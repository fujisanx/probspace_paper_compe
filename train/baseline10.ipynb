{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import Pool\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error,mean_squared_log_error\n",
    "import optuna.integration.lightgbm as lgbo\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "import optuna\n",
    "import umap\n",
    "pd.set_option('display.max_columns', 2000)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "SEED = 777\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(15117, 2002)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df_train = pd.read_pickle('../data/train/full_comp.pickle')\n",
    "df_train = df_train[df_train['cites'].isnull() == False].reset_index(drop=True)\n",
    "df_train['cites'] = np.log1p(df_train['cites'], dtype=np.float64)\n",
    "df_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dorop_cols = []\n",
    "for col in df_train.columns:\n",
    "    if '_x' in col:\n",
    "        dorop_cols.append(col)\n",
    "len(dorop_cols), dorop_cols\n",
    "df_train = df_train.drop(dorop_cols, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  0.090434         0.001255        -0.047360        -0.103646   \n",
       "\n",
       "   roberta_vec_587  roberta_vec_588  roberta_vec_589  roberta_vec_590  \\\n",
       "0         0.041590        12.282816         0.079741        -0.060993   \n",
       "1         0.033566        12.149046         0.052788        -0.019711   \n",
       "2        -0.036725        11.943683         0.009125        -0.021048   \n",
       "3         0.084762        11.718389        -0.038374        -0.011906   \n",
       "4         0.108661        11.651844        -0.063221        -0.013029   \n",
       "\n",
       "   roberta_vec_591  roberta_vec_592  roberta_vec_593  roberta_vec_594  \\\n",
       "0         0.063440        -0.268049        -0.094870         0.024665   \n",
       "1         0.077738        -0.249763        -0.025224         0.030602   \n",
       "2         0.045698        -0.227533        -0.049069        -0.027898   \n",
       "3        -0.001632        -0.224642        -0.079878        -0.099328   \n",
       "4         0.032770        -0.256653         0.004424        -0.158540   \n",
       "\n",
       "   roberta_vec_595  roberta_vec_596  roberta_vec_597  roberta_vec_598  \\\n",
       "0         0.001880         0.187180         0.040667         0.006068   \n",
       "1        -0.058953         0.241092         0.107833        -0.050415   \n",
       "2         0.021803         0.141909         0.143262         0.028343   \n",
       "3         0.124719         0.179040         0.142606        -0.036361   \n",
       "4         0.057062         0.279952         0.185350        -0.026612   \n",
       "\n",
       "   roberta_vec_599  roberta_vec_600  roberta_vec_601  roberta_vec_602  \\\n",
       "0         0.023697         0.187934        -0.221596         0.068942   \n",
       "1        -0.096370         0.116405        -0.205613         0.103472   \n",
       "2         0.013603         0.211610        -0.150498        -0.003177   \n",
       "3        -0.080203         0.120659        -0.049413         0.000616   \n",
       "4        -0.073181         0.144206        -0.092292         0.073268   \n",
       "\n",
       "   roberta_vec_603  roberta_vec_604  roberta_vec_605  roberta_vec_606  \\\n",
       "0         0.005736         0.098046         0.084538         0.091436   \n",
       "1         0.020359         0.087780         0.165174         0.151956   \n",
       "2         0.041957        -0.032630         0.211481         0.146104   \n",
       "3         0.114762         0.136759         0.128589         0.083282   \n",
       "4         0.087679         0.119163         0.097052         0.128711   \n",
       "\n",
       "   roberta_vec_607  roberta_vec_608  roberta_vec_609  roberta_vec_610  \\\n",
       "0        -0.074916         0.082979         0.174427        -0.010615   \n",
       "1        -0.155135         0.048719         0.136277         0.014289   \n",
       "2        -0.185986         0.156548         0.081175         0.050995   \n",
       "3        -0.119280         0.175465         0.175211         0.079239   \n",
       "4        -0.163158         0.196666         0.096923         0.021693   \n",
       "\n",
       "   roberta_vec_611  roberta_vec_612  roberta_vec_613  roberta_vec_614  \\\n",
       "0         0.956542        -0.157765         0.202592        -0.065107   \n",
       "1         0.941260        -0.143702         0.072925        -0.135912   \n",
       "2         0.899738        -0.161586         0.089926        -0.130796   \n",
       "3         0.902776        -0.046504         0.255126        -0.002165   \n",
       "4         0.768567        -0.057968         0.151456        -0.043386   \n",
       "\n",
       "   roberta_vec_615  roberta_vec_616  roberta_vec_617  roberta_vec_618  \\\n",
       "0         0.086034         0.082509         0.116933        -0.130661   \n",
       "1         0.126020         0.117271         0.201530        -0.107007   \n",
       "2        -0.123190         0.086665         0.235653        -0.042723   \n",
       "3        -0.024777         0.089408         0.198304         0.016981   \n",
       "4        -0.004558         0.144182         0.187076         0.072016   \n",
       "\n",
       "   roberta_vec_619  roberta_vec_620  roberta_vec_621  roberta_vec_622  \\\n",
       "0         0.056648        -0.001675        -0.010225         0.085153   \n",
       "1         0.218389         0.065998        -0.083668         0.053277   \n",
       "2         0.308070         0.091798        -0.021499        -0.014432   \n",
       "3         0.230631         0.072909        -0.130362        -0.013177   \n",
       "4         0.245870         0.112004        -0.171178         0.033259   \n",
       "\n",
       "   roberta_vec_623  roberta_vec_624  roberta_vec_625  roberta_vec_626  \\\n",
       "0        -0.174518        -0.064504         0.161261         0.148312   \n",
       "1        -0.147597         0.029762         0.036007        -0.030452   \n",
       "2        -0.194382        -0.075963        -0.058627         0.029009   \n",
       "3        -0.092629        -0.196001         0.119407         0.157684   \n",
       "4        -0.128406        -0.077126         0.080339         0.073449   \n",
       "\n",
       "   roberta_vec_627  roberta_vec_628  roberta_vec_629  roberta_vec_630  \\\n",
       "0        -0.075701        -0.122330        -0.085408         0.294268   \n",
       "1        -0.116128        -0.074170        -0.051684         0.312923   \n",
       "2        -0.040424        -0.000913        -0.051034         0.290411   \n",
       "3        -0.082814         0.007755        -0.103149         0.284048   \n",
       "4        -0.066963         0.034276        -0.062217         0.260491   \n",
       "\n",
       "   roberta_vec_631  roberta_vec_632  roberta_vec_633  roberta_vec_634  \\\n",
       "0        -0.416573         0.074603        -0.209788         0.263318   \n",
       "1        -0.386000         0.135494        -0.153733         0.340041   \n",
       "2        -0.335732         0.170565        -0.109928         0.268078   \n",
       "3        -0.280611         0.074010        -0.182096         0.179465   \n",
       "4        -0.303431         0.150829        -0.128606         0.227643   \n",
       "\n",
       "   roberta_vec_635  roberta_vec_636  roberta_vec_637  roberta_vec_638  \\\n",
       "0         0.310230         0.106331        -0.020113        -0.048917   \n",
       "1         0.280454        -0.007710        -0.049217        -0.141264   \n",
       "2         0.145914        -0.000980        -0.030464        -0.043086   \n",
       "3         0.221553         0.046697        -0.015358         0.039650   \n",
       "4         0.210428         0.000352         0.009954         0.003607   \n",
       "\n",
       "   roberta_vec_639  roberta_vec_640  roberta_vec_641  roberta_vec_642  \\\n",
       "0        -0.070626         0.096161        -0.188461        -0.075923   \n",
       "1         0.125780         0.236436        -0.288352        -0.001873   \n",
       "2         0.071312         0.179723        -0.211874        -0.034422   \n",
       "3         0.081511         0.152500        -0.169331         0.018030   \n",
       "4         0.166239         0.208197        -0.103583        -0.023853   \n",
       "\n",
       "   roberta_vec_643  roberta_vec_644  roberta_vec_645  roberta_vec_646  \\\n",
       "0        -0.234436        -0.020224        -0.090146         0.016027   \n",
       "1        -0.319654         0.173400        -0.170278        -0.004907   \n",
       "2        -0.307077         0.115623        -0.215856         0.036417   \n",
       "3        -0.237969         0.178311        -0.111561         0.018086   \n",
       "4        -0.391357         0.242652        -0.155245         0.061709   \n",
       "\n",
       "   roberta_vec_647  roberta_vec_648  roberta_vec_649  roberta_vec_650  \\\n",
       "0        -0.018889        -0.114500        -0.154983         0.169091   \n",
       "1        -0.229392        -0.096131        -0.110653         0.114807   \n",
       "2         0.080084        -0.064783        -0.117606         0.127752   \n",
       "3         0.100239        -0.067606        -0.186209         0.084413   \n",
       "4         0.067378        -0.110414        -0.202818         0.078849   \n",
       "\n",
       "   roberta_vec_651  roberta_vec_652  roberta_vec_653  roberta_vec_654  \\\n",
       "0         0.098314        -0.164059         0.183723         0.187806   \n",
       "1        -0.077159        -0.306470         0.196648         0.239305   \n",
       "2        -0.012882        -0.205301         0.157589         0.201962   \n",
       "3         0.027594        -0.156649         0.218658         0.102454   \n",
       "4         0.033232        -0.106000         0.203802         0.149935   \n",
       "\n",
       "   roberta_vec_655  roberta_vec_656  roberta_vec_657  roberta_vec_658  \\\n",
       "0        -0.131551        -0.050215        -0.003589         0.114386   \n",
       "1        -0.083120        -0.006266        -0.016332         0.137176   \n",
       "2        -0.038278        -0.064192        -0.049258         0.068961   \n",
       "3        -0.092495        -0.029048         0.020467         0.097208   \n",
       "4        -0.084985         0.015760        -0.048469         0.037743   \n",
       "\n",
       "   roberta_vec_659  roberta_vec_660  roberta_vec_661  roberta_vec_662  \\\n",
       "0        -0.010999        -0.022542         0.019642         0.036398   \n",
       "1        -0.049121         0.032097         0.005268        -0.033742   \n",
       "2        -0.065382         0.155502        -0.050035        -0.160276   \n",
       "3         0.068601         0.086373        -0.188948        -0.147831   \n",
       "4         0.096856         0.143798        -0.136856        -0.153232   \n",
       "\n",
       "   roberta_vec_663  roberta_vec_664  roberta_vec_665  roberta_vec_666  \\\n",
       "0        -0.196691        -0.317724        -0.190680        -0.218587   \n",
       "1        -0.187795        -0.041616        -0.180207        -0.083809   \n",
       "2        -0.177191        -0.369967        -0.243220        -0.121856   \n",
       "3        -0.122629        -0.531026        -0.195737        -0.144605   \n",
       "4        -0.142397        -0.389400        -0.224074        -0.056422   \n",
       "\n",
       "   roberta_vec_667  roberta_vec_668  roberta_vec_669  roberta_vec_670  \\\n",
       "0        -0.481349         0.132161        -0.064054        -0.135440   \n",
       "1        -0.611474         0.132549        -0.097444        -0.145588   \n",
       "2        -0.589593         0.068379        -0.086653        -0.168638   \n",
       "3        -0.535961         0.136488        -0.121504        -0.160985   \n",
       "4        -0.552498         0.065720        -0.176695        -0.079383   \n",
       "\n",
       "   roberta_vec_671  roberta_vec_672  roberta_vec_673  roberta_vec_674  \\\n",
       "0        -0.064407        -0.314367         0.120350        -0.229907   \n",
       "1        -0.041082        -0.310184         0.206319        -0.203444   \n",
       "2        -0.069073        -0.301360         0.129226        -0.260017   \n",
       "3        -0.042444        -0.374259         0.128434        -0.260724   \n",
       "4        -0.159391        -0.431184         0.239871        -0.236745   \n",
       "\n",
       "   roberta_vec_675  roberta_vec_676  roberta_vec_677  roberta_vec_678  \\\n",
       "0        -0.007436        -0.172259         0.153599         0.202984   \n",
       "1        -0.015584        -0.259579         0.176335         0.069659   \n",
       "2         0.015221        -0.192370         0.160849         0.069858   \n",
       "3         0.105819        -0.173876         0.085959        -0.007114   \n",
       "4        -0.009576        -0.231364         0.272019        -0.037922   \n",
       "\n",
       "   roberta_vec_679  roberta_vec_680  roberta_vec_681  roberta_vec_682  \\\n",
       "0         0.076411         0.012433        -0.207095         0.033421   \n",
       "1         0.012299         0.034244        -0.047717         0.055669   \n",
       "2         0.022968         0.029516        -0.123990        -0.063065   \n",
       "3         0.041356        -0.002045        -0.151803        -0.026553   \n",
       "4         0.021827        -0.032716        -0.068437        -0.025033   \n",
       "\n",
       "   roberta_vec_683  roberta_vec_684  roberta_vec_685  roberta_vec_686  \\\n",
       "0        -0.124092         0.215223        -0.086132         0.223904   \n",
       "1         0.003719         0.157838         0.011114         0.159168   \n",
       "2         0.016799         0.150945        -0.042326         0.166288   \n",
       "3        -0.087048         0.191028        -0.033562         0.107551   \n",
       "4        -0.066474         0.184263         0.019257         0.092456   \n",
       "\n",
       "   roberta_vec_687  roberta_vec_688  roberta_vec_689  roberta_vec_690  \\\n",
       "0        -0.158670        -0.441802        -0.005265        -0.076523   \n",
       "1        -0.178828        -0.446934        -0.108646        -0.151625   \n",
       "2        -0.224466        -0.486029        -0.094058        -0.081841   \n",
       "3        -0.343149        -0.405529        -0.163232        -0.044187   \n",
       "4        -0.381083        -0.404728        -0.135549        -0.092909   \n",
       "\n",
       "   roberta_vec_691  roberta_vec_692  roberta_vec_693  roberta_vec_694  \\\n",
       "0         0.007779        -0.222167         0.287094         0.008407   \n",
       "1        -0.023107        -0.169719         0.207508         0.013606   \n",
       "2         0.025652        -0.132757         0.218547         0.033704   \n",
       "3         0.012854        -0.204581         0.237232         0.145715   \n",
       "4        -0.032019        -0.154564         0.245694         0.074664   \n",
       "\n",
       "   roberta_vec_695  roberta_vec_696  roberta_vec_697  roberta_vec_698  \\\n",
       "0        -0.118400         0.111839         0.096921        -0.109888   \n",
       "1        -0.166397         0.069439         0.024586         0.034726   \n",
       "2        -0.149657         0.231645        -0.030209         0.024474   \n",
       "3        -0.125335         0.063213         0.022032        -0.027339   \n",
       "4        -0.105019         0.138730        -0.051127        -0.003614   \n",
       "\n",
       "   roberta_vec_699  roberta_vec_700  roberta_vec_701  roberta_vec_702  \\\n",
       "0        -0.032591        -0.262088        -0.143762         0.001745   \n",
       "1         0.034124        -0.228854        -0.116306         0.059327   \n",
       "2         0.034954        -0.164471         0.055304         0.095869   \n",
       "3         0.020471        -0.217706        -0.001401         0.026243   \n",
       "4        -0.033994        -0.271795        -0.037047         0.037355   \n",
       "\n",
       "   roberta_vec_703  roberta_vec_704  roberta_vec_705  roberta_vec_706  \\\n",
       "0        -0.218345         0.092003         0.029601         0.020191   \n",
       "1        -0.123236         0.111901        -0.049153         0.095247   \n",
       "2        -0.088905         0.045986         0.045848         0.143413   \n",
       "3        -0.193529         0.096699         0.059963         0.140285   \n",
       "4        -0.192260         0.073082         0.057636         0.151957   \n",
       "\n",
       "   roberta_vec_707  roberta_vec_708  roberta_vec_709  roberta_vec_710  \\\n",
       "0         0.111559         0.005574         0.073102         0.047232   \n",
       "1         0.112716        -0.023128         0.058711         0.217587   \n",
       "2         0.154731         0.029386         0.088313         0.053296   \n",
       "3         0.135448         0.055009         0.066489        -0.012882   \n",
       "4         0.184729         0.029239         0.083366        -0.029940   \n",
       "\n",
       "   roberta_vec_711  roberta_vec_712  roberta_vec_713  roberta_vec_714  \\\n",
       "0         0.086060        -0.080468        -0.066836        -0.307097   \n",
       "1        -0.042789        -0.056089         0.032627        -0.305321   \n",
       "2         0.047181        -0.065630         0.069703        -0.299505   \n",
       "3        -0.105614         0.000890         0.034355        -0.243062   \n",
       "4        -0.066711         0.070731         0.069432        -0.227426   \n",
       "\n",
       "   roberta_vec_715  roberta_vec_716  roberta_vec_717  roberta_vec_718  \\\n",
       "0        -0.099523        -0.264944        -0.165200        -0.270445   \n",
       "1         0.011219        -0.307083        -0.103627        -0.063050   \n",
       "2         0.075170        -0.240123        -0.079219        -0.327267   \n",
       "3        -0.077763        -0.166024        -0.111253        -0.237102   \n",
       "4         0.069980        -0.165079        -0.067444        -0.143729   \n",
       "\n",
       "   roberta_vec_719  roberta_vec_720  roberta_vec_721  roberta_vec_722  \\\n",
       "0         0.170967         0.298262         0.029693         0.077656   \n",
       "1         0.115984         0.333934         0.055513         0.008051   \n",
       "2        -0.042809         0.261281         0.048127         0.019458   \n",
       "3        -0.003095         0.293362         0.116610         0.061845   \n",
       "4         0.032275         0.372293         0.041818         0.032854   \n",
       "\n",
       "   roberta_vec_723  roberta_vec_724  roberta_vec_725  roberta_vec_726  \\\n",
       "0         0.060142        -0.168183         0.077958         0.168891   \n",
       "1         0.104721        -0.171578         0.106401         0.010308   \n",
       "2         0.078568        -0.075148         0.000128         0.282459   \n",
       "3        -0.008980        -0.138449         0.131612         0.244518   \n",
       "4         0.146240        -0.137018         0.190802         0.108555   \n",
       "\n",
       "   roberta_vec_727  roberta_vec_728  roberta_vec_729  roberta_vec_730  \\\n",
       "0         0.268703        -0.149408         0.017111        -0.243692   \n",
       "1         0.284912        -0.166739         0.050573        -0.231710   \n",
       "2         0.175455        -0.112744         0.021762        -0.165469   \n",
       "3         0.182840        -0.164329         0.032314        -0.118185   \n",
       "4         0.170401        -0.170474         0.065800        -0.162053   \n",
       "\n",
       "   roberta_vec_731  roberta_vec_732  roberta_vec_733  roberta_vec_734  \\\n",
       "0        -0.190973         0.018555        -0.244975         0.347429   \n",
       "1        -0.157238        -0.059044        -0.136545         0.288966   \n",
       "2        -0.224393         0.010819        -0.105791         0.194084   \n",
       "3        -0.233659        -0.006937        -0.106601         0.157456   \n",
       "4        -0.241368         0.072358        -0.170512         0.181729   \n",
       "\n",
       "   roberta_vec_735  roberta_vec_736  roberta_vec_737  roberta_vec_738  \\\n",
       "0         0.465995         0.145426         0.111209        -0.079083   \n",
       "1         0.348855         0.023084         0.088987        -0.136358   \n",
       "2         0.509446         0.113878         0.128766        -0.175802   \n",
       "3         0.394210         0.110254         0.207392        -0.067668   \n",
       "4         0.394393         0.065402         0.205735        -0.103268   \n",
       "\n",
       "   roberta_vec_739  roberta_vec_740  roberta_vec_741  roberta_vec_742  \\\n",
       "0        -0.170695        -0.197767        -0.445161         0.029228   \n",
       "1        -0.047009        -0.123210        -0.567249        -0.003853   \n",
       "2        -0.106955        -0.074288        -0.475873         0.123529   \n",
       "3        -0.081145        -0.097108        -0.460185        -0.036846   \n",
       "4        -0.039133        -0.052853        -0.421413         0.066120   \n",
       "\n",
       "   roberta_vec_743  roberta_vec_744  roberta_vec_745  roberta_vec_746  \\\n",
       "0         0.070849        -0.132407         0.278908         0.070082   \n",
       "1         0.035672        -0.251738         0.176084         0.085949   \n",
       "2         0.007325        -0.199647         0.186245         0.054156   \n",
       "3        -0.012913        -0.114179         0.144275         0.079694   \n",
       "4        -0.036619        -0.129683         0.152110         0.058852   \n",
       "\n",
       "   roberta_vec_747  roberta_vec_748  roberta_vec_749  roberta_vec_750  \\\n",
       "0        -0.276633         0.595741        -0.456886        -0.110131   \n",
       "1        -0.187019         0.486607        -0.545987        -0.139555   \n",
       "2        -0.242084         0.522150        -0.427954        -0.208415   \n",
       "3        -0.229887         0.509667        -0.542613        -0.205718   \n",
       "4        -0.200716         0.425438        -0.633506        -0.184883   \n",
       "\n",
       "   roberta_vec_751  roberta_vec_752  roberta_vec_753  roberta_vec_754  \\\n",
       "0         0.033729        -0.337985         0.117239         0.234729   \n",
       "1        -0.130976        -0.378054         0.055729         0.309306   \n",
       "2        -0.076315        -0.206017         0.068706         0.202041   \n",
       "3        -0.047161        -0.217698         0.119367         0.281846   \n",
       "4        -0.046711        -0.277424         0.150955         0.174308   \n",
       "\n",
       "   roberta_vec_755  roberta_vec_756  roberta_vec_757  roberta_vec_758  \\\n",
       "0        -0.072439         0.113650         0.048002        -0.158772   \n",
       "1        -0.156030         0.109429        -0.000374        -0.064217   \n",
       "2        -0.066067         0.023081         0.112127        -0.219112   \n",
       "3        -0.062160         0.066264         0.083418        -0.142987   \n",
       "4        -0.191348         0.121687        -0.002648        -0.149996   \n",
       "\n",
       "   roberta_vec_759  roberta_vec_760  roberta_vec_761  roberta_vec_762  \\\n",
       "0        -0.019317         0.086273         0.033125         0.099122   \n",
       "1        -0.051321         0.107150         0.062490         0.142579   \n",
       "2         0.008264        -0.003492         0.049195         0.105896   \n",
       "3        -0.000292        -0.025965         0.085525         0.040288   \n",
       "4         0.036184        -0.095719         0.059427         0.130815   \n",
       "\n",
       "   roberta_vec_763  roberta_vec_764  roberta_vec_765  roberta_vec_766  \\\n",
       "0         0.154706         0.245593         0.108821        -0.001333   \n",
       "1         0.072491         0.352781         0.152339         0.023221   \n",
       "2         0.144375         0.173970         0.156530         0.093464   \n",
       "3         0.167405         0.262107         0.174512        -0.008561   \n",
       "4         0.155930         0.243257         0.070662         0.079929   \n",
       "\n",
       "   roberta_vec_767  fold_no  \n",
       "0         0.027004        4  \n",
       "1        -0.117772        6  \n",
       "2        -0.037191        2  \n",
       "3        -0.022450        3  \n",
       "4        -0.050331        3  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>submitter</th>\n      <th>authors</th>\n      <th>title</th>\n      <th>comments</th>\n      <th>journal-ref</th>\n      <th>doi</th>\n      <th>report-no</th>\n      <th>categories</th>\n      <th>license</th>\n      <th>abstract</th>\n      <th>versions</th>\n      <th>authors_parsed</th>\n      <th>doi_cites</th>\n      <th>cites</th>\n      <th>doi_id</th>\n      <th>pub_publisher</th>\n      <th>pub_journals</th>\n      <th>pub_dois</th>\n      <th>update_date_y</th>\n      <th>first_created_date</th>\n      <th>last_created_date</th>\n      <th>update_year</th>\n      <th>first_created_year</th>\n      <th>last_created_year</th>\n      <th>update_month</th>\n      <th>first_created_month</th>\n      <th>last_created_month</th>\n      <th>update_ym</th>\n      <th>first_created_ym</th>\n      <th>last_created_ym</th>\n      <th>update_day</th>\n      <th>first_created_day</th>\n      <th>last_created_day</th>\n      <th>update_date_unixtime</th>\n      <th>first_created_unixtime</th>\n      <th>last_created_unixtime</th>\n      <th>diff_update_date_unixtime</th>\n      <th>diff_created_unixtime</th>\n      <th>num_created</th>\n      <th>update_date_days</th>\n      <th>first_created_days</th>\n      <th>last_created_days</th>\n      <th>diff_created_days</th>\n      <th>rate_created_days</th>\n      <th>author_first</th>\n      <th>author_num</th>\n      <th>pred_doi_cites</th>\n      <th>category_main_detail</th>\n      <th>category_main</th>\n      <th>cs_y</th>\n      <th>econ_y</th>\n      <th>eess_y</th>\n      <th>math_y</th>\n      <th>nlin_y</th>\n      <th>physics_y</th>\n      <th>stat_y</th>\n      <th>category_name_parent_main_unique</th>\n      <th>category_name_parent_unique</th>\n      <th>category_name_unique</th>\n      <th>acc-phys_y</th>\n      <th>adap-org_y</th>\n      <th>alg-geom_y</th>\n      <th>ao-sci_y</th>\n      <th>astro-ph_y</th>\n      <th>atom-ph_y</th>\n      <th>bayes-an_y</th>\n      <th>chao-dyn_y</th>\n      <th>chem-ph_y</th>\n      <th>cmp-lg_y</th>\n      <th>comp-gas_y</th>\n      <th>cond-mat_y</th>\n      <th>dg-ga_y</th>\n      <th>funct-an_y</th>\n      <th>gr-qc_y</th>\n      <th>hep-ex_y</th>\n      <th>hep-lat_y</th>\n      <th>hep-ph_y</th>\n      <th>hep-th_y</th>\n      <th>math-ph_y</th>\n      <th>mtrl-th_y</th>\n      <th>nucl-ex_y</th>\n      <th>nucl-th_y</th>\n      <th>patt-sol_y</th>\n      <th>plasm-ph_y</th>\n      <th>q-alg_y</th>\n      <th>q-bio_y</th>\n      <th>q-fin_y</th>\n      <th>quant-ph_y</th>\n      <th>solv-int_y</th>\n      <th>supr-con_y</th>\n      <th>acc_y</th>\n      <th>adap_y</th>\n      <th>alg_y</th>\n      <th>ao_y</th>\n      <th>astro_y</th>\n      <th>atom_y</th>\n      <th>bayes_y</th>\n      <th>chao_y</th>\n      <th>chem_y</th>\n      <th>cmp_y</th>\n      <th>comp_y</th>\n      <th>cond_y</th>\n      <th>cs</th>\n      <th>dg_y</th>\n      <th>econ</th>\n      <th>eess</th>\n      <th>funct_y</th>\n      <th>gr_y</th>\n      <th>hep_y</th>\n      <th>math</th>\n      <th>mtrl_y</th>\n      <th>nlin</th>\n      <th>nucl_y</th>\n      <th>patt_y</th>\n      <th>physics</th>\n      <th>plasm_y</th>\n      <th>q_y</th>\n      <th>quant_y</th>\n      <th>solv_y</th>\n      <th>stat</th>\n      <th>supr_y</th>\n      <th>astro-ph.co</th>\n      <th>astro-ph.ep</th>\n      <th>astro-ph.ga</th>\n      <th>astro-ph.he</th>\n      <th>astro-ph.im</th>\n      <th>astro-ph.sr</th>\n      <th>cond-mat.dis-nn</th>\n      <th>cond-mat.mes-hall</th>\n      <th>cond-mat.mtrl-sci</th>\n      <th>cond-mat.other</th>\n      <th>cond-mat.quant-gas</th>\n      <th>cond-mat.soft</th>\n      <th>cond-mat.stat-mech</th>\n      <th>cond-mat.str-el</th>\n      <th>cond-mat.supr-con</th>\n      <th>cs.ai</th>\n      <th>cs.ar</th>\n      <th>cs.cc</th>\n      <th>cs.ce</th>\n      <th>cs.cg</th>\n      <th>cs.cl</th>\n      <th>cs.cr</th>\n      <th>cs.cv</th>\n      <th>cs.cy</th>\n      <th>cs.db</th>\n      <th>cs.dc</th>\n      <th>cs.dl</th>\n      <th>cs.dm</th>\n      <th>cs.ds</th>\n      <th>cs.et</th>\n      <th>cs.fl</th>\n      <th>cs.gl</th>\n      <th>cs.gr</th>\n      <th>cs.gt</th>\n      <th>cs.hc</th>\n      <th>cs.ir</th>\n      <th>cs.it</th>\n      <th>cs.lg</th>\n      <th>cs.lo</th>\n      <th>cs.ma</th>\n      <th>cs.mm</th>\n      <th>cs.ms</th>\n      <th>cs.na</th>\n      <th>cs.ne</th>\n      <th>cs.ni</th>\n      <th>cs.oh</th>\n      <th>cs.os</th>\n      <th>cs.pf</th>\n      <th>cs.pl</th>\n      <th>cs.ro</th>\n      <th>cs.sc</th>\n      <th>cs.sd</th>\n      <th>cs.se</th>\n      <th>cs.si</th>\n      <th>cs.sy</th>\n      <th>econ.em</th>\n      <th>econ.gn</th>\n      <th>econ.th</th>\n      <th>eess.as</th>\n      <th>eess.iv</th>\n      <th>eess.sp</th>\n      <th>eess.sy</th>\n      <th>math.ac</th>\n      <th>math.ag</th>\n      <th>math.ap</th>\n      <th>math.at</th>\n      <th>math.ca</th>\n      <th>math.co</th>\n      <th>math.ct</th>\n      <th>math.cv</th>\n      <th>math.dg</th>\n      <th>math.ds</th>\n      <th>math.fa</th>\n      <th>math.gm</th>\n      <th>math.gn</th>\n      <th>math.gr</th>\n      <th>math.gt</th>\n      <th>math.ho</th>\n      <th>math.it</th>\n      <th>math.kt</th>\n      <th>math.lo</th>\n      <th>math.mg</th>\n      <th>math.mp</th>\n      <th>math.na</th>\n      <th>math.nt</th>\n      <th>math.oa</th>\n      <th>math.oc</th>\n      <th>math.pr</th>\n      <th>math.qa</th>\n      <th>math.ra</th>\n      <th>math.rt</th>\n      <th>math.sg</th>\n      <th>math.sp</th>\n      <th>math.st</th>\n      <th>nlin.ao</th>\n      <th>nlin.cd</th>\n      <th>nlin.cg</th>\n      <th>nlin.ps</th>\n      <th>nlin.si</th>\n      <th>physics.acc-ph</th>\n      <th>physics.ao-ph</th>\n      <th>physics.app-ph</th>\n      <th>physics.atm-clus</th>\n      <th>physics.atom-ph</th>\n      <th>physics.bio-ph</th>\n      <th>physics.chem-ph</th>\n      <th>physics.class-ph</th>\n      <th>physics.comp-ph</th>\n      <th>physics.data-an</th>\n      <th>physics.ed-ph</th>\n      <th>physics.flu-dyn</th>\n      <th>physics.gen-ph</th>\n      <th>physics.geo-ph</th>\n      <th>physics.hist-ph</th>\n      <th>physics.ins-det</th>\n      <th>physics.med-ph</th>\n      <th>physics.optics</th>\n      <th>physics.plasm-ph</th>\n      <th>physics.pop-ph</th>\n      <th>physics.soc-ph</th>\n      <th>physics.space-ph</th>\n      <th>q-bio.bm</th>\n      <th>q-bio.cb</th>\n      <th>q-bio.gn</th>\n      <th>q-bio.mn</th>\n      <th>q-bio.nc</th>\n      <th>q-bio.ot</th>\n      <th>q-bio.pe</th>\n      <th>q-bio.qm</th>\n      <th>q-bio.sc</th>\n      <th>q-bio.to</th>\n      <th>q-fin.cp</th>\n      <th>q-fin.ec</th>\n      <th>q-fin.gn</th>\n      <th>q-fin.mf</th>\n      <th>q-fin.pm</th>\n      <th>q-fin.pr</th>\n      <th>q-fin.rm</th>\n      <th>q-fin.st</th>\n      <th>q-fin.tr</th>\n      <th>stat.ap</th>\n      <th>stat.co</th>\n      <th>stat.me</th>\n      <th>stat.ml</th>\n      <th>stat.ot</th>\n      <th>stat.th</th>\n      <th>submitter_label</th>\n      <th>doi_id_label</th>\n      <th>author_first_label</th>\n      <th>pub_publisher_label</th>\n      <th>license_label</th>\n      <th>category_main_label</th>\n      <th>category_main_detail_label</th>\n      <th>category_name_parent_label</th>\n      <th>category_name_parent_main_label</th>\n      <th>category_name_label</th>\n      <th>doi_cites_mean_author_first_label</th>\n      <th>doi_cites_count_author_first_label</th>\n      <th>doi_cites_sum_author_first_label</th>\n      <th>doi_cites_min_author_first_label</th>\n      <th>doi_cites_max_author_first_label</th>\n      <th>doi_cites_median_author_first_label</th>\n      <th>doi_cites_std_author_first_label</th>\n      <th>doi_cites_q10_author_first_label</th>\n      <th>doi_cites_q25_author_first_label</th>\n      <th>doi_cites_q75_author_first_label</th>\n      <th>doi_cites_mean_doi_id_label</th>\n      <th>doi_cites_count_doi_id_label</th>\n      <th>doi_cites_sum_doi_id_label</th>\n      <th>doi_cites_min_doi_id_label</th>\n      <th>doi_cites_max_doi_id_label</th>\n      <th>doi_cites_median_doi_id_label</th>\n      <th>doi_cites_std_doi_id_label</th>\n      <th>doi_cites_q10_doi_id_label</th>\n      <th>doi_cites_q25_doi_id_label</th>\n      <th>doi_cites_q75_doi_id_label</th>\n      <th>doi_cites_mean_pub_publisher_label</th>\n      <th>doi_cites_count_pub_publisher_label</th>\n      <th>doi_cites_sum_pub_publisher_label</th>\n      <th>doi_cites_min_pub_publisher_label</th>\n      <th>doi_cites_max_pub_publisher_label</th>\n      <th>doi_cites_median_pub_publisher_label</th>\n      <th>doi_cites_std_pub_publisher_label</th>\n      <th>doi_cites_q10_pub_publisher_label</th>\n      <th>doi_cites_q25_pub_publisher_label</th>\n      <th>doi_cites_q75_pub_publisher_label</th>\n      <th>doi_cites_mean_submitter_label</th>\n      <th>doi_cites_count_submitter_label</th>\n      <th>doi_cites_sum_submitter_label</th>\n      <th>doi_cites_min_submitter_label</th>\n      <th>doi_cites_max_submitter_label</th>\n      <th>doi_cites_median_submitter_label</th>\n      <th>doi_cites_std_submitter_label</th>\n      <th>doi_cites_q10_submitter_label</th>\n      <th>doi_cites_q25_submitter_label</th>\n      <th>doi_cites_q75_submitter_label</th>\n      <th>doi_cites_mean_update_ym</th>\n      <th>doi_cites_count_update_ym</th>\n      <th>doi_cites_sum_update_ym</th>\n      <th>doi_cites_min_update_ym</th>\n      <th>doi_cites_max_update_ym</th>\n      <th>doi_cites_median_update_ym</th>\n      <th>doi_cites_std_update_ym</th>\n      <th>doi_cites_q10_update_ym</th>\n      <th>doi_cites_q25_update_ym</th>\n      <th>doi_cites_q75_update_ym</th>\n      <th>doi_cites_mean_first_created_ym</th>\n      <th>doi_cites_count_first_created_ym</th>\n      <th>doi_cites_sum_first_created_ym</th>\n      <th>doi_cites_min_first_created_ym</th>\n      <th>doi_cites_max_first_created_ym</th>\n      <th>doi_cites_median_first_created_ym</th>\n      <th>doi_cites_std_first_created_ym</th>\n      <th>doi_cites_q10_first_created_ym</th>\n      <th>doi_cites_q25_first_created_ym</th>\n      <th>doi_cites_q75_first_created_ym</th>\n      <th>doi_cites_mean_license_label</th>\n      <th>doi_cites_count_license_label</th>\n      <th>doi_cites_sum_license_label</th>\n      <th>doi_cites_min_license_label</th>\n      <th>doi_cites_max_license_label</th>\n      <th>doi_cites_median_license_label</th>\n      <th>doi_cites_std_license_label</th>\n      <th>doi_cites_q10_license_label</th>\n      <th>doi_cites_q25_license_label</th>\n      <th>doi_cites_q75_license_label</th>\n      <th>doi_cites_mean_category_main_label</th>\n      <th>doi_cites_count_category_main_label</th>\n      <th>doi_cites_sum_category_main_label</th>\n      <th>doi_cites_min_category_main_label</th>\n      <th>doi_cites_max_category_main_label</th>\n      <th>doi_cites_median_category_main_label</th>\n      <th>doi_cites_std_category_main_label</th>\n      <th>doi_cites_q10_category_main_label</th>\n      <th>doi_cites_q25_category_main_label</th>\n      <th>doi_cites_q75_category_main_label</th>\n      <th>doi_cites_mean_category_main_detail_label</th>\n      <th>doi_cites_count_category_main_detail_label</th>\n      <th>doi_cites_sum_category_main_detail_label</th>\n      <th>doi_cites_min_category_main_detail_label</th>\n      <th>doi_cites_max_category_main_detail_label</th>\n      <th>doi_cites_median_category_main_detail_label</th>\n      <th>doi_cites_std_category_main_detail_label</th>\n      <th>doi_cites_q10_category_main_detail_label</th>\n      <th>doi_cites_q25_category_main_detail_label</th>\n      <th>doi_cites_q75_category_main_detail_label</th>\n      <th>doi_cites_mean_category_name_parent_label</th>\n      <th>doi_cites_count_category_name_parent_label</th>\n      <th>doi_cites_sum_category_name_parent_label</th>\n      <th>doi_cites_min_category_name_parent_label</th>\n      <th>doi_cites_max_category_name_parent_label</th>\n      <th>doi_cites_median_category_name_parent_label</th>\n      <th>doi_cites_std_category_name_parent_label</th>\n      <th>doi_cites_q10_category_name_parent_label</th>\n      <th>doi_cites_q25_category_name_parent_label</th>\n      <th>doi_cites_q75_category_name_parent_label</th>\n      <th>doi_cites_mean_category_name_parent_main_label</th>\n      <th>doi_cites_count_category_name_parent_main_label</th>\n      <th>doi_cites_sum_category_name_parent_main_label</th>\n      <th>doi_cites_min_category_name_parent_main_label</th>\n      <th>doi_cites_max_category_name_parent_main_label</th>\n      <th>doi_cites_median_category_name_parent_main_label</th>\n      <th>doi_cites_std_category_name_parent_main_label</th>\n      <th>doi_cites_q10_category_name_parent_main_label</th>\n      <th>doi_cites_q25_category_name_parent_main_label</th>\n      <th>doi_cites_q75_category_name_parent_main_label</th>\n      <th>doi_cites_mean_category_name_label</th>\n      <th>doi_cites_count_category_name_label</th>\n      <th>doi_cites_sum_category_name_label</th>\n      <th>doi_cites_min_category_name_label</th>\n      <th>doi_cites_max_category_name_label</th>\n      <th>doi_cites_median_category_name_label</th>\n      <th>doi_cites_std_category_name_label</th>\n      <th>doi_cites_q10_category_name_label</th>\n      <th>doi_cites_q25_category_name_label</th>\n      <th>doi_cites_q75_category_name_label</th>\n      <th>pred_doi_cites_mean_author_first_label</th>\n      <th>pred_doi_cites_count_author_first_label</th>\n      <th>pred_doi_cites_sum_author_first_label</th>\n      <th>pred_doi_cites_min_author_first_label</th>\n      <th>pred_doi_cites_max_author_first_label</th>\n      <th>pred_doi_cites_median_author_first_label</th>\n      <th>pred_doi_cites_std_author_first_label</th>\n      <th>pred_doi_cites_q10_author_first_label</th>\n      <th>pred_doi_cites_q25_author_first_label</th>\n      <th>pred_doi_cites_q75_author_first_label</th>\n      <th>pred_doi_cites_mean_doi_id_label</th>\n      <th>pred_doi_cites_count_doi_id_label</th>\n      <th>pred_doi_cites_sum_doi_id_label</th>\n      <th>pred_doi_cites_min_doi_id_label</th>\n      <th>pred_doi_cites_max_doi_id_label</th>\n      <th>pred_doi_cites_median_doi_id_label</th>\n      <th>pred_doi_cites_std_doi_id_label</th>\n      <th>pred_doi_cites_q10_doi_id_label</th>\n      <th>pred_doi_cites_q25_doi_id_label</th>\n      <th>pred_doi_cites_q75_doi_id_label</th>\n      <th>pred_doi_cites_mean_pub_publisher_label</th>\n      <th>pred_doi_cites_count_pub_publisher_label</th>\n      <th>pred_doi_cites_sum_pub_publisher_label</th>\n      <th>pred_doi_cites_min_pub_publisher_label</th>\n      <th>pred_doi_cites_max_pub_publisher_label</th>\n      <th>pred_doi_cites_median_pub_publisher_label</th>\n      <th>pred_doi_cites_std_pub_publisher_label</th>\n      <th>pred_doi_cites_q10_pub_publisher_label</th>\n      <th>pred_doi_cites_q25_pub_publisher_label</th>\n      <th>pred_doi_cites_q75_pub_publisher_label</th>\n      <th>pred_doi_cites_mean_submitter_label</th>\n      <th>pred_doi_cites_count_submitter_label</th>\n      <th>pred_doi_cites_sum_submitter_label</th>\n      <th>pred_doi_cites_min_submitter_label</th>\n      <th>pred_doi_cites_max_submitter_label</th>\n      <th>pred_doi_cites_median_submitter_label</th>\n      <th>pred_doi_cites_std_submitter_label</th>\n      <th>pred_doi_cites_q10_submitter_label</th>\n      <th>pred_doi_cites_q25_submitter_label</th>\n      <th>pred_doi_cites_q75_submitter_label</th>\n      <th>pred_doi_cites_mean_update_ym</th>\n      <th>pred_doi_cites_count_update_ym</th>\n      <th>pred_doi_cites_sum_update_ym</th>\n      <th>pred_doi_cites_min_update_ym</th>\n      <th>pred_doi_cites_max_update_ym</th>\n      <th>pred_doi_cites_median_update_ym</th>\n      <th>pred_doi_cites_std_update_ym</th>\n      <th>pred_doi_cites_q10_update_ym</th>\n      <th>pred_doi_cites_q25_update_ym</th>\n      <th>pred_doi_cites_q75_update_ym</th>\n      <th>pred_doi_cites_mean_first_created_ym</th>\n      <th>pred_doi_cites_count_first_created_ym</th>\n      <th>pred_doi_cites_sum_first_created_ym</th>\n      <th>pred_doi_cites_min_first_created_ym</th>\n      <th>pred_doi_cites_max_first_created_ym</th>\n      <th>pred_doi_cites_median_first_created_ym</th>\n      <th>pred_doi_cites_std_first_created_ym</th>\n      <th>pred_doi_cites_q10_first_created_ym</th>\n      <th>pred_doi_cites_q25_first_created_ym</th>\n      <th>pred_doi_cites_q75_first_created_ym</th>\n      <th>pred_doi_cites_mean_license_label</th>\n      <th>pred_doi_cites_count_license_label</th>\n      <th>pred_doi_cites_sum_license_label</th>\n      <th>pred_doi_cites_min_license_label</th>\n      <th>pred_doi_cites_max_license_label</th>\n      <th>pred_doi_cites_median_license_label</th>\n      <th>pred_doi_cites_std_license_label</th>\n      <th>pred_doi_cites_q10_license_label</th>\n      <th>pred_doi_cites_q25_license_label</th>\n      <th>pred_doi_cites_q75_license_label</th>\n      <th>pred_doi_cites_mean_category_main_label</th>\n      <th>pred_doi_cites_count_category_main_label</th>\n      <th>pred_doi_cites_sum_category_main_label</th>\n      <th>pred_doi_cites_min_category_main_label</th>\n      <th>pred_doi_cites_max_category_main_label</th>\n      <th>pred_doi_cites_median_category_main_label</th>\n      <th>pred_doi_cites_std_category_main_label</th>\n      <th>pred_doi_cites_q10_category_main_label</th>\n      <th>pred_doi_cites_q25_category_main_label</th>\n      <th>pred_doi_cites_q75_category_main_label</th>\n      <th>pred_doi_cites_mean_category_main_detail_label</th>\n      <th>pred_doi_cites_count_category_main_detail_label</th>\n      <th>pred_doi_cites_sum_category_main_detail_label</th>\n      <th>pred_doi_cites_min_category_main_detail_label</th>\n      <th>pred_doi_cites_max_category_main_detail_label</th>\n      <th>pred_doi_cites_median_category_main_detail_label</th>\n      <th>pred_doi_cites_std_category_main_detail_label</th>\n      <th>pred_doi_cites_q10_category_main_detail_label</th>\n      <th>pred_doi_cites_q25_category_main_detail_label</th>\n      <th>pred_doi_cites_q75_category_main_detail_label</th>\n      <th>pred_doi_cites_mean_category_name_parent_label</th>\n      <th>pred_doi_cites_count_category_name_parent_label</th>\n      <th>pred_doi_cites_sum_category_name_parent_label</th>\n      <th>pred_doi_cites_min_category_name_parent_label</th>\n      <th>pred_doi_cites_max_category_name_parent_label</th>\n      <th>pred_doi_cites_median_category_name_parent_label</th>\n      <th>pred_doi_cites_std_category_name_parent_label</th>\n      <th>pred_doi_cites_q10_category_name_parent_label</th>\n      <th>pred_doi_cites_q25_category_name_parent_label</th>\n      <th>pred_doi_cites_q75_category_name_parent_label</th>\n      <th>pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>pred_doi_cites_count_category_name_parent_main_label</th>\n      <th>pred_doi_cites_sum_category_name_parent_main_label</th>\n      <th>pred_doi_cites_min_category_name_parent_main_label</th>\n      <th>pred_doi_cites_max_category_name_parent_main_label</th>\n      <th>pred_doi_cites_median_category_name_parent_main_label</th>\n      <th>pred_doi_cites_std_category_name_parent_main_label</th>\n      <th>pred_doi_cites_q10_category_name_parent_main_label</th>\n      <th>pred_doi_cites_q25_category_name_parent_main_label</th>\n      <th>pred_doi_cites_q75_category_name_parent_main_label</th>\n      <th>pred_doi_cites_mean_category_name_label</th>\n      <th>pred_doi_cites_count_category_name_label</th>\n      <th>pred_doi_cites_sum_category_name_label</th>\n      <th>pred_doi_cites_min_category_name_label</th>\n      <th>pred_doi_cites_max_category_name_label</th>\n      <th>pred_doi_cites_median_category_name_label</th>\n      <th>pred_doi_cites_std_category_name_label</th>\n      <th>pred_doi_cites_q10_category_name_label</th>\n      <th>pred_doi_cites_q25_category_name_label</th>\n      <th>pred_doi_cites_q75_category_name_label</th>\n      <th>diff_doi_cites_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_pred_doi_cites</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_main_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_category_main_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_main_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_main_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_update_ym</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_license_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_license_label</th>\n      <th>diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_license_label</th>\n      <th>diff_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_label</th>\n      <th>is_null_comments</th>\n      <th>is_null_report-no</th>\n      <th>is_null_journal-ref</th>\n      <th>roberta_vec_0</th>\n      <th>roberta_vec_1</th>\n      <th>roberta_vec_2</th>\n      <th>roberta_vec_3</th>\n      <th>roberta_vec_4</th>\n      <th>roberta_vec_5</th>\n      <th>roberta_vec_6</th>\n      <th>roberta_vec_7</th>\n      <th>roberta_vec_8</th>\n      <th>roberta_vec_9</th>\n      <th>roberta_vec_10</th>\n      <th>roberta_vec_11</th>\n      <th>roberta_vec_12</th>\n      <th>roberta_vec_13</th>\n      <th>roberta_vec_14</th>\n      <th>roberta_vec_15</th>\n      <th>roberta_vec_16</th>\n      <th>roberta_vec_17</th>\n      <th>roberta_vec_18</th>\n      <th>roberta_vec_19</th>\n      <th>roberta_vec_20</th>\n      <th>roberta_vec_21</th>\n      <th>roberta_vec_22</th>\n      <th>roberta_vec_23</th>\n      <th>roberta_vec_24</th>\n      <th>roberta_vec_25</th>\n      <th>roberta_vec_26</th>\n      <th>roberta_vec_27</th>\n      <th>roberta_vec_28</th>\n      <th>roberta_vec_29</th>\n      <th>roberta_vec_30</th>\n      <th>roberta_vec_31</th>\n      <th>roberta_vec_32</th>\n      <th>roberta_vec_33</th>\n      <th>roberta_vec_34</th>\n      <th>roberta_vec_35</th>\n      <th>roberta_vec_36</th>\n      <th>roberta_vec_37</th>\n      <th>roberta_vec_38</th>\n      <th>roberta_vec_39</th>\n      <th>roberta_vec_40</th>\n      <th>roberta_vec_41</th>\n      <th>roberta_vec_42</th>\n      <th>roberta_vec_43</th>\n      <th>roberta_vec_44</th>\n      <th>roberta_vec_45</th>\n      <th>roberta_vec_46</th>\n      <th>roberta_vec_47</th>\n      <th>roberta_vec_48</th>\n      <th>roberta_vec_49</th>\n      <th>roberta_vec_50</th>\n      <th>roberta_vec_51</th>\n      <th>roberta_vec_52</th>\n      <th>roberta_vec_53</th>\n      <th>roberta_vec_54</th>\n      <th>roberta_vec_55</th>\n      <th>roberta_vec_56</th>\n      <th>roberta_vec_57</th>\n      <th>roberta_vec_58</th>\n      <th>roberta_vec_59</th>\n      <th>roberta_vec_60</th>\n      <th>roberta_vec_61</th>\n      <th>roberta_vec_62</th>\n      <th>roberta_vec_63</th>\n      <th>roberta_vec_64</th>\n      <th>roberta_vec_65</th>\n      <th>roberta_vec_66</th>\n      <th>roberta_vec_67</th>\n      <th>roberta_vec_68</th>\n      <th>roberta_vec_69</th>\n      <th>roberta_vec_70</th>\n      <th>roberta_vec_71</th>\n      <th>roberta_vec_72</th>\n      <th>roberta_vec_73</th>\n      <th>roberta_vec_74</th>\n      <th>roberta_vec_75</th>\n      <th>roberta_vec_76</th>\n      <th>roberta_vec_77</th>\n      <th>roberta_vec_78</th>\n      <th>roberta_vec_79</th>\n      <th>roberta_vec_80</th>\n      <th>roberta_vec_81</th>\n      <th>roberta_vec_82</th>\n      <th>roberta_vec_83</th>\n      <th>roberta_vec_84</th>\n      <th>roberta_vec_85</th>\n      <th>roberta_vec_86</th>\n      <th>roberta_vec_87</th>\n      <th>roberta_vec_88</th>\n      <th>roberta_vec_89</th>\n      <th>roberta_vec_90</th>\n      <th>roberta_vec_91</th>\n      <th>roberta_vec_92</th>\n      <th>roberta_vec_93</th>\n      <th>roberta_vec_94</th>\n      <th>roberta_vec_95</th>\n      <th>roberta_vec_96</th>\n      <th>roberta_vec_97</th>\n      <th>roberta_vec_98</th>\n      <th>roberta_vec_99</th>\n      <th>roberta_vec_100</th>\n      <th>roberta_vec_101</th>\n      <th>roberta_vec_102</th>\n      <th>roberta_vec_103</th>\n      <th>roberta_vec_104</th>\n      <th>roberta_vec_105</th>\n      <th>roberta_vec_106</th>\n      <th>roberta_vec_107</th>\n      <th>roberta_vec_108</th>\n      <th>roberta_vec_109</th>\n      <th>roberta_vec_110</th>\n      <th>roberta_vec_111</th>\n      <th>roberta_vec_112</th>\n      <th>roberta_vec_113</th>\n      <th>roberta_vec_114</th>\n      <th>roberta_vec_115</th>\n      <th>roberta_vec_116</th>\n      <th>roberta_vec_117</th>\n      <th>roberta_vec_118</th>\n      <th>roberta_vec_119</th>\n      <th>roberta_vec_120</th>\n      <th>roberta_vec_121</th>\n      <th>roberta_vec_122</th>\n      <th>roberta_vec_123</th>\n      <th>roberta_vec_124</th>\n      <th>roberta_vec_125</th>\n      <th>roberta_vec_126</th>\n      <th>roberta_vec_127</th>\n      <th>roberta_vec_128</th>\n      <th>roberta_vec_129</th>\n      <th>roberta_vec_130</th>\n      <th>roberta_vec_131</th>\n      <th>roberta_vec_132</th>\n      <th>roberta_vec_133</th>\n      <th>roberta_vec_134</th>\n      <th>roberta_vec_135</th>\n      <th>roberta_vec_136</th>\n      <th>roberta_vec_137</th>\n      <th>roberta_vec_138</th>\n      <th>roberta_vec_139</th>\n      <th>roberta_vec_140</th>\n      <th>roberta_vec_141</th>\n      <th>roberta_vec_142</th>\n      <th>roberta_vec_143</th>\n      <th>roberta_vec_144</th>\n      <th>roberta_vec_145</th>\n      <th>roberta_vec_146</th>\n      <th>roberta_vec_147</th>\n      <th>roberta_vec_148</th>\n      <th>roberta_vec_149</th>\n      <th>roberta_vec_150</th>\n      <th>roberta_vec_151</th>\n      <th>roberta_vec_152</th>\n      <th>roberta_vec_153</th>\n      <th>roberta_vec_154</th>\n      <th>roberta_vec_155</th>\n      <th>roberta_vec_156</th>\n      <th>roberta_vec_157</th>\n      <th>roberta_vec_158</th>\n      <th>roberta_vec_159</th>\n      <th>roberta_vec_160</th>\n      <th>roberta_vec_161</th>\n      <th>roberta_vec_162</th>\n      <th>roberta_vec_163</th>\n      <th>roberta_vec_164</th>\n      <th>roberta_vec_165</th>\n      <th>roberta_vec_166</th>\n      <th>roberta_vec_167</th>\n      <th>roberta_vec_168</th>\n      <th>roberta_vec_169</th>\n      <th>roberta_vec_170</th>\n      <th>roberta_vec_171</th>\n      <th>roberta_vec_172</th>\n      <th>roberta_vec_173</th>\n      <th>roberta_vec_174</th>\n      <th>roberta_vec_175</th>\n      <th>roberta_vec_176</th>\n      <th>roberta_vec_177</th>\n      <th>roberta_vec_178</th>\n      <th>roberta_vec_179</th>\n      <th>roberta_vec_180</th>\n      <th>roberta_vec_181</th>\n      <th>roberta_vec_182</th>\n      <th>roberta_vec_183</th>\n      <th>roberta_vec_184</th>\n      <th>roberta_vec_185</th>\n      <th>roberta_vec_186</th>\n      <th>roberta_vec_187</th>\n      <th>roberta_vec_188</th>\n      <th>roberta_vec_189</th>\n      <th>roberta_vec_190</th>\n      <th>roberta_vec_191</th>\n      <th>roberta_vec_192</th>\n      <th>roberta_vec_193</th>\n      <th>roberta_vec_194</th>\n      <th>roberta_vec_195</th>\n      <th>roberta_vec_196</th>\n      <th>roberta_vec_197</th>\n      <th>roberta_vec_198</th>\n      <th>roberta_vec_199</th>\n      <th>roberta_vec_200</th>\n      <th>roberta_vec_201</th>\n      <th>roberta_vec_202</th>\n      <th>roberta_vec_203</th>\n      <th>roberta_vec_204</th>\n      <th>roberta_vec_205</th>\n      <th>roberta_vec_206</th>\n      <th>roberta_vec_207</th>\n      <th>roberta_vec_208</th>\n      <th>roberta_vec_209</th>\n      <th>roberta_vec_210</th>\n      <th>roberta_vec_211</th>\n      <th>roberta_vec_212</th>\n      <th>roberta_vec_213</th>\n      <th>roberta_vec_214</th>\n      <th>roberta_vec_215</th>\n      <th>roberta_vec_216</th>\n      <th>roberta_vec_217</th>\n      <th>roberta_vec_218</th>\n      <th>roberta_vec_219</th>\n      <th>roberta_vec_220</th>\n      <th>roberta_vec_221</th>\n      <th>roberta_vec_222</th>\n      <th>roberta_vec_223</th>\n      <th>roberta_vec_224</th>\n      <th>roberta_vec_225</th>\n      <th>roberta_vec_226</th>\n      <th>roberta_vec_227</th>\n      <th>roberta_vec_228</th>\n      <th>roberta_vec_229</th>\n      <th>roberta_vec_230</th>\n      <th>roberta_vec_231</th>\n      <th>roberta_vec_232</th>\n      <th>roberta_vec_233</th>\n      <th>roberta_vec_234</th>\n      <th>roberta_vec_235</th>\n      <th>roberta_vec_236</th>\n      <th>roberta_vec_237</th>\n      <th>roberta_vec_238</th>\n      <th>roberta_vec_239</th>\n      <th>roberta_vec_240</th>\n      <th>roberta_vec_241</th>\n      <th>roberta_vec_242</th>\n      <th>roberta_vec_243</th>\n      <th>roberta_vec_244</th>\n      <th>roberta_vec_245</th>\n      <th>roberta_vec_246</th>\n      <th>roberta_vec_247</th>\n      <th>roberta_vec_248</th>\n      <th>roberta_vec_249</th>\n      <th>roberta_vec_250</th>\n      <th>roberta_vec_251</th>\n      <th>roberta_vec_252</th>\n      <th>roberta_vec_253</th>\n      <th>roberta_vec_254</th>\n      <th>roberta_vec_255</th>\n      <th>roberta_vec_256</th>\n      <th>roberta_vec_257</th>\n      <th>roberta_vec_258</th>\n      <th>roberta_vec_259</th>\n      <th>roberta_vec_260</th>\n      <th>roberta_vec_261</th>\n      <th>roberta_vec_262</th>\n      <th>roberta_vec_263</th>\n      <th>roberta_vec_264</th>\n      <th>roberta_vec_265</th>\n      <th>roberta_vec_266</th>\n      <th>roberta_vec_267</th>\n      <th>roberta_vec_268</th>\n      <th>roberta_vec_269</th>\n      <th>roberta_vec_270</th>\n      <th>roberta_vec_271</th>\n      <th>roberta_vec_272</th>\n      <th>roberta_vec_273</th>\n      <th>roberta_vec_274</th>\n      <th>roberta_vec_275</th>\n      <th>roberta_vec_276</th>\n      <th>roberta_vec_277</th>\n      <th>roberta_vec_278</th>\n      <th>roberta_vec_279</th>\n      <th>roberta_vec_280</th>\n      <th>roberta_vec_281</th>\n      <th>roberta_vec_282</th>\n      <th>roberta_vec_283</th>\n      <th>roberta_vec_284</th>\n      <th>roberta_vec_285</th>\n      <th>roberta_vec_286</th>\n      <th>roberta_vec_287</th>\n      <th>roberta_vec_288</th>\n      <th>roberta_vec_289</th>\n      <th>roberta_vec_290</th>\n      <th>roberta_vec_291</th>\n      <th>roberta_vec_292</th>\n      <th>roberta_vec_293</th>\n      <th>roberta_vec_294</th>\n      <th>roberta_vec_295</th>\n      <th>roberta_vec_296</th>\n      <th>roberta_vec_297</th>\n      <th>roberta_vec_298</th>\n      <th>roberta_vec_299</th>\n      <th>roberta_vec_300</th>\n      <th>roberta_vec_301</th>\n      <th>roberta_vec_302</th>\n      <th>roberta_vec_303</th>\n      <th>roberta_vec_304</th>\n      <th>roberta_vec_305</th>\n      <th>roberta_vec_306</th>\n      <th>roberta_vec_307</th>\n      <th>roberta_vec_308</th>\n      <th>roberta_vec_309</th>\n      <th>roberta_vec_310</th>\n      <th>roberta_vec_311</th>\n      <th>roberta_vec_312</th>\n      <th>roberta_vec_313</th>\n      <th>roberta_vec_314</th>\n      <th>roberta_vec_315</th>\n      <th>roberta_vec_316</th>\n      <th>roberta_vec_317</th>\n      <th>roberta_vec_318</th>\n      <th>roberta_vec_319</th>\n      <th>roberta_vec_320</th>\n      <th>roberta_vec_321</th>\n      <th>roberta_vec_322</th>\n      <th>roberta_vec_323</th>\n      <th>roberta_vec_324</th>\n      <th>roberta_vec_325</th>\n      <th>roberta_vec_326</th>\n      <th>roberta_vec_327</th>\n      <th>roberta_vec_328</th>\n      <th>roberta_vec_329</th>\n      <th>roberta_vec_330</th>\n      <th>roberta_vec_331</th>\n      <th>roberta_vec_332</th>\n      <th>roberta_vec_333</th>\n      <th>roberta_vec_334</th>\n      <th>roberta_vec_335</th>\n      <th>roberta_vec_336</th>\n      <th>roberta_vec_337</th>\n      <th>roberta_vec_338</th>\n      <th>roberta_vec_339</th>\n      <th>roberta_vec_340</th>\n      <th>roberta_vec_341</th>\n      <th>roberta_vec_342</th>\n      <th>roberta_vec_343</th>\n      <th>roberta_vec_344</th>\n      <th>roberta_vec_345</th>\n      <th>roberta_vec_346</th>\n      <th>roberta_vec_347</th>\n      <th>roberta_vec_348</th>\n      <th>roberta_vec_349</th>\n      <th>roberta_vec_350</th>\n      <th>roberta_vec_351</th>\n      <th>roberta_vec_352</th>\n      <th>roberta_vec_353</th>\n      <th>roberta_vec_354</th>\n      <th>roberta_vec_355</th>\n      <th>roberta_vec_356</th>\n      <th>roberta_vec_357</th>\n      <th>roberta_vec_358</th>\n      <th>roberta_vec_359</th>\n      <th>roberta_vec_360</th>\n      <th>roberta_vec_361</th>\n      <th>roberta_vec_362</th>\n      <th>roberta_vec_363</th>\n      <th>roberta_vec_364</th>\n      <th>roberta_vec_365</th>\n      <th>roberta_vec_366</th>\n      <th>roberta_vec_367</th>\n      <th>roberta_vec_368</th>\n      <th>roberta_vec_369</th>\n      <th>roberta_vec_370</th>\n      <th>roberta_vec_371</th>\n      <th>roberta_vec_372</th>\n      <th>roberta_vec_373</th>\n      <th>roberta_vec_374</th>\n      <th>roberta_vec_375</th>\n      <th>roberta_vec_376</th>\n      <th>roberta_vec_377</th>\n      <th>roberta_vec_378</th>\n      <th>roberta_vec_379</th>\n      <th>roberta_vec_380</th>\n      <th>roberta_vec_381</th>\n      <th>roberta_vec_382</th>\n      <th>roberta_vec_383</th>\n      <th>roberta_vec_384</th>\n      <th>roberta_vec_385</th>\n      <th>roberta_vec_386</th>\n      <th>roberta_vec_387</th>\n      <th>roberta_vec_388</th>\n      <th>roberta_vec_389</th>\n      <th>roberta_vec_390</th>\n      <th>roberta_vec_391</th>\n      <th>roberta_vec_392</th>\n      <th>roberta_vec_393</th>\n      <th>roberta_vec_394</th>\n      <th>roberta_vec_395</th>\n      <th>roberta_vec_396</th>\n      <th>roberta_vec_397</th>\n      <th>roberta_vec_398</th>\n      <th>roberta_vec_399</th>\n      <th>roberta_vec_400</th>\n      <th>roberta_vec_401</th>\n      <th>roberta_vec_402</th>\n      <th>roberta_vec_403</th>\n      <th>roberta_vec_404</th>\n      <th>roberta_vec_405</th>\n      <th>roberta_vec_406</th>\n      <th>roberta_vec_407</th>\n      <th>roberta_vec_408</th>\n      <th>roberta_vec_409</th>\n      <th>roberta_vec_410</th>\n      <th>roberta_vec_411</th>\n      <th>roberta_vec_412</th>\n      <th>roberta_vec_413</th>\n      <th>roberta_vec_414</th>\n      <th>roberta_vec_415</th>\n      <th>roberta_vec_416</th>\n      <th>roberta_vec_417</th>\n      <th>roberta_vec_418</th>\n      <th>roberta_vec_419</th>\n      <th>roberta_vec_420</th>\n      <th>roberta_vec_421</th>\n      <th>roberta_vec_422</th>\n      <th>roberta_vec_423</th>\n      <th>roberta_vec_424</th>\n      <th>roberta_vec_425</th>\n      <th>roberta_vec_426</th>\n      <th>roberta_vec_427</th>\n      <th>roberta_vec_428</th>\n      <th>roberta_vec_429</th>\n      <th>roberta_vec_430</th>\n      <th>roberta_vec_431</th>\n      <th>roberta_vec_432</th>\n      <th>roberta_vec_433</th>\n      <th>roberta_vec_434</th>\n      <th>roberta_vec_435</th>\n      <th>roberta_vec_436</th>\n      <th>roberta_vec_437</th>\n      <th>roberta_vec_438</th>\n      <th>roberta_vec_439</th>\n      <th>roberta_vec_440</th>\n      <th>roberta_vec_441</th>\n      <th>roberta_vec_442</th>\n      <th>roberta_vec_443</th>\n      <th>roberta_vec_444</th>\n      <th>roberta_vec_445</th>\n      <th>roberta_vec_446</th>\n      <th>roberta_vec_447</th>\n      <th>roberta_vec_448</th>\n      <th>roberta_vec_449</th>\n      <th>roberta_vec_450</th>\n      <th>roberta_vec_451</th>\n      <th>roberta_vec_452</th>\n      <th>roberta_vec_453</th>\n      <th>roberta_vec_454</th>\n      <th>roberta_vec_455</th>\n      <th>roberta_vec_456</th>\n      <th>roberta_vec_457</th>\n      <th>roberta_vec_458</th>\n      <th>roberta_vec_459</th>\n      <th>roberta_vec_460</th>\n      <th>roberta_vec_461</th>\n      <th>roberta_vec_462</th>\n      <th>roberta_vec_463</th>\n      <th>roberta_vec_464</th>\n      <th>roberta_vec_465</th>\n      <th>roberta_vec_466</th>\n      <th>roberta_vec_467</th>\n      <th>roberta_vec_468</th>\n      <th>roberta_vec_469</th>\n      <th>roberta_vec_470</th>\n      <th>roberta_vec_471</th>\n      <th>roberta_vec_472</th>\n      <th>roberta_vec_473</th>\n      <th>roberta_vec_474</th>\n      <th>roberta_vec_475</th>\n      <th>roberta_vec_476</th>\n      <th>roberta_vec_477</th>\n      <th>roberta_vec_478</th>\n      <th>roberta_vec_479</th>\n      <th>roberta_vec_480</th>\n      <th>roberta_vec_481</th>\n      <th>roberta_vec_482</th>\n      <th>roberta_vec_483</th>\n      <th>roberta_vec_484</th>\n      <th>roberta_vec_485</th>\n      <th>roberta_vec_486</th>\n      <th>roberta_vec_487</th>\n      <th>roberta_vec_488</th>\n      <th>roberta_vec_489</th>\n      <th>roberta_vec_490</th>\n      <th>roberta_vec_491</th>\n      <th>roberta_vec_492</th>\n      <th>roberta_vec_493</th>\n      <th>roberta_vec_494</th>\n      <th>roberta_vec_495</th>\n      <th>roberta_vec_496</th>\n      <th>roberta_vec_497</th>\n      <th>roberta_vec_498</th>\n      <th>roberta_vec_499</th>\n      <th>roberta_vec_500</th>\n      <th>roberta_vec_501</th>\n      <th>roberta_vec_502</th>\n      <th>roberta_vec_503</th>\n      <th>roberta_vec_504</th>\n      <th>roberta_vec_505</th>\n      <th>roberta_vec_506</th>\n      <th>roberta_vec_507</th>\n      <th>roberta_vec_508</th>\n      <th>roberta_vec_509</th>\n      <th>roberta_vec_510</th>\n      <th>roberta_vec_511</th>\n      <th>roberta_vec_512</th>\n      <th>roberta_vec_513</th>\n      <th>roberta_vec_514</th>\n      <th>roberta_vec_515</th>\n      <th>roberta_vec_516</th>\n      <th>roberta_vec_517</th>\n      <th>roberta_vec_518</th>\n      <th>roberta_vec_519</th>\n      <th>roberta_vec_520</th>\n      <th>roberta_vec_521</th>\n      <th>roberta_vec_522</th>\n      <th>roberta_vec_523</th>\n      <th>roberta_vec_524</th>\n      <th>roberta_vec_525</th>\n      <th>roberta_vec_526</th>\n      <th>roberta_vec_527</th>\n      <th>roberta_vec_528</th>\n      <th>roberta_vec_529</th>\n      <th>roberta_vec_530</th>\n      <th>roberta_vec_531</th>\n      <th>roberta_vec_532</th>\n      <th>roberta_vec_533</th>\n      <th>roberta_vec_534</th>\n      <th>roberta_vec_535</th>\n      <th>roberta_vec_536</th>\n      <th>roberta_vec_537</th>\n      <th>roberta_vec_538</th>\n      <th>roberta_vec_539</th>\n      <th>roberta_vec_540</th>\n      <th>roberta_vec_541</th>\n      <th>roberta_vec_542</th>\n      <th>roberta_vec_543</th>\n      <th>roberta_vec_544</th>\n      <th>roberta_vec_545</th>\n      <th>roberta_vec_546</th>\n      <th>roberta_vec_547</th>\n      <th>roberta_vec_548</th>\n      <th>roberta_vec_549</th>\n      <th>roberta_vec_550</th>\n      <th>roberta_vec_551</th>\n      <th>roberta_vec_552</th>\n      <th>roberta_vec_553</th>\n      <th>roberta_vec_554</th>\n      <th>roberta_vec_555</th>\n      <th>roberta_vec_556</th>\n      <th>roberta_vec_557</th>\n      <th>roberta_vec_558</th>\n      <th>roberta_vec_559</th>\n      <th>roberta_vec_560</th>\n      <th>roberta_vec_561</th>\n      <th>roberta_vec_562</th>\n      <th>roberta_vec_563</th>\n      <th>roberta_vec_564</th>\n      <th>roberta_vec_565</th>\n      <th>roberta_vec_566</th>\n      <th>roberta_vec_567</th>\n      <th>roberta_vec_568</th>\n      <th>roberta_vec_569</th>\n      <th>roberta_vec_570</th>\n      <th>roberta_vec_571</th>\n      <th>roberta_vec_572</th>\n      <th>roberta_vec_573</th>\n      <th>roberta_vec_574</th>\n      <th>roberta_vec_575</th>\n      <th>roberta_vec_576</th>\n      <th>roberta_vec_577</th>\n      <th>roberta_vec_578</th>\n      <th>roberta_vec_579</th>\n      <th>roberta_vec_580</th>\n      <th>roberta_vec_581</th>\n      <th>roberta_vec_582</th>\n      <th>roberta_vec_583</th>\n      <th>roberta_vec_584</th>\n      <th>roberta_vec_585</th>\n      <th>roberta_vec_586</th>\n      <th>roberta_vec_587</th>\n      <th>roberta_vec_588</th>\n      <th>roberta_vec_589</th>\n      <th>roberta_vec_590</th>\n      <th>roberta_vec_591</th>\n      <th>roberta_vec_592</th>\n      <th>roberta_vec_593</th>\n      <th>roberta_vec_594</th>\n      <th>roberta_vec_595</th>\n      <th>roberta_vec_596</th>\n      <th>roberta_vec_597</th>\n      <th>roberta_vec_598</th>\n      <th>roberta_vec_599</th>\n      <th>roberta_vec_600</th>\n      <th>roberta_vec_601</th>\n      <th>roberta_vec_602</th>\n      <th>roberta_vec_603</th>\n      <th>roberta_vec_604</th>\n      <th>roberta_vec_605</th>\n      <th>roberta_vec_606</th>\n      <th>roberta_vec_607</th>\n      <th>roberta_vec_608</th>\n      <th>roberta_vec_609</th>\n      <th>roberta_vec_610</th>\n      <th>roberta_vec_611</th>\n      <th>roberta_vec_612</th>\n      <th>roberta_vec_613</th>\n      <th>roberta_vec_614</th>\n      <th>roberta_vec_615</th>\n      <th>roberta_vec_616</th>\n      <th>roberta_vec_617</th>\n      <th>roberta_vec_618</th>\n      <th>roberta_vec_619</th>\n      <th>roberta_vec_620</th>\n      <th>roberta_vec_621</th>\n      <th>roberta_vec_622</th>\n      <th>roberta_vec_623</th>\n      <th>roberta_vec_624</th>\n      <th>roberta_vec_625</th>\n      <th>roberta_vec_626</th>\n      <th>roberta_vec_627</th>\n      <th>roberta_vec_628</th>\n      <th>roberta_vec_629</th>\n      <th>roberta_vec_630</th>\n      <th>roberta_vec_631</th>\n      <th>roberta_vec_632</th>\n      <th>roberta_vec_633</th>\n      <th>roberta_vec_634</th>\n      <th>roberta_vec_635</th>\n      <th>roberta_vec_636</th>\n      <th>roberta_vec_637</th>\n      <th>roberta_vec_638</th>\n      <th>roberta_vec_639</th>\n      <th>roberta_vec_640</th>\n      <th>roberta_vec_641</th>\n      <th>roberta_vec_642</th>\n      <th>roberta_vec_643</th>\n      <th>roberta_vec_644</th>\n      <th>roberta_vec_645</th>\n      <th>roberta_vec_646</th>\n      <th>roberta_vec_647</th>\n      <th>roberta_vec_648</th>\n      <th>roberta_vec_649</th>\n      <th>roberta_vec_650</th>\n      <th>roberta_vec_651</th>\n      <th>roberta_vec_652</th>\n      <th>roberta_vec_653</th>\n      <th>roberta_vec_654</th>\n      <th>roberta_vec_655</th>\n      <th>roberta_vec_656</th>\n      <th>roberta_vec_657</th>\n      <th>roberta_vec_658</th>\n      <th>roberta_vec_659</th>\n      <th>roberta_vec_660</th>\n      <th>roberta_vec_661</th>\n      <th>roberta_vec_662</th>\n      <th>roberta_vec_663</th>\n      <th>roberta_vec_664</th>\n      <th>roberta_vec_665</th>\n      <th>roberta_vec_666</th>\n      <th>roberta_vec_667</th>\n      <th>roberta_vec_668</th>\n      <th>roberta_vec_669</th>\n      <th>roberta_vec_670</th>\n      <th>roberta_vec_671</th>\n      <th>roberta_vec_672</th>\n      <th>roberta_vec_673</th>\n      <th>roberta_vec_674</th>\n      <th>roberta_vec_675</th>\n      <th>roberta_vec_676</th>\n      <th>roberta_vec_677</th>\n      <th>roberta_vec_678</th>\n      <th>roberta_vec_679</th>\n      <th>roberta_vec_680</th>\n      <th>roberta_vec_681</th>\n      <th>roberta_vec_682</th>\n      <th>roberta_vec_683</th>\n      <th>roberta_vec_684</th>\n      <th>roberta_vec_685</th>\n      <th>roberta_vec_686</th>\n      <th>roberta_vec_687</th>\n      <th>roberta_vec_688</th>\n      <th>roberta_vec_689</th>\n      <th>roberta_vec_690</th>\n      <th>roberta_vec_691</th>\n      <th>roberta_vec_692</th>\n      <th>roberta_vec_693</th>\n      <th>roberta_vec_694</th>\n      <th>roberta_vec_695</th>\n      <th>roberta_vec_696</th>\n      <th>roberta_vec_697</th>\n      <th>roberta_vec_698</th>\n      <th>roberta_vec_699</th>\n      <th>roberta_vec_700</th>\n      <th>roberta_vec_701</th>\n      <th>roberta_vec_702</th>\n      <th>roberta_vec_703</th>\n      <th>roberta_vec_704</th>\n      <th>roberta_vec_705</th>\n      <th>roberta_vec_706</th>\n      <th>roberta_vec_707</th>\n      <th>roberta_vec_708</th>\n      <th>roberta_vec_709</th>\n      <th>roberta_vec_710</th>\n      <th>roberta_vec_711</th>\n      <th>roberta_vec_712</th>\n      <th>roberta_vec_713</th>\n      <th>roberta_vec_714</th>\n      <th>roberta_vec_715</th>\n      <th>roberta_vec_716</th>\n      <th>roberta_vec_717</th>\n      <th>roberta_vec_718</th>\n      <th>roberta_vec_719</th>\n      <th>roberta_vec_720</th>\n      <th>roberta_vec_721</th>\n      <th>roberta_vec_722</th>\n      <th>roberta_vec_723</th>\n      <th>roberta_vec_724</th>\n      <th>roberta_vec_725</th>\n      <th>roberta_vec_726</th>\n      <th>roberta_vec_727</th>\n      <th>roberta_vec_728</th>\n      <th>roberta_vec_729</th>\n      <th>roberta_vec_730</th>\n      <th>roberta_vec_731</th>\n      <th>roberta_vec_732</th>\n      <th>roberta_vec_733</th>\n      <th>roberta_vec_734</th>\n      <th>roberta_vec_735</th>\n      <th>roberta_vec_736</th>\n      <th>roberta_vec_737</th>\n      <th>roberta_vec_738</th>\n      <th>roberta_vec_739</th>\n      <th>roberta_vec_740</th>\n      <th>roberta_vec_741</th>\n      <th>roberta_vec_742</th>\n      <th>roberta_vec_743</th>\n      <th>roberta_vec_744</th>\n      <th>roberta_vec_745</th>\n      <th>roberta_vec_746</th>\n      <th>roberta_vec_747</th>\n      <th>roberta_vec_748</th>\n      <th>roberta_vec_749</th>\n      <th>roberta_vec_750</th>\n      <th>roberta_vec_751</th>\n      <th>roberta_vec_752</th>\n      <th>roberta_vec_753</th>\n      <th>roberta_vec_754</th>\n      <th>roberta_vec_755</th>\n      <th>roberta_vec_756</th>\n      <th>roberta_vec_757</th>\n      <th>roberta_vec_758</th>\n      <th>roberta_vec_759</th>\n      <th>roberta_vec_760</th>\n      <th>roberta_vec_761</th>\n      <th>roberta_vec_762</th>\n      <th>roberta_vec_763</th>\n      <th>roberta_vec_764</th>\n      <th>roberta_vec_765</th>\n      <th>roberta_vec_766</th>\n      <th>roberta_vec_767</th>\n      <th>fold_no</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1403.7138</td>\n      <td>Aigen Li</td>\n      <td>Qi Li, S.L. Liang, Aigen Li (University of Mis...</td>\n      <td>Spectropolarimetric Constraints on the Nature ...</td>\n      <td>5 pages, 2 figures; accepted for publication i...</td>\n      <td>None</td>\n      <td>10.1093/mnrasl/slu021</td>\n      <td>None</td>\n      <td>astro-ph.GA</td>\n      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n      <td>While it is well recognized that interstella...</td>\n      <td>[{'version': 'v1', 'created': 'Thu, 27 Mar 201...</td>\n      <td>[[Li, Qi, , University of Missouri], [Liang, S...</td>\n      <td>2.197266</td>\n      <td>2.079442</td>\n      <td>10.1093</td>\n      <td>Oxford University Press</td>\n      <td>5.832031</td>\n      <td>1091568.0</td>\n      <td>2015-06-19</td>\n      <td>2014-03-27 17:25:40+00:00</td>\n      <td>2014-03-27 17:25:40+00:00</td>\n      <td>2015</td>\n      <td>2014</td>\n      <td>2014</td>\n      <td>6</td>\n      <td>3</td>\n      <td>3</td>\n      <td>201506</td>\n      <td>201403</td>\n      <td>201403</td>\n      <td>19</td>\n      <td>27</td>\n      <td>27</td>\n      <td>1.434672e+09</td>\n      <td>1.395941e+09</td>\n      <td>1.395941e+09</td>\n      <td>38730860</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>16605</td>\n      <td>16156</td>\n      <td>16156</td>\n      <td>0</td>\n      <td>0.500000</td>\n      <td>Li</td>\n      <td>3</td>\n      <td>2.197266</td>\n      <td>astro-ph</td>\n      <td>astro-ph</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>astro</td>\n      <td>astro-ph</td>\n      <td>astro-ph.ga</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3033</td>\n      <td>53</td>\n      <td>60233</td>\n      <td>344</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>92</td>\n      <td>88</td>\n      <td>1965</td>\n      <td>1.975586</td>\n      <td>6310</td>\n      <td>12464.000000</td>\n      <td>0.000000</td>\n      <td>8.492188</td>\n      <td>1.946289</td>\n      <td>1.362305</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>2.890625</td>\n      <td>2.072266</td>\n      <td>29728</td>\n      <td>61618.074219</td>\n      <td>0.0</td>\n      <td>8.078125</td>\n      <td>2.080078</td>\n      <td>1.298828</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>3.044922</td>\n      <td>2.072266</td>\n      <td>29728</td>\n      <td>61618.074219</td>\n      <td>0.0</td>\n      <td>8.078125</td>\n      <td>2.080078</td>\n      <td>1.298828</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>3.044922</td>\n      <td>2.281250</td>\n      <td>43</td>\n      <td>98.06250</td>\n      <td>0.000000</td>\n      <td>4.093750</td>\n      <td>2.302734</td>\n      <td>0.999023</td>\n      <td>0.774414</td>\n      <td>1.946289</td>\n      <td>2.917969</td>\n      <td>2.416016</td>\n      <td>75054</td>\n      <td>181269.656250</td>\n      <td>0.0</td>\n      <td>9.531250</td>\n      <td>2.484375</td>\n      <td>1.274414</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.294922</td>\n      <td>2.304688</td>\n      <td>4588</td>\n      <td>10568.0</td>\n      <td>0.0</td>\n      <td>6.882812</td>\n      <td>2.302734</td>\n      <td>1.259766</td>\n      <td>0.693359</td>\n      <td>1.386719</td>\n      <td>3.177734</td>\n      <td>1.940430</td>\n      <td>599833</td>\n      <td>1163680.25</td>\n      <td>0.0</td>\n      <td>9.101562</td>\n      <td>1.946289</td>\n      <td>1.349609</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>2.890625</td>\n      <td>2.640625</td>\n      <td>195890</td>\n      <td>517164.906250</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.773438</td>\n      <td>1.436523</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.664062</td>\n      <td>2.640625</td>\n      <td>195890</td>\n      <td>517164.906250</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.773438</td>\n      <td>1.436523</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.664062</td>\n      <td>2.714844</td>\n      <td>145677</td>\n      <td>395449.468750</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.833984</td>\n      <td>1.429688</td>\n      <td>0.693359</td>\n      <td>1.791992</td>\n      <td>3.712891</td>\n      <td>2.714844</td>\n      <td>145677</td>\n      <td>395449.468750</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.833984</td>\n      <td>1.429688</td>\n      <td>0.693359</td>\n      <td>1.791992</td>\n      <td>3.712891</td>\n      <td>2.189453</td>\n      <td>18375</td>\n      <td>40248.957031</td>\n      <td>0.0</td>\n      <td>7.996094</td>\n      <td>2.302734</td>\n      <td>1.303711</td>\n      <td>0.000000</td>\n      <td>1.386719</td>\n      <td>3.134766</td>\n      <td>1.975586</td>\n      <td>6310</td>\n      <td>12472.000000</td>\n      <td>0.000000</td>\n      <td>8.492188</td>\n      <td>1.946289</td>\n      <td>1.362305</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>2.890625</td>\n      <td>2.072266</td>\n      <td>29728</td>\n      <td>61621.222656</td>\n      <td>0.0</td>\n      <td>8.078125</td>\n      <td>2.080078</td>\n      <td>1.298828</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>3.044922</td>\n      <td>2.072266</td>\n      <td>29728</td>\n      <td>61621.222656</td>\n      <td>0.0</td>\n      <td>8.078125</td>\n      <td>2.080078</td>\n      <td>1.298828</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>3.044922</td>\n      <td>2.281250</td>\n      <td>43</td>\n      <td>98.06250</td>\n      <td>0.000000</td>\n      <td>4.093750</td>\n      <td>2.302734</td>\n      <td>0.999023</td>\n      <td>0.774414</td>\n      <td>1.946289</td>\n      <td>2.917969</td>\n      <td>2.416016</td>\n      <td>75054</td>\n      <td>181276.968750</td>\n      <td>0.0</td>\n      <td>9.531250</td>\n      <td>2.484375</td>\n      <td>1.274414</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.294922</td>\n      <td>2.304688</td>\n      <td>4588</td>\n      <td>10568.0</td>\n      <td>0.0</td>\n      <td>6.882812</td>\n      <td>2.302734</td>\n      <td>1.259766</td>\n      <td>0.693359</td>\n      <td>1.386719</td>\n      <td>3.177734</td>\n      <td>1.940430</td>\n      <td>599833</td>\n      <td>1.163743e+06</td>\n      <td>0.0</td>\n      <td>9.101562</td>\n      <td>1.946289</td>\n      <td>1.349609</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>2.890625</td>\n      <td>2.640625</td>\n      <td>195890</td>\n      <td>517176.90625</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.773438</td>\n      <td>1.436523</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.664062</td>\n      <td>2.640625</td>\n      <td>195890</td>\n      <td>517176.90625</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.773438</td>\n      <td>1.436523</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.664062</td>\n      <td>2.714844</td>\n      <td>145677</td>\n      <td>395457.687500</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.833984</td>\n      <td>1.429688</td>\n      <td>0.693359</td>\n      <td>1.791992</td>\n      <td>3.712891</td>\n      <td>2.714844</td>\n      <td>145677</td>\n      <td>395457.687500</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.833984</td>\n      <td>1.429688</td>\n      <td>0.693359</td>\n      <td>1.791992</td>\n      <td>3.712891</td>\n      <td>2.191406</td>\n      <td>18375</td>\n      <td>40250.773438</td>\n      <td>0.0</td>\n      <td>7.996094</td>\n      <td>2.302734</td>\n      <td>1.303711</td>\n      <td>0.000000</td>\n      <td>1.386719</td>\n      <td>3.134766</td>\n      <td>-0.083679</td>\n      <td>0.974609</td>\n      <td>0.124512</td>\n      <td>1.040039</td>\n      <td>0.221313</td>\n      <td>1.074219</td>\n      <td>0.124512</td>\n      <td>1.040039</td>\n      <td>-0.218018</td>\n      <td>0.936035</td>\n      <td>-0.106506</td>\n      <td>0.967773</td>\n      <td>0.257324</td>\n      <td>1.087891</td>\n      <td>-0.442871</td>\n      <td>0.878418</td>\n      <td>-0.442871</td>\n      <td>0.878418</td>\n      <td>-0.517578</td>\n      <td>0.860840</td>\n      <td>-0.517578</td>\n      <td>0.860840</td>\n      <td>0.006805</td>\n      <td>1.001953</td>\n      <td>-0.000041</td>\n      <td>1.0</td>\n      <td>-0.083923</td>\n      <td>0.974609</td>\n      <td>0.124390</td>\n      <td>1.040039</td>\n      <td>0.221313</td>\n      <td>1.074219</td>\n      <td>0.124390</td>\n      <td>1.040039</td>\n      <td>-0.218018</td>\n      <td>0.936035</td>\n      <td>-0.106628</td>\n      <td>0.967773</td>\n      <td>0.257080</td>\n      <td>1.087891</td>\n      <td>-0.442871</td>\n      <td>0.878418</td>\n      <td>-0.442871</td>\n      <td>0.878418</td>\n      <td>-0.517578</td>\n      <td>0.860840</td>\n      <td>-0.517578</td>\n      <td>0.860840</td>\n      <td>0.006706</td>\n      <td>1.001953</td>\n      <td>0.208130</td>\n      <td>1.067383</td>\n      <td>0.304932</td>\n      <td>1.102539</td>\n      <td>0.208130</td>\n      <td>1.067383</td>\n      <td>-0.134277</td>\n      <td>0.960449</td>\n      <td>-0.022827</td>\n      <td>0.993164</td>\n      <td>0.340820</td>\n      <td>1.116211</td>\n      <td>-0.359131</td>\n      <td>0.901367</td>\n      <td>-0.359131</td>\n      <td>0.901367</td>\n      <td>-0.433594</td>\n      <td>0.883301</td>\n      <td>-0.433594</td>\n      <td>0.883301</td>\n      <td>0.090515</td>\n      <td>1.028320</td>\n      <td>0.083618</td>\n      <td>1.026367</td>\n      <td>-0.000227</td>\n      <td>1.0</td>\n      <td>0.208130</td>\n      <td>1.067383</td>\n      <td>0.304932</td>\n      <td>1.102539</td>\n      <td>0.208130</td>\n      <td>1.067383</td>\n      <td>-0.134399</td>\n      <td>0.960449</td>\n      <td>-0.022934</td>\n      <td>0.993164</td>\n      <td>0.340820</td>\n      <td>1.116211</td>\n      <td>-0.359131</td>\n      <td>0.901367</td>\n      <td>-0.359131</td>\n      <td>0.901367</td>\n      <td>-0.433594</td>\n      <td>0.883301</td>\n      <td>-0.433594</td>\n      <td>0.883301</td>\n      <td>0.090393</td>\n      <td>1.028320</td>\n      <td>0.096863</td>\n      <td>1.032227</td>\n      <td>0.000000</td>\n      <td>1.00000</td>\n      <td>-0.342529</td>\n      <td>0.899902</td>\n      <td>-0.230957</td>\n      <td>0.930176</td>\n      <td>0.132690</td>\n      <td>1.044922</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.641602</td>\n      <td>0.827148</td>\n      <td>-0.641602</td>\n      <td>0.827148</td>\n      <td>-0.117676</td>\n      <td>0.962891</td>\n      <td>-0.124512</td>\n      <td>0.960938</td>\n      <td>-0.208374</td>\n      <td>0.936523</td>\n      <td>-0.000106</td>\n      <td>1.0</td>\n      <td>0.096802</td>\n      <td>1.032227</td>\n      <td>-0.000106</td>\n      <td>1.00000</td>\n      <td>-0.342529</td>\n      <td>0.899902</td>\n      <td>-0.231079</td>\n      <td>0.930176</td>\n      <td>0.132568</td>\n      <td>1.044922</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.642090</td>\n      <td>0.827148</td>\n      <td>-0.642090</td>\n      <td>0.827148</td>\n      <td>-0.117798</td>\n      <td>0.962891</td>\n      <td>-0.096863</td>\n      <td>0.968262</td>\n      <td>-0.439453</td>\n      <td>0.871582</td>\n      <td>-0.327881</td>\n      <td>0.900879</td>\n      <td>0.035858</td>\n      <td>1.011719</td>\n      <td>-0.664062</td>\n      <td>0.817383</td>\n      <td>-0.664062</td>\n      <td>0.817383</td>\n      <td>-0.738770</td>\n      <td>0.801270</td>\n      <td>-0.738770</td>\n      <td>0.801270</td>\n      <td>-0.214600</td>\n      <td>0.932617</td>\n      <td>-0.221436</td>\n      <td>0.930664</td>\n      <td>-0.305176</td>\n      <td>0.906738</td>\n      <td>-0.096985</td>\n      <td>0.968262</td>\n      <td>-0.000098</td>\n      <td>1.0</td>\n      <td>-0.096985</td>\n      <td>0.968262</td>\n      <td>-0.439453</td>\n      <td>0.871094</td>\n      <td>-0.327881</td>\n      <td>0.900879</td>\n      <td>0.035736</td>\n      <td>1.011719</td>\n      <td>-0.664062</td>\n      <td>0.817383</td>\n      <td>-0.664062</td>\n      <td>0.817383</td>\n      <td>-0.738770</td>\n      <td>0.801270</td>\n      <td>-0.738770</td>\n      <td>0.801270</td>\n      <td>-0.214722</td>\n      <td>0.932617</td>\n      <td>-0.342529</td>\n      <td>0.899902</td>\n      <td>-0.230957</td>\n      <td>0.930176</td>\n      <td>0.132690</td>\n      <td>1.044922</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.641602</td>\n      <td>0.827148</td>\n      <td>-0.641602</td>\n      <td>0.827148</td>\n      <td>-0.117676</td>\n      <td>0.962891</td>\n      <td>-0.124512</td>\n      <td>0.960938</td>\n      <td>-0.208374</td>\n      <td>0.936523</td>\n      <td>-0.000106</td>\n      <td>1.000000</td>\n      <td>0.096802</td>\n      <td>1.032227</td>\n      <td>-0.000106</td>\n      <td>1.0</td>\n      <td>-0.342529</td>\n      <td>0.899902</td>\n      <td>-0.231079</td>\n      <td>0.930176</td>\n      <td>0.132568</td>\n      <td>1.044922</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.642090</td>\n      <td>0.827148</td>\n      <td>-0.642090</td>\n      <td>0.827148</td>\n      <td>-0.117798</td>\n      <td>0.962891</td>\n      <td>0.111450</td>\n      <td>1.034180</td>\n      <td>0.475098</td>\n      <td>1.162109</td>\n      <td>-0.224854</td>\n      <td>0.937988</td>\n      <td>-0.224854</td>\n      <td>0.937988</td>\n      <td>-0.299316</td>\n      <td>0.919434</td>\n      <td>-0.299316</td>\n      <td>0.919434</td>\n      <td>0.224731</td>\n      <td>1.070312</td>\n      <td>0.217896</td>\n      <td>1.068359</td>\n      <td>0.134033</td>\n      <td>1.041016</td>\n      <td>0.342285</td>\n      <td>1.111328</td>\n      <td>0.439209</td>\n      <td>1.147461</td>\n      <td>0.342285</td>\n      <td>1.111328</td>\n      <td>-0.000097</td>\n      <td>1.0</td>\n      <td>0.111328</td>\n      <td>1.034180</td>\n      <td>0.475098</td>\n      <td>1.161133</td>\n      <td>-0.224976</td>\n      <td>0.937988</td>\n      <td>-0.224976</td>\n      <td>0.937988</td>\n      <td>-0.299316</td>\n      <td>0.919434</td>\n      <td>-0.299316</td>\n      <td>0.919434</td>\n      <td>0.224731</td>\n      <td>1.070312</td>\n      <td>0.363770</td>\n      <td>1.124023</td>\n      <td>-0.336426</td>\n      <td>0.907715</td>\n      <td>-0.336426</td>\n      <td>0.907715</td>\n      <td>-0.410889</td>\n      <td>0.889160</td>\n      <td>-0.410889</td>\n      <td>0.889160</td>\n      <td>0.113342</td>\n      <td>1.035156</td>\n      <td>0.106445</td>\n      <td>1.033203</td>\n      <td>0.022598</td>\n      <td>1.006836</td>\n      <td>0.230957</td>\n      <td>1.075195</td>\n      <td>0.327881</td>\n      <td>1.110352</td>\n      <td>0.230957</td>\n      <td>1.075195</td>\n      <td>-0.111572</td>\n      <td>0.967285</td>\n      <td>-0.000104</td>\n      <td>1.0</td>\n      <td>0.363525</td>\n      <td>1.124023</td>\n      <td>-0.336426</td>\n      <td>0.907715</td>\n      <td>-0.336426</td>\n      <td>0.907715</td>\n      <td>-0.410889</td>\n      <td>0.889160</td>\n      <td>-0.410889</td>\n      <td>0.889160</td>\n      <td>0.113220</td>\n      <td>1.035156</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.774414</td>\n      <td>0.791504</td>\n      <td>-0.774414</td>\n      <td>0.791504</td>\n      <td>-0.250488</td>\n      <td>0.921387</td>\n      <td>-0.257324</td>\n      <td>0.919434</td>\n      <td>-0.341064</td>\n      <td>0.895996</td>\n      <td>-0.132812</td>\n      <td>0.956543</td>\n      <td>-0.035950</td>\n      <td>0.987793</td>\n      <td>-0.132812</td>\n      <td>0.956543</td>\n      <td>-0.475342</td>\n      <td>0.860840</td>\n      <td>-0.363770</td>\n      <td>0.889648</td>\n      <td>-0.000105</td>\n      <td>1.0</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.774414</td>\n      <td>0.791504</td>\n      <td>-0.774414</td>\n      <td>0.791504</td>\n      <td>-0.250488</td>\n      <td>0.921387</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.074463</td>\n      <td>0.979980</td>\n      <td>-0.074463</td>\n      <td>0.979980</td>\n      <td>0.449707</td>\n      <td>1.140625</td>\n      <td>0.442871</td>\n      <td>1.138672</td>\n      <td>0.358887</td>\n      <td>1.109375</td>\n      <td>0.567383</td>\n      <td>1.184570</td>\n      <td>0.664062</td>\n      <td>1.223633</td>\n      <td>0.567383</td>\n      <td>1.184570</td>\n      <td>0.224731</td>\n      <td>1.065430</td>\n      <td>0.336182</td>\n      <td>1.101562</td>\n      <td>0.700195</td>\n      <td>1.238281</td>\n      <td>-0.000061</td>\n      <td>1.0</td>\n      <td>-0.000061</td>\n      <td>1.0</td>\n      <td>-0.074524</td>\n      <td>0.979980</td>\n      <td>-0.074524</td>\n      <td>0.979980</td>\n      <td>0.449463</td>\n      <td>1.140625</td>\n      <td>-0.074463</td>\n      <td>0.979980</td>\n      <td>-0.074463</td>\n      <td>0.979980</td>\n      <td>0.449707</td>\n      <td>1.140625</td>\n      <td>0.442871</td>\n      <td>1.138672</td>\n      <td>0.358887</td>\n      <td>1.109375</td>\n      <td>0.567383</td>\n      <td>1.184570</td>\n      <td>0.664062</td>\n      <td>1.223633</td>\n      <td>0.567383</td>\n      <td>1.184570</td>\n      <td>0.224731</td>\n      <td>1.065430</td>\n      <td>0.336182</td>\n      <td>1.101562</td>\n      <td>0.700195</td>\n      <td>1.238281</td>\n      <td>-0.000061</td>\n      <td>1.0</td>\n      <td>-0.000061</td>\n      <td>1.0</td>\n      <td>-0.074524</td>\n      <td>0.979980</td>\n      <td>-0.074524</td>\n      <td>0.979980</td>\n      <td>0.449463</td>\n      <td>1.140625</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.523926</td>\n      <td>1.164062</td>\n      <td>0.517090</td>\n      <td>1.162109</td>\n      <td>0.433350</td>\n      <td>1.131836</td>\n      <td>0.641602</td>\n      <td>1.208984</td>\n      <td>0.738770</td>\n      <td>1.248047</td>\n      <td>0.641602</td>\n      <td>1.208984</td>\n      <td>0.299316</td>\n      <td>1.087891</td>\n      <td>0.410645</td>\n      <td>1.124023</td>\n      <td>0.774414</td>\n      <td>1.263672</td>\n      <td>0.074402</td>\n      <td>1.020508</td>\n      <td>0.074402</td>\n      <td>1.020508</td>\n      <td>-0.000057</td>\n      <td>1.0</td>\n      <td>-0.000057</td>\n      <td>1.000000</td>\n      <td>0.523926</td>\n      <td>1.164062</td>\n      <td>0.523926</td>\n      <td>1.164062</td>\n      <td>0.517090</td>\n      <td>1.162109</td>\n      <td>0.433350</td>\n      <td>1.131836</td>\n      <td>0.641602</td>\n      <td>1.208984</td>\n      <td>0.738770</td>\n      <td>1.248047</td>\n      <td>0.641602</td>\n      <td>1.208984</td>\n      <td>0.299316</td>\n      <td>1.087891</td>\n      <td>0.410645</td>\n      <td>1.124023</td>\n      <td>0.774414</td>\n      <td>1.263672</td>\n      <td>0.074402</td>\n      <td>1.020508</td>\n      <td>0.074402</td>\n      <td>1.020508</td>\n      <td>-0.000057</td>\n      <td>1.000000</td>\n      <td>-0.000057</td>\n      <td>1.0</td>\n      <td>0.523926</td>\n      <td>1.164062</td>\n      <td>-0.006847</td>\n      <td>0.998047</td>\n      <td>-0.090698</td>\n      <td>0.972168</td>\n      <td>0.117615</td>\n      <td>1.038086</td>\n      <td>0.214478</td>\n      <td>1.072266</td>\n      <td>0.117615</td>\n      <td>1.038086</td>\n      <td>-0.224854</td>\n      <td>0.934082</td>\n      <td>-0.113403</td>\n      <td>0.965820</td>\n      <td>0.250244</td>\n      <td>1.084961</td>\n      <td>-0.449707</td>\n      <td>0.876465</td>\n      <td>-0.449707</td>\n      <td>0.876465</td>\n      <td>-0.524414</td>\n      <td>0.858887</td>\n      <td>-0.524414</td>\n      <td>0.858887</td>\n      <td>-0.000099</td>\n      <td>1.0</td>\n      <td>-0.083862</td>\n      <td>0.974609</td>\n      <td>0.124451</td>\n      <td>1.040039</td>\n      <td>0.221313</td>\n      <td>1.074219</td>\n      <td>0.124451</td>\n      <td>1.040039</td>\n      <td>-0.218018</td>\n      <td>0.936035</td>\n      <td>-0.106567</td>\n      <td>0.967773</td>\n      <td>0.257080</td>\n      <td>1.087891</td>\n      <td>-0.442871</td>\n      <td>0.878418</td>\n      <td>-0.442871</td>\n      <td>0.878418</td>\n      <td>-0.517578</td>\n      <td>0.860840</td>\n      <td>-0.517578</td>\n      <td>0.860840</td>\n      <td>0.006748</td>\n      <td>1.001953</td>\n      <td>0.208252</td>\n      <td>1.067383</td>\n      <td>0.305176</td>\n      <td>1.102539</td>\n      <td>0.208252</td>\n      <td>1.067383</td>\n      <td>-0.134155</td>\n      <td>0.960938</td>\n      <td>-0.022705</td>\n      <td>0.993164</td>\n      <td>0.341064</td>\n      <td>1.116211</td>\n      <td>-0.358887</td>\n      <td>0.901367</td>\n      <td>-0.358887</td>\n      <td>0.901367</td>\n      <td>-0.433594</td>\n      <td>0.883301</td>\n      <td>-0.433594</td>\n      <td>0.883301</td>\n      <td>0.090637</td>\n      <td>1.028320</td>\n      <td>0.096863</td>\n      <td>1.032227</td>\n      <td>0.000000</td>\n      <td>1.00000</td>\n      <td>-0.342529</td>\n      <td>0.899902</td>\n      <td>-0.230957</td>\n      <td>0.930176</td>\n      <td>0.132690</td>\n      <td>1.044922</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.641602</td>\n      <td>0.827148</td>\n      <td>-0.641602</td>\n      <td>0.827148</td>\n      <td>-0.117676</td>\n      <td>0.962891</td>\n      <td>-0.096863</td>\n      <td>0.968262</td>\n      <td>-0.439453</td>\n      <td>0.871582</td>\n      <td>-0.327881</td>\n      <td>0.900879</td>\n      <td>0.035858</td>\n      <td>1.011719</td>\n      <td>-0.664062</td>\n      <td>0.817383</td>\n      <td>-0.664062</td>\n      <td>0.817383</td>\n      <td>-0.738770</td>\n      <td>0.801270</td>\n      <td>-0.738770</td>\n      <td>0.801270</td>\n      <td>-0.214600</td>\n      <td>0.932617</td>\n      <td>-0.342529</td>\n      <td>0.899902</td>\n      <td>-0.230957</td>\n      <td>0.930176</td>\n      <td>0.132690</td>\n      <td>1.044922</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.641602</td>\n      <td>0.827148</td>\n      <td>-0.641602</td>\n      <td>0.827148</td>\n      <td>-0.117676</td>\n      <td>0.962891</td>\n      <td>0.111450</td>\n      <td>1.034180</td>\n      <td>0.475098</td>\n      <td>1.161133</td>\n      <td>-0.224854</td>\n      <td>0.937988</td>\n      <td>-0.224854</td>\n      <td>0.937988</td>\n      <td>-0.299316</td>\n      <td>0.919434</td>\n      <td>-0.299316</td>\n      <td>0.919434</td>\n      <td>0.224731</td>\n      <td>1.070312</td>\n      <td>0.363770</td>\n      <td>1.124023</td>\n      <td>-0.336182</td>\n      <td>0.907715</td>\n      <td>-0.336182</td>\n      <td>0.907715</td>\n      <td>-0.410889</td>\n      <td>0.889648</td>\n      <td>-0.410889</td>\n      <td>0.889648</td>\n      <td>0.113342</td>\n      <td>1.035156</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.774414</td>\n      <td>0.791504</td>\n      <td>-0.774414</td>\n      <td>0.791504</td>\n      <td>-0.250488</td>\n      <td>0.921387</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.074463</td>\n      <td>0.979980</td>\n      <td>-0.074463</td>\n      <td>0.979980</td>\n      <td>0.449707</td>\n      <td>1.140625</td>\n      <td>-0.074463</td>\n      <td>0.979980</td>\n      <td>-0.074463</td>\n      <td>0.979980</td>\n      <td>0.449707</td>\n      <td>1.140625</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.523926</td>\n      <td>1.164062</td>\n      <td>0.523926</td>\n      <td>1.164062</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.029891</td>\n      <td>0.073973</td>\n      <td>-0.094534</td>\n      <td>0.126522</td>\n      <td>0.181991</td>\n      <td>0.180853</td>\n      <td>0.266522</td>\n      <td>0.097094</td>\n      <td>-0.123095</td>\n      <td>-0.095310</td>\n      <td>-0.179767</td>\n      <td>-0.457392</td>\n      <td>-0.080620</td>\n      <td>0.089512</td>\n      <td>0.082543</td>\n      <td>0.058552</td>\n      <td>0.027953</td>\n      <td>0.203759</td>\n      <td>0.072223</td>\n      <td>0.050499</td>\n      <td>-0.263804</td>\n      <td>0.327592</td>\n      <td>-0.103400</td>\n      <td>-0.121484</td>\n      <td>0.250285</td>\n      <td>0.074155</td>\n      <td>0.144880</td>\n      <td>0.259375</td>\n      <td>-0.288159</td>\n      <td>0.185025</td>\n      <td>-0.030508</td>\n      <td>0.070924</td>\n      <td>0.074667</td>\n      <td>0.036750</td>\n      <td>-0.061805</td>\n      <td>0.060579</td>\n      <td>-0.036916</td>\n      <td>0.039057</td>\n      <td>-0.296855</td>\n      <td>0.009289</td>\n      <td>-0.116685</td>\n      <td>0.715590</td>\n      <td>0.101258</td>\n      <td>-0.220261</td>\n      <td>-0.076486</td>\n      <td>-0.188758</td>\n      <td>-0.017060</td>\n      <td>0.037125</td>\n      <td>0.057709</td>\n      <td>0.166822</td>\n      <td>-0.043452</td>\n      <td>-0.019516</td>\n      <td>-0.173006</td>\n      <td>0.089757</td>\n      <td>-0.190869</td>\n      <td>0.095406</td>\n      <td>0.060052</td>\n      <td>-0.132407</td>\n      <td>0.005342</td>\n      <td>0.033320</td>\n      <td>0.024427</td>\n      <td>-0.118014</td>\n      <td>0.251357</td>\n      <td>0.040674</td>\n      <td>-0.027957</td>\n      <td>-0.026394</td>\n      <td>0.024263</td>\n      <td>0.372732</td>\n      <td>-0.025801</td>\n      <td>-0.246002</td>\n      <td>0.109832</td>\n      <td>-0.017500</td>\n      <td>-0.001231</td>\n      <td>0.143727</td>\n      <td>0.264198</td>\n      <td>0.107633</td>\n      <td>0.060340</td>\n      <td>-0.318682</td>\n      <td>0.006398</td>\n      <td>0.099760</td>\n      <td>-0.036413</td>\n      <td>0.055242</td>\n      <td>1.750735</td>\n      <td>0.118613</td>\n      <td>0.177189</td>\n      <td>-0.080642</td>\n      <td>0.044355</td>\n      <td>0.100218</td>\n      <td>-0.051002</td>\n      <td>-0.008707</td>\n      <td>-0.134474</td>\n      <td>-0.004757</td>\n      <td>-0.011541</td>\n      <td>-0.217713</td>\n      <td>-0.042322</td>\n      <td>0.052909</td>\n      <td>0.071231</td>\n      <td>-1.152318</td>\n      <td>-0.056206</td>\n      <td>-0.060179</td>\n      <td>0.189230</td>\n      <td>-0.190384</td>\n      <td>-0.025456</td>\n      <td>-0.110303</td>\n      <td>-0.197674</td>\n      <td>0.367068</td>\n      <td>0.155162</td>\n      <td>0.074811</td>\n      <td>0.099640</td>\n      <td>-0.164789</td>\n      <td>-0.005851</td>\n      <td>0.046500</td>\n      <td>-0.082355</td>\n      <td>0.045490</td>\n      <td>0.089042</td>\n      <td>-0.197250</td>\n      <td>0.304634</td>\n      <td>0.225055</td>\n      <td>-0.028008</td>\n      <td>0.400766</td>\n      <td>0.061376</td>\n      <td>0.030082</td>\n      <td>0.118571</td>\n      <td>-0.081431</td>\n      <td>0.120853</td>\n      <td>0.292195</td>\n      <td>-0.141171</td>\n      <td>0.152237</td>\n      <td>-0.059104</td>\n      <td>0.141703</td>\n      <td>0.194321</td>\n      <td>-0.700524</td>\n      <td>-0.223517</td>\n      <td>-0.198950</td>\n      <td>-0.048475</td>\n      <td>0.045590</td>\n      <td>0.032415</td>\n      <td>0.005680</td>\n      <td>-0.050341</td>\n      <td>0.018153</td>\n      <td>0.211495</td>\n      <td>0.210816</td>\n      <td>-0.227017</td>\n      <td>-0.245958</td>\n      <td>0.129851</td>\n      <td>0.194608</td>\n      <td>-0.074843</td>\n      <td>-0.347806</td>\n      <td>0.008898</td>\n      <td>0.330879</td>\n      <td>0.291852</td>\n      <td>0.063475</td>\n      <td>-0.278245</td>\n      <td>-0.312510</td>\n      <td>-0.021102</td>\n      <td>0.703451</td>\n      <td>0.140124</td>\n      <td>-0.089052</td>\n      <td>-0.132394</td>\n      <td>0.020074</td>\n      <td>-0.022465</td>\n      <td>-0.017278</td>\n      <td>0.210464</td>\n      <td>-0.011987</td>\n      <td>0.079176</td>\n      <td>0.043043</td>\n      <td>-0.103480</td>\n      <td>0.110626</td>\n      <td>-0.036202</td>\n      <td>0.118242</td>\n      <td>-0.063492</td>\n      <td>-0.068646</td>\n      <td>0.113279</td>\n      <td>0.071068</td>\n      <td>-0.098169</td>\n      <td>-0.019378</td>\n      <td>-0.040457</td>\n      <td>-0.086607</td>\n      <td>0.030731</td>\n      <td>0.072026</td>\n      <td>-0.039192</td>\n      <td>0.114556</td>\n      <td>-0.050776</td>\n      <td>-0.167901</td>\n      <td>0.117924</td>\n      <td>0.215771</td>\n      <td>-0.063886</td>\n      <td>0.233640</td>\n      <td>0.017298</td>\n      <td>-0.082844</td>\n      <td>-0.067025</td>\n      <td>-0.149464</td>\n      <td>-0.188189</td>\n      <td>-0.230590</td>\n      <td>-0.033488</td>\n      <td>-0.188438</td>\n      <td>0.027001</td>\n      <td>0.040944</td>\n      <td>0.488956</td>\n      <td>-0.082526</td>\n      <td>0.172990</td>\n      <td>-0.299410</td>\n      <td>0.126077</td>\n      <td>-0.053900</td>\n      <td>0.195646</td>\n      <td>0.008763</td>\n      <td>-0.449042</td>\n      <td>0.146812</td>\n      <td>0.036107</td>\n      <td>-0.134739</td>\n      <td>0.037157</td>\n      <td>-0.022641</td>\n      <td>-0.343432</td>\n      <td>0.162106</td>\n      <td>-0.195311</td>\n      <td>0.222396</td>\n      <td>0.097842</td>\n      <td>-0.307882</td>\n      <td>-0.325424</td>\n      <td>-0.210876</td>\n      <td>0.034118</td>\n      <td>0.114727</td>\n      <td>-0.092626</td>\n      <td>0.038293</td>\n      <td>-0.076999</td>\n      <td>0.035965</td>\n      <td>-0.001640</td>\n      <td>-0.067477</td>\n      <td>0.206820</td>\n      <td>-0.090941</td>\n      <td>0.011562</td>\n      <td>0.523355</td>\n      <td>0.205579</td>\n      <td>0.027943</td>\n      <td>0.256336</td>\n      <td>-0.408816</td>\n      <td>-0.036333</td>\n      <td>-0.039951</td>\n      <td>-0.067919</td>\n      <td>-0.153039</td>\n      <td>-0.669205</td>\n      <td>0.106416</td>\n      <td>0.140609</td>\n      <td>0.073663</td>\n      <td>-0.095072</td>\n      <td>-0.029422</td>\n      <td>-0.021315</td>\n      <td>0.024537</td>\n      <td>0.019863</td>\n      <td>0.111300</td>\n      <td>-0.087268</td>\n      <td>0.033974</td>\n      <td>-0.041300</td>\n      <td>-0.002832</td>\n      <td>0.191807</td>\n      <td>-0.201867</td>\n      <td>0.066451</td>\n      <td>-0.023439</td>\n      <td>-0.336640</td>\n      <td>0.714376</td>\n      <td>0.090345</td>\n      <td>-0.191597</td>\n      <td>0.216644</td>\n      <td>-0.025642</td>\n      <td>-0.009782</td>\n      <td>-0.170917</td>\n      <td>-0.068642</td>\n      <td>-0.000975</td>\n      <td>0.141942</td>\n      <td>-0.020858</td>\n      <td>-0.093970</td>\n      <td>0.041412</td>\n      <td>0.061818</td>\n      <td>-0.048434</td>\n      <td>-0.040401</td>\n      <td>0.484307</td>\n      <td>0.199968</td>\n      <td>-0.079365</td>\n      <td>-0.282826</td>\n      <td>0.018812</td>\n      <td>0.142837</td>\n      <td>-0.237518</td>\n      <td>0.177559</td>\n      <td>0.108307</td>\n      <td>0.015854</td>\n      <td>-0.030182</td>\n      <td>-0.045865</td>\n      <td>-0.018895</td>\n      <td>-0.130665</td>\n      <td>0.158553</td>\n      <td>-0.120776</td>\n      <td>-0.060394</td>\n      <td>-0.245842</td>\n      <td>-0.171578</td>\n      <td>0.217652</td>\n      <td>-0.069561</td>\n      <td>0.218517</td>\n      <td>-0.073254</td>\n      <td>-0.029057</td>\n      <td>-0.045591</td>\n      <td>0.122068</td>\n      <td>0.059425</td>\n      <td>0.108973</td>\n      <td>0.037294</td>\n      <td>-0.031758</td>\n      <td>0.092954</td>\n      <td>-0.298806</td>\n      <td>0.093344</td>\n      <td>0.245105</td>\n      <td>-0.030890</td>\n      <td>-0.109563</td>\n      <td>0.203928</td>\n      <td>-0.128601</td>\n      <td>-0.157510</td>\n      <td>0.035108</td>\n      <td>0.130013</td>\n      <td>-0.028601</td>\n      <td>-0.050875</td>\n      <td>-0.020912</td>\n      <td>-0.041617</td>\n      <td>-0.225915</td>\n      <td>-0.147597</td>\n      <td>-0.346384</td>\n      <td>-0.230136</td>\n      <td>-0.450810</td>\n      <td>0.134377</td>\n      <td>-0.004222</td>\n      <td>0.075372</td>\n      <td>0.246987</td>\n      <td>0.189751</td>\n      <td>1.171893</td>\n      <td>0.423259</td>\n      <td>0.129495</td>\n      <td>0.047323</td>\n      <td>0.245403</td>\n      <td>0.009631</td>\n      <td>-0.154849</td>\n      <td>0.136841</td>\n      <td>0.310707</td>\n      <td>0.053092</td>\n      <td>-0.118639</td>\n      <td>0.132450</td>\n      <td>0.005078</td>\n      <td>0.145224</td>\n      <td>0.315543</td>\n      <td>-0.036221</td>\n      <td>0.078925</td>\n      <td>0.070699</td>\n      <td>0.323266</td>\n      <td>-0.204535</td>\n      <td>-0.196910</td>\n      <td>-0.252790</td>\n      <td>-0.010706</td>\n      <td>-0.160570</td>\n      <td>0.057044</td>\n      <td>0.159736</td>\n      <td>-0.112820</td>\n      <td>-0.079733</td>\n      <td>-0.325507</td>\n      <td>-0.188891</td>\n      <td>-0.037424</td>\n      <td>0.284523</td>\n      <td>0.069744</td>\n      <td>0.039888</td>\n      <td>0.027148</td>\n      <td>-0.180135</td>\n      <td>0.270566</td>\n      <td>0.013305</td>\n      <td>0.162727</td>\n      <td>-0.054204</td>\n      <td>0.033835</td>\n      <td>-0.054199</td>\n      <td>-0.152402</td>\n      <td>-0.004757</td>\n      <td>0.042156</td>\n      <td>-0.095999</td>\n      <td>0.147516</td>\n      <td>-0.211751</td>\n      <td>-0.189056</td>\n      <td>0.024325</td>\n      <td>0.074092</td>\n      <td>0.093858</td>\n      <td>-0.106786</td>\n      <td>-0.024229</td>\n      <td>0.361901</td>\n      <td>-0.034408</td>\n      <td>0.173214</td>\n      <td>0.111462</td>\n      <td>0.031852</td>\n      <td>-0.091530</td>\n      <td>0.117739</td>\n      <td>-0.177065</td>\n      <td>-0.025684</td>\n      <td>-0.056748</td>\n      <td>0.097014</td>\n      <td>-0.064555</td>\n      <td>0.157628</td>\n      <td>-0.842130</td>\n      <td>0.027633</td>\n      <td>-0.110726</td>\n      <td>-0.012435</td>\n      <td>0.236703</td>\n      <td>-0.001034</td>\n      <td>-0.164259</td>\n      <td>-0.005667</td>\n      <td>0.162612</td>\n      <td>-0.022558</td>\n      <td>-0.138819</td>\n      <td>-0.067203</td>\n      <td>0.209579</td>\n      <td>0.166321</td>\n      <td>0.142380</td>\n      <td>0.005088</td>\n      <td>-0.166263</td>\n      <td>0.056212</td>\n      <td>0.058532</td>\n      <td>-0.172003</td>\n      <td>-0.141869</td>\n      <td>-0.301240</td>\n      <td>-0.030682</td>\n      <td>-0.228836</td>\n      <td>-0.144127</td>\n      <td>-0.019697</td>\n      <td>-0.055351</td>\n      <td>-0.520402</td>\n      <td>-0.117021</td>\n      <td>0.305170</td>\n      <td>-0.069852</td>\n      <td>0.036632</td>\n      <td>0.167410</td>\n      <td>0.169146</td>\n      <td>0.017619</td>\n      <td>-0.193883</td>\n      <td>-0.067916</td>\n      <td>-0.273727</td>\n      <td>-0.106562</td>\n      <td>0.097620</td>\n      <td>-0.108914</td>\n      <td>-0.194292</td>\n      <td>-0.050219</td>\n      <td>-0.271828</td>\n      <td>0.080720</td>\n      <td>-0.100197</td>\n      <td>0.024852</td>\n      <td>-0.165310</td>\n      <td>-0.122422</td>\n      <td>0.084847</td>\n      <td>-0.124941</td>\n      <td>-0.003572</td>\n      <td>-0.107315</td>\n      <td>-0.084064</td>\n      <td>-0.034131</td>\n      <td>-0.471144</td>\n      <td>-1.553909</td>\n      <td>0.427042</td>\n      <td>0.133481</td>\n      <td>-0.149348</td>\n      <td>0.085869</td>\n      <td>-0.026381</td>\n      <td>-0.150482</td>\n      <td>0.119490</td>\n      <td>-0.034511</td>\n      <td>0.125529</td>\n      <td>-0.220760</td>\n      <td>0.051272</td>\n      <td>-0.168972</td>\n      <td>0.112018</td>\n      <td>-0.060398</td>\n      <td>0.114152</td>\n      <td>-0.003922</td>\n      <td>0.152598</td>\n      <td>-0.037112</td>\n      <td>-0.117118</td>\n      <td>-0.172868</td>\n      <td>-0.065486</td>\n      <td>-0.066613</td>\n      <td>-0.090606</td>\n      <td>0.447557</td>\n      <td>0.253628</td>\n      <td>0.099496</td>\n      <td>-0.023720</td>\n      <td>0.066293</td>\n      <td>0.149939</td>\n      <td>-0.021434</td>\n      <td>0.059437</td>\n      <td>-0.148900</td>\n      <td>0.022031</td>\n      <td>0.026973</td>\n      <td>0.262776</td>\n      <td>-0.259509</td>\n      <td>-0.190479</td>\n      <td>-0.141458</td>\n      <td>0.170991</td>\n      <td>0.259469</td>\n      <td>0.175812</td>\n      <td>0.248569</td>\n      <td>0.095650</td>\n      <td>-0.208740</td>\n      <td>-0.208450</td>\n      <td>0.113769</td>\n      <td>0.068898</td>\n      <td>0.112756</td>\n      <td>-0.066803</td>\n      <td>0.130623</td>\n      <td>-0.038461</td>\n      <td>-0.045274</td>\n      <td>-0.221130</td>\n      <td>0.057998</td>\n      <td>0.130247</td>\n      <td>0.042116</td>\n      <td>-0.316556</td>\n      <td>0.075787</td>\n      <td>0.082944</td>\n      <td>0.086548</td>\n      <td>0.098292</td>\n      <td>0.178536</td>\n      <td>0.093488</td>\n      <td>-0.212282</td>\n      <td>0.030044</td>\n      <td>0.373361</td>\n      <td>0.072374</td>\n      <td>0.007171</td>\n      <td>0.050314</td>\n      <td>0.078846</td>\n      <td>-0.125926</td>\n      <td>0.117607</td>\n      <td>-0.182267</td>\n      <td>-0.018583</td>\n      <td>0.010673</td>\n      <td>-0.101328</td>\n      <td>0.451349</td>\n      <td>-0.107005</td>\n      <td>0.303963</td>\n      <td>0.109850</td>\n      <td>-0.300183</td>\n      <td>-0.026005</td>\n      <td>0.120307</td>\n      <td>-0.143743</td>\n      <td>-0.053908</td>\n      <td>0.038879</td>\n      <td>-0.207219</td>\n      <td>0.136312</td>\n      <td>0.055207</td>\n      <td>0.080172</td>\n      <td>-0.067663</td>\n      <td>-0.183928</td>\n      <td>0.204984</td>\n      <td>-0.001481</td>\n      <td>0.160431</td>\n      <td>0.064906</td>\n      <td>-0.046666</td>\n      <td>0.114594</td>\n      <td>0.185505</td>\n      <td>0.041588</td>\n      <td>-0.106086</td>\n      <td>0.324065</td>\n      <td>-0.014526</td>\n      <td>-0.138719</td>\n      <td>-0.217771</td>\n      <td>0.388382</td>\n      <td>0.036732</td>\n      <td>0.290681</td>\n      <td>-0.118576</td>\n      <td>-0.056008</td>\n      <td>0.174637</td>\n      <td>-0.108908</td>\n      <td>0.021874</td>\n      <td>0.055263</td>\n      <td>0.050176</td>\n      <td>0.018494</td>\n      <td>1.638244</td>\n      <td>0.292534</td>\n      <td>-0.157802</td>\n      <td>-0.064801</td>\n      <td>0.262028</td>\n      <td>0.053501</td>\n      <td>-0.101853</td>\n      <td>0.026971</td>\n      <td>0.278975</td>\n      <td>0.123910</td>\n      <td>0.080051</td>\n      <td>-0.116013</td>\n      <td>-0.184120</td>\n      <td>0.175828</td>\n      <td>-0.047083</td>\n      <td>-0.045451</td>\n      <td>-0.185876</td>\n      <td>0.041590</td>\n      <td>12.282816</td>\n      <td>0.079741</td>\n      <td>-0.060993</td>\n      <td>0.063440</td>\n      <td>-0.268049</td>\n      <td>-0.094870</td>\n      <td>0.024665</td>\n      <td>0.001880</td>\n      <td>0.187180</td>\n      <td>0.040667</td>\n      <td>0.006068</td>\n      <td>0.023697</td>\n      <td>0.187934</td>\n      <td>-0.221596</td>\n      <td>0.068942</td>\n      <td>0.005736</td>\n      <td>0.098046</td>\n      <td>0.084538</td>\n      <td>0.091436</td>\n      <td>-0.074916</td>\n      <td>0.082979</td>\n      <td>0.174427</td>\n      <td>-0.010615</td>\n      <td>0.956542</td>\n      <td>-0.157765</td>\n      <td>0.202592</td>\n      <td>-0.065107</td>\n      <td>0.086034</td>\n      <td>0.082509</td>\n      <td>0.116933</td>\n      <td>-0.130661</td>\n      <td>0.056648</td>\n      <td>-0.001675</td>\n      <td>-0.010225</td>\n      <td>0.085153</td>\n      <td>-0.174518</td>\n      <td>-0.064504</td>\n      <td>0.161261</td>\n      <td>0.148312</td>\n      <td>-0.075701</td>\n      <td>-0.122330</td>\n      <td>-0.085408</td>\n      <td>0.294268</td>\n      <td>-0.416573</td>\n      <td>0.074603</td>\n      <td>-0.209788</td>\n      <td>0.263318</td>\n      <td>0.310230</td>\n      <td>0.106331</td>\n      <td>-0.020113</td>\n      <td>-0.048917</td>\n      <td>-0.070626</td>\n      <td>0.096161</td>\n      <td>-0.188461</td>\n      <td>-0.075923</td>\n      <td>-0.234436</td>\n      <td>-0.020224</td>\n      <td>-0.090146</td>\n      <td>0.016027</td>\n      <td>-0.018889</td>\n      <td>-0.114500</td>\n      <td>-0.154983</td>\n      <td>0.169091</td>\n      <td>0.098314</td>\n      <td>-0.164059</td>\n      <td>0.183723</td>\n      <td>0.187806</td>\n      <td>-0.131551</td>\n      <td>-0.050215</td>\n      <td>-0.003589</td>\n      <td>0.114386</td>\n      <td>-0.010999</td>\n      <td>-0.022542</td>\n      <td>0.019642</td>\n      <td>0.036398</td>\n      <td>-0.196691</td>\n      <td>-0.317724</td>\n      <td>-0.190680</td>\n      <td>-0.218587</td>\n      <td>-0.481349</td>\n      <td>0.132161</td>\n      <td>-0.064054</td>\n      <td>-0.135440</td>\n      <td>-0.064407</td>\n      <td>-0.314367</td>\n      <td>0.120350</td>\n      <td>-0.229907</td>\n      <td>-0.007436</td>\n      <td>-0.172259</td>\n      <td>0.153599</td>\n      <td>0.202984</td>\n      <td>0.076411</td>\n      <td>0.012433</td>\n      <td>-0.207095</td>\n      <td>0.033421</td>\n      <td>-0.124092</td>\n      <td>0.215223</td>\n      <td>-0.086132</td>\n      <td>0.223904</td>\n      <td>-0.158670</td>\n      <td>-0.441802</td>\n      <td>-0.005265</td>\n      <td>-0.076523</td>\n      <td>0.007779</td>\n      <td>-0.222167</td>\n      <td>0.287094</td>\n      <td>0.008407</td>\n      <td>-0.118400</td>\n      <td>0.111839</td>\n      <td>0.096921</td>\n      <td>-0.109888</td>\n      <td>-0.032591</td>\n      <td>-0.262088</td>\n      <td>-0.143762</td>\n      <td>0.001745</td>\n      <td>-0.218345</td>\n      <td>0.092003</td>\n      <td>0.029601</td>\n      <td>0.020191</td>\n      <td>0.111559</td>\n      <td>0.005574</td>\n      <td>0.073102</td>\n      <td>0.047232</td>\n      <td>0.086060</td>\n      <td>-0.080468</td>\n      <td>-0.066836</td>\n      <td>-0.307097</td>\n      <td>-0.099523</td>\n      <td>-0.264944</td>\n      <td>-0.165200</td>\n      <td>-0.270445</td>\n      <td>0.170967</td>\n      <td>0.298262</td>\n      <td>0.029693</td>\n      <td>0.077656</td>\n      <td>0.060142</td>\n      <td>-0.168183</td>\n      <td>0.077958</td>\n      <td>0.168891</td>\n      <td>0.268703</td>\n      <td>-0.149408</td>\n      <td>0.017111</td>\n      <td>-0.243692</td>\n      <td>-0.190973</td>\n      <td>0.018555</td>\n      <td>-0.244975</td>\n      <td>0.347429</td>\n      <td>0.465995</td>\n      <td>0.145426</td>\n      <td>0.111209</td>\n      <td>-0.079083</td>\n      <td>-0.170695</td>\n      <td>-0.197767</td>\n      <td>-0.445161</td>\n      <td>0.029228</td>\n      <td>0.070849</td>\n      <td>-0.132407</td>\n      <td>0.278908</td>\n      <td>0.070082</td>\n      <td>-0.276633</td>\n      <td>0.595741</td>\n      <td>-0.456886</td>\n      <td>-0.110131</td>\n      <td>0.033729</td>\n      <td>-0.337985</td>\n      <td>0.117239</td>\n      <td>0.234729</td>\n      <td>-0.072439</td>\n      <td>0.113650</td>\n      <td>0.048002</td>\n      <td>-0.158772</td>\n      <td>-0.019317</td>\n      <td>0.086273</td>\n      <td>0.033125</td>\n      <td>0.099122</td>\n      <td>0.154706</td>\n      <td>0.245593</td>\n      <td>0.108821</td>\n      <td>-0.001333</td>\n      <td>0.027004</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1405.5857</td>\n      <td>Michael Mortonson</td>\n      <td>Michael J. Mortonson, Uro\\v{s} Seljak</td>\n      <td>A joint analysis of Planck and BICEP2 B modes ...</td>\n      <td>13 pages, 4 figures; submitted to JCAP; refere...</td>\n      <td>JCAP10(2014)035</td>\n      <td>10.1088/1475-7516/2014/10/035</td>\n      <td>None</td>\n      <td>astro-ph.CO gr-qc hep-ph hep-th</td>\n      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n      <td>We analyze BICEP2 and Planck data using a mo...</td>\n      <td>[{'version': 'v1', 'created': 'Thu, 22 May 201...</td>\n      <td>[[Mortonson, Michael J., ], [Seljak, Uro, ]]</td>\n      <td>4.812500</td>\n      <td>5.241747</td>\n      <td>10.1088</td>\n      <td>IOP Publishing</td>\n      <td>4.605469</td>\n      <td>510044.0</td>\n      <td>2014-10-17</td>\n      <td>2014-05-22 18:53:31+00:00</td>\n      <td>2014-09-26 18:28:05+00:00</td>\n      <td>2014</td>\n      <td>2014</td>\n      <td>2014</td>\n      <td>10</td>\n      <td>5</td>\n      <td>9</td>\n      <td>201410</td>\n      <td>201405</td>\n      <td>201409</td>\n      <td>17</td>\n      <td>22</td>\n      <td>26</td>\n      <td>1.413504e+09</td>\n      <td>1.400785e+09</td>\n      <td>1.411756e+09</td>\n      <td>12719189</td>\n      <td>10971274.0</td>\n      <td>2</td>\n      <td>16360</td>\n      <td>16212</td>\n      <td>16339</td>\n      <td>127</td>\n      <td>42.656250</td>\n      <td>Mortonson</td>\n      <td>2</td>\n      <td>4.812500</td>\n      <td>astro-ph</td>\n      <td>astro-ph</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>astro gr hep hep</td>\n      <td>astro-ph gr-qc hep-ph hep-th</td>\n      <td>astro-ph.co gr-qc hep-ph hep-th</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>122888</td>\n      <td>49</td>\n      <td>70004</td>\n      <td>209</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>493</td>\n      <td>416</td>\n      <td>1288</td>\n      <td>3.462891</td>\n      <td>17</td>\n      <td>58.875000</td>\n      <td>1.946289</td>\n      <td>4.812500</td>\n      <td>3.218750</td>\n      <td>0.885742</td>\n      <td>2.417969</td>\n      <td>2.890625</td>\n      <td>4.160156</td>\n      <td>2.314453</td>\n      <td>105982</td>\n      <td>245209.000000</td>\n      <td>0.0</td>\n      <td>8.742188</td>\n      <td>2.302734</td>\n      <td>1.347656</td>\n      <td>0.693359</td>\n      <td>1.386719</td>\n      <td>3.294922</td>\n      <td>2.314453</td>\n      <td>105982</td>\n      <td>245209.000000</td>\n      <td>0.0</td>\n      <td>8.742188</td>\n      <td>2.302734</td>\n      <td>1.347656</td>\n      <td>0.693359</td>\n      <td>1.386719</td>\n      <td>3.294922</td>\n      <td>3.556641</td>\n      <td>16</td>\n      <td>56.90625</td>\n      <td>2.197266</td>\n      <td>4.812500</td>\n      <td>3.275391</td>\n      <td>0.820801</td>\n      <td>2.699219</td>\n      <td>2.968750</td>\n      <td>4.242188</td>\n      <td>2.310547</td>\n      <td>4152</td>\n      <td>9594.140625</td>\n      <td>0.0</td>\n      <td>7.281250</td>\n      <td>2.302734</td>\n      <td>1.355469</td>\n      <td>0.693359</td>\n      <td>1.386719</td>\n      <td>3.218750</td>\n      <td>2.257812</td>\n      <td>4372</td>\n      <td>9872.0</td>\n      <td>0.0</td>\n      <td>7.730469</td>\n      <td>2.302734</td>\n      <td>1.231445</td>\n      <td>0.693359</td>\n      <td>1.386719</td>\n      <td>3.091797</td>\n      <td>1.940430</td>\n      <td>599833</td>\n      <td>1163680.25</td>\n      <td>0.0</td>\n      <td>9.101562</td>\n      <td>1.946289</td>\n      <td>1.349609</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>2.890625</td>\n      <td>2.640625</td>\n      <td>195890</td>\n      <td>517164.906250</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.773438</td>\n      <td>1.436523</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.664062</td>\n      <td>2.640625</td>\n      <td>195890</td>\n      <td>517164.906250</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.773438</td>\n      <td>1.436523</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.664062</td>\n      <td>2.880859</td>\n      <td>3846</td>\n      <td>11079.309570</td>\n      <td>0.0</td>\n      <td>8.039062</td>\n      <td>2.945312</td>\n      <td>1.387695</td>\n      <td>1.098633</td>\n      <td>1.946289</td>\n      <td>3.828125</td>\n      <td>2.873047</td>\n      <td>3883</td>\n      <td>11159.622070</td>\n      <td>0.0</td>\n      <td>8.039062</td>\n      <td>2.945312</td>\n      <td>1.389648</td>\n      <td>1.098633</td>\n      <td>1.946289</td>\n      <td>3.806641</td>\n      <td>2.542969</td>\n      <td>2108</td>\n      <td>5358.716309</td>\n      <td>0.0</td>\n      <td>7.472656</td>\n      <td>2.638672</td>\n      <td>1.283203</td>\n      <td>0.693359</td>\n      <td>1.791992</td>\n      <td>3.400391</td>\n      <td>3.462891</td>\n      <td>17</td>\n      <td>58.875000</td>\n      <td>1.946289</td>\n      <td>4.812500</td>\n      <td>3.218750</td>\n      <td>0.885742</td>\n      <td>2.417969</td>\n      <td>2.890625</td>\n      <td>4.160156</td>\n      <td>2.314453</td>\n      <td>105982</td>\n      <td>245219.015625</td>\n      <td>0.0</td>\n      <td>8.742188</td>\n      <td>2.302734</td>\n      <td>1.347656</td>\n      <td>0.693359</td>\n      <td>1.386719</td>\n      <td>3.294922</td>\n      <td>2.314453</td>\n      <td>105982</td>\n      <td>245219.015625</td>\n      <td>0.0</td>\n      <td>8.742188</td>\n      <td>2.302734</td>\n      <td>1.347656</td>\n      <td>0.693359</td>\n      <td>1.386719</td>\n      <td>3.294922</td>\n      <td>3.556641</td>\n      <td>16</td>\n      <td>56.90625</td>\n      <td>2.197266</td>\n      <td>4.812500</td>\n      <td>3.275391</td>\n      <td>0.820801</td>\n      <td>2.699219</td>\n      <td>2.968750</td>\n      <td>4.242188</td>\n      <td>2.310547</td>\n      <td>4152</td>\n      <td>9594.564453</td>\n      <td>0.0</td>\n      <td>7.281250</td>\n      <td>2.302734</td>\n      <td>1.355469</td>\n      <td>0.693359</td>\n      <td>1.386719</td>\n      <td>3.218750</td>\n      <td>2.257812</td>\n      <td>4372</td>\n      <td>9872.0</td>\n      <td>0.0</td>\n      <td>7.730469</td>\n      <td>2.302734</td>\n      <td>1.231445</td>\n      <td>0.693359</td>\n      <td>1.386719</td>\n      <td>3.091797</td>\n      <td>1.940430</td>\n      <td>599833</td>\n      <td>1.163743e+06</td>\n      <td>0.0</td>\n      <td>9.101562</td>\n      <td>1.946289</td>\n      <td>1.349609</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>2.890625</td>\n      <td>2.640625</td>\n      <td>195890</td>\n      <td>517176.90625</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.773438</td>\n      <td>1.436523</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.664062</td>\n      <td>2.640625</td>\n      <td>195890</td>\n      <td>517176.90625</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.773438</td>\n      <td>1.436523</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.664062</td>\n      <td>2.880859</td>\n      <td>3846</td>\n      <td>11079.509766</td>\n      <td>0.0</td>\n      <td>8.039062</td>\n      <td>2.945312</td>\n      <td>1.387695</td>\n      <td>1.098633</td>\n      <td>1.946289</td>\n      <td>3.828125</td>\n      <td>2.873047</td>\n      <td>3883</td>\n      <td>11159.831055</td>\n      <td>0.0</td>\n      <td>8.039062</td>\n      <td>2.945312</td>\n      <td>1.389648</td>\n      <td>1.098633</td>\n      <td>1.946289</td>\n      <td>3.806641</td>\n      <td>2.542969</td>\n      <td>2108</td>\n      <td>5358.861328</td>\n      <td>0.0</td>\n      <td>7.472656</td>\n      <td>2.638672</td>\n      <td>1.283203</td>\n      <td>0.693359</td>\n      <td>1.791992</td>\n      <td>3.400391</td>\n      <td>1.254883</td>\n      <td>1.275391</td>\n      <td>2.498047</td>\n      <td>1.753906</td>\n      <td>1.349609</td>\n      <td>1.302734</td>\n      <td>2.498047</td>\n      <td>1.753906</td>\n      <td>2.501953</td>\n      <td>1.755859</td>\n      <td>2.554688</td>\n      <td>1.784180</td>\n      <td>2.873047</td>\n      <td>1.976562</td>\n      <td>2.171875</td>\n      <td>1.596680</td>\n      <td>2.171875</td>\n      <td>1.596680</td>\n      <td>1.931641</td>\n      <td>1.498047</td>\n      <td>1.938477</td>\n      <td>1.500000</td>\n      <td>2.269531</td>\n      <td>1.640625</td>\n      <td>-0.000316</td>\n      <td>1.0</td>\n      <td>1.254883</td>\n      <td>1.275391</td>\n      <td>2.498047</td>\n      <td>1.753906</td>\n      <td>1.349609</td>\n      <td>1.302734</td>\n      <td>2.498047</td>\n      <td>1.753906</td>\n      <td>2.501953</td>\n      <td>1.755859</td>\n      <td>2.554688</td>\n      <td>1.784180</td>\n      <td>2.873047</td>\n      <td>1.976562</td>\n      <td>2.171875</td>\n      <td>1.596680</td>\n      <td>2.171875</td>\n      <td>1.596680</td>\n      <td>1.931641</td>\n      <td>1.498047</td>\n      <td>1.938477</td>\n      <td>1.500000</td>\n      <td>2.269531</td>\n      <td>1.640625</td>\n      <td>1.244141</td>\n      <td>1.375000</td>\n      <td>0.094788</td>\n      <td>1.021484</td>\n      <td>1.244141</td>\n      <td>1.375000</td>\n      <td>1.247070</td>\n      <td>1.376953</td>\n      <td>1.298828</td>\n      <td>1.398438</td>\n      <td>1.617188</td>\n      <td>1.549805</td>\n      <td>0.917480</td>\n      <td>1.251953</td>\n      <td>0.917480</td>\n      <td>1.251953</td>\n      <td>0.676758</td>\n      <td>1.174805</td>\n      <td>0.683594</td>\n      <td>1.176758</td>\n      <td>1.015625</td>\n      <td>1.287109</td>\n      <td>-1.254883</td>\n      <td>0.784180</td>\n      <td>0.000019</td>\n      <td>1.0</td>\n      <td>1.243164</td>\n      <td>1.375000</td>\n      <td>0.094788</td>\n      <td>1.021484</td>\n      <td>1.243164</td>\n      <td>1.375000</td>\n      <td>1.246094</td>\n      <td>1.376953</td>\n      <td>1.298828</td>\n      <td>1.398438</td>\n      <td>1.617188</td>\n      <td>1.549805</td>\n      <td>0.917480</td>\n      <td>1.251953</td>\n      <td>0.917480</td>\n      <td>1.251953</td>\n      <td>0.676758</td>\n      <td>1.174805</td>\n      <td>0.683594</td>\n      <td>1.176758</td>\n      <td>1.015625</td>\n      <td>1.286133</td>\n      <td>-1.148438</td>\n      <td>0.742676</td>\n      <td>0.000000</td>\n      <td>1.00000</td>\n      <td>0.002958</td>\n      <td>1.000977</td>\n      <td>0.055389</td>\n      <td>1.016602</td>\n      <td>0.373779</td>\n      <td>1.126953</td>\n      <td>-0.326416</td>\n      <td>0.910156</td>\n      <td>-0.326416</td>\n      <td>0.910156</td>\n      <td>-0.566895</td>\n      <td>0.854004</td>\n      <td>-0.560059</td>\n      <td>0.855469</td>\n      <td>-0.228394</td>\n      <td>0.935547</td>\n      <td>-2.498047</td>\n      <td>0.570312</td>\n      <td>-1.244141</td>\n      <td>0.727051</td>\n      <td>-0.000095</td>\n      <td>1.0</td>\n      <td>-1.148438</td>\n      <td>0.742676</td>\n      <td>-0.000095</td>\n      <td>1.00000</td>\n      <td>0.002855</td>\n      <td>1.000977</td>\n      <td>0.055267</td>\n      <td>1.016602</td>\n      <td>0.373535</td>\n      <td>1.126953</td>\n      <td>-0.326416</td>\n      <td>0.910156</td>\n      <td>-0.326416</td>\n      <td>0.910156</td>\n      <td>-0.566895</td>\n      <td>0.854004</td>\n      <td>-0.560547</td>\n      <td>0.855469</td>\n      <td>-0.228516</td>\n      <td>0.935547</td>\n      <td>1.148438</td>\n      <td>1.346680</td>\n      <td>1.152344</td>\n      <td>1.347656</td>\n      <td>1.204102</td>\n      <td>1.369141</td>\n      <td>1.522461</td>\n      <td>1.517578</td>\n      <td>0.822754</td>\n      <td>1.225586</td>\n      <td>0.822754</td>\n      <td>1.225586</td>\n      <td>0.582031</td>\n      <td>1.150391</td>\n      <td>0.588867</td>\n      <td>1.152344</td>\n      <td>0.920410</td>\n      <td>1.259766</td>\n      <td>-1.349609</td>\n      <td>0.767578</td>\n      <td>-0.094788</td>\n      <td>0.979004</td>\n      <td>1.148438</td>\n      <td>1.346680</td>\n      <td>-0.000005</td>\n      <td>1.0</td>\n      <td>1.148438</td>\n      <td>1.346680</td>\n      <td>1.151367</td>\n      <td>1.347656</td>\n      <td>1.204102</td>\n      <td>1.369141</td>\n      <td>1.522461</td>\n      <td>1.517578</td>\n      <td>0.822266</td>\n      <td>1.225586</td>\n      <td>0.822266</td>\n      <td>1.225586</td>\n      <td>0.582031</td>\n      <td>1.150391</td>\n      <td>0.588379</td>\n      <td>1.152344</td>\n      <td>0.920410</td>\n      <td>1.259766</td>\n      <td>0.002958</td>\n      <td>1.000977</td>\n      <td>0.055389</td>\n      <td>1.016602</td>\n      <td>0.373779</td>\n      <td>1.126953</td>\n      <td>-0.326416</td>\n      <td>0.910156</td>\n      <td>-0.326416</td>\n      <td>0.910156</td>\n      <td>-0.566895</td>\n      <td>0.854004</td>\n      <td>-0.560059</td>\n      <td>0.855469</td>\n      <td>-0.228394</td>\n      <td>0.935547</td>\n      <td>-2.498047</td>\n      <td>0.570312</td>\n      <td>-1.244141</td>\n      <td>0.727051</td>\n      <td>-0.000095</td>\n      <td>1.000000</td>\n      <td>-1.148438</td>\n      <td>0.742676</td>\n      <td>-0.000095</td>\n      <td>1.0</td>\n      <td>0.002855</td>\n      <td>1.000977</td>\n      <td>0.055267</td>\n      <td>1.016602</td>\n      <td>0.373535</td>\n      <td>1.126953</td>\n      <td>-0.326416</td>\n      <td>0.910156</td>\n      <td>-0.326416</td>\n      <td>0.910156</td>\n      <td>-0.566895</td>\n      <td>0.854004</td>\n      <td>-0.560547</td>\n      <td>0.855469</td>\n      <td>-0.228516</td>\n      <td>0.935547</td>\n      <td>0.052429</td>\n      <td>1.015625</td>\n      <td>0.370605</td>\n      <td>1.125977</td>\n      <td>-0.329346</td>\n      <td>0.909668</td>\n      <td>-0.329346</td>\n      <td>0.909668</td>\n      <td>-0.569824</td>\n      <td>0.853027</td>\n      <td>-0.563477</td>\n      <td>0.854492</td>\n      <td>-0.231323</td>\n      <td>0.934570</td>\n      <td>-2.501953</td>\n      <td>0.569824</td>\n      <td>-1.247070</td>\n      <td>0.726562</td>\n      <td>-0.003052</td>\n      <td>0.999023</td>\n      <td>-1.152344</td>\n      <td>0.741699</td>\n      <td>-0.003052</td>\n      <td>0.999023</td>\n      <td>-0.000102</td>\n      <td>1.0</td>\n      <td>0.052307</td>\n      <td>1.015625</td>\n      <td>0.370605</td>\n      <td>1.125977</td>\n      <td>-0.329346</td>\n      <td>0.909668</td>\n      <td>-0.329346</td>\n      <td>0.909668</td>\n      <td>-0.569824</td>\n      <td>0.853027</td>\n      <td>-0.563477</td>\n      <td>0.854492</td>\n      <td>-0.231445</td>\n      <td>0.934570</td>\n      <td>0.318359</td>\n      <td>1.108398</td>\n      <td>-0.381836</td>\n      <td>0.895020</td>\n      <td>-0.381836</td>\n      <td>0.895020</td>\n      <td>-0.622559</td>\n      <td>0.839844</td>\n      <td>-0.615723</td>\n      <td>0.841309</td>\n      <td>-0.283691</td>\n      <td>0.919922</td>\n      <td>-2.554688</td>\n      <td>0.560547</td>\n      <td>-1.298828</td>\n      <td>0.714844</td>\n      <td>-0.055481</td>\n      <td>0.983398</td>\n      <td>-1.204102</td>\n      <td>0.729980</td>\n      <td>-0.055481</td>\n      <td>0.983398</td>\n      <td>-0.052551</td>\n      <td>0.984375</td>\n      <td>-0.000117</td>\n      <td>1.0</td>\n      <td>0.318115</td>\n      <td>1.108398</td>\n      <td>-0.381836</td>\n      <td>0.895020</td>\n      <td>-0.381836</td>\n      <td>0.895020</td>\n      <td>-0.622559</td>\n      <td>0.839355</td>\n      <td>-0.615723</td>\n      <td>0.840820</td>\n      <td>-0.283936</td>\n      <td>0.919922</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.940918</td>\n      <td>0.757812</td>\n      <td>-0.934082</td>\n      <td>0.758789</td>\n      <td>-0.602051</td>\n      <td>0.830078</td>\n      <td>-2.873047</td>\n      <td>0.505859</td>\n      <td>-1.617188</td>\n      <td>0.645020</td>\n      <td>-0.373779</td>\n      <td>0.887207</td>\n      <td>-1.522461</td>\n      <td>0.658691</td>\n      <td>-0.373779</td>\n      <td>0.887207</td>\n      <td>-0.370850</td>\n      <td>0.888184</td>\n      <td>-0.318359</td>\n      <td>0.902344</td>\n      <td>-0.000105</td>\n      <td>1.0</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.940918</td>\n      <td>0.757812</td>\n      <td>-0.934082</td>\n      <td>0.758789</td>\n      <td>-0.602051</td>\n      <td>0.830078</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.240601</td>\n      <td>0.937988</td>\n      <td>-0.233887</td>\n      <td>0.939453</td>\n      <td>0.098022</td>\n      <td>1.027344</td>\n      <td>-2.171875</td>\n      <td>0.626465</td>\n      <td>-0.917480</td>\n      <td>0.798828</td>\n      <td>0.326416</td>\n      <td>1.098633</td>\n      <td>-0.822754</td>\n      <td>0.815918</td>\n      <td>0.326416</td>\n      <td>1.098633</td>\n      <td>0.329346</td>\n      <td>1.099609</td>\n      <td>0.381592</td>\n      <td>1.117188</td>\n      <td>0.700195</td>\n      <td>1.238281</td>\n      <td>-0.000061</td>\n      <td>1.0</td>\n      <td>-0.000061</td>\n      <td>1.0</td>\n      <td>-0.240723</td>\n      <td>0.937988</td>\n      <td>-0.233887</td>\n      <td>0.939453</td>\n      <td>0.097900</td>\n      <td>1.027344</td>\n      <td>-0.240601</td>\n      <td>0.937988</td>\n      <td>-0.233887</td>\n      <td>0.939453</td>\n      <td>0.098022</td>\n      <td>1.027344</td>\n      <td>-2.171875</td>\n      <td>0.626465</td>\n      <td>-0.917480</td>\n      <td>0.798828</td>\n      <td>0.326416</td>\n      <td>1.098633</td>\n      <td>-0.822754</td>\n      <td>0.815918</td>\n      <td>0.326416</td>\n      <td>1.098633</td>\n      <td>0.329346</td>\n      <td>1.099609</td>\n      <td>0.381592</td>\n      <td>1.117188</td>\n      <td>0.700195</td>\n      <td>1.238281</td>\n      <td>-0.000061</td>\n      <td>1.0</td>\n      <td>-0.000061</td>\n      <td>1.0</td>\n      <td>-0.240723</td>\n      <td>0.937988</td>\n      <td>-0.233887</td>\n      <td>0.939453</td>\n      <td>0.097900</td>\n      <td>1.027344</td>\n      <td>0.006767</td>\n      <td>1.001953</td>\n      <td>0.338623</td>\n      <td>1.095703</td>\n      <td>-1.931641</td>\n      <td>0.667480</td>\n      <td>-0.676758</td>\n      <td>0.851562</td>\n      <td>0.566895</td>\n      <td>1.170898</td>\n      <td>-0.582031</td>\n      <td>0.869629</td>\n      <td>0.566895</td>\n      <td>1.170898</td>\n      <td>0.569824</td>\n      <td>1.171875</td>\n      <td>0.622559</td>\n      <td>1.191406</td>\n      <td>0.940430</td>\n      <td>1.320312</td>\n      <td>0.240601</td>\n      <td>1.066406</td>\n      <td>0.240601</td>\n      <td>1.066406</td>\n      <td>-0.000052</td>\n      <td>1.0</td>\n      <td>0.006714</td>\n      <td>1.001953</td>\n      <td>0.338623</td>\n      <td>1.095703</td>\n      <td>0.331787</td>\n      <td>1.093750</td>\n      <td>-1.938477</td>\n      <td>0.666504</td>\n      <td>-0.683594</td>\n      <td>0.850098</td>\n      <td>0.560059</td>\n      <td>1.168945</td>\n      <td>-0.588867</td>\n      <td>0.868164</td>\n      <td>0.560059</td>\n      <td>1.168945</td>\n      <td>0.562988</td>\n      <td>1.169922</td>\n      <td>0.615723</td>\n      <td>1.188477</td>\n      <td>0.934082</td>\n      <td>1.317383</td>\n      <td>0.233887</td>\n      <td>1.064453</td>\n      <td>0.233887</td>\n      <td>1.064453</td>\n      <td>-0.006817</td>\n      <td>0.998047</td>\n      <td>-0.000054</td>\n      <td>1.0</td>\n      <td>0.331787</td>\n      <td>1.093750</td>\n      <td>-2.269531</td>\n      <td>0.609375</td>\n      <td>-1.015625</td>\n      <td>0.777344</td>\n      <td>0.228271</td>\n      <td>1.069336</td>\n      <td>-0.920410</td>\n      <td>0.793945</td>\n      <td>0.228271</td>\n      <td>1.069336</td>\n      <td>0.231201</td>\n      <td>1.070312</td>\n      <td>0.283691</td>\n      <td>1.086914</td>\n      <td>0.602051</td>\n      <td>1.205078</td>\n      <td>-0.098083</td>\n      <td>0.973145</td>\n      <td>-0.098083</td>\n      <td>0.973145</td>\n      <td>-0.338623</td>\n      <td>0.912598</td>\n      <td>-0.332031</td>\n      <td>0.914551</td>\n      <td>-0.000069</td>\n      <td>1.0</td>\n      <td>1.254883</td>\n      <td>1.275391</td>\n      <td>2.498047</td>\n      <td>1.753906</td>\n      <td>1.349609</td>\n      <td>1.302734</td>\n      <td>2.498047</td>\n      <td>1.753906</td>\n      <td>2.501953</td>\n      <td>1.755859</td>\n      <td>2.554688</td>\n      <td>1.784180</td>\n      <td>2.873047</td>\n      <td>1.976562</td>\n      <td>2.171875</td>\n      <td>1.596680</td>\n      <td>2.171875</td>\n      <td>1.596680</td>\n      <td>1.931641</td>\n      <td>1.498047</td>\n      <td>1.938477</td>\n      <td>1.500000</td>\n      <td>2.269531</td>\n      <td>1.640625</td>\n      <td>1.243164</td>\n      <td>1.375000</td>\n      <td>0.094788</td>\n      <td>1.021484</td>\n      <td>1.243164</td>\n      <td>1.375000</td>\n      <td>1.246094</td>\n      <td>1.376953</td>\n      <td>1.298828</td>\n      <td>1.398438</td>\n      <td>1.617188</td>\n      <td>1.549805</td>\n      <td>0.916992</td>\n      <td>1.251953</td>\n      <td>0.916992</td>\n      <td>1.251953</td>\n      <td>0.676758</td>\n      <td>1.174805</td>\n      <td>0.683594</td>\n      <td>1.176758</td>\n      <td>1.015625</td>\n      <td>1.286133</td>\n      <td>-1.148438</td>\n      <td>0.742676</td>\n      <td>0.000000</td>\n      <td>1.00000</td>\n      <td>0.002951</td>\n      <td>1.000977</td>\n      <td>0.055359</td>\n      <td>1.016602</td>\n      <td>0.373779</td>\n      <td>1.126953</td>\n      <td>-0.326416</td>\n      <td>0.910156</td>\n      <td>-0.326416</td>\n      <td>0.910156</td>\n      <td>-0.566895</td>\n      <td>0.854004</td>\n      <td>-0.560059</td>\n      <td>0.855469</td>\n      <td>-0.228394</td>\n      <td>0.935547</td>\n      <td>1.148438</td>\n      <td>1.346680</td>\n      <td>1.151367</td>\n      <td>1.347656</td>\n      <td>1.204102</td>\n      <td>1.369141</td>\n      <td>1.522461</td>\n      <td>1.517578</td>\n      <td>0.822266</td>\n      <td>1.225586</td>\n      <td>0.822266</td>\n      <td>1.225586</td>\n      <td>0.582031</td>\n      <td>1.150391</td>\n      <td>0.588379</td>\n      <td>1.152344</td>\n      <td>0.920410</td>\n      <td>1.259766</td>\n      <td>0.002951</td>\n      <td>1.000977</td>\n      <td>0.055359</td>\n      <td>1.016602</td>\n      <td>0.373779</td>\n      <td>1.126953</td>\n      <td>-0.326416</td>\n      <td>0.910156</td>\n      <td>-0.326416</td>\n      <td>0.910156</td>\n      <td>-0.566895</td>\n      <td>0.854004</td>\n      <td>-0.560059</td>\n      <td>0.855469</td>\n      <td>-0.228394</td>\n      <td>0.935547</td>\n      <td>0.052429</td>\n      <td>1.015625</td>\n      <td>0.370605</td>\n      <td>1.125977</td>\n      <td>-0.329346</td>\n      <td>0.909668</td>\n      <td>-0.329346</td>\n      <td>0.909668</td>\n      <td>-0.569824</td>\n      <td>0.853027</td>\n      <td>-0.562988</td>\n      <td>0.854492</td>\n      <td>-0.231323</td>\n      <td>0.934570</td>\n      <td>0.318359</td>\n      <td>1.108398</td>\n      <td>-0.381836</td>\n      <td>0.895020</td>\n      <td>-0.381836</td>\n      <td>0.895020</td>\n      <td>-0.622559</td>\n      <td>0.839844</td>\n      <td>-0.615723</td>\n      <td>0.841309</td>\n      <td>-0.283691</td>\n      <td>0.919922</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.940918</td>\n      <td>0.757812</td>\n      <td>-0.934082</td>\n      <td>0.758789</td>\n      <td>-0.602051</td>\n      <td>0.830078</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.240601</td>\n      <td>0.937988</td>\n      <td>-0.233887</td>\n      <td>0.939453</td>\n      <td>0.097961</td>\n      <td>1.027344</td>\n      <td>-0.240601</td>\n      <td>0.937988</td>\n      <td>-0.233887</td>\n      <td>0.939453</td>\n      <td>0.097961</td>\n      <td>1.027344</td>\n      <td>0.006763</td>\n      <td>1.001953</td>\n      <td>0.338623</td>\n      <td>1.095703</td>\n      <td>0.331787</td>\n      <td>1.093750</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.106689</td>\n      <td>0.096661</td>\n      <td>-0.085257</td>\n      <td>0.170849</td>\n      <td>0.228557</td>\n      <td>-0.015929</td>\n      <td>0.218602</td>\n      <td>0.115935</td>\n      <td>-0.114532</td>\n      <td>-0.041913</td>\n      <td>-0.280049</td>\n      <td>-0.316037</td>\n      <td>0.081699</td>\n      <td>0.000767</td>\n      <td>0.008402</td>\n      <td>0.159766</td>\n      <td>0.165167</td>\n      <td>0.177519</td>\n      <td>0.025201</td>\n      <td>0.106031</td>\n      <td>-0.182975</td>\n      <td>0.322555</td>\n      <td>-0.148209</td>\n      <td>-0.066234</td>\n      <td>0.360090</td>\n      <td>0.112757</td>\n      <td>0.080112</td>\n      <td>0.180167</td>\n      <td>-0.232795</td>\n      <td>0.117121</td>\n      <td>-0.000931</td>\n      <td>0.187347</td>\n      <td>0.064031</td>\n      <td>-0.040666</td>\n      <td>-0.050130</td>\n      <td>-0.061977</td>\n      <td>-0.007930</td>\n      <td>0.057334</td>\n      <td>-0.383450</td>\n      <td>0.079792</td>\n      <td>-0.039399</td>\n      <td>0.720806</td>\n      <td>-0.007725</td>\n      <td>-0.171891</td>\n      <td>-0.094048</td>\n      <td>-0.186083</td>\n      <td>0.009172</td>\n      <td>0.122302</td>\n      <td>-0.048451</td>\n      <td>0.140538</td>\n      <td>0.067924</td>\n      <td>-0.064853</td>\n      <td>-0.116790</td>\n      <td>0.019235</td>\n      <td>-0.042266</td>\n      <td>0.030180</td>\n      <td>-0.022746</td>\n      <td>0.147852</td>\n      <td>-0.005656</td>\n      <td>-0.027981</td>\n      <td>-0.025677</td>\n      <td>-0.317784</td>\n      <td>0.172559</td>\n      <td>0.265649</td>\n      <td>-0.004519</td>\n      <td>-0.007689</td>\n      <td>0.058715</td>\n      <td>0.233586</td>\n      <td>0.010346</td>\n      <td>-0.230091</td>\n      <td>-0.027315</td>\n      <td>-0.108018</td>\n      <td>0.033987</td>\n      <td>0.177319</td>\n      <td>0.159197</td>\n      <td>0.201489</td>\n      <td>0.148096</td>\n      <td>-0.681110</td>\n      <td>0.082591</td>\n      <td>0.205589</td>\n      <td>-0.072989</td>\n      <td>0.098726</td>\n      <td>1.460827</td>\n      <td>0.140312</td>\n      <td>0.129674</td>\n      <td>-0.100670</td>\n      <td>0.116050</td>\n      <td>0.083768</td>\n      <td>-0.044950</td>\n      <td>0.046189</td>\n      <td>-0.126911</td>\n      <td>-0.083236</td>\n      <td>-0.018723</td>\n      <td>-0.108265</td>\n      <td>-0.096291</td>\n      <td>-0.032141</td>\n      <td>0.030658</td>\n      <td>-1.423888</td>\n      <td>-0.018308</td>\n      <td>0.142786</td>\n      <td>0.098601</td>\n      <td>-0.203690</td>\n      <td>0.037845</td>\n      <td>-0.115890</td>\n      <td>-0.169963</td>\n      <td>0.434114</td>\n      <td>0.137431</td>\n      <td>0.093100</td>\n      <td>-0.006186</td>\n      <td>-0.111833</td>\n      <td>0.065362</td>\n      <td>0.113184</td>\n      <td>-0.104002</td>\n      <td>0.101748</td>\n      <td>0.098081</td>\n      <td>-0.103681</td>\n      <td>0.325966</td>\n      <td>0.243259</td>\n      <td>-0.155149</td>\n      <td>0.515708</td>\n      <td>0.039121</td>\n      <td>0.077220</td>\n      <td>0.087235</td>\n      <td>-0.028862</td>\n      <td>0.145868</td>\n      <td>0.163139</td>\n      <td>-0.020414</td>\n      <td>0.204764</td>\n      <td>-0.078171</td>\n      <td>0.234116</td>\n      <td>0.157518</td>\n      <td>-0.588080</td>\n      <td>-0.148251</td>\n      <td>-0.188761</td>\n      <td>-0.151361</td>\n      <td>0.079774</td>\n      <td>-0.083731</td>\n      <td>0.071460</td>\n      <td>-0.004751</td>\n      <td>0.042767</td>\n      <td>0.143576</td>\n      <td>0.157891</td>\n      <td>-0.209118</td>\n      <td>-0.197583</td>\n      <td>0.192532</td>\n      <td>0.118830</td>\n      <td>-0.032486</td>\n      <td>-0.197717</td>\n      <td>0.048301</td>\n      <td>0.166421</td>\n      <td>0.214181</td>\n      <td>0.148172</td>\n      <td>-0.289947</td>\n      <td>-0.323667</td>\n      <td>-0.150069</td>\n      <td>0.683203</td>\n      <td>0.108692</td>\n      <td>-0.145632</td>\n      <td>0.014729</td>\n      <td>-0.017408</td>\n      <td>0.033193</td>\n      <td>-0.085116</td>\n      <td>0.081296</td>\n      <td>0.014131</td>\n      <td>0.086181</td>\n      <td>-0.064274</td>\n      <td>-0.116177</td>\n      <td>0.068913</td>\n      <td>-0.100193</td>\n      <td>0.020076</td>\n      <td>-0.113650</td>\n      <td>-0.090350</td>\n      <td>0.143324</td>\n      <td>0.119548</td>\n      <td>-0.080569</td>\n      <td>0.082065</td>\n      <td>-0.137666</td>\n      <td>-0.063878</td>\n      <td>0.058815</td>\n      <td>0.093614</td>\n      <td>-0.023716</td>\n      <td>0.074879</td>\n      <td>0.098216</td>\n      <td>-0.122689</td>\n      <td>0.107038</td>\n      <td>-0.002196</td>\n      <td>-0.111367</td>\n      <td>0.323567</td>\n      <td>-0.017652</td>\n      <td>-0.080741</td>\n      <td>-0.024784</td>\n      <td>-0.084675</td>\n      <td>-0.156599</td>\n      <td>-0.215438</td>\n      <td>-0.034031</td>\n      <td>-0.152236</td>\n      <td>-0.050263</td>\n      <td>0.055357</td>\n      <td>0.250663</td>\n      <td>-0.114731</td>\n      <td>0.136917</td>\n      <td>-0.320303</td>\n      <td>0.062832</td>\n      <td>-0.018570</td>\n      <td>0.204020</td>\n      <td>0.014725</td>\n      <td>-0.373413</td>\n      <td>-0.021981</td>\n      <td>0.012825</td>\n      <td>-0.023030</td>\n      <td>0.082520</td>\n      <td>0.091580</td>\n      <td>-0.273733</td>\n      <td>0.183646</td>\n      <td>-0.168271</td>\n      <td>0.139121</td>\n      <td>0.060510</td>\n      <td>-0.232226</td>\n      <td>-0.308105</td>\n      <td>-0.158718</td>\n      <td>-0.017783</td>\n      <td>0.086882</td>\n      <td>-0.141424</td>\n      <td>-0.012542</td>\n      <td>0.042945</td>\n      <td>-0.085448</td>\n      <td>0.067201</td>\n      <td>-0.119579</td>\n      <td>0.102467</td>\n      <td>-0.116019</td>\n      <td>-0.001257</td>\n      <td>0.404416</td>\n      <td>0.313694</td>\n      <td>0.050169</td>\n      <td>0.136860</td>\n      <td>-0.399030</td>\n      <td>-0.009352</td>\n      <td>0.021727</td>\n      <td>-0.164164</td>\n      <td>-0.157287</td>\n      <td>-0.768654</td>\n      <td>-0.000166</td>\n      <td>0.200911</td>\n      <td>0.110998</td>\n      <td>-0.090584</td>\n      <td>0.149695</td>\n      <td>-0.045115</td>\n      <td>-0.195699</td>\n      <td>0.052729</td>\n      <td>0.004195</td>\n      <td>-0.107189</td>\n      <td>0.144304</td>\n      <td>0.064344</td>\n      <td>0.065908</td>\n      <td>0.139589</td>\n      <td>-0.243548</td>\n      <td>0.096933</td>\n      <td>-0.134869</td>\n      <td>-0.331963</td>\n      <td>0.616384</td>\n      <td>0.033699</td>\n      <td>-0.121195</td>\n      <td>0.234892</td>\n      <td>-0.057047</td>\n      <td>0.016235</td>\n      <td>-0.134531</td>\n      <td>-0.027028</td>\n      <td>0.054334</td>\n      <td>0.122324</td>\n      <td>0.036071</td>\n      <td>-0.150864</td>\n      <td>0.093658</td>\n      <td>0.074198</td>\n      <td>-0.090122</td>\n      <td>-0.135492</td>\n      <td>0.493441</td>\n      <td>0.256015</td>\n      <td>-0.025004</td>\n      <td>-0.157481</td>\n      <td>0.027424</td>\n      <td>0.141670</td>\n      <td>-0.048801</td>\n      <td>0.066857</td>\n      <td>0.179784</td>\n      <td>-0.089254</td>\n      <td>-0.078412</td>\n      <td>0.242003</td>\n      <td>-0.044503</td>\n      <td>-0.168966</td>\n      <td>0.102760</td>\n      <td>-0.125682</td>\n      <td>-0.131750</td>\n      <td>-0.152472</td>\n      <td>0.020987</td>\n      <td>0.194332</td>\n      <td>0.036416</td>\n      <td>0.194594</td>\n      <td>-0.058079</td>\n      <td>0.028480</td>\n      <td>-0.148669</td>\n      <td>0.128975</td>\n      <td>-0.070672</td>\n      <td>0.124587</td>\n      <td>0.045444</td>\n      <td>-0.065784</td>\n      <td>0.118355</td>\n      <td>-0.162230</td>\n      <td>-0.000975</td>\n      <td>0.182277</td>\n      <td>-0.058043</td>\n      <td>-0.192583</td>\n      <td>0.084746</td>\n      <td>-0.051868</td>\n      <td>-0.021684</td>\n      <td>0.029572</td>\n      <td>0.190434</td>\n      <td>-0.153322</td>\n      <td>0.100547</td>\n      <td>0.016438</td>\n      <td>0.030090</td>\n      <td>-0.271785</td>\n      <td>-0.001377</td>\n      <td>-0.202443</td>\n      <td>-0.226051</td>\n      <td>-0.264157</td>\n      <td>0.139300</td>\n      <td>-0.073914</td>\n      <td>0.093044</td>\n      <td>0.219541</td>\n      <td>0.154412</td>\n      <td>1.327022</td>\n      <td>0.563168</td>\n      <td>0.175710</td>\n      <td>-0.080630</td>\n      <td>0.245853</td>\n      <td>0.024140</td>\n      <td>-0.058418</td>\n      <td>0.054246</td>\n      <td>0.274013</td>\n      <td>0.039049</td>\n      <td>-0.051425</td>\n      <td>-0.057492</td>\n      <td>0.106365</td>\n      <td>0.058885</td>\n      <td>0.303400</td>\n      <td>-0.059952</td>\n      <td>0.052255</td>\n      <td>0.078477</td>\n      <td>0.344559</td>\n      <td>-0.156540</td>\n      <td>-0.149459</td>\n      <td>-0.175235</td>\n      <td>-0.096360</td>\n      <td>-0.321122</td>\n      <td>-0.092529</td>\n      <td>0.067954</td>\n      <td>-0.236221</td>\n      <td>-0.039793</td>\n      <td>-0.225340</td>\n      <td>-0.383308</td>\n      <td>-0.152813</td>\n      <td>0.224674</td>\n      <td>-0.044086</td>\n      <td>0.104212</td>\n      <td>0.177586</td>\n      <td>-0.204635</td>\n      <td>0.228664</td>\n      <td>0.109329</td>\n      <td>0.121903</td>\n      <td>-0.115773</td>\n      <td>0.104601</td>\n      <td>0.057888</td>\n      <td>-0.117652</td>\n      <td>-0.104324</td>\n      <td>0.134173</td>\n      <td>-0.259472</td>\n      <td>0.159142</td>\n      <td>-0.186426</td>\n      <td>-0.224667</td>\n      <td>-0.031908</td>\n      <td>-0.023992</td>\n      <td>0.157893</td>\n      <td>-0.224642</td>\n      <td>-0.044007</td>\n      <td>0.370970</td>\n      <td>-0.098588</td>\n      <td>0.083123</td>\n      <td>0.246624</td>\n      <td>0.078750</td>\n      <td>-0.054559</td>\n      <td>0.067571</td>\n      <td>-0.114809</td>\n      <td>0.026363</td>\n      <td>-0.128940</td>\n      <td>0.083838</td>\n      <td>-0.018011</td>\n      <td>0.121785</td>\n      <td>-0.604013</td>\n      <td>-0.070415</td>\n      <td>-0.022270</td>\n      <td>0.078345</td>\n      <td>0.118066</td>\n      <td>0.044453</td>\n      <td>-0.071485</td>\n      <td>0.014087</td>\n      <td>0.180780</td>\n      <td>-0.089880</td>\n      <td>-0.139726</td>\n      <td>-0.107553</td>\n      <td>0.292497</td>\n      <td>0.134242</td>\n      <td>0.189744</td>\n      <td>0.030346</td>\n      <td>-0.071608</td>\n      <td>-0.018470</td>\n      <td>-0.009388</td>\n      <td>-0.091223</td>\n      <td>-0.236851</td>\n      <td>-0.263282</td>\n      <td>-0.085916</td>\n      <td>-0.031939</td>\n      <td>-0.157399</td>\n      <td>-0.042684</td>\n      <td>-0.029077</td>\n      <td>-0.435612</td>\n      <td>-0.142579</td>\n      <td>0.212879</td>\n      <td>0.009944</td>\n      <td>-0.037457</td>\n      <td>0.158548</td>\n      <td>0.110534</td>\n      <td>0.083527</td>\n      <td>-0.231483</td>\n      <td>-0.060833</td>\n      <td>-0.183733</td>\n      <td>-0.097880</td>\n      <td>0.107157</td>\n      <td>-0.217850</td>\n      <td>-0.115877</td>\n      <td>0.037825</td>\n      <td>-0.354344</td>\n      <td>0.019821</td>\n      <td>0.013490</td>\n      <td>-0.039675</td>\n      <td>-0.151258</td>\n      <td>-0.062437</td>\n      <td>0.077217</td>\n      <td>-0.082840</td>\n      <td>-0.161044</td>\n      <td>-0.140004</td>\n      <td>-0.228191</td>\n      <td>0.043486</td>\n      <td>-0.327433</td>\n      <td>-1.464426</td>\n      <td>0.309512</td>\n      <td>0.200035</td>\n      <td>-0.070795</td>\n      <td>0.041317</td>\n      <td>-0.029839</td>\n      <td>-0.136768</td>\n      <td>0.170875</td>\n      <td>0.063316</td>\n      <td>0.135547</td>\n      <td>-0.269064</td>\n      <td>0.036440</td>\n      <td>-0.200457</td>\n      <td>0.228488</td>\n      <td>-0.106061</td>\n      <td>-0.013615</td>\n      <td>0.074039</td>\n      <td>0.312686</td>\n      <td>-0.094369</td>\n      <td>-0.138735</td>\n      <td>-0.125981</td>\n      <td>-0.109813</td>\n      <td>-0.042848</td>\n      <td>-0.132928</td>\n      <td>0.246403</td>\n      <td>0.147859</td>\n      <td>-0.050107</td>\n      <td>-0.188052</td>\n      <td>0.072340</td>\n      <td>0.110755</td>\n      <td>0.007728</td>\n      <td>0.031509</td>\n      <td>-0.112875</td>\n      <td>0.024343</td>\n      <td>0.077814</td>\n      <td>0.176009</td>\n      <td>-0.202625</td>\n      <td>-0.309267</td>\n      <td>-0.066109</td>\n      <td>0.231504</td>\n      <td>0.191174</td>\n      <td>0.179446</td>\n      <td>0.323706</td>\n      <td>-0.049855</td>\n      <td>-0.350672</td>\n      <td>-0.306750</td>\n      <td>0.069974</td>\n      <td>0.060304</td>\n      <td>0.074133</td>\n      <td>-0.099113</td>\n      <td>0.230257</td>\n      <td>0.037989</td>\n      <td>-0.070723</td>\n      <td>-0.195232</td>\n      <td>0.034662</td>\n      <td>0.098447</td>\n      <td>-0.072708</td>\n      <td>-0.251961</td>\n      <td>-0.015479</td>\n      <td>0.030078</td>\n      <td>0.109286</td>\n      <td>0.063225</td>\n      <td>0.163723</td>\n      <td>0.100954</td>\n      <td>-0.040606</td>\n      <td>0.081477</td>\n      <td>0.457815</td>\n      <td>0.069638</td>\n      <td>-0.022119</td>\n      <td>0.068916</td>\n      <td>-0.029299</td>\n      <td>-0.047933</td>\n      <td>0.130911</td>\n      <td>-0.107221</td>\n      <td>0.010681</td>\n      <td>0.017530</td>\n      <td>-0.012998</td>\n      <td>0.315592</td>\n      <td>-0.132335</td>\n      <td>0.325063</td>\n      <td>0.033086</td>\n      <td>-0.284308</td>\n      <td>-0.074773</td>\n      <td>0.102727</td>\n      <td>-0.216311</td>\n      <td>0.059021</td>\n      <td>0.078432</td>\n      <td>-0.181384</td>\n      <td>0.158258</td>\n      <td>0.045622</td>\n      <td>0.032226</td>\n      <td>0.096019</td>\n      <td>-0.211077</td>\n      <td>0.075733</td>\n      <td>-0.044160</td>\n      <td>0.129608</td>\n      <td>0.076760</td>\n      <td>-0.075360</td>\n      <td>0.466614</td>\n      <td>0.090322</td>\n      <td>0.150747</td>\n      <td>-0.085852</td>\n      <td>0.307366</td>\n      <td>0.205635</td>\n      <td>-0.059250</td>\n      <td>-0.317916</td>\n      <td>0.371092</td>\n      <td>0.113249</td>\n      <td>0.095618</td>\n      <td>-0.035221</td>\n      <td>-0.096874</td>\n      <td>0.058885</td>\n      <td>-0.083357</td>\n      <td>-0.082363</td>\n      <td>0.053920</td>\n      <td>0.117225</td>\n      <td>-0.099438</td>\n      <td>1.554892</td>\n      <td>0.286132</td>\n      <td>-0.303300</td>\n      <td>0.046252</td>\n      <td>0.269663</td>\n      <td>0.030778</td>\n      <td>-0.066629</td>\n      <td>0.099054</td>\n      <td>0.214823</td>\n      <td>0.186653</td>\n      <td>0.099686</td>\n      <td>-0.048642</td>\n      <td>-0.147051</td>\n      <td>0.068232</td>\n      <td>-0.058560</td>\n      <td>0.071003</td>\n      <td>0.050570</td>\n      <td>0.033566</td>\n      <td>12.149046</td>\n      <td>0.052788</td>\n      <td>-0.019711</td>\n      <td>0.077738</td>\n      <td>-0.249763</td>\n      <td>-0.025224</td>\n      <td>0.030602</td>\n      <td>-0.058953</td>\n      <td>0.241092</td>\n      <td>0.107833</td>\n      <td>-0.050415</td>\n      <td>-0.096370</td>\n      <td>0.116405</td>\n      <td>-0.205613</td>\n      <td>0.103472</td>\n      <td>0.020359</td>\n      <td>0.087780</td>\n      <td>0.165174</td>\n      <td>0.151956</td>\n      <td>-0.155135</td>\n      <td>0.048719</td>\n      <td>0.136277</td>\n      <td>0.014289</td>\n      <td>0.941260</td>\n      <td>-0.143702</td>\n      <td>0.072925</td>\n      <td>-0.135912</td>\n      <td>0.126020</td>\n      <td>0.117271</td>\n      <td>0.201530</td>\n      <td>-0.107007</td>\n      <td>0.218389</td>\n      <td>0.065998</td>\n      <td>-0.083668</td>\n      <td>0.053277</td>\n      <td>-0.147597</td>\n      <td>0.029762</td>\n      <td>0.036007</td>\n      <td>-0.030452</td>\n      <td>-0.116128</td>\n      <td>-0.074170</td>\n      <td>-0.051684</td>\n      <td>0.312923</td>\n      <td>-0.386000</td>\n      <td>0.135494</td>\n      <td>-0.153733</td>\n      <td>0.340041</td>\n      <td>0.280454</td>\n      <td>-0.007710</td>\n      <td>-0.049217</td>\n      <td>-0.141264</td>\n      <td>0.125780</td>\n      <td>0.236436</td>\n      <td>-0.288352</td>\n      <td>-0.001873</td>\n      <td>-0.319654</td>\n      <td>0.173400</td>\n      <td>-0.170278</td>\n      <td>-0.004907</td>\n      <td>-0.229392</td>\n      <td>-0.096131</td>\n      <td>-0.110653</td>\n      <td>0.114807</td>\n      <td>-0.077159</td>\n      <td>-0.306470</td>\n      <td>0.196648</td>\n      <td>0.239305</td>\n      <td>-0.083120</td>\n      <td>-0.006266</td>\n      <td>-0.016332</td>\n      <td>0.137176</td>\n      <td>-0.049121</td>\n      <td>0.032097</td>\n      <td>0.005268</td>\n      <td>-0.033742</td>\n      <td>-0.187795</td>\n      <td>-0.041616</td>\n      <td>-0.180207</td>\n      <td>-0.083809</td>\n      <td>-0.611474</td>\n      <td>0.132549</td>\n      <td>-0.097444</td>\n      <td>-0.145588</td>\n      <td>-0.041082</td>\n      <td>-0.310184</td>\n      <td>0.206319</td>\n      <td>-0.203444</td>\n      <td>-0.015584</td>\n      <td>-0.259579</td>\n      <td>0.176335</td>\n      <td>0.069659</td>\n      <td>0.012299</td>\n      <td>0.034244</td>\n      <td>-0.047717</td>\n      <td>0.055669</td>\n      <td>0.003719</td>\n      <td>0.157838</td>\n      <td>0.011114</td>\n      <td>0.159168</td>\n      <td>-0.178828</td>\n      <td>-0.446934</td>\n      <td>-0.108646</td>\n      <td>-0.151625</td>\n      <td>-0.023107</td>\n      <td>-0.169719</td>\n      <td>0.207508</td>\n      <td>0.013606</td>\n      <td>-0.166397</td>\n      <td>0.069439</td>\n      <td>0.024586</td>\n      <td>0.034726</td>\n      <td>0.034124</td>\n      <td>-0.228854</td>\n      <td>-0.116306</td>\n      <td>0.059327</td>\n      <td>-0.123236</td>\n      <td>0.111901</td>\n      <td>-0.049153</td>\n      <td>0.095247</td>\n      <td>0.112716</td>\n      <td>-0.023128</td>\n      <td>0.058711</td>\n      <td>0.217587</td>\n      <td>-0.042789</td>\n      <td>-0.056089</td>\n      <td>0.032627</td>\n      <td>-0.305321</td>\n      <td>0.011219</td>\n      <td>-0.307083</td>\n      <td>-0.103627</td>\n      <td>-0.063050</td>\n      <td>0.115984</td>\n      <td>0.333934</td>\n      <td>0.055513</td>\n      <td>0.008051</td>\n      <td>0.104721</td>\n      <td>-0.171578</td>\n      <td>0.106401</td>\n      <td>0.010308</td>\n      <td>0.284912</td>\n      <td>-0.166739</td>\n      <td>0.050573</td>\n      <td>-0.231710</td>\n      <td>-0.157238</td>\n      <td>-0.059044</td>\n      <td>-0.136545</td>\n      <td>0.288966</td>\n      <td>0.348855</td>\n      <td>0.023084</td>\n      <td>0.088987</td>\n      <td>-0.136358</td>\n      <td>-0.047009</td>\n      <td>-0.123210</td>\n      <td>-0.567249</td>\n      <td>-0.003853</td>\n      <td>0.035672</td>\n      <td>-0.251738</td>\n      <td>0.176084</td>\n      <td>0.085949</td>\n      <td>-0.187019</td>\n      <td>0.486607</td>\n      <td>-0.545987</td>\n      <td>-0.139555</td>\n      <td>-0.130976</td>\n      <td>-0.378054</td>\n      <td>0.055729</td>\n      <td>0.309306</td>\n      <td>-0.156030</td>\n      <td>0.109429</td>\n      <td>-0.000374</td>\n      <td>-0.064217</td>\n      <td>-0.051321</td>\n      <td>0.107150</td>\n      <td>0.062490</td>\n      <td>0.142579</td>\n      <td>0.072491</td>\n      <td>0.352781</td>\n      <td>0.152339</td>\n      <td>0.023221</td>\n      <td>-0.117772</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1807.01034</td>\n      <td>Evangelos Thomas Karamatskos</td>\n      <td>Evangelos T. Karamatskos, Sebastian Raabe, Ter...</td>\n      <td>Molecular movie of ultrafast coherent rotation...</td>\n      <td>9 Figures</td>\n      <td>Nat Commun 10, 3364 (2019)</td>\n      <td>10.1038/s41467-019-11122-y</td>\n      <td>None</td>\n      <td>physics.chem-ph physics.atom-ph quant-ph</td>\n      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n      <td>Recording molecular movies on ultrafast time...</td>\n      <td>[{'version': 'v1', 'created': 'Tue, 3 Jul 2018...</td>\n      <td>[[Karamatskos, Evangelos T., ], [Raabe, Sebast...</td>\n      <td>1.946289</td>\n      <td>2.197225</td>\n      <td>10.1038</td>\n      <td>Nature Publishing Group</td>\n      <td>5.105469</td>\n      <td>866445.0</td>\n      <td>2020-05-19</td>\n      <td>2018-07-03 09:11:19+00:00</td>\n      <td>2019-05-09 12:01:21+00:00</td>\n      <td>2020</td>\n      <td>2018</td>\n      <td>2019</td>\n      <td>5</td>\n      <td>7</td>\n      <td>5</td>\n      <td>202005</td>\n      <td>201807</td>\n      <td>201905</td>\n      <td>19</td>\n      <td>3</td>\n      <td>9</td>\n      <td>1.589846e+09</td>\n      <td>1.530609e+09</td>\n      <td>1.557403e+09</td>\n      <td>59237321</td>\n      <td>26794202.0</td>\n      <td>2</td>\n      <td>18401</td>\n      <td>17715</td>\n      <td>18025</td>\n      <td>310</td>\n      <td>103.687500</td>\n      <td>Karamatskos</td>\n      <td>13</td>\n      <td>1.946289</td>\n      <td>physics</td>\n      <td>physics</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>physics physics quant</td>\n      <td>physics physics quant-ph</td>\n      <td>physics.atom-ph physics.chem-ph quant-ph</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>52993</td>\n      <td>22</td>\n      <td>50948</td>\n      <td>327</td>\n      <td>0</td>\n      <td>29</td>\n      <td>29</td>\n      <td>3527</td>\n      <td>2806</td>\n      <td>22852</td>\n      <td>1.573242</td>\n      <td>3</td>\n      <td>4.718750</td>\n      <td>0.693359</td>\n      <td>2.080078</td>\n      <td>1.946289</td>\n      <td>0.764648</td>\n      <td>0.943848</td>\n      <td>1.319336</td>\n      <td>2.011719</td>\n      <td>3.093750</td>\n      <td>11868</td>\n      <td>36725.164062</td>\n      <td>0.0</td>\n      <td>9.265625</td>\n      <td>3.134766</td>\n      <td>1.694336</td>\n      <td>0.693359</td>\n      <td>1.946289</td>\n      <td>4.316406</td>\n      <td>3.093750</td>\n      <td>11868</td>\n      <td>36725.164062</td>\n      <td>0.0</td>\n      <td>9.265625</td>\n      <td>3.134766</td>\n      <td>1.694336</td>\n      <td>0.693359</td>\n      <td>1.946289</td>\n      <td>4.316406</td>\n      <td>1.573242</td>\n      <td>3</td>\n      <td>4.71875</td>\n      <td>0.693359</td>\n      <td>2.080078</td>\n      <td>1.946289</td>\n      <td>0.764648</td>\n      <td>0.943848</td>\n      <td>1.319336</td>\n      <td>2.011719</td>\n      <td>0.235229</td>\n      <td>6130</td>\n      <td>1441.699829</td>\n      <td>0.0</td>\n      <td>6.515625</td>\n      <td>0.000000</td>\n      <td>0.611816</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.221680</td>\n      <td>5360</td>\n      <td>6548.0</td>\n      <td>0.0</td>\n      <td>5.140625</td>\n      <td>1.098633</td>\n      <td>0.966797</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>1.946289</td>\n      <td>1.940430</td>\n      <td>599833</td>\n      <td>1163680.25</td>\n      <td>0.0</td>\n      <td>9.101562</td>\n      <td>1.946289</td>\n      <td>1.349609</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>2.890625</td>\n      <td>1.880859</td>\n      <td>70889</td>\n      <td>133323.875000</td>\n      <td>0.0</td>\n      <td>8.406250</td>\n      <td>1.791992</td>\n      <td>1.330078</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>2.833984</td>\n      <td>1.880859</td>\n      <td>70889</td>\n      <td>133323.875000</td>\n      <td>0.0</td>\n      <td>8.406250</td>\n      <td>1.791992</td>\n      <td>1.330078</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>2.833984</td>\n      <td>2.138672</td>\n      <td>1099</td>\n      <td>2350.435059</td>\n      <td>0.0</td>\n      <td>6.667969</td>\n      <td>2.197266</td>\n      <td>1.338867</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>2.996094</td>\n      <td>2.138672</td>\n      <td>1099</td>\n      <td>2350.435059</td>\n      <td>0.0</td>\n      <td>6.667969</td>\n      <td>2.197266</td>\n      <td>1.338867</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>2.996094</td>\n      <td>2.371094</td>\n      <td>107</td>\n      <td>253.668579</td>\n      <td>0.0</td>\n      <td>6.667969</td>\n      <td>2.302734</td>\n      <td>1.276367</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.218750</td>\n      <td>1.573242</td>\n      <td>3</td>\n      <td>4.718750</td>\n      <td>0.693359</td>\n      <td>2.080078</td>\n      <td>1.946289</td>\n      <td>0.765137</td>\n      <td>0.943848</td>\n      <td>1.320312</td>\n      <td>2.013672</td>\n      <td>3.093750</td>\n      <td>11868</td>\n      <td>36725.636719</td>\n      <td>0.0</td>\n      <td>9.265625</td>\n      <td>3.134766</td>\n      <td>1.694336</td>\n      <td>0.693359</td>\n      <td>1.946289</td>\n      <td>4.316406</td>\n      <td>3.093750</td>\n      <td>11868</td>\n      <td>36725.636719</td>\n      <td>0.0</td>\n      <td>9.265625</td>\n      <td>3.134766</td>\n      <td>1.694336</td>\n      <td>0.693359</td>\n      <td>1.946289</td>\n      <td>4.316406</td>\n      <td>1.573242</td>\n      <td>3</td>\n      <td>4.71875</td>\n      <td>0.693359</td>\n      <td>2.080078</td>\n      <td>1.946289</td>\n      <td>0.765137</td>\n      <td>0.943848</td>\n      <td>1.320312</td>\n      <td>2.013672</td>\n      <td>0.235229</td>\n      <td>6130</td>\n      <td>1441.907227</td>\n      <td>0.0</td>\n      <td>6.515625</td>\n      <td>0.000000</td>\n      <td>0.611816</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.221680</td>\n      <td>5360</td>\n      <td>6548.0</td>\n      <td>0.0</td>\n      <td>5.140625</td>\n      <td>1.098633</td>\n      <td>0.966797</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>1.946289</td>\n      <td>1.940430</td>\n      <td>599833</td>\n      <td>1.163743e+06</td>\n      <td>0.0</td>\n      <td>9.101562</td>\n      <td>1.946289</td>\n      <td>1.349609</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>2.890625</td>\n      <td>1.880859</td>\n      <td>70889</td>\n      <td>133331.53125</td>\n      <td>0.0</td>\n      <td>8.406250</td>\n      <td>1.791992</td>\n      <td>1.330078</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>2.833984</td>\n      <td>1.880859</td>\n      <td>70889</td>\n      <td>133331.53125</td>\n      <td>0.0</td>\n      <td>8.406250</td>\n      <td>1.791992</td>\n      <td>1.330078</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>2.833984</td>\n      <td>2.138672</td>\n      <td>1099</td>\n      <td>2350.564453</td>\n      <td>0.0</td>\n      <td>6.667969</td>\n      <td>2.197266</td>\n      <td>1.338867</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>2.996094</td>\n      <td>2.138672</td>\n      <td>1099</td>\n      <td>2350.564453</td>\n      <td>0.0</td>\n      <td>6.667969</td>\n      <td>2.197266</td>\n      <td>1.338867</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>2.996094</td>\n      <td>2.371094</td>\n      <td>107</td>\n      <td>253.680664</td>\n      <td>0.0</td>\n      <td>6.667969</td>\n      <td>2.302734</td>\n      <td>1.276367</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.218750</td>\n      <td>0.373047</td>\n      <td>1.144531</td>\n      <td>-1.148438</td>\n      <td>0.719727</td>\n      <td>0.373047</td>\n      <td>1.144531</td>\n      <td>-1.148438</td>\n      <td>0.719727</td>\n      <td>1.710938</td>\n      <td>2.384766</td>\n      <td>0.724121</td>\n      <td>1.326172</td>\n      <td>0.005905</td>\n      <td>1.001953</td>\n      <td>0.065186</td>\n      <td>1.022461</td>\n      <td>0.065186</td>\n      <td>1.022461</td>\n      <td>-0.192749</td>\n      <td>0.938477</td>\n      <td>-0.192749</td>\n      <td>0.938477</td>\n      <td>-0.424805</td>\n      <td>0.874023</td>\n      <td>-0.000379</td>\n      <td>1.0</td>\n      <td>0.372559</td>\n      <td>1.144531</td>\n      <td>-1.148438</td>\n      <td>0.719238</td>\n      <td>0.372559</td>\n      <td>1.144531</td>\n      <td>-1.148438</td>\n      <td>0.719238</td>\n      <td>1.710938</td>\n      <td>2.384766</td>\n      <td>0.724121</td>\n      <td>1.326172</td>\n      <td>0.005798</td>\n      <td>1.001953</td>\n      <td>0.065063</td>\n      <td>1.022461</td>\n      <td>0.065063</td>\n      <td>1.022461</td>\n      <td>-0.192871</td>\n      <td>0.938477</td>\n      <td>-0.192871</td>\n      <td>0.938477</td>\n      <td>-0.425049</td>\n      <td>0.874023</td>\n      <td>-1.521484</td>\n      <td>0.628418</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-1.521484</td>\n      <td>0.628418</td>\n      <td>1.337891</td>\n      <td>2.082031</td>\n      <td>0.351318</td>\n      <td>1.158203</td>\n      <td>-0.367188</td>\n      <td>0.875000</td>\n      <td>-0.307861</td>\n      <td>0.893066</td>\n      <td>-0.307861</td>\n      <td>0.893066</td>\n      <td>-0.565918</td>\n      <td>0.819824</td>\n      <td>-0.565918</td>\n      <td>0.819824</td>\n      <td>-0.797852</td>\n      <td>0.763184</td>\n      <td>-0.373535</td>\n      <td>0.873047</td>\n      <td>-0.000409</td>\n      <td>1.0</td>\n      <td>-1.521484</td>\n      <td>0.628418</td>\n      <td>-0.000409</td>\n      <td>1.000000</td>\n      <td>-1.521484</td>\n      <td>0.628418</td>\n      <td>1.337891</td>\n      <td>2.082031</td>\n      <td>0.351074</td>\n      <td>1.158203</td>\n      <td>-0.367188</td>\n      <td>0.875000</td>\n      <td>-0.308105</td>\n      <td>0.893066</td>\n      <td>-0.308105</td>\n      <td>0.893066</td>\n      <td>-0.565918</td>\n      <td>0.819824</td>\n      <td>-0.565918</td>\n      <td>0.819824</td>\n      <td>-0.797852</td>\n      <td>0.763184</td>\n      <td>1.521484</td>\n      <td>1.591797</td>\n      <td>0.000000</td>\n      <td>1.00000</td>\n      <td>2.859375</td>\n      <td>3.314453</td>\n      <td>1.873047</td>\n      <td>1.842773</td>\n      <td>1.154297</td>\n      <td>1.392578</td>\n      <td>1.213867</td>\n      <td>1.420898</td>\n      <td>1.213867</td>\n      <td>1.420898</td>\n      <td>0.955566</td>\n      <td>1.304688</td>\n      <td>0.955566</td>\n      <td>1.304688</td>\n      <td>0.723633</td>\n      <td>1.214844</td>\n      <td>1.148438</td>\n      <td>1.389648</td>\n      <td>1.521484</td>\n      <td>1.590820</td>\n      <td>-0.000040</td>\n      <td>1.0</td>\n      <td>1.521484</td>\n      <td>1.590820</td>\n      <td>-0.000040</td>\n      <td>1.00000</td>\n      <td>2.859375</td>\n      <td>3.314453</td>\n      <td>1.873047</td>\n      <td>1.842773</td>\n      <td>1.154297</td>\n      <td>1.392578</td>\n      <td>1.213867</td>\n      <td>1.420898</td>\n      <td>1.213867</td>\n      <td>1.420898</td>\n      <td>0.955566</td>\n      <td>1.304688</td>\n      <td>0.955566</td>\n      <td>1.304688</td>\n      <td>0.723633</td>\n      <td>1.214844</td>\n      <td>-1.521484</td>\n      <td>0.628418</td>\n      <td>1.337891</td>\n      <td>2.082031</td>\n      <td>0.351318</td>\n      <td>1.158203</td>\n      <td>-0.367188</td>\n      <td>0.875000</td>\n      <td>-0.307861</td>\n      <td>0.893066</td>\n      <td>-0.307861</td>\n      <td>0.893066</td>\n      <td>-0.565918</td>\n      <td>0.819824</td>\n      <td>-0.565918</td>\n      <td>0.819824</td>\n      <td>-0.797852</td>\n      <td>0.763184</td>\n      <td>-0.373535</td>\n      <td>0.873047</td>\n      <td>-0.000409</td>\n      <td>1.000000</td>\n      <td>-1.521484</td>\n      <td>0.628418</td>\n      <td>-0.000409</td>\n      <td>1.0</td>\n      <td>-1.521484</td>\n      <td>0.628418</td>\n      <td>1.337891</td>\n      <td>2.082031</td>\n      <td>0.351074</td>\n      <td>1.158203</td>\n      <td>-0.367188</td>\n      <td>0.875000</td>\n      <td>-0.308105</td>\n      <td>0.893066</td>\n      <td>-0.308105</td>\n      <td>0.893066</td>\n      <td>-0.565918</td>\n      <td>0.819824</td>\n      <td>-0.565918</td>\n      <td>0.819824</td>\n      <td>-0.797852</td>\n      <td>0.763184</td>\n      <td>2.859375</td>\n      <td>3.314453</td>\n      <td>1.873047</td>\n      <td>1.842773</td>\n      <td>1.154297</td>\n      <td>1.392578</td>\n      <td>1.213867</td>\n      <td>1.420898</td>\n      <td>1.213867</td>\n      <td>1.420898</td>\n      <td>0.955566</td>\n      <td>1.304688</td>\n      <td>0.955566</td>\n      <td>1.304688</td>\n      <td>0.723633</td>\n      <td>1.214844</td>\n      <td>1.148438</td>\n      <td>1.389648</td>\n      <td>1.521484</td>\n      <td>1.590820</td>\n      <td>-0.000040</td>\n      <td>1.000000</td>\n      <td>1.521484</td>\n      <td>1.590820</td>\n      <td>-0.000040</td>\n      <td>1.0</td>\n      <td>2.859375</td>\n      <td>3.314453</td>\n      <td>1.873047</td>\n      <td>1.842773</td>\n      <td>1.154297</td>\n      <td>1.392578</td>\n      <td>1.213867</td>\n      <td>1.420898</td>\n      <td>1.213867</td>\n      <td>1.420898</td>\n      <td>0.955566</td>\n      <td>1.304688</td>\n      <td>0.955566</td>\n      <td>1.304688</td>\n      <td>0.723633</td>\n      <td>1.214844</td>\n      <td>-0.986328</td>\n      <td>0.556152</td>\n      <td>-1.705078</td>\n      <td>0.420166</td>\n      <td>-1.645508</td>\n      <td>0.428711</td>\n      <td>-1.645508</td>\n      <td>0.428711</td>\n      <td>-1.903320</td>\n      <td>0.393555</td>\n      <td>-1.903320</td>\n      <td>0.393555</td>\n      <td>-2.134766</td>\n      <td>0.366455</td>\n      <td>-1.710938</td>\n      <td>0.419189</td>\n      <td>-1.337891</td>\n      <td>0.479980</td>\n      <td>-2.859375</td>\n      <td>0.301758</td>\n      <td>-1.337891</td>\n      <td>0.479980</td>\n      <td>-2.859375</td>\n      <td>0.301758</td>\n      <td>-0.000034</td>\n      <td>1.0</td>\n      <td>-0.986328</td>\n      <td>0.556152</td>\n      <td>-1.705078</td>\n      <td>0.420166</td>\n      <td>-1.645508</td>\n      <td>0.428711</td>\n      <td>-1.645508</td>\n      <td>0.428711</td>\n      <td>-1.903320</td>\n      <td>0.393555</td>\n      <td>-1.903320</td>\n      <td>0.393555</td>\n      <td>-2.134766</td>\n      <td>0.366455</td>\n      <td>-0.718262</td>\n      <td>0.755859</td>\n      <td>-0.659180</td>\n      <td>0.770996</td>\n      <td>-0.659180</td>\n      <td>0.770996</td>\n      <td>-0.916992</td>\n      <td>0.708008</td>\n      <td>-0.916992</td>\n      <td>0.708008</td>\n      <td>-1.149414</td>\n      <td>0.659180</td>\n      <td>-0.724609</td>\n      <td>0.753906</td>\n      <td>-0.351562</td>\n      <td>0.863281</td>\n      <td>-1.873047</td>\n      <td>0.542480</td>\n      <td>-0.351562</td>\n      <td>0.863281</td>\n      <td>-1.873047</td>\n      <td>0.542480</td>\n      <td>0.986328</td>\n      <td>1.798828</td>\n      <td>-0.000137</td>\n      <td>1.0</td>\n      <td>-0.718750</td>\n      <td>0.755371</td>\n      <td>-0.659180</td>\n      <td>0.770996</td>\n      <td>-0.659180</td>\n      <td>0.770996</td>\n      <td>-0.917480</td>\n      <td>0.708008</td>\n      <td>-0.917480</td>\n      <td>0.708008</td>\n      <td>-1.149414</td>\n      <td>0.659180</td>\n      <td>0.059265</td>\n      <td>1.020508</td>\n      <td>0.059265</td>\n      <td>1.020508</td>\n      <td>-0.198730</td>\n      <td>0.936523</td>\n      <td>-0.198730</td>\n      <td>0.936523</td>\n      <td>-0.430664</td>\n      <td>0.872070</td>\n      <td>-0.006283</td>\n      <td>0.998047</td>\n      <td>0.366699</td>\n      <td>1.142578</td>\n      <td>-1.154297</td>\n      <td>0.718262</td>\n      <td>0.366699</td>\n      <td>1.142578</td>\n      <td>-1.154297</td>\n      <td>0.718262</td>\n      <td>1.705078</td>\n      <td>2.380859</td>\n      <td>0.718262</td>\n      <td>1.323242</td>\n      <td>-0.000105</td>\n      <td>1.0</td>\n      <td>0.059143</td>\n      <td>1.020508</td>\n      <td>0.059143</td>\n      <td>1.020508</td>\n      <td>-0.198853</td>\n      <td>0.936523</td>\n      <td>-0.198853</td>\n      <td>0.936523</td>\n      <td>-0.430908</td>\n      <td>0.872070</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.258057</td>\n      <td>0.917969</td>\n      <td>-0.258057</td>\n      <td>0.917969</td>\n      <td>-0.489990</td>\n      <td>0.854492</td>\n      <td>-0.065552</td>\n      <td>0.977539</td>\n      <td>0.307617</td>\n      <td>1.119141</td>\n      <td>-1.213867</td>\n      <td>0.703613</td>\n      <td>0.307617</td>\n      <td>1.119141</td>\n      <td>-1.213867</td>\n      <td>0.703613</td>\n      <td>1.645508</td>\n      <td>2.332031</td>\n      <td>0.659180</td>\n      <td>1.296875</td>\n      <td>-0.059357</td>\n      <td>0.979980</td>\n      <td>-0.000108</td>\n      <td>1.0</td>\n      <td>-0.000108</td>\n      <td>1.0</td>\n      <td>-0.258057</td>\n      <td>0.917969</td>\n      <td>-0.258057</td>\n      <td>0.917969</td>\n      <td>-0.489990</td>\n      <td>0.854492</td>\n      <td>-0.258057</td>\n      <td>0.917969</td>\n      <td>-0.258057</td>\n      <td>0.917969</td>\n      <td>-0.489990</td>\n      <td>0.854492</td>\n      <td>-0.065552</td>\n      <td>0.977539</td>\n      <td>0.307617</td>\n      <td>1.119141</td>\n      <td>-1.213867</td>\n      <td>0.703613</td>\n      <td>0.307617</td>\n      <td>1.119141</td>\n      <td>-1.213867</td>\n      <td>0.703613</td>\n      <td>1.645508</td>\n      <td>2.332031</td>\n      <td>0.659180</td>\n      <td>1.296875</td>\n      <td>-0.059357</td>\n      <td>0.979980</td>\n      <td>-0.000108</td>\n      <td>1.0</td>\n      <td>-0.000108</td>\n      <td>1.0</td>\n      <td>-0.258057</td>\n      <td>0.917969</td>\n      <td>-0.258057</td>\n      <td>0.917969</td>\n      <td>-0.489990</td>\n      <td>0.854492</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.232056</td>\n      <td>0.931152</td>\n      <td>0.192383</td>\n      <td>1.065430</td>\n      <td>0.565430</td>\n      <td>1.219727</td>\n      <td>-0.955566</td>\n      <td>0.766602</td>\n      <td>0.565430</td>\n      <td>1.219727</td>\n      <td>-0.955566</td>\n      <td>0.766602</td>\n      <td>1.903320</td>\n      <td>2.541016</td>\n      <td>0.916992</td>\n      <td>1.413086</td>\n      <td>0.198608</td>\n      <td>1.067383</td>\n      <td>0.257812</td>\n      <td>1.089844</td>\n      <td>0.257812</td>\n      <td>1.089844</td>\n      <td>-0.000118</td>\n      <td>1.0</td>\n      <td>-0.000118</td>\n      <td>1.000000</td>\n      <td>-0.232178</td>\n      <td>0.931152</td>\n      <td>-0.232056</td>\n      <td>0.931152</td>\n      <td>0.192383</td>\n      <td>1.065430</td>\n      <td>0.565430</td>\n      <td>1.219727</td>\n      <td>-0.955566</td>\n      <td>0.766602</td>\n      <td>0.565430</td>\n      <td>1.219727</td>\n      <td>-0.955566</td>\n      <td>0.766602</td>\n      <td>1.903320</td>\n      <td>2.541016</td>\n      <td>0.916992</td>\n      <td>1.413086</td>\n      <td>0.198608</td>\n      <td>1.067383</td>\n      <td>0.257812</td>\n      <td>1.089844</td>\n      <td>0.257812</td>\n      <td>1.089844</td>\n      <td>-0.000118</td>\n      <td>1.000000</td>\n      <td>-0.000118</td>\n      <td>1.0</td>\n      <td>-0.232178</td>\n      <td>0.931152</td>\n      <td>0.424561</td>\n      <td>1.144531</td>\n      <td>0.797363</td>\n      <td>1.309570</td>\n      <td>-0.723633</td>\n      <td>0.823242</td>\n      <td>0.797363</td>\n      <td>1.309570</td>\n      <td>-0.723633</td>\n      <td>0.823242</td>\n      <td>2.134766</td>\n      <td>2.728516</td>\n      <td>1.149414</td>\n      <td>1.517578</td>\n      <td>0.430664</td>\n      <td>1.146484</td>\n      <td>0.489990</td>\n      <td>1.169922</td>\n      <td>0.489990</td>\n      <td>1.169922</td>\n      <td>0.231934</td>\n      <td>1.074219</td>\n      <td>0.231934</td>\n      <td>1.074219</td>\n      <td>-0.000113</td>\n      <td>1.0</td>\n      <td>0.373047</td>\n      <td>1.144531</td>\n      <td>-1.148438</td>\n      <td>0.719727</td>\n      <td>0.373047</td>\n      <td>1.144531</td>\n      <td>-1.148438</td>\n      <td>0.719727</td>\n      <td>1.710938</td>\n      <td>2.384766</td>\n      <td>0.724609</td>\n      <td>1.326172</td>\n      <td>0.006176</td>\n      <td>1.001953</td>\n      <td>0.065430</td>\n      <td>1.022461</td>\n      <td>0.065430</td>\n      <td>1.022461</td>\n      <td>-0.192505</td>\n      <td>0.938477</td>\n      <td>-0.192505</td>\n      <td>0.938477</td>\n      <td>-0.424561</td>\n      <td>0.874023</td>\n      <td>-1.521484</td>\n      <td>0.628418</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-1.521484</td>\n      <td>0.628418</td>\n      <td>1.337891</td>\n      <td>2.083984</td>\n      <td>0.351562</td>\n      <td>1.158203</td>\n      <td>-0.366943</td>\n      <td>0.875000</td>\n      <td>-0.307617</td>\n      <td>0.893066</td>\n      <td>-0.307617</td>\n      <td>0.893066</td>\n      <td>-0.565430</td>\n      <td>0.819824</td>\n      <td>-0.565430</td>\n      <td>0.819824</td>\n      <td>-0.797363</td>\n      <td>0.763184</td>\n      <td>1.521484</td>\n      <td>1.590820</td>\n      <td>0.000000</td>\n      <td>1.00000</td>\n      <td>2.859375</td>\n      <td>3.314453</td>\n      <td>1.873047</td>\n      <td>1.842773</td>\n      <td>1.154297</td>\n      <td>1.392578</td>\n      <td>1.213867</td>\n      <td>1.420898</td>\n      <td>1.213867</td>\n      <td>1.420898</td>\n      <td>0.955566</td>\n      <td>1.304688</td>\n      <td>0.955566</td>\n      <td>1.304688</td>\n      <td>0.723633</td>\n      <td>1.214844</td>\n      <td>-1.521484</td>\n      <td>0.628418</td>\n      <td>1.337891</td>\n      <td>2.083984</td>\n      <td>0.351562</td>\n      <td>1.158203</td>\n      <td>-0.366943</td>\n      <td>0.875000</td>\n      <td>-0.307617</td>\n      <td>0.893066</td>\n      <td>-0.307617</td>\n      <td>0.893066</td>\n      <td>-0.565430</td>\n      <td>0.819824</td>\n      <td>-0.565430</td>\n      <td>0.819824</td>\n      <td>-0.797363</td>\n      <td>0.763184</td>\n      <td>2.859375</td>\n      <td>3.314453</td>\n      <td>1.873047</td>\n      <td>1.842773</td>\n      <td>1.154297</td>\n      <td>1.392578</td>\n      <td>1.213867</td>\n      <td>1.420898</td>\n      <td>1.213867</td>\n      <td>1.420898</td>\n      <td>0.955566</td>\n      <td>1.304688</td>\n      <td>0.955566</td>\n      <td>1.304688</td>\n      <td>0.723633</td>\n      <td>1.214844</td>\n      <td>-0.986328</td>\n      <td>0.556152</td>\n      <td>-1.705078</td>\n      <td>0.420166</td>\n      <td>-1.645508</td>\n      <td>0.428711</td>\n      <td>-1.645508</td>\n      <td>0.428711</td>\n      <td>-1.903320</td>\n      <td>0.393555</td>\n      <td>-1.903320</td>\n      <td>0.393555</td>\n      <td>-2.134766</td>\n      <td>0.366455</td>\n      <td>-0.718262</td>\n      <td>0.755859</td>\n      <td>-0.659180</td>\n      <td>0.770996</td>\n      <td>-0.659180</td>\n      <td>0.770996</td>\n      <td>-0.916992</td>\n      <td>0.708008</td>\n      <td>-0.916992</td>\n      <td>0.708008</td>\n      <td>-1.149414</td>\n      <td>0.659180</td>\n      <td>0.059265</td>\n      <td>1.020508</td>\n      <td>0.059265</td>\n      <td>1.020508</td>\n      <td>-0.198730</td>\n      <td>0.936523</td>\n      <td>-0.198730</td>\n      <td>0.936523</td>\n      <td>-0.430664</td>\n      <td>0.872070</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.258057</td>\n      <td>0.917969</td>\n      <td>-0.258057</td>\n      <td>0.917969</td>\n      <td>-0.489990</td>\n      <td>0.854492</td>\n      <td>-0.258057</td>\n      <td>0.917969</td>\n      <td>-0.258057</td>\n      <td>0.917969</td>\n      <td>-0.489990</td>\n      <td>0.854492</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.232056</td>\n      <td>0.931152</td>\n      <td>-0.232056</td>\n      <td>0.931152</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.058636</td>\n      <td>0.010714</td>\n      <td>-0.059075</td>\n      <td>0.174498</td>\n      <td>0.100554</td>\n      <td>-0.131889</td>\n      <td>0.225161</td>\n      <td>0.244286</td>\n      <td>0.006053</td>\n      <td>-0.035511</td>\n      <td>-0.138970</td>\n      <td>-0.300824</td>\n      <td>0.026777</td>\n      <td>-0.094231</td>\n      <td>0.070053</td>\n      <td>0.049529</td>\n      <td>0.032565</td>\n      <td>0.228549</td>\n      <td>-0.029385</td>\n      <td>0.202228</td>\n      <td>-0.196451</td>\n      <td>0.268336</td>\n      <td>-0.141619</td>\n      <td>-0.045674</td>\n      <td>0.251531</td>\n      <td>0.092396</td>\n      <td>-0.032728</td>\n      <td>0.143906</td>\n      <td>-0.253902</td>\n      <td>0.106192</td>\n      <td>0.025324</td>\n      <td>0.149742</td>\n      <td>-0.020356</td>\n      <td>-0.044923</td>\n      <td>-0.098189</td>\n      <td>-0.001313</td>\n      <td>-0.009215</td>\n      <td>0.025668</td>\n      <td>-0.242498</td>\n      <td>0.037107</td>\n      <td>-0.144369</td>\n      <td>0.629059</td>\n      <td>0.006858</td>\n      <td>-0.194995</td>\n      <td>-0.103956</td>\n      <td>-0.204220</td>\n      <td>0.076918</td>\n      <td>0.008045</td>\n      <td>-0.109790</td>\n      <td>0.213696</td>\n      <td>0.041697</td>\n      <td>0.010409</td>\n      <td>-0.092023</td>\n      <td>0.012192</td>\n      <td>0.049983</td>\n      <td>0.049678</td>\n      <td>0.025156</td>\n      <td>0.043410</td>\n      <td>0.065244</td>\n      <td>0.022134</td>\n      <td>0.073444</td>\n      <td>-0.187696</td>\n      <td>0.129353</td>\n      <td>0.149432</td>\n      <td>-0.172874</td>\n      <td>-0.044035</td>\n      <td>0.021388</td>\n      <td>0.312965</td>\n      <td>0.098221</td>\n      <td>-0.124847</td>\n      <td>0.055834</td>\n      <td>-0.125175</td>\n      <td>0.032435</td>\n      <td>0.128881</td>\n      <td>0.235134</td>\n      <td>0.140699</td>\n      <td>0.069918</td>\n      <td>-0.672858</td>\n      <td>0.009851</td>\n      <td>-0.029000</td>\n      <td>0.027043</td>\n      <td>-0.002515</td>\n      <td>1.163201</td>\n      <td>0.212105</td>\n      <td>0.192237</td>\n      <td>-0.029575</td>\n      <td>0.056986</td>\n      <td>0.041314</td>\n      <td>-0.052864</td>\n      <td>0.035421</td>\n      <td>0.022062</td>\n      <td>0.015643</td>\n      <td>-0.026877</td>\n      <td>0.081834</td>\n      <td>-0.032967</td>\n      <td>0.044244</td>\n      <td>0.107404</td>\n      <td>-0.799779</td>\n      <td>0.027633</td>\n      <td>0.050448</td>\n      <td>0.150503</td>\n      <td>-0.204648</td>\n      <td>0.100943</td>\n      <td>-0.063438</td>\n      <td>-0.115660</td>\n      <td>0.354749</td>\n      <td>0.053366</td>\n      <td>0.041253</td>\n      <td>-0.015168</td>\n      <td>-0.017427</td>\n      <td>0.013218</td>\n      <td>0.014239</td>\n      <td>-0.009076</td>\n      <td>0.149112</td>\n      <td>0.058685</td>\n      <td>-0.025347</td>\n      <td>0.161608</td>\n      <td>0.215013</td>\n      <td>-0.115882</td>\n      <td>0.378832</td>\n      <td>0.067918</td>\n      <td>0.053657</td>\n      <td>0.035087</td>\n      <td>0.019467</td>\n      <td>0.232082</td>\n      <td>0.133354</td>\n      <td>-0.099733</td>\n      <td>0.182296</td>\n      <td>-0.092707</td>\n      <td>0.146196</td>\n      <td>0.166810</td>\n      <td>-0.540334</td>\n      <td>-0.156816</td>\n      <td>-0.073869</td>\n      <td>-0.132254</td>\n      <td>0.040842</td>\n      <td>0.107210</td>\n      <td>0.153356</td>\n      <td>-0.050587</td>\n      <td>0.014879</td>\n      <td>0.059130</td>\n      <td>0.212193</td>\n      <td>-0.147306</td>\n      <td>-0.299204</td>\n      <td>0.169198</td>\n      <td>0.166360</td>\n      <td>-0.019866</td>\n      <td>-0.202794</td>\n      <td>0.077016</td>\n      <td>0.236030</td>\n      <td>0.133821</td>\n      <td>0.009633</td>\n      <td>-0.222554</td>\n      <td>-0.305388</td>\n      <td>0.086404</td>\n      <td>0.491844</td>\n      <td>0.043066</td>\n      <td>-0.104762</td>\n      <td>-0.050825</td>\n      <td>0.050087</td>\n      <td>0.077291</td>\n      <td>-0.013840</td>\n      <td>0.137109</td>\n      <td>-0.019664</td>\n      <td>0.106395</td>\n      <td>-0.061957</td>\n      <td>-0.056608</td>\n      <td>0.063086</td>\n      <td>-0.059875</td>\n      <td>0.010025</td>\n      <td>-0.054847</td>\n      <td>-0.039138</td>\n      <td>0.221609</td>\n      <td>0.029139</td>\n      <td>-0.082917</td>\n      <td>0.006236</td>\n      <td>-0.024076</td>\n      <td>-0.006428</td>\n      <td>-0.076604</td>\n      <td>0.005966</td>\n      <td>-0.058579</td>\n      <td>0.032553</td>\n      <td>-0.010159</td>\n      <td>-0.121083</td>\n      <td>0.096812</td>\n      <td>0.032963</td>\n      <td>0.008011</td>\n      <td>0.204217</td>\n      <td>-0.026281</td>\n      <td>-0.026648</td>\n      <td>-0.083031</td>\n      <td>-0.098149</td>\n      <td>-0.115228</td>\n      <td>-0.168603</td>\n      <td>0.034523</td>\n      <td>-0.229638</td>\n      <td>-0.015499</td>\n      <td>0.099428</td>\n      <td>0.316498</td>\n      <td>-0.059244</td>\n      <td>0.043453</td>\n      <td>-0.238379</td>\n      <td>-0.048957</td>\n      <td>0.029714</td>\n      <td>0.175877</td>\n      <td>-0.058545</td>\n      <td>-0.337008</td>\n      <td>0.139788</td>\n      <td>0.067161</td>\n      <td>-0.145479</td>\n      <td>0.020695</td>\n      <td>0.009076</td>\n      <td>-0.301170</td>\n      <td>0.149199</td>\n      <td>-0.067805</td>\n      <td>0.244772</td>\n      <td>0.102992</td>\n      <td>-0.486507</td>\n      <td>-0.255377</td>\n      <td>-0.370387</td>\n      <td>0.036015</td>\n      <td>0.131491</td>\n      <td>-0.050544</td>\n      <td>-0.000680</td>\n      <td>-0.045226</td>\n      <td>-0.034644</td>\n      <td>-0.040375</td>\n      <td>-0.047943</td>\n      <td>0.050667</td>\n      <td>-0.214451</td>\n      <td>0.039135</td>\n      <td>0.437912</td>\n      <td>0.147950</td>\n      <td>0.118242</td>\n      <td>0.141982</td>\n      <td>-0.171082</td>\n      <td>0.024772</td>\n      <td>-0.093310</td>\n      <td>-0.102587</td>\n      <td>-0.174004</td>\n      <td>-0.434453</td>\n      <td>-0.017174</td>\n      <td>0.117015</td>\n      <td>0.044358</td>\n      <td>-0.163389</td>\n      <td>0.056085</td>\n      <td>-0.041381</td>\n      <td>-0.064689</td>\n      <td>0.120794</td>\n      <td>-0.096786</td>\n      <td>0.035183</td>\n      <td>0.131595</td>\n      <td>-0.048496</td>\n      <td>0.094129</td>\n      <td>0.370886</td>\n      <td>-0.129678</td>\n      <td>0.024195</td>\n      <td>-0.052577</td>\n      <td>-0.168958</td>\n      <td>0.702454</td>\n      <td>-0.024561</td>\n      <td>-0.081442</td>\n      <td>0.230987</td>\n      <td>0.017456</td>\n      <td>-0.031369</td>\n      <td>-0.184837</td>\n      <td>-0.079899</td>\n      <td>-0.001711</td>\n      <td>0.116212</td>\n      <td>0.031062</td>\n      <td>-0.043929</td>\n      <td>0.058226</td>\n      <td>0.036402</td>\n      <td>-0.061046</td>\n      <td>-0.103579</td>\n      <td>0.487620</td>\n      <td>0.306732</td>\n      <td>-0.080666</td>\n      <td>-0.075670</td>\n      <td>-0.113858</td>\n      <td>0.228503</td>\n      <td>-0.137072</td>\n      <td>0.171435</td>\n      <td>0.000850</td>\n      <td>-0.092887</td>\n      <td>-0.006098</td>\n      <td>0.034071</td>\n      <td>-0.045583</td>\n      <td>-0.092175</td>\n      <td>0.170488</td>\n      <td>-0.092354</td>\n      <td>-0.064047</td>\n      <td>-0.077924</td>\n      <td>0.002353</td>\n      <td>0.212249</td>\n      <td>0.072543</td>\n      <td>0.185454</td>\n      <td>-0.137102</td>\n      <td>-0.034992</td>\n      <td>-0.068608</td>\n      <td>0.063293</td>\n      <td>-0.081857</td>\n      <td>-0.036655</td>\n      <td>-0.024877</td>\n      <td>-0.054160</td>\n      <td>-0.003148</td>\n      <td>-0.295286</td>\n      <td>-0.001143</td>\n      <td>0.103214</td>\n      <td>0.067657</td>\n      <td>-0.173375</td>\n      <td>0.118085</td>\n      <td>-0.092448</td>\n      <td>-0.070693</td>\n      <td>0.000386</td>\n      <td>0.146617</td>\n      <td>-0.203845</td>\n      <td>0.104012</td>\n      <td>0.035923</td>\n      <td>-0.065729</td>\n      <td>-0.255621</td>\n      <td>-0.018872</td>\n      <td>-0.235537</td>\n      <td>-0.139847</td>\n      <td>-0.229877</td>\n      <td>0.091465</td>\n      <td>-0.129228</td>\n      <td>0.063348</td>\n      <td>0.222444</td>\n      <td>0.167723</td>\n      <td>0.799401</td>\n      <td>0.287181</td>\n      <td>0.244295</td>\n      <td>-0.091952</td>\n      <td>0.189623</td>\n      <td>-0.004588</td>\n      <td>-0.096270</td>\n      <td>0.103717</td>\n      <td>0.122914</td>\n      <td>0.180498</td>\n      <td>-0.085829</td>\n      <td>0.037453</td>\n      <td>0.083641</td>\n      <td>-0.022197</td>\n      <td>0.278891</td>\n      <td>0.031745</td>\n      <td>0.040040</td>\n      <td>0.174038</td>\n      <td>0.237011</td>\n      <td>-0.129716</td>\n      <td>-0.090428</td>\n      <td>-0.092568</td>\n      <td>0.020385</td>\n      <td>-0.290315</td>\n      <td>-0.136768</td>\n      <td>0.094372</td>\n      <td>-0.239339</td>\n      <td>0.066254</td>\n      <td>-0.219372</td>\n      <td>-0.013867</td>\n      <td>-0.058819</td>\n      <td>0.167249</td>\n      <td>0.088246</td>\n      <td>0.139254</td>\n      <td>0.158456</td>\n      <td>-0.214302</td>\n      <td>0.059863</td>\n      <td>0.048369</td>\n      <td>0.140961</td>\n      <td>-0.052258</td>\n      <td>0.022215</td>\n      <td>0.117193</td>\n      <td>-0.198312</td>\n      <td>0.046149</td>\n      <td>0.065080</td>\n      <td>-0.131723</td>\n      <td>0.148405</td>\n      <td>-0.105136</td>\n      <td>-0.017348</td>\n      <td>0.017469</td>\n      <td>0.055048</td>\n      <td>0.062849</td>\n      <td>-0.145111</td>\n      <td>-0.090956</td>\n      <td>0.347868</td>\n      <td>-0.173408</td>\n      <td>0.239250</td>\n      <td>0.233804</td>\n      <td>-0.050822</td>\n      <td>-0.043957</td>\n      <td>-0.081328</td>\n      <td>-0.130969</td>\n      <td>0.090637</td>\n      <td>-0.005476</td>\n      <td>0.005979</td>\n      <td>-0.095841</td>\n      <td>-0.004443</td>\n      <td>-0.573272</td>\n      <td>0.052719</td>\n      <td>-0.039369</td>\n      <td>-0.043112</td>\n      <td>0.149305</td>\n      <td>0.013323</td>\n      <td>-0.167854</td>\n      <td>0.013867</td>\n      <td>0.147719</td>\n      <td>-0.077423</td>\n      <td>-0.168064</td>\n      <td>0.023604</td>\n      <td>0.206980</td>\n      <td>0.096800</td>\n      <td>0.054415</td>\n      <td>-0.025650</td>\n      <td>-0.099143</td>\n      <td>-0.107413</td>\n      <td>0.009039</td>\n      <td>-0.051372</td>\n      <td>-0.071684</td>\n      <td>-0.337983</td>\n      <td>-0.022913</td>\n      <td>-0.115648</td>\n      <td>-0.147720</td>\n      <td>0.076459</td>\n      <td>-0.032694</td>\n      <td>-0.405611</td>\n      <td>-0.093737</td>\n      <td>0.162254</td>\n      <td>-0.128603</td>\n      <td>-0.059068</td>\n      <td>0.152106</td>\n      <td>0.116981</td>\n      <td>0.034100</td>\n      <td>-0.091319</td>\n      <td>-0.047329</td>\n      <td>-0.220269</td>\n      <td>-0.181284</td>\n      <td>0.147484</td>\n      <td>-0.030241</td>\n      <td>-0.094176</td>\n      <td>-0.028390</td>\n      <td>-0.423256</td>\n      <td>-0.008630</td>\n      <td>-0.089388</td>\n      <td>-0.051802</td>\n      <td>-0.187190</td>\n      <td>-0.056604</td>\n      <td>0.039992</td>\n      <td>-0.045386</td>\n      <td>-0.159299</td>\n      <td>0.065161</td>\n      <td>-0.119729</td>\n      <td>0.084508</td>\n      <td>-0.238439</td>\n      <td>-1.245493</td>\n      <td>0.316492</td>\n      <td>0.124741</td>\n      <td>-0.094329</td>\n      <td>0.040135</td>\n      <td>-0.053970</td>\n      <td>-0.139376</td>\n      <td>0.224051</td>\n      <td>-0.015139</td>\n      <td>0.030831</td>\n      <td>-0.236975</td>\n      <td>0.074075</td>\n      <td>-0.125315</td>\n      <td>0.195212</td>\n      <td>-0.053042</td>\n      <td>-0.068390</td>\n      <td>-0.011739</td>\n      <td>0.259602</td>\n      <td>-0.029778</td>\n      <td>-0.204091</td>\n      <td>-0.161495</td>\n      <td>-0.101882</td>\n      <td>0.029371</td>\n      <td>-0.096685</td>\n      <td>0.242557</td>\n      <td>0.128139</td>\n      <td>-0.102149</td>\n      <td>-0.031175</td>\n      <td>0.023084</td>\n      <td>0.090913</td>\n      <td>-0.008003</td>\n      <td>-0.002542</td>\n      <td>-0.130041</td>\n      <td>0.017542</td>\n      <td>-0.032975</td>\n      <td>0.165599</td>\n      <td>-0.146561</td>\n      <td>-0.315001</td>\n      <td>-0.144236</td>\n      <td>0.203106</td>\n      <td>0.171700</td>\n      <td>0.188711</td>\n      <td>0.191581</td>\n      <td>0.262250</td>\n      <td>-0.171416</td>\n      <td>-0.238761</td>\n      <td>0.053921</td>\n      <td>0.084004</td>\n      <td>0.037835</td>\n      <td>-0.001942</td>\n      <td>0.284443</td>\n      <td>-0.001259</td>\n      <td>-0.081281</td>\n      <td>-0.196211</td>\n      <td>-0.050664</td>\n      <td>0.086285</td>\n      <td>-0.074844</td>\n      <td>-0.334811</td>\n      <td>-0.061478</td>\n      <td>0.093135</td>\n      <td>0.079076</td>\n      <td>0.130750</td>\n      <td>0.078746</td>\n      <td>0.000870</td>\n      <td>-0.100240</td>\n      <td>0.161888</td>\n      <td>0.394872</td>\n      <td>-0.018865</td>\n      <td>-0.040577</td>\n      <td>0.049906</td>\n      <td>0.058182</td>\n      <td>0.003539</td>\n      <td>0.048700</td>\n      <td>-0.061696</td>\n      <td>0.023898</td>\n      <td>0.019088</td>\n      <td>0.065765</td>\n      <td>0.357156</td>\n      <td>-0.058940</td>\n      <td>0.223929</td>\n      <td>-0.029957</td>\n      <td>-0.171474</td>\n      <td>-0.064602</td>\n      <td>0.085286</td>\n      <td>-0.104078</td>\n      <td>0.013963</td>\n      <td>0.001375</td>\n      <td>-0.199610</td>\n      <td>0.120454</td>\n      <td>0.029318</td>\n      <td>-0.056383</td>\n      <td>0.008633</td>\n      <td>-0.183540</td>\n      <td>0.099670</td>\n      <td>0.110537</td>\n      <td>0.078604</td>\n      <td>0.184638</td>\n      <td>0.028015</td>\n      <td>0.213840</td>\n      <td>0.150936</td>\n      <td>0.156741</td>\n      <td>-0.150252</td>\n      <td>0.356990</td>\n      <td>0.111836</td>\n      <td>-0.013739</td>\n      <td>-0.165121</td>\n      <td>0.342721</td>\n      <td>-0.046606</td>\n      <td>0.091505</td>\n      <td>-0.017373</td>\n      <td>0.065098</td>\n      <td>0.136549</td>\n      <td>-0.198081</td>\n      <td>-0.075013</td>\n      <td>0.051467</td>\n      <td>0.043023</td>\n      <td>0.005960</td>\n      <td>1.449147</td>\n      <td>0.158738</td>\n      <td>-0.154632</td>\n      <td>-0.046137</td>\n      <td>0.202906</td>\n      <td>0.046001</td>\n      <td>0.046088</td>\n      <td>-0.092596</td>\n      <td>0.124631</td>\n      <td>0.058095</td>\n      <td>0.245624</td>\n      <td>-0.043082</td>\n      <td>-0.064476</td>\n      <td>0.022578</td>\n      <td>-0.102995</td>\n      <td>-0.200476</td>\n      <td>0.017085</td>\n      <td>-0.036725</td>\n      <td>11.943683</td>\n      <td>0.009125</td>\n      <td>-0.021048</td>\n      <td>0.045698</td>\n      <td>-0.227533</td>\n      <td>-0.049069</td>\n      <td>-0.027898</td>\n      <td>0.021803</td>\n      <td>0.141909</td>\n      <td>0.143262</td>\n      <td>0.028343</td>\n      <td>0.013603</td>\n      <td>0.211610</td>\n      <td>-0.150498</td>\n      <td>-0.003177</td>\n      <td>0.041957</td>\n      <td>-0.032630</td>\n      <td>0.211481</td>\n      <td>0.146104</td>\n      <td>-0.185986</td>\n      <td>0.156548</td>\n      <td>0.081175</td>\n      <td>0.050995</td>\n      <td>0.899738</td>\n      <td>-0.161586</td>\n      <td>0.089926</td>\n      <td>-0.130796</td>\n      <td>-0.123190</td>\n      <td>0.086665</td>\n      <td>0.235653</td>\n      <td>-0.042723</td>\n      <td>0.308070</td>\n      <td>0.091798</td>\n      <td>-0.021499</td>\n      <td>-0.014432</td>\n      <td>-0.194382</td>\n      <td>-0.075963</td>\n      <td>-0.058627</td>\n      <td>0.029009</td>\n      <td>-0.040424</td>\n      <td>-0.000913</td>\n      <td>-0.051034</td>\n      <td>0.290411</td>\n      <td>-0.335732</td>\n      <td>0.170565</td>\n      <td>-0.109928</td>\n      <td>0.268078</td>\n      <td>0.145914</td>\n      <td>-0.000980</td>\n      <td>-0.030464</td>\n      <td>-0.043086</td>\n      <td>0.071312</td>\n      <td>0.179723</td>\n      <td>-0.211874</td>\n      <td>-0.034422</td>\n      <td>-0.307077</td>\n      <td>0.115623</td>\n      <td>-0.215856</td>\n      <td>0.036417</td>\n      <td>0.080084</td>\n      <td>-0.064783</td>\n      <td>-0.117606</td>\n      <td>0.127752</td>\n      <td>-0.012882</td>\n      <td>-0.205301</td>\n      <td>0.157589</td>\n      <td>0.201962</td>\n      <td>-0.038278</td>\n      <td>-0.064192</td>\n      <td>-0.049258</td>\n      <td>0.068961</td>\n      <td>-0.065382</td>\n      <td>0.155502</td>\n      <td>-0.050035</td>\n      <td>-0.160276</td>\n      <td>-0.177191</td>\n      <td>-0.369967</td>\n      <td>-0.243220</td>\n      <td>-0.121856</td>\n      <td>-0.589593</td>\n      <td>0.068379</td>\n      <td>-0.086653</td>\n      <td>-0.168638</td>\n      <td>-0.069073</td>\n      <td>-0.301360</td>\n      <td>0.129226</td>\n      <td>-0.260017</td>\n      <td>0.015221</td>\n      <td>-0.192370</td>\n      <td>0.160849</td>\n      <td>0.069858</td>\n      <td>0.022968</td>\n      <td>0.029516</td>\n      <td>-0.123990</td>\n      <td>-0.063065</td>\n      <td>0.016799</td>\n      <td>0.150945</td>\n      <td>-0.042326</td>\n      <td>0.166288</td>\n      <td>-0.224466</td>\n      <td>-0.486029</td>\n      <td>-0.094058</td>\n      <td>-0.081841</td>\n      <td>0.025652</td>\n      <td>-0.132757</td>\n      <td>0.218547</td>\n      <td>0.033704</td>\n      <td>-0.149657</td>\n      <td>0.231645</td>\n      <td>-0.030209</td>\n      <td>0.024474</td>\n      <td>0.034954</td>\n      <td>-0.164471</td>\n      <td>0.055304</td>\n      <td>0.095869</td>\n      <td>-0.088905</td>\n      <td>0.045986</td>\n      <td>0.045848</td>\n      <td>0.143413</td>\n      <td>0.154731</td>\n      <td>0.029386</td>\n      <td>0.088313</td>\n      <td>0.053296</td>\n      <td>0.047181</td>\n      <td>-0.065630</td>\n      <td>0.069703</td>\n      <td>-0.299505</td>\n      <td>0.075170</td>\n      <td>-0.240123</td>\n      <td>-0.079219</td>\n      <td>-0.327267</td>\n      <td>-0.042809</td>\n      <td>0.261281</td>\n      <td>0.048127</td>\n      <td>0.019458</td>\n      <td>0.078568</td>\n      <td>-0.075148</td>\n      <td>0.000128</td>\n      <td>0.282459</td>\n      <td>0.175455</td>\n      <td>-0.112744</td>\n      <td>0.021762</td>\n      <td>-0.165469</td>\n      <td>-0.224393</td>\n      <td>0.010819</td>\n      <td>-0.105791</td>\n      <td>0.194084</td>\n      <td>0.509446</td>\n      <td>0.113878</td>\n      <td>0.128766</td>\n      <td>-0.175802</td>\n      <td>-0.106955</td>\n      <td>-0.074288</td>\n      <td>-0.475873</td>\n      <td>0.123529</td>\n      <td>0.007325</td>\n      <td>-0.199647</td>\n      <td>0.186245</td>\n      <td>0.054156</td>\n      <td>-0.242084</td>\n      <td>0.522150</td>\n      <td>-0.427954</td>\n      <td>-0.208415</td>\n      <td>-0.076315</td>\n      <td>-0.206017</td>\n      <td>0.068706</td>\n      <td>0.202041</td>\n      <td>-0.066067</td>\n      <td>0.023081</td>\n      <td>0.112127</td>\n      <td>-0.219112</td>\n      <td>0.008264</td>\n      <td>-0.003492</td>\n      <td>0.049195</td>\n      <td>0.105896</td>\n      <td>0.144375</td>\n      <td>0.173970</td>\n      <td>0.156530</td>\n      <td>0.093464</td>\n      <td>-0.037191</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>astro-ph/9908243</td>\n      <td>Peter Meszaros</td>\n      <td>C. Weth (1, 2), P. Meszaros (1,3,4), T. Kallma...</td>\n      <td>Early X-ray/UV Line Signatures of GRB Progenit...</td>\n      <td>revisions to ApJ ms first submitted 8/21/99; u...</td>\n      <td>Astrophys.J. 534 (2000) 581-586</td>\n      <td>10.1086/308792</td>\n      <td>None</td>\n      <td>astro-ph</td>\n      <td>None</td>\n      <td>We calculate the X-ray/UV spectral line sign...</td>\n      <td>[{'version': 'v1', 'created': 'Sat, 21 Aug 199...</td>\n      <td>[[Weth, C., ], [Meszaros, P., ], [Kallman, T.,...</td>\n      <td>3.554688</td>\n      <td>1.098612</td>\n      <td>10.1086</td>\n      <td>The University of Chicago Press</td>\n      <td>4.250000</td>\n      <td>325332.0</td>\n      <td>2009-10-31</td>\n      <td>1999-08-21 18:39:47+00:00</td>\n      <td>1999-12-02 20:50:59+00:00</td>\n      <td>2009</td>\n      <td>1999</td>\n      <td>1999</td>\n      <td>10</td>\n      <td>8</td>\n      <td>12</td>\n      <td>200910</td>\n      <td>199908</td>\n      <td>199912</td>\n      <td>31</td>\n      <td>21</td>\n      <td>2</td>\n      <td>1.256947e+09</td>\n      <td>9.352608e+08</td>\n      <td>9.441679e+08</td>\n      <td>321686413</td>\n      <td>8907072.0</td>\n      <td>3</td>\n      <td>14548</td>\n      <td>10824</td>\n      <td>10927</td>\n      <td>103</td>\n      <td>26.000000</td>\n      <td>Weth</td>\n      <td>4</td>\n      <td>3.554688</td>\n      <td>astro-ph</td>\n      <td>astro-ph</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>astro</td>\n      <td>astro-ph</td>\n      <td>astro-ph</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>141917</td>\n      <td>47</td>\n      <td>106908</td>\n      <td>488</td>\n      <td>8</td>\n      <td>4</td>\n      <td>4</td>\n      <td>92</td>\n      <td>88</td>\n      <td>131</td>\n      <td>3.554688</td>\n      <td>1</td>\n      <td>3.554688</td>\n      <td>3.554688</td>\n      <td>3.554688</td>\n      <td>3.554688</td>\n      <td>NaN</td>\n      <td>3.554688</td>\n      <td>3.554688</td>\n      <td>3.554688</td>\n      <td>3.656250</td>\n      <td>28742</td>\n      <td>105086.937500</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>3.664062</td>\n      <td>1.098633</td>\n      <td>2.302734</td>\n      <td>2.996094</td>\n      <td>4.355469</td>\n      <td>3.656250</td>\n      <td>28742</td>\n      <td>105086.937500</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>3.664062</td>\n      <td>1.098633</td>\n      <td>2.302734</td>\n      <td>2.996094</td>\n      <td>4.355469</td>\n      <td>3.724609</td>\n      <td>46</td>\n      <td>171.25000</td>\n      <td>0.000000</td>\n      <td>6.652344</td>\n      <td>3.636719</td>\n      <td>1.689453</td>\n      <td>1.498047</td>\n      <td>2.753906</td>\n      <td>4.882812</td>\n      <td>2.621094</td>\n      <td>63528</td>\n      <td>166519.625000</td>\n      <td>0.0</td>\n      <td>8.304688</td>\n      <td>2.638672</td>\n      <td>1.384766</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.583984</td>\n      <td>2.615234</td>\n      <td>1343</td>\n      <td>3512.0</td>\n      <td>0.0</td>\n      <td>7.585938</td>\n      <td>2.638672</td>\n      <td>1.470703</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.611328</td>\n      <td>2.646484</td>\n      <td>290654</td>\n      <td>769432.75</td>\n      <td>0.0</td>\n      <td>10.132812</td>\n      <td>2.708984</td>\n      <td>1.405273</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.611328</td>\n      <td>2.640625</td>\n      <td>195890</td>\n      <td>517164.906250</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.773438</td>\n      <td>1.436523</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.664062</td>\n      <td>2.640625</td>\n      <td>195890</td>\n      <td>517164.906250</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.773438</td>\n      <td>1.436523</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.664062</td>\n      <td>2.714844</td>\n      <td>145677</td>\n      <td>395449.468750</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.833984</td>\n      <td>1.429688</td>\n      <td>0.693359</td>\n      <td>1.791992</td>\n      <td>3.712891</td>\n      <td>2.714844</td>\n      <td>145677</td>\n      <td>395449.468750</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.833984</td>\n      <td>1.429688</td>\n      <td>0.693359</td>\n      <td>1.791992</td>\n      <td>3.712891</td>\n      <td>3.175781</td>\n      <td>65431</td>\n      <td>207840.250000</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>3.332031</td>\n      <td>1.387695</td>\n      <td>1.098633</td>\n      <td>2.398438</td>\n      <td>4.109375</td>\n      <td>3.554688</td>\n      <td>1</td>\n      <td>3.554688</td>\n      <td>3.554688</td>\n      <td>3.554688</td>\n      <td>3.554688</td>\n      <td>NaN</td>\n      <td>3.554688</td>\n      <td>3.554688</td>\n      <td>3.554688</td>\n      <td>3.656250</td>\n      <td>28742</td>\n      <td>105086.835938</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>3.664062</td>\n      <td>1.098633</td>\n      <td>2.302734</td>\n      <td>2.996094</td>\n      <td>4.355469</td>\n      <td>3.656250</td>\n      <td>28742</td>\n      <td>105086.835938</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>3.664062</td>\n      <td>1.098633</td>\n      <td>2.302734</td>\n      <td>2.996094</td>\n      <td>4.355469</td>\n      <td>3.724609</td>\n      <td>46</td>\n      <td>171.25000</td>\n      <td>0.000000</td>\n      <td>6.652344</td>\n      <td>3.636719</td>\n      <td>1.688477</td>\n      <td>1.498047</td>\n      <td>2.753906</td>\n      <td>4.882812</td>\n      <td>2.621094</td>\n      <td>63528</td>\n      <td>166524.562500</td>\n      <td>0.0</td>\n      <td>8.304688</td>\n      <td>2.638672</td>\n      <td>1.384766</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.583984</td>\n      <td>2.615234</td>\n      <td>1343</td>\n      <td>3512.0</td>\n      <td>0.0</td>\n      <td>7.585938</td>\n      <td>2.638672</td>\n      <td>1.470703</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.611328</td>\n      <td>2.646484</td>\n      <td>290654</td>\n      <td>7.694539e+05</td>\n      <td>0.0</td>\n      <td>10.132812</td>\n      <td>2.708984</td>\n      <td>1.405273</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.611328</td>\n      <td>2.640625</td>\n      <td>195890</td>\n      <td>517176.90625</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.773438</td>\n      <td>1.436523</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.664062</td>\n      <td>2.640625</td>\n      <td>195890</td>\n      <td>517176.90625</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.773438</td>\n      <td>1.436523</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.664062</td>\n      <td>2.714844</td>\n      <td>145677</td>\n      <td>395457.687500</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.833984</td>\n      <td>1.429688</td>\n      <td>0.693359</td>\n      <td>1.791992</td>\n      <td>3.712891</td>\n      <td>2.714844</td>\n      <td>145677</td>\n      <td>395457.687500</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.833984</td>\n      <td>1.429688</td>\n      <td>0.693359</td>\n      <td>1.791992</td>\n      <td>3.712891</td>\n      <td>3.175781</td>\n      <td>65431</td>\n      <td>207841.625000</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>3.332031</td>\n      <td>1.387695</td>\n      <td>1.098633</td>\n      <td>2.398438</td>\n      <td>4.109375</td>\n      <td>-0.168701</td>\n      <td>0.964355</td>\n      <td>-0.100891</td>\n      <td>0.978516</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.100891</td>\n      <td>0.978516</td>\n      <td>0.934082</td>\n      <td>1.257812</td>\n      <td>0.940918</td>\n      <td>1.260742</td>\n      <td>0.908203</td>\n      <td>1.249023</td>\n      <td>0.915039</td>\n      <td>1.250977</td>\n      <td>0.915039</td>\n      <td>1.250977</td>\n      <td>0.840820</td>\n      <td>1.226562</td>\n      <td>0.840820</td>\n      <td>1.226562</td>\n      <td>0.378906</td>\n      <td>1.090820</td>\n      <td>0.000661</td>\n      <td>1.0</td>\n      <td>-0.168579</td>\n      <td>0.964355</td>\n      <td>-0.100891</td>\n      <td>0.978516</td>\n      <td>0.000661</td>\n      <td>1.000000</td>\n      <td>-0.100891</td>\n      <td>0.978516</td>\n      <td>0.934082</td>\n      <td>1.257812</td>\n      <td>0.940918</td>\n      <td>1.259766</td>\n      <td>0.908203</td>\n      <td>1.249023</td>\n      <td>0.915039</td>\n      <td>1.250977</td>\n      <td>0.915039</td>\n      <td>1.250977</td>\n      <td>0.840820</td>\n      <td>1.226562</td>\n      <td>0.840820</td>\n      <td>1.226562</td>\n      <td>0.378906</td>\n      <td>1.090820</td>\n      <td>0.067871</td>\n      <td>1.014648</td>\n      <td>0.168701</td>\n      <td>1.037109</td>\n      <td>0.067871</td>\n      <td>1.014648</td>\n      <td>1.102539</td>\n      <td>1.304688</td>\n      <td>1.109375</td>\n      <td>1.306641</td>\n      <td>1.077148</td>\n      <td>1.294922</td>\n      <td>1.083984</td>\n      <td>1.297852</td>\n      <td>1.083984</td>\n      <td>1.297852</td>\n      <td>1.009766</td>\n      <td>1.271484</td>\n      <td>1.009766</td>\n      <td>1.271484</td>\n      <td>0.547363</td>\n      <td>1.130859</td>\n      <td>0.169434</td>\n      <td>1.037109</td>\n      <td>0.000169</td>\n      <td>1.0</td>\n      <td>0.067871</td>\n      <td>1.014648</td>\n      <td>0.169434</td>\n      <td>1.037109</td>\n      <td>0.067871</td>\n      <td>1.014648</td>\n      <td>1.102539</td>\n      <td>1.304688</td>\n      <td>1.109375</td>\n      <td>1.306641</td>\n      <td>1.077148</td>\n      <td>1.294922</td>\n      <td>1.083984</td>\n      <td>1.297852</td>\n      <td>1.083984</td>\n      <td>1.297852</td>\n      <td>1.009766</td>\n      <td>1.271484</td>\n      <td>1.009766</td>\n      <td>1.271484</td>\n      <td>0.547363</td>\n      <td>1.130859</td>\n      <td>0.100891</td>\n      <td>1.022461</td>\n      <td>0.000000</td>\n      <td>1.00000</td>\n      <td>1.035156</td>\n      <td>1.286133</td>\n      <td>1.041992</td>\n      <td>1.288086</td>\n      <td>1.008789</td>\n      <td>1.276367</td>\n      <td>1.016602</td>\n      <td>1.279297</td>\n      <td>1.016602</td>\n      <td>1.279297</td>\n      <td>0.941895</td>\n      <td>1.253906</td>\n      <td>0.941895</td>\n      <td>1.253906</td>\n      <td>0.479736</td>\n      <td>1.115234</td>\n      <td>0.101501</td>\n      <td>1.022461</td>\n      <td>-0.067688</td>\n      <td>0.985840</td>\n      <td>0.000003</td>\n      <td>1.0</td>\n      <td>0.101501</td>\n      <td>1.022461</td>\n      <td>0.000003</td>\n      <td>1.00000</td>\n      <td>1.035156</td>\n      <td>1.286133</td>\n      <td>1.041992</td>\n      <td>1.288086</td>\n      <td>1.008789</td>\n      <td>1.276367</td>\n      <td>1.015625</td>\n      <td>1.279297</td>\n      <td>1.015625</td>\n      <td>1.279297</td>\n      <td>0.941406</td>\n      <td>1.253906</td>\n      <td>0.941406</td>\n      <td>1.253906</td>\n      <td>0.479736</td>\n      <td>1.115234</td>\n      <td>-0.100891</td>\n      <td>0.978516</td>\n      <td>0.934082</td>\n      <td>1.257812</td>\n      <td>0.940918</td>\n      <td>1.260742</td>\n      <td>0.908203</td>\n      <td>1.249023</td>\n      <td>0.915039</td>\n      <td>1.250977</td>\n      <td>0.915039</td>\n      <td>1.250977</td>\n      <td>0.840820</td>\n      <td>1.226562</td>\n      <td>0.840820</td>\n      <td>1.226562</td>\n      <td>0.378906</td>\n      <td>1.090820</td>\n      <td>0.000661</td>\n      <td>1.000000</td>\n      <td>-0.168579</td>\n      <td>0.964355</td>\n      <td>-0.100891</td>\n      <td>0.978516</td>\n      <td>0.000661</td>\n      <td>1.0</td>\n      <td>-0.100891</td>\n      <td>0.978516</td>\n      <td>0.934082</td>\n      <td>1.257812</td>\n      <td>0.940918</td>\n      <td>1.259766</td>\n      <td>0.908203</td>\n      <td>1.249023</td>\n      <td>0.915039</td>\n      <td>1.250977</td>\n      <td>0.915039</td>\n      <td>1.250977</td>\n      <td>0.840820</td>\n      <td>1.226562</td>\n      <td>0.840820</td>\n      <td>1.226562</td>\n      <td>0.378906</td>\n      <td>1.090820</td>\n      <td>1.035156</td>\n      <td>1.286133</td>\n      <td>1.041992</td>\n      <td>1.288086</td>\n      <td>1.008789</td>\n      <td>1.276367</td>\n      <td>1.016602</td>\n      <td>1.279297</td>\n      <td>1.016602</td>\n      <td>1.279297</td>\n      <td>0.941895</td>\n      <td>1.253906</td>\n      <td>0.941895</td>\n      <td>1.253906</td>\n      <td>0.479736</td>\n      <td>1.115234</td>\n      <td>0.101501</td>\n      <td>1.022461</td>\n      <td>-0.067688</td>\n      <td>0.985840</td>\n      <td>0.000003</td>\n      <td>1.000000</td>\n      <td>0.101501</td>\n      <td>1.022461</td>\n      <td>0.000003</td>\n      <td>1.0</td>\n      <td>1.035156</td>\n      <td>1.286133</td>\n      <td>1.041992</td>\n      <td>1.288086</td>\n      <td>1.008789</td>\n      <td>1.276367</td>\n      <td>1.015625</td>\n      <td>1.279297</td>\n      <td>1.015625</td>\n      <td>1.279297</td>\n      <td>0.941406</td>\n      <td>1.253906</td>\n      <td>0.941406</td>\n      <td>1.253906</td>\n      <td>0.479736</td>\n      <td>1.115234</td>\n      <td>0.006626</td>\n      <td>1.001953</td>\n      <td>-0.026047</td>\n      <td>0.992676</td>\n      <td>-0.018875</td>\n      <td>0.994629</td>\n      <td>-0.018875</td>\n      <td>0.994629</td>\n      <td>-0.093384</td>\n      <td>0.975098</td>\n      <td>-0.093384</td>\n      <td>0.975098</td>\n      <td>-0.555176</td>\n      <td>0.867188</td>\n      <td>-0.933594</td>\n      <td>0.794922</td>\n      <td>-1.102539</td>\n      <td>0.766602</td>\n      <td>-1.035156</td>\n      <td>0.777832</td>\n      <td>-0.933594</td>\n      <td>0.794922</td>\n      <td>-1.035156</td>\n      <td>0.777832</td>\n      <td>-0.000078</td>\n      <td>1.0</td>\n      <td>0.006557</td>\n      <td>1.001953</td>\n      <td>-0.026123</td>\n      <td>0.992676</td>\n      <td>-0.018936</td>\n      <td>0.994629</td>\n      <td>-0.018936</td>\n      <td>0.994629</td>\n      <td>-0.093445</td>\n      <td>0.974609</td>\n      <td>-0.093445</td>\n      <td>0.974609</td>\n      <td>-0.555176</td>\n      <td>0.867188</td>\n      <td>-0.032684</td>\n      <td>0.991211</td>\n      <td>-0.025497</td>\n      <td>0.993164</td>\n      <td>-0.025497</td>\n      <td>0.993164</td>\n      <td>-0.099976</td>\n      <td>0.973145</td>\n      <td>-0.099976</td>\n      <td>0.973145</td>\n      <td>-0.562012</td>\n      <td>0.865234</td>\n      <td>-0.939941</td>\n      <td>0.793457</td>\n      <td>-1.109375</td>\n      <td>0.765137</td>\n      <td>-1.041992</td>\n      <td>0.776367</td>\n      <td>-0.939941</td>\n      <td>0.793457</td>\n      <td>-1.041992</td>\n      <td>0.776367</td>\n      <td>-0.006702</td>\n      <td>0.998047</td>\n      <td>-0.000069</td>\n      <td>1.0</td>\n      <td>-0.032745</td>\n      <td>0.991211</td>\n      <td>-0.025558</td>\n      <td>0.993164</td>\n      <td>-0.025558</td>\n      <td>0.993164</td>\n      <td>-0.100037</td>\n      <td>0.973145</td>\n      <td>-0.100037</td>\n      <td>0.973145</td>\n      <td>-0.562012</td>\n      <td>0.865234</td>\n      <td>0.007168</td>\n      <td>1.001953</td>\n      <td>0.007168</td>\n      <td>1.001953</td>\n      <td>-0.067322</td>\n      <td>0.981934</td>\n      <td>-0.067322</td>\n      <td>0.981934</td>\n      <td>-0.529297</td>\n      <td>0.873047</td>\n      <td>-0.907227</td>\n      <td>0.800781</td>\n      <td>-1.077148</td>\n      <td>0.771973</td>\n      <td>-1.008789</td>\n      <td>0.783203</td>\n      <td>-0.907227</td>\n      <td>0.800781</td>\n      <td>-1.008789</td>\n      <td>0.783203</td>\n      <td>0.025970</td>\n      <td>1.006836</td>\n      <td>0.032593</td>\n      <td>1.008789</td>\n      <td>-0.000073</td>\n      <td>1.0</td>\n      <td>0.007107</td>\n      <td>1.001953</td>\n      <td>0.007107</td>\n      <td>1.001953</td>\n      <td>-0.067383</td>\n      <td>0.981934</td>\n      <td>-0.067383</td>\n      <td>0.981934</td>\n      <td>-0.529297</td>\n      <td>0.873047</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.074463</td>\n      <td>0.979980</td>\n      <td>-0.074463</td>\n      <td>0.979980</td>\n      <td>-0.536621</td>\n      <td>0.871582</td>\n      <td>-0.914551</td>\n      <td>0.799316</td>\n      <td>-1.083984</td>\n      <td>0.770508</td>\n      <td>-1.016602</td>\n      <td>0.781738</td>\n      <td>-0.914551</td>\n      <td>0.799316</td>\n      <td>-1.016602</td>\n      <td>0.781738</td>\n      <td>0.018799</td>\n      <td>1.004883</td>\n      <td>0.025436</td>\n      <td>1.006836</td>\n      <td>-0.007240</td>\n      <td>0.998047</td>\n      <td>-0.000061</td>\n      <td>1.0</td>\n      <td>-0.000061</td>\n      <td>1.0</td>\n      <td>-0.074524</td>\n      <td>0.979980</td>\n      <td>-0.074524</td>\n      <td>0.979980</td>\n      <td>-0.536621</td>\n      <td>0.871582</td>\n      <td>-0.074463</td>\n      <td>0.979980</td>\n      <td>-0.074463</td>\n      <td>0.979980</td>\n      <td>-0.536621</td>\n      <td>0.871582</td>\n      <td>-0.914551</td>\n      <td>0.799316</td>\n      <td>-1.083984</td>\n      <td>0.770508</td>\n      <td>-1.016602</td>\n      <td>0.781738</td>\n      <td>-0.914551</td>\n      <td>0.799316</td>\n      <td>-1.016602</td>\n      <td>0.781738</td>\n      <td>0.018799</td>\n      <td>1.004883</td>\n      <td>0.025436</td>\n      <td>1.006836</td>\n      <td>-0.007240</td>\n      <td>0.998047</td>\n      <td>-0.000061</td>\n      <td>1.0</td>\n      <td>-0.000061</td>\n      <td>1.0</td>\n      <td>-0.074524</td>\n      <td>0.979980</td>\n      <td>-0.074524</td>\n      <td>0.979980</td>\n      <td>-0.536621</td>\n      <td>0.871582</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.461914</td>\n      <td>0.889160</td>\n      <td>-0.840332</td>\n      <td>0.815430</td>\n      <td>-1.009766</td>\n      <td>0.786133</td>\n      <td>-0.941406</td>\n      <td>0.797852</td>\n      <td>-0.840332</td>\n      <td>0.815430</td>\n      <td>-0.941406</td>\n      <td>0.797852</td>\n      <td>0.093262</td>\n      <td>1.025391</td>\n      <td>0.099915</td>\n      <td>1.027344</td>\n      <td>0.067261</td>\n      <td>1.018555</td>\n      <td>0.074402</td>\n      <td>1.020508</td>\n      <td>0.074402</td>\n      <td>1.020508</td>\n      <td>-0.000057</td>\n      <td>1.0</td>\n      <td>-0.000057</td>\n      <td>1.000000</td>\n      <td>-0.461914</td>\n      <td>0.889160</td>\n      <td>-0.461914</td>\n      <td>0.889160</td>\n      <td>-0.840332</td>\n      <td>0.815430</td>\n      <td>-1.009766</td>\n      <td>0.786133</td>\n      <td>-0.941406</td>\n      <td>0.797852</td>\n      <td>-0.840332</td>\n      <td>0.815430</td>\n      <td>-0.941406</td>\n      <td>0.797852</td>\n      <td>0.093262</td>\n      <td>1.025391</td>\n      <td>0.099915</td>\n      <td>1.027344</td>\n      <td>0.067261</td>\n      <td>1.018555</td>\n      <td>0.074402</td>\n      <td>1.020508</td>\n      <td>0.074402</td>\n      <td>1.020508</td>\n      <td>-0.000057</td>\n      <td>1.000000</td>\n      <td>-0.000057</td>\n      <td>1.0</td>\n      <td>-0.461914</td>\n      <td>0.889160</td>\n      <td>-0.378174</td>\n      <td>0.916992</td>\n      <td>-0.547363</td>\n      <td>0.884277</td>\n      <td>-0.479736</td>\n      <td>0.896973</td>\n      <td>-0.378174</td>\n      <td>0.916992</td>\n      <td>-0.479736</td>\n      <td>0.896973</td>\n      <td>0.555176</td>\n      <td>1.153320</td>\n      <td>0.562012</td>\n      <td>1.155273</td>\n      <td>0.529297</td>\n      <td>1.145508</td>\n      <td>0.536133</td>\n      <td>1.147461</td>\n      <td>0.536133</td>\n      <td>1.147461</td>\n      <td>0.461914</td>\n      <td>1.124023</td>\n      <td>0.461914</td>\n      <td>1.124023</td>\n      <td>-0.000021</td>\n      <td>1.0</td>\n      <td>-0.169189</td>\n      <td>0.964355</td>\n      <td>-0.101501</td>\n      <td>0.978027</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.101501</td>\n      <td>0.978027</td>\n      <td>0.933594</td>\n      <td>1.257812</td>\n      <td>0.939941</td>\n      <td>1.259766</td>\n      <td>0.907227</td>\n      <td>1.249023</td>\n      <td>0.914551</td>\n      <td>1.250977</td>\n      <td>0.914551</td>\n      <td>1.250977</td>\n      <td>0.839844</td>\n      <td>1.226562</td>\n      <td>0.839844</td>\n      <td>1.226562</td>\n      <td>0.378174</td>\n      <td>1.090820</td>\n      <td>0.067688</td>\n      <td>1.014648</td>\n      <td>0.169189</td>\n      <td>1.037109</td>\n      <td>0.067688</td>\n      <td>1.014648</td>\n      <td>1.102539</td>\n      <td>1.304688</td>\n      <td>1.109375</td>\n      <td>1.306641</td>\n      <td>1.076172</td>\n      <td>1.294922</td>\n      <td>1.083984</td>\n      <td>1.297852</td>\n      <td>1.083984</td>\n      <td>1.297852</td>\n      <td>1.009766</td>\n      <td>1.271484</td>\n      <td>1.009766</td>\n      <td>1.271484</td>\n      <td>0.547363</td>\n      <td>1.130859</td>\n      <td>0.101501</td>\n      <td>1.022461</td>\n      <td>0.000000</td>\n      <td>1.00000</td>\n      <td>1.035156</td>\n      <td>1.286133</td>\n      <td>1.041992</td>\n      <td>1.288086</td>\n      <td>1.008789</td>\n      <td>1.276367</td>\n      <td>1.015625</td>\n      <td>1.279297</td>\n      <td>1.015625</td>\n      <td>1.279297</td>\n      <td>0.941406</td>\n      <td>1.253906</td>\n      <td>0.941406</td>\n      <td>1.253906</td>\n      <td>0.479736</td>\n      <td>1.115234</td>\n      <td>-0.101501</td>\n      <td>0.978027</td>\n      <td>0.933594</td>\n      <td>1.257812</td>\n      <td>0.939941</td>\n      <td>1.259766</td>\n      <td>0.907227</td>\n      <td>1.249023</td>\n      <td>0.914551</td>\n      <td>1.250977</td>\n      <td>0.914551</td>\n      <td>1.250977</td>\n      <td>0.839844</td>\n      <td>1.226562</td>\n      <td>0.839844</td>\n      <td>1.226562</td>\n      <td>0.378174</td>\n      <td>1.090820</td>\n      <td>1.035156</td>\n      <td>1.286133</td>\n      <td>1.041992</td>\n      <td>1.288086</td>\n      <td>1.008789</td>\n      <td>1.276367</td>\n      <td>1.015625</td>\n      <td>1.279297</td>\n      <td>1.015625</td>\n      <td>1.279297</td>\n      <td>0.941406</td>\n      <td>1.253906</td>\n      <td>0.941406</td>\n      <td>1.253906</td>\n      <td>0.479736</td>\n      <td>1.115234</td>\n      <td>0.006634</td>\n      <td>1.001953</td>\n      <td>-0.026047</td>\n      <td>0.992676</td>\n      <td>-0.018860</td>\n      <td>0.994629</td>\n      <td>-0.018860</td>\n      <td>0.994629</td>\n      <td>-0.093323</td>\n      <td>0.975098</td>\n      <td>-0.093323</td>\n      <td>0.975098</td>\n      <td>-0.555176</td>\n      <td>0.867188</td>\n      <td>-0.032684</td>\n      <td>0.991211</td>\n      <td>-0.025497</td>\n      <td>0.993164</td>\n      <td>-0.025497</td>\n      <td>0.993164</td>\n      <td>-0.099976</td>\n      <td>0.973145</td>\n      <td>-0.099976</td>\n      <td>0.973145</td>\n      <td>-0.562012</td>\n      <td>0.865234</td>\n      <td>0.007179</td>\n      <td>1.001953</td>\n      <td>0.007179</td>\n      <td>1.001953</td>\n      <td>-0.067322</td>\n      <td>0.981934</td>\n      <td>-0.067322</td>\n      <td>0.981934</td>\n      <td>-0.529297</td>\n      <td>0.873535</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.074463</td>\n      <td>0.979980</td>\n      <td>-0.074463</td>\n      <td>0.979980</td>\n      <td>-0.536133</td>\n      <td>0.871582</td>\n      <td>-0.074463</td>\n      <td>0.979980</td>\n      <td>-0.074463</td>\n      <td>0.979980</td>\n      <td>-0.536133</td>\n      <td>0.871582</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.461914</td>\n      <td>0.889648</td>\n      <td>-0.461914</td>\n      <td>0.889648</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.014395</td>\n      <td>0.040316</td>\n      <td>-0.006585</td>\n      <td>0.126848</td>\n      <td>0.212726</td>\n      <td>0.070803</td>\n      <td>0.241755</td>\n      <td>0.267412</td>\n      <td>-0.064004</td>\n      <td>-0.098026</td>\n      <td>-0.104129</td>\n      <td>-0.278143</td>\n      <td>-0.045478</td>\n      <td>-0.180248</td>\n      <td>0.162043</td>\n      <td>0.022745</td>\n      <td>0.039454</td>\n      <td>0.249000</td>\n      <td>0.066742</td>\n      <td>0.067162</td>\n      <td>-0.166020</td>\n      <td>0.239773</td>\n      <td>-0.052797</td>\n      <td>-0.072319</td>\n      <td>0.256792</td>\n      <td>0.137566</td>\n      <td>-0.085245</td>\n      <td>0.269796</td>\n      <td>-0.185596</td>\n      <td>0.050790</td>\n      <td>0.060654</td>\n      <td>0.019494</td>\n      <td>-0.013807</td>\n      <td>-0.025799</td>\n      <td>-0.139750</td>\n      <td>-0.012520</td>\n      <td>-0.024701</td>\n      <td>0.014373</td>\n      <td>-0.240063</td>\n      <td>0.095081</td>\n      <td>-0.253646</td>\n      <td>0.697968</td>\n      <td>0.111205</td>\n      <td>-0.203138</td>\n      <td>0.005281</td>\n      <td>-0.266690</td>\n      <td>0.091751</td>\n      <td>0.135966</td>\n      <td>-0.105726</td>\n      <td>0.140257</td>\n      <td>-0.127193</td>\n      <td>-0.068365</td>\n      <td>-0.097209</td>\n      <td>0.085209</td>\n      <td>-0.004637</td>\n      <td>0.020475</td>\n      <td>0.044677</td>\n      <td>0.073270</td>\n      <td>0.017781</td>\n      <td>0.129403</td>\n      <td>0.017203</td>\n      <td>-0.092696</td>\n      <td>0.130777</td>\n      <td>0.266385</td>\n      <td>-0.089212</td>\n      <td>-0.030700</td>\n      <td>0.016042</td>\n      <td>0.238878</td>\n      <td>-0.016649</td>\n      <td>-0.135385</td>\n      <td>0.039690</td>\n      <td>0.004393</td>\n      <td>-0.035969</td>\n      <td>0.105605</td>\n      <td>0.194074</td>\n      <td>0.008125</td>\n      <td>0.031153</td>\n      <td>-0.883288</td>\n      <td>-0.005592</td>\n      <td>0.035557</td>\n      <td>0.007486</td>\n      <td>0.086736</td>\n      <td>1.020782</td>\n      <td>0.146921</td>\n      <td>0.084700</td>\n      <td>-0.114722</td>\n      <td>0.002327</td>\n      <td>0.059267</td>\n      <td>0.023035</td>\n      <td>0.099756</td>\n      <td>-0.049315</td>\n      <td>0.000573</td>\n      <td>0.046366</td>\n      <td>-0.028310</td>\n      <td>-0.049696</td>\n      <td>0.060778</td>\n      <td>0.167912</td>\n      <td>-0.898141</td>\n      <td>-0.032359</td>\n      <td>0.047238</td>\n      <td>0.150196</td>\n      <td>-0.177945</td>\n      <td>0.095856</td>\n      <td>-0.055000</td>\n      <td>-0.187189</td>\n      <td>0.262604</td>\n      <td>0.134465</td>\n      <td>0.088091</td>\n      <td>-0.049437</td>\n      <td>-0.083170</td>\n      <td>-0.023037</td>\n      <td>-0.068187</td>\n      <td>-0.111820</td>\n      <td>0.220422</td>\n      <td>0.070814</td>\n      <td>-0.023280</td>\n      <td>0.192262</td>\n      <td>0.173185</td>\n      <td>-0.119966</td>\n      <td>0.318646</td>\n      <td>0.118725</td>\n      <td>0.066591</td>\n      <td>0.089681</td>\n      <td>-0.034637</td>\n      <td>0.179630</td>\n      <td>0.213383</td>\n      <td>-0.141436</td>\n      <td>0.154860</td>\n      <td>-0.064743</td>\n      <td>0.185055</td>\n      <td>0.077914</td>\n      <td>-0.570457</td>\n      <td>-0.169875</td>\n      <td>-0.086884</td>\n      <td>-0.011990</td>\n      <td>-0.002227</td>\n      <td>0.063539</td>\n      <td>0.116127</td>\n      <td>-0.022118</td>\n      <td>0.059615</td>\n      <td>0.151680</td>\n      <td>0.312870</td>\n      <td>-0.080331</td>\n      <td>-0.185251</td>\n      <td>0.175763</td>\n      <td>0.068033</td>\n      <td>-0.027991</td>\n      <td>-0.290627</td>\n      <td>0.001316</td>\n      <td>0.197334</td>\n      <td>0.173489</td>\n      <td>-0.009123</td>\n      <td>-0.255338</td>\n      <td>-0.276321</td>\n      <td>0.010751</td>\n      <td>0.323599</td>\n      <td>0.013591</td>\n      <td>-0.194354</td>\n      <td>-0.098156</td>\n      <td>0.085560</td>\n      <td>-0.013342</td>\n      <td>-0.056274</td>\n      <td>0.211737</td>\n      <td>-0.078829</td>\n      <td>0.143575</td>\n      <td>-0.016382</td>\n      <td>-0.113807</td>\n      <td>0.161036</td>\n      <td>-0.000522</td>\n      <td>-0.010081</td>\n      <td>-0.096027</td>\n      <td>-0.045169</td>\n      <td>0.205903</td>\n      <td>0.101635</td>\n      <td>-0.072784</td>\n      <td>-0.035826</td>\n      <td>-0.054418</td>\n      <td>-0.015063</td>\n      <td>-0.017550</td>\n      <td>0.041290</td>\n      <td>-0.098889</td>\n      <td>0.006737</td>\n      <td>-0.031829</td>\n      <td>-0.178458</td>\n      <td>0.072357</td>\n      <td>-0.011122</td>\n      <td>-0.114164</td>\n      <td>0.076534</td>\n      <td>-0.012500</td>\n      <td>-0.056580</td>\n      <td>0.054856</td>\n      <td>-0.035538</td>\n      <td>-0.118811</td>\n      <td>-0.221217</td>\n      <td>0.070619</td>\n      <td>-0.299445</td>\n      <td>0.028102</td>\n      <td>0.048711</td>\n      <td>0.318984</td>\n      <td>-0.028681</td>\n      <td>0.082179</td>\n      <td>-0.241116</td>\n      <td>0.132850</td>\n      <td>0.053384</td>\n      <td>0.305334</td>\n      <td>-0.076399</td>\n      <td>-0.382406</td>\n      <td>0.124662</td>\n      <td>-0.000141</td>\n      <td>-0.153085</td>\n      <td>0.013562</td>\n      <td>0.131116</td>\n      <td>-0.305380</td>\n      <td>0.137779</td>\n      <td>-0.123351</td>\n      <td>0.113804</td>\n      <td>0.023366</td>\n      <td>-0.691986</td>\n      <td>-0.226234</td>\n      <td>-0.439358</td>\n      <td>0.156078</td>\n      <td>0.122454</td>\n      <td>-0.069900</td>\n      <td>0.014667</td>\n      <td>-0.093751</td>\n      <td>0.159044</td>\n      <td>-0.017086</td>\n      <td>-0.039828</td>\n      <td>0.137597</td>\n      <td>-0.134000</td>\n      <td>0.075241</td>\n      <td>0.460601</td>\n      <td>0.194682</td>\n      <td>0.106394</td>\n      <td>0.162251</td>\n      <td>-0.102255</td>\n      <td>0.033476</td>\n      <td>-0.023335</td>\n      <td>-0.084755</td>\n      <td>-0.162164</td>\n      <td>-0.383545</td>\n      <td>0.146981</td>\n      <td>0.046603</td>\n      <td>0.103725</td>\n      <td>-0.108111</td>\n      <td>0.102718</td>\n      <td>-0.021282</td>\n      <td>-0.076587</td>\n      <td>0.044816</td>\n      <td>0.003569</td>\n      <td>-0.005044</td>\n      <td>0.158080</td>\n      <td>-0.036099</td>\n      <td>0.006859</td>\n      <td>0.275014</td>\n      <td>-0.085838</td>\n      <td>0.033226</td>\n      <td>-0.072988</td>\n      <td>-0.239376</td>\n      <td>0.588016</td>\n      <td>-0.049332</td>\n      <td>-0.138504</td>\n      <td>0.101199</td>\n      <td>0.034493</td>\n      <td>0.032310</td>\n      <td>-0.264636</td>\n      <td>-0.105204</td>\n      <td>-0.040831</td>\n      <td>0.109986</td>\n      <td>0.060860</td>\n      <td>-0.030608</td>\n      <td>0.004095</td>\n      <td>0.141148</td>\n      <td>-0.083403</td>\n      <td>-0.120084</td>\n      <td>0.314562</td>\n      <td>0.314164</td>\n      <td>0.013409</td>\n      <td>-0.059688</td>\n      <td>-0.075537</td>\n      <td>0.334556</td>\n      <td>-0.105893</td>\n      <td>0.028918</td>\n      <td>0.010418</td>\n      <td>-0.023963</td>\n      <td>-0.046750</td>\n      <td>0.004263</td>\n      <td>-0.138153</td>\n      <td>-0.079462</td>\n      <td>0.132184</td>\n      <td>-0.119096</td>\n      <td>-0.074472</td>\n      <td>-0.074425</td>\n      <td>0.039214</td>\n      <td>0.126273</td>\n      <td>0.043046</td>\n      <td>0.176546</td>\n      <td>0.026117</td>\n      <td>0.049155</td>\n      <td>-0.032219</td>\n      <td>0.038669</td>\n      <td>-0.068398</td>\n      <td>0.004747</td>\n      <td>0.048858</td>\n      <td>0.003641</td>\n      <td>0.043219</td>\n      <td>-0.279176</td>\n      <td>0.090642</td>\n      <td>0.193314</td>\n      <td>0.061296</td>\n      <td>-0.109620</td>\n      <td>0.101936</td>\n      <td>-0.032699</td>\n      <td>-0.070150</td>\n      <td>-0.077110</td>\n      <td>0.178501</td>\n      <td>-0.188987</td>\n      <td>-0.040373</td>\n      <td>-0.107662</td>\n      <td>-0.129262</td>\n      <td>-0.210910</td>\n      <td>-0.047515</td>\n      <td>-0.215605</td>\n      <td>-0.128098</td>\n      <td>-0.242442</td>\n      <td>0.137022</td>\n      <td>-0.169469</td>\n      <td>0.104505</td>\n      <td>0.274242</td>\n      <td>0.228840</td>\n      <td>0.659030</td>\n      <td>0.473674</td>\n      <td>0.191182</td>\n      <td>-0.052772</td>\n      <td>0.183293</td>\n      <td>-0.009096</td>\n      <td>-0.136143</td>\n      <td>0.134588</td>\n      <td>0.214055</td>\n      <td>0.031961</td>\n      <td>-0.084362</td>\n      <td>-0.019983</td>\n      <td>0.056233</td>\n      <td>0.041232</td>\n      <td>0.187793</td>\n      <td>0.034025</td>\n      <td>0.013881</td>\n      <td>0.126630</td>\n      <td>0.313428</td>\n      <td>-0.203654</td>\n      <td>-0.141390</td>\n      <td>-0.106300</td>\n      <td>-0.041599</td>\n      <td>-0.254144</td>\n      <td>-0.114789</td>\n      <td>0.216905</td>\n      <td>-0.174713</td>\n      <td>0.047400</td>\n      <td>-0.276530</td>\n      <td>0.135743</td>\n      <td>-0.015669</td>\n      <td>0.212420</td>\n      <td>0.064641</td>\n      <td>0.046356</td>\n      <td>0.086537</td>\n      <td>-0.300819</td>\n      <td>0.133173</td>\n      <td>-0.029847</td>\n      <td>0.144262</td>\n      <td>0.038215</td>\n      <td>-0.018609</td>\n      <td>0.144560</td>\n      <td>-0.080554</td>\n      <td>0.099522</td>\n      <td>0.082846</td>\n      <td>-0.104098</td>\n      <td>0.163453</td>\n      <td>-0.105722</td>\n      <td>-0.005788</td>\n      <td>-0.079458</td>\n      <td>0.056530</td>\n      <td>0.134542</td>\n      <td>-0.117285</td>\n      <td>-0.009504</td>\n      <td>0.333778</td>\n      <td>-0.093358</td>\n      <td>0.198542</td>\n      <td>0.162139</td>\n      <td>-0.148620</td>\n      <td>0.014203</td>\n      <td>-0.000961</td>\n      <td>-0.143421</td>\n      <td>0.018741</td>\n      <td>-0.023881</td>\n      <td>0.017504</td>\n      <td>-0.033296</td>\n      <td>0.043634</td>\n      <td>-0.530796</td>\n      <td>0.016288</td>\n      <td>-0.041590</td>\n      <td>-0.121004</td>\n      <td>0.256529</td>\n      <td>0.055697</td>\n      <td>-0.096192</td>\n      <td>-0.044014</td>\n      <td>0.176081</td>\n      <td>-0.094061</td>\n      <td>-0.120798</td>\n      <td>-0.024787</td>\n      <td>0.217521</td>\n      <td>0.074970</td>\n      <td>0.030706</td>\n      <td>-0.016892</td>\n      <td>-0.058280</td>\n      <td>-0.024367</td>\n      <td>-0.010494</td>\n      <td>-0.087810</td>\n      <td>-0.152776</td>\n      <td>-0.413415</td>\n      <td>-0.098316</td>\n      <td>-0.120644</td>\n      <td>-0.166903</td>\n      <td>0.086750</td>\n      <td>-0.031066</td>\n      <td>-0.417200</td>\n      <td>-0.031581</td>\n      <td>0.078387</td>\n      <td>-0.099511</td>\n      <td>0.016763</td>\n      <td>0.107805</td>\n      <td>0.152398</td>\n      <td>0.009406</td>\n      <td>-0.141831</td>\n      <td>-0.021426</td>\n      <td>-0.239185</td>\n      <td>-0.113952</td>\n      <td>0.039260</td>\n      <td>-0.066326</td>\n      <td>-0.128038</td>\n      <td>-0.101552</td>\n      <td>-0.372797</td>\n      <td>-0.023403</td>\n      <td>-0.097901</td>\n      <td>-0.039467</td>\n      <td>-0.150162</td>\n      <td>-0.075887</td>\n      <td>0.080679</td>\n      <td>-0.151617</td>\n      <td>-0.170695</td>\n      <td>0.000198</td>\n      <td>-0.042573</td>\n      <td>0.073403</td>\n      <td>-0.279140</td>\n      <td>-1.319828</td>\n      <td>0.277992</td>\n      <td>0.139795</td>\n      <td>-0.015011</td>\n      <td>-0.009274</td>\n      <td>-0.042717</td>\n      <td>-0.184760</td>\n      <td>0.291450</td>\n      <td>0.024838</td>\n      <td>0.028786</td>\n      <td>-0.244488</td>\n      <td>0.077660</td>\n      <td>-0.054341</td>\n      <td>0.235243</td>\n      <td>0.013562</td>\n      <td>-0.009636</td>\n      <td>-0.077241</td>\n      <td>0.207004</td>\n      <td>-0.115475</td>\n      <td>-0.193482</td>\n      <td>-0.142165</td>\n      <td>-0.115125</td>\n      <td>-0.034100</td>\n      <td>-0.115934</td>\n      <td>0.287656</td>\n      <td>0.145933</td>\n      <td>-0.093721</td>\n      <td>-0.035607</td>\n      <td>0.061535</td>\n      <td>0.120488</td>\n      <td>0.001517</td>\n      <td>-0.049003</td>\n      <td>-0.010541</td>\n      <td>0.026895</td>\n      <td>0.003773</td>\n      <td>0.213621</td>\n      <td>-0.145385</td>\n      <td>-0.092696</td>\n      <td>-0.139265</td>\n      <td>0.183296</td>\n      <td>0.240703</td>\n      <td>0.191990</td>\n      <td>0.190071</td>\n      <td>0.378330</td>\n      <td>-0.253577</td>\n      <td>-0.093517</td>\n      <td>0.072363</td>\n      <td>0.100284</td>\n      <td>0.115021</td>\n      <td>0.015604</td>\n      <td>0.025725</td>\n      <td>0.001527</td>\n      <td>-0.058733</td>\n      <td>-0.178453</td>\n      <td>0.039573</td>\n      <td>0.127935</td>\n      <td>0.004134</td>\n      <td>-0.256633</td>\n      <td>-0.137457</td>\n      <td>0.030484</td>\n      <td>0.081752</td>\n      <td>0.053804</td>\n      <td>0.018789</td>\n      <td>-0.000192</td>\n      <td>-0.123750</td>\n      <td>0.157126</td>\n      <td>0.309610</td>\n      <td>-0.003232</td>\n      <td>0.046127</td>\n      <td>0.078381</td>\n      <td>0.061917</td>\n      <td>0.015205</td>\n      <td>0.121157</td>\n      <td>-0.103559</td>\n      <td>-0.020847</td>\n      <td>-0.047766</td>\n      <td>-0.020523</td>\n      <td>0.301164</td>\n      <td>-0.085818</td>\n      <td>0.247234</td>\n      <td>-0.023593</td>\n      <td>-0.147539</td>\n      <td>-0.038534</td>\n      <td>0.039494</td>\n      <td>-0.194933</td>\n      <td>0.007838</td>\n      <td>-0.099993</td>\n      <td>-0.166663</td>\n      <td>0.063348</td>\n      <td>-0.041038</td>\n      <td>0.000037</td>\n      <td>-0.008255</td>\n      <td>-0.187349</td>\n      <td>0.238425</td>\n      <td>0.068897</td>\n      <td>0.083307</td>\n      <td>0.144361</td>\n      <td>-0.050611</td>\n      <td>0.421962</td>\n      <td>0.117088</td>\n      <td>0.146283</td>\n      <td>-0.136847</td>\n      <td>0.257327</td>\n      <td>0.109962</td>\n      <td>-0.036141</td>\n      <td>-0.100092</td>\n      <td>0.454077</td>\n      <td>-0.019662</td>\n      <td>0.073650</td>\n      <td>-0.050929</td>\n      <td>0.004876</td>\n      <td>0.077912</td>\n      <td>-0.092465</td>\n      <td>0.008673</td>\n      <td>0.043924</td>\n      <td>-0.102756</td>\n      <td>-0.011966</td>\n      <td>1.478923</td>\n      <td>0.141116</td>\n      <td>-0.104184</td>\n      <td>-0.212622</td>\n      <td>0.308052</td>\n      <td>0.027975</td>\n      <td>-0.096544</td>\n      <td>-0.030955</td>\n      <td>0.156021</td>\n      <td>0.030313</td>\n      <td>0.225954</td>\n      <td>-0.095054</td>\n      <td>-0.028980</td>\n      <td>0.018185</td>\n      <td>0.025814</td>\n      <td>-0.195067</td>\n      <td>-0.167623</td>\n      <td>0.084762</td>\n      <td>11.718389</td>\n      <td>-0.038374</td>\n      <td>-0.011906</td>\n      <td>-0.001632</td>\n      <td>-0.224642</td>\n      <td>-0.079878</td>\n      <td>-0.099328</td>\n      <td>0.124719</td>\n      <td>0.179040</td>\n      <td>0.142606</td>\n      <td>-0.036361</td>\n      <td>-0.080203</td>\n      <td>0.120659</td>\n      <td>-0.049413</td>\n      <td>0.000616</td>\n      <td>0.114762</td>\n      <td>0.136759</td>\n      <td>0.128589</td>\n      <td>0.083282</td>\n      <td>-0.119280</td>\n      <td>0.175465</td>\n      <td>0.175211</td>\n      <td>0.079239</td>\n      <td>0.902776</td>\n      <td>-0.046504</td>\n      <td>0.255126</td>\n      <td>-0.002165</td>\n      <td>-0.024777</td>\n      <td>0.089408</td>\n      <td>0.198304</td>\n      <td>0.016981</td>\n      <td>0.230631</td>\n      <td>0.072909</td>\n      <td>-0.130362</td>\n      <td>-0.013177</td>\n      <td>-0.092629</td>\n      <td>-0.196001</td>\n      <td>0.119407</td>\n      <td>0.157684</td>\n      <td>-0.082814</td>\n      <td>0.007755</td>\n      <td>-0.103149</td>\n      <td>0.284048</td>\n      <td>-0.280611</td>\n      <td>0.074010</td>\n      <td>-0.182096</td>\n      <td>0.179465</td>\n      <td>0.221553</td>\n      <td>0.046697</td>\n      <td>-0.015358</td>\n      <td>0.039650</td>\n      <td>0.081511</td>\n      <td>0.152500</td>\n      <td>-0.169331</td>\n      <td>0.018030</td>\n      <td>-0.237969</td>\n      <td>0.178311</td>\n      <td>-0.111561</td>\n      <td>0.018086</td>\n      <td>0.100239</td>\n      <td>-0.067606</td>\n      <td>-0.186209</td>\n      <td>0.084413</td>\n      <td>0.027594</td>\n      <td>-0.156649</td>\n      <td>0.218658</td>\n      <td>0.102454</td>\n      <td>-0.092495</td>\n      <td>-0.029048</td>\n      <td>0.020467</td>\n      <td>0.097208</td>\n      <td>0.068601</td>\n      <td>0.086373</td>\n      <td>-0.188948</td>\n      <td>-0.147831</td>\n      <td>-0.122629</td>\n      <td>-0.531026</td>\n      <td>-0.195737</td>\n      <td>-0.144605</td>\n      <td>-0.535961</td>\n      <td>0.136488</td>\n      <td>-0.121504</td>\n      <td>-0.160985</td>\n      <td>-0.042444</td>\n      <td>-0.374259</td>\n      <td>0.128434</td>\n      <td>-0.260724</td>\n      <td>0.105819</td>\n      <td>-0.173876</td>\n      <td>0.085959</td>\n      <td>-0.007114</td>\n      <td>0.041356</td>\n      <td>-0.002045</td>\n      <td>-0.151803</td>\n      <td>-0.026553</td>\n      <td>-0.087048</td>\n      <td>0.191028</td>\n      <td>-0.033562</td>\n      <td>0.107551</td>\n      <td>-0.343149</td>\n      <td>-0.405529</td>\n      <td>-0.163232</td>\n      <td>-0.044187</td>\n      <td>0.012854</td>\n      <td>-0.204581</td>\n      <td>0.237232</td>\n      <td>0.145715</td>\n      <td>-0.125335</td>\n      <td>0.063213</td>\n      <td>0.022032</td>\n      <td>-0.027339</td>\n      <td>0.020471</td>\n      <td>-0.217706</td>\n      <td>-0.001401</td>\n      <td>0.026243</td>\n      <td>-0.193529</td>\n      <td>0.096699</td>\n      <td>0.059963</td>\n      <td>0.140285</td>\n      <td>0.135448</td>\n      <td>0.055009</td>\n      <td>0.066489</td>\n      <td>-0.012882</td>\n      <td>-0.105614</td>\n      <td>0.000890</td>\n      <td>0.034355</td>\n      <td>-0.243062</td>\n      <td>-0.077763</td>\n      <td>-0.166024</td>\n      <td>-0.111253</td>\n      <td>-0.237102</td>\n      <td>-0.003095</td>\n      <td>0.293362</td>\n      <td>0.116610</td>\n      <td>0.061845</td>\n      <td>-0.008980</td>\n      <td>-0.138449</td>\n      <td>0.131612</td>\n      <td>0.244518</td>\n      <td>0.182840</td>\n      <td>-0.164329</td>\n      <td>0.032314</td>\n      <td>-0.118185</td>\n      <td>-0.233659</td>\n      <td>-0.006937</td>\n      <td>-0.106601</td>\n      <td>0.157456</td>\n      <td>0.394210</td>\n      <td>0.110254</td>\n      <td>0.207392</td>\n      <td>-0.067668</td>\n      <td>-0.081145</td>\n      <td>-0.097108</td>\n      <td>-0.460185</td>\n      <td>-0.036846</td>\n      <td>-0.012913</td>\n      <td>-0.114179</td>\n      <td>0.144275</td>\n      <td>0.079694</td>\n      <td>-0.229887</td>\n      <td>0.509667</td>\n      <td>-0.542613</td>\n      <td>-0.205718</td>\n      <td>-0.047161</td>\n      <td>-0.217698</td>\n      <td>0.119367</td>\n      <td>0.281846</td>\n      <td>-0.062160</td>\n      <td>0.066264</td>\n      <td>0.083418</td>\n      <td>-0.142987</td>\n      <td>-0.000292</td>\n      <td>-0.025965</td>\n      <td>0.085525</td>\n      <td>0.040288</td>\n      <td>0.167405</td>\n      <td>0.262107</td>\n      <td>0.174512</td>\n      <td>-0.008561</td>\n      <td>-0.022450</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>hep-ph/0103252</td>\n      <td>Tommy Ohlsson</td>\n      <td>Tommy Ohlsson, Hakan Snellman</td>\n      <td>Neutrino oscillations with three flavors in ma...</td>\n      <td>13 pages, 8 figures, RevTeX. Final version to ...</td>\n      <td>Eur.Phys.J.C20:507-515,2001</td>\n      <td>10.1007/s100520100687</td>\n      <td>TUM-HEP-405/01</td>\n      <td>hep-ph</td>\n      <td>None</td>\n      <td>In this paper, we discuss the evolution oper...</td>\n      <td>[{'version': 'v1', 'created': 'Fri, 23 Mar 200...</td>\n      <td>[[Ohlsson, Tommy, ], [Snellman, Hakan, ]]</td>\n      <td>2.996094</td>\n      <td>2.708050</td>\n      <td>10.1007</td>\n      <td>Springer-Verlag</td>\n      <td>7.910156</td>\n      <td>4329354.0</td>\n      <td>2010-05-28</td>\n      <td>2001-03-23 13:55:22+00:00</td>\n      <td>2001-05-04 14:16:14+00:00</td>\n      <td>2010</td>\n      <td>2001</td>\n      <td>2001</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>201005</td>\n      <td>200103</td>\n      <td>200105</td>\n      <td>28</td>\n      <td>23</td>\n      <td>4</td>\n      <td>1.275005e+09</td>\n      <td>9.853557e+08</td>\n      <td>9.889858e+08</td>\n      <td>289649078</td>\n      <td>3630052.0</td>\n      <td>2</td>\n      <td>14757</td>\n      <td>11404</td>\n      <td>11446</td>\n      <td>42</td>\n      <td>14.335938</td>\n      <td>Ohlsson</td>\n      <td>2</td>\n      <td>2.996094</td>\n      <td>hep-ph</td>\n      <td>hep-ph</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>hep</td>\n      <td>hep-ph</td>\n      <td>hep-ph</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>179924</td>\n      <td>10</td>\n      <td>74502</td>\n      <td>440</td>\n      <td>8</td>\n      <td>20</td>\n      <td>20</td>\n      <td>2948</td>\n      <td>2330</td>\n      <td>17403</td>\n      <td>2.146484</td>\n      <td>37</td>\n      <td>79.375000</td>\n      <td>0.000000</td>\n      <td>4.906250</td>\n      <td>2.398438</td>\n      <td>1.270508</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>3.177734</td>\n      <td>1.708008</td>\n      <td>69644</td>\n      <td>118928.890625</td>\n      <td>0.0</td>\n      <td>7.730469</td>\n      <td>1.609375</td>\n      <td>1.254883</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>2.564453</td>\n      <td>1.746094</td>\n      <td>85372</td>\n      <td>149083.671875</td>\n      <td>0.0</td>\n      <td>7.730469</td>\n      <td>1.791992</td>\n      <td>1.250000</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>2.638672</td>\n      <td>2.419922</td>\n      <td>43</td>\n      <td>104.00000</td>\n      <td>0.000000</td>\n      <td>4.906250</td>\n      <td>2.773438</td>\n      <td>1.196289</td>\n      <td>0.693359</td>\n      <td>1.869141</td>\n      <td>3.177734</td>\n      <td>2.728516</td>\n      <td>2548</td>\n      <td>6952.146973</td>\n      <td>0.0</td>\n      <td>7.511719</td>\n      <td>2.773438</td>\n      <td>1.313477</td>\n      <td>0.693359</td>\n      <td>1.946289</td>\n      <td>3.636719</td>\n      <td>2.707031</td>\n      <td>1941</td>\n      <td>5256.0</td>\n      <td>0.0</td>\n      <td>7.250000</td>\n      <td>2.773438</td>\n      <td>1.438477</td>\n      <td>0.693359</td>\n      <td>1.791992</td>\n      <td>3.664062</td>\n      <td>2.646484</td>\n      <td>290654</td>\n      <td>769432.75</td>\n      <td>0.0</td>\n      <td>10.132812</td>\n      <td>2.708984</td>\n      <td>1.405273</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.611328</td>\n      <td>2.162109</td>\n      <td>82437</td>\n      <td>178245.640625</td>\n      <td>0.0</td>\n      <td>8.476562</td>\n      <td>2.197266</td>\n      <td>1.360352</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>3.134766</td>\n      <td>2.162109</td>\n      <td>82437</td>\n      <td>178245.640625</td>\n      <td>0.0</td>\n      <td>8.476562</td>\n      <td>2.197266</td>\n      <td>1.360352</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>3.134766</td>\n      <td>2.062500</td>\n      <td>51453</td>\n      <td>106164.593750</td>\n      <td>0.0</td>\n      <td>8.421875</td>\n      <td>2.080078</td>\n      <td>1.354492</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>3.044922</td>\n      <td>2.041016</td>\n      <td>107865</td>\n      <td>220126.531250</td>\n      <td>0.0</td>\n      <td>8.523438</td>\n      <td>2.080078</td>\n      <td>1.330078</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>2.996094</td>\n      <td>2.062500</td>\n      <td>51453</td>\n      <td>106164.593750</td>\n      <td>0.0</td>\n      <td>8.421875</td>\n      <td>2.080078</td>\n      <td>1.354492</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>3.044922</td>\n      <td>2.146484</td>\n      <td>37</td>\n      <td>79.375000</td>\n      <td>0.000000</td>\n      <td>4.906250</td>\n      <td>2.398438</td>\n      <td>1.270508</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>3.177734</td>\n      <td>1.708008</td>\n      <td>69644</td>\n      <td>118937.000000</td>\n      <td>0.0</td>\n      <td>7.730469</td>\n      <td>1.609375</td>\n      <td>1.254883</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>2.564453</td>\n      <td>1.746094</td>\n      <td>85372</td>\n      <td>149093.718750</td>\n      <td>0.0</td>\n      <td>7.730469</td>\n      <td>1.791992</td>\n      <td>1.250000</td>\n      <td>0.000000</td>\n      <td>0.693359</td>\n      <td>2.638672</td>\n      <td>2.419922</td>\n      <td>43</td>\n      <td>104.00000</td>\n      <td>0.000000</td>\n      <td>4.906250</td>\n      <td>2.773438</td>\n      <td>1.196289</td>\n      <td>0.693359</td>\n      <td>1.869141</td>\n      <td>3.177734</td>\n      <td>2.728516</td>\n      <td>2548</td>\n      <td>6952.290039</td>\n      <td>0.0</td>\n      <td>7.511719</td>\n      <td>2.773438</td>\n      <td>1.313477</td>\n      <td>0.693359</td>\n      <td>1.946289</td>\n      <td>3.636719</td>\n      <td>2.707031</td>\n      <td>1941</td>\n      <td>5256.0</td>\n      <td>0.0</td>\n      <td>7.250000</td>\n      <td>2.773438</td>\n      <td>1.438477</td>\n      <td>0.693359</td>\n      <td>1.791992</td>\n      <td>3.664062</td>\n      <td>2.646484</td>\n      <td>290654</td>\n      <td>7.694539e+05</td>\n      <td>0.0</td>\n      <td>10.132812</td>\n      <td>2.708984</td>\n      <td>1.405273</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.611328</td>\n      <td>2.162109</td>\n      <td>82437</td>\n      <td>178253.53125</td>\n      <td>0.0</td>\n      <td>8.476562</td>\n      <td>2.197266</td>\n      <td>1.360352</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>3.134766</td>\n      <td>2.162109</td>\n      <td>82437</td>\n      <td>178253.53125</td>\n      <td>0.0</td>\n      <td>8.476562</td>\n      <td>2.197266</td>\n      <td>1.360352</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>3.134766</td>\n      <td>2.062500</td>\n      <td>51453</td>\n      <td>106169.703125</td>\n      <td>0.0</td>\n      <td>8.421875</td>\n      <td>2.080078</td>\n      <td>1.354492</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>3.044922</td>\n      <td>2.041016</td>\n      <td>107865</td>\n      <td>220137.921875</td>\n      <td>0.0</td>\n      <td>8.523438</td>\n      <td>2.080078</td>\n      <td>1.330078</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>2.996094</td>\n      <td>2.062500</td>\n      <td>51453</td>\n      <td>106169.703125</td>\n      <td>0.0</td>\n      <td>8.421875</td>\n      <td>2.080078</td>\n      <td>1.354492</td>\n      <td>0.000000</td>\n      <td>1.098633</td>\n      <td>3.044922</td>\n      <td>0.576660</td>\n      <td>1.168945</td>\n      <td>1.288086</td>\n      <td>1.475586</td>\n      <td>0.850098</td>\n      <td>1.270508</td>\n      <td>1.249023</td>\n      <td>1.455078</td>\n      <td>0.267334</td>\n      <td>1.071289</td>\n      <td>0.287842</td>\n      <td>1.077148</td>\n      <td>0.348389</td>\n      <td>1.095703</td>\n      <td>0.833496</td>\n      <td>1.263672</td>\n      <td>0.833496</td>\n      <td>1.263672</td>\n      <td>0.932617</td>\n      <td>1.304688</td>\n      <td>0.955078</td>\n      <td>1.314453</td>\n      <td>0.932617</td>\n      <td>1.304688</td>\n      <td>-0.000361</td>\n      <td>1.0</td>\n      <td>0.576660</td>\n      <td>1.168945</td>\n      <td>1.288086</td>\n      <td>1.475586</td>\n      <td>0.849609</td>\n      <td>1.270508</td>\n      <td>1.249023</td>\n      <td>1.455078</td>\n      <td>0.267090</td>\n      <td>1.071289</td>\n      <td>0.287842</td>\n      <td>1.077148</td>\n      <td>0.348389</td>\n      <td>1.095703</td>\n      <td>0.833496</td>\n      <td>1.263672</td>\n      <td>0.833496</td>\n      <td>1.263672</td>\n      <td>0.932129</td>\n      <td>1.304688</td>\n      <td>0.955078</td>\n      <td>1.314453</td>\n      <td>0.932129</td>\n      <td>1.304688</td>\n      <td>0.711426</td>\n      <td>1.262695</td>\n      <td>0.273193</td>\n      <td>1.086914</td>\n      <td>0.672852</td>\n      <td>1.245117</td>\n      <td>-0.309570</td>\n      <td>0.916992</td>\n      <td>-0.289062</td>\n      <td>0.921875</td>\n      <td>-0.228271</td>\n      <td>0.937500</td>\n      <td>0.256836</td>\n      <td>1.081055</td>\n      <td>0.256836</td>\n      <td>1.081055</td>\n      <td>0.355713</td>\n      <td>1.116211</td>\n      <td>0.378174</td>\n      <td>1.124023</td>\n      <td>0.355713</td>\n      <td>1.116211</td>\n      <td>-0.577148</td>\n      <td>0.855469</td>\n      <td>-0.000100</td>\n      <td>1.0</td>\n      <td>0.711426</td>\n      <td>1.262695</td>\n      <td>0.273193</td>\n      <td>1.086914</td>\n      <td>0.672363</td>\n      <td>1.245117</td>\n      <td>-0.309570</td>\n      <td>0.916992</td>\n      <td>-0.289062</td>\n      <td>0.921875</td>\n      <td>-0.228394</td>\n      <td>0.937500</td>\n      <td>0.256592</td>\n      <td>1.081055</td>\n      <td>0.256592</td>\n      <td>1.081055</td>\n      <td>0.355469</td>\n      <td>1.116211</td>\n      <td>0.378174</td>\n      <td>1.124023</td>\n      <td>0.355469</td>\n      <td>1.116211</td>\n      <td>-0.438232</td>\n      <td>0.860840</td>\n      <td>-0.038605</td>\n      <td>0.98584</td>\n      <td>-1.020508</td>\n      <td>0.726074</td>\n      <td>-1.000000</td>\n      <td>0.730469</td>\n      <td>-0.939453</td>\n      <td>0.742188</td>\n      <td>-0.454590</td>\n      <td>0.856445</td>\n      <td>-0.454590</td>\n      <td>0.856445</td>\n      <td>-0.355713</td>\n      <td>0.883789</td>\n      <td>-0.333008</td>\n      <td>0.890625</td>\n      <td>-0.355713</td>\n      <td>0.883789</td>\n      <td>-1.288086</td>\n      <td>0.677734</td>\n      <td>-0.711426</td>\n      <td>0.791992</td>\n      <td>-0.000116</td>\n      <td>1.0</td>\n      <td>-0.438232</td>\n      <td>0.860840</td>\n      <td>-0.038727</td>\n      <td>0.98584</td>\n      <td>-1.020508</td>\n      <td>0.726074</td>\n      <td>-1.000000</td>\n      <td>0.730469</td>\n      <td>-0.939453</td>\n      <td>0.742188</td>\n      <td>-0.454590</td>\n      <td>0.856445</td>\n      <td>-0.454590</td>\n      <td>0.856445</td>\n      <td>-0.355713</td>\n      <td>0.883789</td>\n      <td>-0.333252</td>\n      <td>0.890625</td>\n      <td>-0.355713</td>\n      <td>0.883789</td>\n      <td>0.399658</td>\n      <td>1.145508</td>\n      <td>-0.582520</td>\n      <td>0.843750</td>\n      <td>-0.562012</td>\n      <td>0.848633</td>\n      <td>-0.501465</td>\n      <td>0.862305</td>\n      <td>-0.016357</td>\n      <td>0.994629</td>\n      <td>-0.016357</td>\n      <td>0.994629</td>\n      <td>0.082520</td>\n      <td>1.027344</td>\n      <td>0.105103</td>\n      <td>1.034180</td>\n      <td>0.082520</td>\n      <td>1.027344</td>\n      <td>-0.850098</td>\n      <td>0.787109</td>\n      <td>-0.273193</td>\n      <td>0.919922</td>\n      <td>0.437988</td>\n      <td>1.162109</td>\n      <td>-0.000063</td>\n      <td>1.0</td>\n      <td>0.399414</td>\n      <td>1.145508</td>\n      <td>-0.582520</td>\n      <td>0.843750</td>\n      <td>-0.562012</td>\n      <td>0.848633</td>\n      <td>-0.501465</td>\n      <td>0.862305</td>\n      <td>-0.016464</td>\n      <td>0.994629</td>\n      <td>-0.016464</td>\n      <td>0.994629</td>\n      <td>0.082397</td>\n      <td>1.027344</td>\n      <td>0.104980</td>\n      <td>1.034180</td>\n      <td>0.082397</td>\n      <td>1.027344</td>\n      <td>-0.982422</td>\n      <td>0.736328</td>\n      <td>-0.961426</td>\n      <td>0.740723</td>\n      <td>-0.900879</td>\n      <td>0.752930</td>\n      <td>-0.416016</td>\n      <td>0.868652</td>\n      <td>-0.416016</td>\n      <td>0.868652</td>\n      <td>-0.317139</td>\n      <td>0.896484</td>\n      <td>-0.294434</td>\n      <td>0.903320</td>\n      <td>-0.317139</td>\n      <td>0.896484</td>\n      <td>-1.250000</td>\n      <td>0.687012</td>\n      <td>-0.672852</td>\n      <td>0.803223</td>\n      <td>0.038483</td>\n      <td>1.014648</td>\n      <td>-0.399658</td>\n      <td>0.873047</td>\n      <td>-0.000118</td>\n      <td>1.0</td>\n      <td>-0.982422</td>\n      <td>0.736328</td>\n      <td>-0.961914</td>\n      <td>0.740723</td>\n      <td>-0.900879</td>\n      <td>0.752930</td>\n      <td>-0.416016</td>\n      <td>0.868652</td>\n      <td>-0.416016</td>\n      <td>0.868652</td>\n      <td>-0.317139</td>\n      <td>0.896484</td>\n      <td>-0.294678</td>\n      <td>0.903320</td>\n      <td>-0.317139</td>\n      <td>0.896484</td>\n      <td>0.020538</td>\n      <td>1.005859</td>\n      <td>0.081238</td>\n      <td>1.022461</td>\n      <td>0.566406</td>\n      <td>1.178711</td>\n      <td>0.566406</td>\n      <td>1.178711</td>\n      <td>0.665039</td>\n      <td>1.216797</td>\n      <td>0.687500</td>\n      <td>1.226562</td>\n      <td>0.665039</td>\n      <td>1.216797</td>\n      <td>-0.267578</td>\n      <td>0.933105</td>\n      <td>0.309326</td>\n      <td>1.090820</td>\n      <td>1.020508</td>\n      <td>1.376953</td>\n      <td>0.582520</td>\n      <td>1.185547</td>\n      <td>0.981934</td>\n      <td>1.357422</td>\n      <td>-0.000056</td>\n      <td>1.0</td>\n      <td>0.020493</td>\n      <td>1.005859</td>\n      <td>0.081177</td>\n      <td>1.022461</td>\n      <td>0.566406</td>\n      <td>1.178711</td>\n      <td>0.566406</td>\n      <td>1.178711</td>\n      <td>0.665039</td>\n      <td>1.216797</td>\n      <td>0.687500</td>\n      <td>1.226562</td>\n      <td>0.665039</td>\n      <td>1.216797</td>\n      <td>0.060669</td>\n      <td>1.016602</td>\n      <td>0.545898</td>\n      <td>1.172852</td>\n      <td>0.545898</td>\n      <td>1.172852</td>\n      <td>0.644531</td>\n      <td>1.209961</td>\n      <td>0.666992</td>\n      <td>1.219727</td>\n      <td>0.644531</td>\n      <td>1.209961</td>\n      <td>-0.288086</td>\n      <td>0.927734</td>\n      <td>0.288818</td>\n      <td>1.084961</td>\n      <td>1.000000</td>\n      <td>1.369141</td>\n      <td>0.562012</td>\n      <td>1.178711</td>\n      <td>0.961426</td>\n      <td>1.350586</td>\n      <td>-0.020599</td>\n      <td>0.994629</td>\n      <td>-0.000053</td>\n      <td>1.0</td>\n      <td>0.060608</td>\n      <td>1.016602</td>\n      <td>0.545410</td>\n      <td>1.172852</td>\n      <td>0.545410</td>\n      <td>1.172852</td>\n      <td>0.644531</td>\n      <td>1.209961</td>\n      <td>0.666992</td>\n      <td>1.219727</td>\n      <td>0.644531</td>\n      <td>1.209961</td>\n      <td>0.485107</td>\n      <td>1.153320</td>\n      <td>0.485107</td>\n      <td>1.153320</td>\n      <td>0.583984</td>\n      <td>1.190430</td>\n      <td>0.606445</td>\n      <td>1.199219</td>\n      <td>0.583984</td>\n      <td>1.190430</td>\n      <td>-0.348877</td>\n      <td>0.912598</td>\n      <td>0.228149</td>\n      <td>1.066406</td>\n      <td>0.939453</td>\n      <td>1.346680</td>\n      <td>0.501465</td>\n      <td>1.159180</td>\n      <td>0.900879</td>\n      <td>1.328125</td>\n      <td>-0.081299</td>\n      <td>0.978027</td>\n      <td>-0.060730</td>\n      <td>0.983398</td>\n      <td>-0.000073</td>\n      <td>1.0</td>\n      <td>0.484863</td>\n      <td>1.153320</td>\n      <td>0.484863</td>\n      <td>1.153320</td>\n      <td>0.583984</td>\n      <td>1.190430</td>\n      <td>0.606445</td>\n      <td>1.199219</td>\n      <td>0.583984</td>\n      <td>1.190430</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.098877</td>\n      <td>1.032227</td>\n      <td>0.121460</td>\n      <td>1.040039</td>\n      <td>0.098877</td>\n      <td>1.032227</td>\n      <td>-0.833984</td>\n      <td>0.791504</td>\n      <td>-0.256836</td>\n      <td>0.924805</td>\n      <td>0.454346</td>\n      <td>1.167969</td>\n      <td>0.016296</td>\n      <td>1.004883</td>\n      <td>0.415771</td>\n      <td>1.151367</td>\n      <td>-0.566406</td>\n      <td>0.848145</td>\n      <td>-0.545898</td>\n      <td>0.853027</td>\n      <td>-0.485107</td>\n      <td>0.867188</td>\n      <td>-0.000096</td>\n      <td>1.0</td>\n      <td>-0.000096</td>\n      <td>1.0</td>\n      <td>0.098755</td>\n      <td>1.032227</td>\n      <td>0.121338</td>\n      <td>1.040039</td>\n      <td>0.098755</td>\n      <td>1.032227</td>\n      <td>0.098877</td>\n      <td>1.032227</td>\n      <td>0.121460</td>\n      <td>1.040039</td>\n      <td>0.098877</td>\n      <td>1.032227</td>\n      <td>-0.833984</td>\n      <td>0.791504</td>\n      <td>-0.256836</td>\n      <td>0.924805</td>\n      <td>0.454346</td>\n      <td>1.167969</td>\n      <td>0.016296</td>\n      <td>1.004883</td>\n      <td>0.415771</td>\n      <td>1.151367</td>\n      <td>-0.566406</td>\n      <td>0.848145</td>\n      <td>-0.545898</td>\n      <td>0.853027</td>\n      <td>-0.485107</td>\n      <td>0.867188</td>\n      <td>-0.000096</td>\n      <td>1.0</td>\n      <td>-0.000096</td>\n      <td>1.0</td>\n      <td>0.098755</td>\n      <td>1.032227</td>\n      <td>0.121338</td>\n      <td>1.040039</td>\n      <td>0.098755</td>\n      <td>1.032227</td>\n      <td>0.022568</td>\n      <td>1.007812</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.932617</td>\n      <td>0.766602</td>\n      <td>-0.355713</td>\n      <td>0.895996</td>\n      <td>0.355469</td>\n      <td>1.130859</td>\n      <td>-0.082581</td>\n      <td>0.973633</td>\n      <td>0.316895</td>\n      <td>1.115234</td>\n      <td>-0.665039</td>\n      <td>0.821777</td>\n      <td>-0.644531</td>\n      <td>0.826172</td>\n      <td>-0.583984</td>\n      <td>0.839844</td>\n      <td>-0.098999</td>\n      <td>0.968750</td>\n      <td>-0.098999</td>\n      <td>0.968750</td>\n      <td>-0.000099</td>\n      <td>1.0</td>\n      <td>0.022461</td>\n      <td>1.007812</td>\n      <td>-0.000099</td>\n      <td>1.000000</td>\n      <td>-0.022568</td>\n      <td>0.992676</td>\n      <td>-0.955566</td>\n      <td>0.760742</td>\n      <td>-0.378418</td>\n      <td>0.889160</td>\n      <td>0.333008</td>\n      <td>1.123047</td>\n      <td>-0.105164</td>\n      <td>0.966797</td>\n      <td>0.294434</td>\n      <td>1.107422</td>\n      <td>-0.687988</td>\n      <td>0.815430</td>\n      <td>-0.666992</td>\n      <td>0.819824</td>\n      <td>-0.606445</td>\n      <td>0.833496</td>\n      <td>-0.121521</td>\n      <td>0.961426</td>\n      <td>-0.121521</td>\n      <td>0.961426</td>\n      <td>-0.022675</td>\n      <td>0.992676</td>\n      <td>-0.000106</td>\n      <td>1.0</td>\n      <td>-0.022675</td>\n      <td>0.992676</td>\n      <td>-0.932617</td>\n      <td>0.766602</td>\n      <td>-0.355713</td>\n      <td>0.895996</td>\n      <td>0.355469</td>\n      <td>1.130859</td>\n      <td>-0.082581</td>\n      <td>0.973633</td>\n      <td>0.316895</td>\n      <td>1.115234</td>\n      <td>-0.665039</td>\n      <td>0.821777</td>\n      <td>-0.644531</td>\n      <td>0.826172</td>\n      <td>-0.583984</td>\n      <td>0.839844</td>\n      <td>-0.098999</td>\n      <td>0.968750</td>\n      <td>-0.098999</td>\n      <td>0.968750</td>\n      <td>-0.000099</td>\n      <td>1.000000</td>\n      <td>0.022461</td>\n      <td>1.007812</td>\n      <td>-0.000099</td>\n      <td>1.0</td>\n      <td>0.577148</td>\n      <td>1.168945</td>\n      <td>1.288086</td>\n      <td>1.475586</td>\n      <td>0.850098</td>\n      <td>1.270508</td>\n      <td>1.250000</td>\n      <td>1.455078</td>\n      <td>0.267578</td>\n      <td>1.071289</td>\n      <td>0.288086</td>\n      <td>1.078125</td>\n      <td>0.348877</td>\n      <td>1.095703</td>\n      <td>0.833984</td>\n      <td>1.263672</td>\n      <td>0.833984</td>\n      <td>1.263672</td>\n      <td>0.932617</td>\n      <td>1.304688</td>\n      <td>0.955078</td>\n      <td>1.314453</td>\n      <td>0.932617</td>\n      <td>1.304688</td>\n      <td>0.711426</td>\n      <td>1.262695</td>\n      <td>0.273193</td>\n      <td>1.086914</td>\n      <td>0.672852</td>\n      <td>1.245117</td>\n      <td>-0.309326</td>\n      <td>0.916992</td>\n      <td>-0.288818</td>\n      <td>0.921875</td>\n      <td>-0.228271</td>\n      <td>0.937500</td>\n      <td>0.256836</td>\n      <td>1.081055</td>\n      <td>0.256836</td>\n      <td>1.081055</td>\n      <td>0.355713</td>\n      <td>1.116211</td>\n      <td>0.378174</td>\n      <td>1.124023</td>\n      <td>0.355713</td>\n      <td>1.116211</td>\n      <td>-0.438232</td>\n      <td>0.860840</td>\n      <td>-0.038605</td>\n      <td>0.98584</td>\n      <td>-1.020508</td>\n      <td>0.726074</td>\n      <td>-1.000000</td>\n      <td>0.730469</td>\n      <td>-0.939453</td>\n      <td>0.742188</td>\n      <td>-0.454590</td>\n      <td>0.856445</td>\n      <td>-0.454590</td>\n      <td>0.856445</td>\n      <td>-0.355713</td>\n      <td>0.883789</td>\n      <td>-0.333008</td>\n      <td>0.890625</td>\n      <td>-0.355713</td>\n      <td>0.883789</td>\n      <td>0.399414</td>\n      <td>1.145508</td>\n      <td>-0.582520</td>\n      <td>0.843750</td>\n      <td>-0.562012</td>\n      <td>0.848633</td>\n      <td>-0.501465</td>\n      <td>0.862305</td>\n      <td>-0.016403</td>\n      <td>0.994629</td>\n      <td>-0.016403</td>\n      <td>0.994629</td>\n      <td>0.082458</td>\n      <td>1.027344</td>\n      <td>0.105042</td>\n      <td>1.034180</td>\n      <td>0.082458</td>\n      <td>1.027344</td>\n      <td>-0.981934</td>\n      <td>0.736816</td>\n      <td>-0.961426</td>\n      <td>0.740723</td>\n      <td>-0.900879</td>\n      <td>0.752930</td>\n      <td>-0.416016</td>\n      <td>0.868652</td>\n      <td>-0.416016</td>\n      <td>0.868652</td>\n      <td>-0.317139</td>\n      <td>0.896484</td>\n      <td>-0.294434</td>\n      <td>0.903320</td>\n      <td>-0.317139</td>\n      <td>0.896484</td>\n      <td>0.020538</td>\n      <td>1.005859</td>\n      <td>0.081238</td>\n      <td>1.022461</td>\n      <td>0.566406</td>\n      <td>1.178711</td>\n      <td>0.566406</td>\n      <td>1.178711</td>\n      <td>0.665039</td>\n      <td>1.216797</td>\n      <td>0.687500</td>\n      <td>1.226562</td>\n      <td>0.665039</td>\n      <td>1.216797</td>\n      <td>0.060669</td>\n      <td>1.016602</td>\n      <td>0.545898</td>\n      <td>1.172852</td>\n      <td>0.545898</td>\n      <td>1.172852</td>\n      <td>0.644531</td>\n      <td>1.209961</td>\n      <td>0.666992</td>\n      <td>1.219727</td>\n      <td>0.644531</td>\n      <td>1.209961</td>\n      <td>0.485107</td>\n      <td>1.153320</td>\n      <td>0.485107</td>\n      <td>1.153320</td>\n      <td>0.583984</td>\n      <td>1.190430</td>\n      <td>0.606445</td>\n      <td>1.199219</td>\n      <td>0.583984</td>\n      <td>1.190430</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.098877</td>\n      <td>1.032227</td>\n      <td>0.121460</td>\n      <td>1.040039</td>\n      <td>0.098877</td>\n      <td>1.032227</td>\n      <td>0.098877</td>\n      <td>1.032227</td>\n      <td>0.121460</td>\n      <td>1.040039</td>\n      <td>0.098877</td>\n      <td>1.032227</td>\n      <td>0.022568</td>\n      <td>1.007812</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.022568</td>\n      <td>0.992676</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.045024</td>\n      <td>0.061441</td>\n      <td>0.026114</td>\n      <td>0.186450</td>\n      <td>0.307958</td>\n      <td>0.072466</td>\n      <td>0.257011</td>\n      <td>0.268695</td>\n      <td>-0.061639</td>\n      <td>-0.094779</td>\n      <td>-0.121741</td>\n      <td>-0.250290</td>\n      <td>0.009397</td>\n      <td>-0.171356</td>\n      <td>0.094235</td>\n      <td>0.088163</td>\n      <td>0.067743</td>\n      <td>0.185107</td>\n      <td>-0.061381</td>\n      <td>0.002100</td>\n      <td>-0.185709</td>\n      <td>0.153642</td>\n      <td>-0.202368</td>\n      <td>-0.085331</td>\n      <td>0.208823</td>\n      <td>0.157383</td>\n      <td>0.015314</td>\n      <td>0.177820</td>\n      <td>-0.241283</td>\n      <td>0.041038</td>\n      <td>0.094112</td>\n      <td>0.104222</td>\n      <td>0.013423</td>\n      <td>-0.014741</td>\n      <td>-0.061360</td>\n      <td>-0.059740</td>\n      <td>-0.093536</td>\n      <td>0.040848</td>\n      <td>-0.202473</td>\n      <td>0.025084</td>\n      <td>-0.203459</td>\n      <td>0.682886</td>\n      <td>0.079395</td>\n      <td>-0.135364</td>\n      <td>0.007933</td>\n      <td>-0.294504</td>\n      <td>0.030884</td>\n      <td>0.089232</td>\n      <td>-0.133020</td>\n      <td>0.212464</td>\n      <td>-0.078407</td>\n      <td>-0.028086</td>\n      <td>-0.133585</td>\n      <td>0.022868</td>\n      <td>0.065664</td>\n      <td>-0.041062</td>\n      <td>0.066463</td>\n      <td>0.231022</td>\n      <td>0.091057</td>\n      <td>0.108240</td>\n      <td>-0.002117</td>\n      <td>-0.076744</td>\n      <td>0.075257</td>\n      <td>0.284456</td>\n      <td>-0.055942</td>\n      <td>-0.030060</td>\n      <td>0.067542</td>\n      <td>0.255630</td>\n      <td>-0.089150</td>\n      <td>-0.129420</td>\n      <td>0.024403</td>\n      <td>-0.132183</td>\n      <td>0.009920</td>\n      <td>0.183269</td>\n      <td>0.181521</td>\n      <td>0.086513</td>\n      <td>0.028355</td>\n      <td>-1.615930</td>\n      <td>0.053578</td>\n      <td>0.018184</td>\n      <td>-0.019003</td>\n      <td>0.063449</td>\n      <td>0.743330</td>\n      <td>0.188787</td>\n      <td>0.200535</td>\n      <td>0.015952</td>\n      <td>0.006422</td>\n      <td>0.062562</td>\n      <td>0.046106</td>\n      <td>0.124889</td>\n      <td>-0.079480</td>\n      <td>0.002685</td>\n      <td>0.075083</td>\n      <td>0.010744</td>\n      <td>0.105034</td>\n      <td>0.040061</td>\n      <td>0.111879</td>\n      <td>-0.977696</td>\n      <td>-0.022940</td>\n      <td>0.056089</td>\n      <td>0.138631</td>\n      <td>-0.157961</td>\n      <td>0.105337</td>\n      <td>-0.055579</td>\n      <td>-0.115602</td>\n      <td>0.285886</td>\n      <td>0.045770</td>\n      <td>0.106218</td>\n      <td>-0.065913</td>\n      <td>-0.167793</td>\n      <td>0.023226</td>\n      <td>0.022350</td>\n      <td>-0.047034</td>\n      <td>0.229539</td>\n      <td>0.035684</td>\n      <td>-0.092818</td>\n      <td>0.221824</td>\n      <td>0.156967</td>\n      <td>-0.169832</td>\n      <td>0.327100</td>\n      <td>0.018974</td>\n      <td>0.088442</td>\n      <td>-0.020907</td>\n      <td>0.093161</td>\n      <td>0.274468</td>\n      <td>0.168415</td>\n      <td>-0.037457</td>\n      <td>0.175039</td>\n      <td>-0.142163</td>\n      <td>0.243652</td>\n      <td>0.014862</td>\n      <td>-0.555045</td>\n      <td>-0.209317</td>\n      <td>-0.178369</td>\n      <td>-0.164118</td>\n      <td>0.051424</td>\n      <td>-0.082794</td>\n      <td>0.191170</td>\n      <td>-0.029072</td>\n      <td>-0.070847</td>\n      <td>0.096028</td>\n      <td>0.228004</td>\n      <td>-0.033884</td>\n      <td>-0.169825</td>\n      <td>0.137567</td>\n      <td>0.060474</td>\n      <td>-0.040224</td>\n      <td>-0.217791</td>\n      <td>0.022308</td>\n      <td>0.112378</td>\n      <td>0.103478</td>\n      <td>0.103084</td>\n      <td>-0.190879</td>\n      <td>-0.231992</td>\n      <td>0.096861</td>\n      <td>0.480173</td>\n      <td>-0.007636</td>\n      <td>-0.238974</td>\n      <td>-0.039465</td>\n      <td>0.191938</td>\n      <td>-0.051125</td>\n      <td>-0.074155</td>\n      <td>0.140969</td>\n      <td>-0.006411</td>\n      <td>0.176998</td>\n      <td>-0.057825</td>\n      <td>-0.125296</td>\n      <td>0.150800</td>\n      <td>0.127588</td>\n      <td>0.067897</td>\n      <td>-0.174030</td>\n      <td>0.017394</td>\n      <td>0.159570</td>\n      <td>0.147627</td>\n      <td>-0.074560</td>\n      <td>-0.031528</td>\n      <td>-0.117566</td>\n      <td>-0.025238</td>\n      <td>-0.107860</td>\n      <td>-0.012475</td>\n      <td>-0.104021</td>\n      <td>-0.033846</td>\n      <td>0.017542</td>\n      <td>-0.159165</td>\n      <td>0.066054</td>\n      <td>0.018775</td>\n      <td>-0.129476</td>\n      <td>0.088261</td>\n      <td>-0.091472</td>\n      <td>-0.106108</td>\n      <td>0.033383</td>\n      <td>-0.059297</td>\n      <td>-0.149944</td>\n      <td>-0.225755</td>\n      <td>-0.040606</td>\n      <td>-0.255070</td>\n      <td>0.002444</td>\n      <td>0.015893</td>\n      <td>0.263539</td>\n      <td>-0.037503</td>\n      <td>0.085573</td>\n      <td>-0.341842</td>\n      <td>0.117793</td>\n      <td>0.108258</td>\n      <td>0.339525</td>\n      <td>-0.068815</td>\n      <td>-0.333756</td>\n      <td>0.159553</td>\n      <td>0.021009</td>\n      <td>-0.091770</td>\n      <td>0.019718</td>\n      <td>0.084636</td>\n      <td>-0.292808</td>\n      <td>0.024914</td>\n      <td>-0.091420</td>\n      <td>0.053642</td>\n      <td>0.082632</td>\n      <td>-0.439348</td>\n      <td>-0.191939</td>\n      <td>-0.403662</td>\n      <td>0.101641</td>\n      <td>0.094936</td>\n      <td>-0.089206</td>\n      <td>-0.114034</td>\n      <td>-0.169870</td>\n      <td>0.130361</td>\n      <td>0.037054</td>\n      <td>-0.010573</td>\n      <td>0.186188</td>\n      <td>-0.106118</td>\n      <td>0.006441</td>\n      <td>0.393563</td>\n      <td>0.241401</td>\n      <td>0.142665</td>\n      <td>0.118523</td>\n      <td>-0.175827</td>\n      <td>0.054609</td>\n      <td>0.018519</td>\n      <td>-0.108494</td>\n      <td>-0.214344</td>\n      <td>-0.456074</td>\n      <td>0.061460</td>\n      <td>0.057831</td>\n      <td>0.066821</td>\n      <td>-0.116205</td>\n      <td>0.153234</td>\n      <td>-0.086866</td>\n      <td>-0.012465</td>\n      <td>0.003945</td>\n      <td>-0.012793</td>\n      <td>0.024195</td>\n      <td>0.139661</td>\n      <td>-0.016817</td>\n      <td>0.034005</td>\n      <td>0.274794</td>\n      <td>-0.135067</td>\n      <td>-0.052155</td>\n      <td>-0.058958</td>\n      <td>-0.287165</td>\n      <td>0.617193</td>\n      <td>-0.129067</td>\n      <td>-0.179277</td>\n      <td>0.089431</td>\n      <td>0.050735</td>\n      <td>-0.007037</td>\n      <td>-0.317692</td>\n      <td>-0.169178</td>\n      <td>-0.068490</td>\n      <td>0.101219</td>\n      <td>0.009539</td>\n      <td>-0.107717</td>\n      <td>0.026075</td>\n      <td>0.086839</td>\n      <td>-0.047483</td>\n      <td>-0.136969</td>\n      <td>0.334363</td>\n      <td>0.318387</td>\n      <td>-0.046161</td>\n      <td>-0.106427</td>\n      <td>0.008149</td>\n      <td>0.325522</td>\n      <td>0.050240</td>\n      <td>0.056993</td>\n      <td>0.093178</td>\n      <td>-0.104203</td>\n      <td>0.007165</td>\n      <td>0.109145</td>\n      <td>-0.136339</td>\n      <td>-0.118205</td>\n      <td>0.202302</td>\n      <td>-0.060391</td>\n      <td>-0.194238</td>\n      <td>-0.057531</td>\n      <td>0.073516</td>\n      <td>0.095490</td>\n      <td>-0.006624</td>\n      <td>0.187697</td>\n      <td>-0.016216</td>\n      <td>-0.026481</td>\n      <td>-0.066832</td>\n      <td>0.036074</td>\n      <td>-0.071059</td>\n      <td>-0.020087</td>\n      <td>0.007296</td>\n      <td>-0.039265</td>\n      <td>0.145286</td>\n      <td>-0.287755</td>\n      <td>0.020310</td>\n      <td>0.132173</td>\n      <td>0.017299</td>\n      <td>-0.163920</td>\n      <td>0.018398</td>\n      <td>-0.051605</td>\n      <td>-0.025069</td>\n      <td>-0.092841</td>\n      <td>0.270640</td>\n      <td>-0.188679</td>\n      <td>0.062747</td>\n      <td>-0.007382</td>\n      <td>-0.068917</td>\n      <td>-0.196340</td>\n      <td>-0.014721</td>\n      <td>-0.193743</td>\n      <td>-0.104232</td>\n      <td>-0.219494</td>\n      <td>0.128811</td>\n      <td>-0.148385</td>\n      <td>0.093468</td>\n      <td>0.203894</td>\n      <td>0.202032</td>\n      <td>0.623950</td>\n      <td>0.543432</td>\n      <td>0.165606</td>\n      <td>-0.006492</td>\n      <td>0.246928</td>\n      <td>-0.007816</td>\n      <td>0.061278</td>\n      <td>0.083360</td>\n      <td>0.166092</td>\n      <td>0.119673</td>\n      <td>-0.104286</td>\n      <td>-0.087636</td>\n      <td>0.106660</td>\n      <td>0.010581</td>\n      <td>0.121943</td>\n      <td>0.002054</td>\n      <td>-0.032377</td>\n      <td>0.255948</td>\n      <td>0.312340</td>\n      <td>-0.340801</td>\n      <td>-0.229477</td>\n      <td>-0.089041</td>\n      <td>-0.092818</td>\n      <td>-0.233366</td>\n      <td>-0.083138</td>\n      <td>0.132514</td>\n      <td>-0.276736</td>\n      <td>0.018394</td>\n      <td>-0.222276</td>\n      <td>0.052933</td>\n      <td>-0.076984</td>\n      <td>0.201275</td>\n      <td>0.034494</td>\n      <td>0.026687</td>\n      <td>0.179343</td>\n      <td>-0.316101</td>\n      <td>-0.171431</td>\n      <td>-0.055398</td>\n      <td>0.100052</td>\n      <td>0.002029</td>\n      <td>0.021735</td>\n      <td>0.134354</td>\n      <td>-0.062151</td>\n      <td>0.116655</td>\n      <td>0.056437</td>\n      <td>-0.156781</td>\n      <td>0.131438</td>\n      <td>-0.122826</td>\n      <td>-0.012687</td>\n      <td>-0.153846</td>\n      <td>-0.012327</td>\n      <td>0.148026</td>\n      <td>-0.099284</td>\n      <td>-0.004246</td>\n      <td>0.327164</td>\n      <td>-0.133463</td>\n      <td>0.190452</td>\n      <td>0.223688</td>\n      <td>-0.069656</td>\n      <td>0.040422</td>\n      <td>-0.026260</td>\n      <td>-0.125241</td>\n      <td>0.027271</td>\n      <td>-0.011622</td>\n      <td>-0.010560</td>\n      <td>-0.049907</td>\n      <td>0.017461</td>\n      <td>-0.552175</td>\n      <td>0.026533</td>\n      <td>-0.036820</td>\n      <td>-0.000572</td>\n      <td>0.167837</td>\n      <td>0.068831</td>\n      <td>-0.044935</td>\n      <td>-0.014204</td>\n      <td>0.098796</td>\n      <td>-0.086490</td>\n      <td>-0.107535</td>\n      <td>-0.015403</td>\n      <td>0.194117</td>\n      <td>0.076873</td>\n      <td>0.127647</td>\n      <td>0.002150</td>\n      <td>-0.032479</td>\n      <td>0.042913</td>\n      <td>0.024105</td>\n      <td>-0.060284</td>\n      <td>-0.139178</td>\n      <td>-0.323357</td>\n      <td>-0.080555</td>\n      <td>-0.101109</td>\n      <td>-0.111122</td>\n      <td>0.082361</td>\n      <td>-0.090455</td>\n      <td>-0.517197</td>\n      <td>-0.067904</td>\n      <td>0.092758</td>\n      <td>-0.099924</td>\n      <td>-0.086709</td>\n      <td>0.199176</td>\n      <td>0.141999</td>\n      <td>0.092574</td>\n      <td>-0.120088</td>\n      <td>0.046302</td>\n      <td>-0.116925</td>\n      <td>-0.097921</td>\n      <td>0.037665</td>\n      <td>-0.058858</td>\n      <td>-0.050301</td>\n      <td>-0.093208</td>\n      <td>-0.452632</td>\n      <td>-0.104972</td>\n      <td>-0.097232</td>\n      <td>-0.044161</td>\n      <td>-0.160200</td>\n      <td>-0.096984</td>\n      <td>0.183310</td>\n      <td>-0.075674</td>\n      <td>-0.174416</td>\n      <td>-0.002953</td>\n      <td>-0.125316</td>\n      <td>0.022790</td>\n      <td>-0.325339</td>\n      <td>-1.249750</td>\n      <td>0.219210</td>\n      <td>0.112360</td>\n      <td>-0.027278</td>\n      <td>-0.026580</td>\n      <td>0.037978</td>\n      <td>-0.131886</td>\n      <td>0.285838</td>\n      <td>0.101624</td>\n      <td>0.100467</td>\n      <td>-0.274340</td>\n      <td>0.110626</td>\n      <td>-0.060746</td>\n      <td>0.268230</td>\n      <td>0.014230</td>\n      <td>-0.085340</td>\n      <td>-0.004362</td>\n      <td>0.237103</td>\n      <td>-0.127168</td>\n      <td>-0.253778</td>\n      <td>-0.125134</td>\n      <td>-0.144491</td>\n      <td>-0.060425</td>\n      <td>-0.220046</td>\n      <td>0.189528</td>\n      <td>0.214894</td>\n      <td>-0.068495</td>\n      <td>-0.009231</td>\n      <td>0.054502</td>\n      <td>0.090229</td>\n      <td>0.058186</td>\n      <td>0.016010</td>\n      <td>-0.028923</td>\n      <td>0.033826</td>\n      <td>0.007451</td>\n      <td>0.216144</td>\n      <td>-0.181054</td>\n      <td>-0.078201</td>\n      <td>-0.105774</td>\n      <td>0.225342</td>\n      <td>0.156310</td>\n      <td>0.191633</td>\n      <td>0.140339</td>\n      <td>0.310926</td>\n      <td>-0.164473</td>\n      <td>-0.166424</td>\n      <td>0.105870</td>\n      <td>0.122402</td>\n      <td>0.091046</td>\n      <td>-0.012213</td>\n      <td>0.057316</td>\n      <td>-0.016780</td>\n      <td>-0.070268</td>\n      <td>-0.214911</td>\n      <td>0.048223</td>\n      <td>0.114932</td>\n      <td>-0.035823</td>\n      <td>-0.263190</td>\n      <td>-0.165005</td>\n      <td>0.004527</td>\n      <td>0.098826</td>\n      <td>-0.015102</td>\n      <td>0.064898</td>\n      <td>0.020789</td>\n      <td>-0.102969</td>\n      <td>0.155976</td>\n      <td>0.334598</td>\n      <td>0.052263</td>\n      <td>0.014203</td>\n      <td>0.065645</td>\n      <td>0.021419</td>\n      <td>0.070152</td>\n      <td>0.060167</td>\n      <td>-0.075937</td>\n      <td>0.015064</td>\n      <td>0.033468</td>\n      <td>-0.011410</td>\n      <td>0.168035</td>\n      <td>-0.125233</td>\n      <td>0.246304</td>\n      <td>-0.048883</td>\n      <td>-0.210810</td>\n      <td>-0.090323</td>\n      <td>0.038766</td>\n      <td>-0.120851</td>\n      <td>0.086392</td>\n      <td>-0.097071</td>\n      <td>0.058819</td>\n      <td>0.102806</td>\n      <td>0.069638</td>\n      <td>0.018521</td>\n      <td>0.013644</td>\n      <td>-0.117160</td>\n      <td>0.269430</td>\n      <td>0.057706</td>\n      <td>0.085797</td>\n      <td>0.251591</td>\n      <td>-0.049474</td>\n      <td>0.498698</td>\n      <td>0.142232</td>\n      <td>0.112757</td>\n      <td>-0.088355</td>\n      <td>0.217441</td>\n      <td>0.224559</td>\n      <td>0.044062</td>\n      <td>-0.157754</td>\n      <td>0.462205</td>\n      <td>0.071578</td>\n      <td>0.045937</td>\n      <td>-0.017116</td>\n      <td>0.034172</td>\n      <td>0.005457</td>\n      <td>-0.087265</td>\n      <td>-0.045315</td>\n      <td>0.067920</td>\n      <td>-0.030377</td>\n      <td>-0.006359</td>\n      <td>1.455204</td>\n      <td>0.188507</td>\n      <td>-0.205016</td>\n      <td>-0.068080</td>\n      <td>0.292195</td>\n      <td>0.041927</td>\n      <td>-0.174890</td>\n      <td>-0.046047</td>\n      <td>0.206588</td>\n      <td>0.064226</td>\n      <td>0.367615</td>\n      <td>-0.070862</td>\n      <td>0.009342</td>\n      <td>0.090434</td>\n      <td>0.001255</td>\n      <td>-0.047360</td>\n      <td>-0.103646</td>\n      <td>0.108661</td>\n      <td>11.651844</td>\n      <td>-0.063221</td>\n      <td>-0.013029</td>\n      <td>0.032770</td>\n      <td>-0.256653</td>\n      <td>0.004424</td>\n      <td>-0.158540</td>\n      <td>0.057062</td>\n      <td>0.279952</td>\n      <td>0.185350</td>\n      <td>-0.026612</td>\n      <td>-0.073181</td>\n      <td>0.144206</td>\n      <td>-0.092292</td>\n      <td>0.073268</td>\n      <td>0.087679</td>\n      <td>0.119163</td>\n      <td>0.097052</td>\n      <td>0.128711</td>\n      <td>-0.163158</td>\n      <td>0.196666</td>\n      <td>0.096923</td>\n      <td>0.021693</td>\n      <td>0.768567</td>\n      <td>-0.057968</td>\n      <td>0.151456</td>\n      <td>-0.043386</td>\n      <td>-0.004558</td>\n      <td>0.144182</td>\n      <td>0.187076</td>\n      <td>0.072016</td>\n      <td>0.245870</td>\n      <td>0.112004</td>\n      <td>-0.171178</td>\n      <td>0.033259</td>\n      <td>-0.128406</td>\n      <td>-0.077126</td>\n      <td>0.080339</td>\n      <td>0.073449</td>\n      <td>-0.066963</td>\n      <td>0.034276</td>\n      <td>-0.062217</td>\n      <td>0.260491</td>\n      <td>-0.303431</td>\n      <td>0.150829</td>\n      <td>-0.128606</td>\n      <td>0.227643</td>\n      <td>0.210428</td>\n      <td>0.000352</td>\n      <td>0.009954</td>\n      <td>0.003607</td>\n      <td>0.166239</td>\n      <td>0.208197</td>\n      <td>-0.103583</td>\n      <td>-0.023853</td>\n      <td>-0.391357</td>\n      <td>0.242652</td>\n      <td>-0.155245</td>\n      <td>0.061709</td>\n      <td>0.067378</td>\n      <td>-0.110414</td>\n      <td>-0.202818</td>\n      <td>0.078849</td>\n      <td>0.033232</td>\n      <td>-0.106000</td>\n      <td>0.203802</td>\n      <td>0.149935</td>\n      <td>-0.084985</td>\n      <td>0.015760</td>\n      <td>-0.048469</td>\n      <td>0.037743</td>\n      <td>0.096856</td>\n      <td>0.143798</td>\n      <td>-0.136856</td>\n      <td>-0.153232</td>\n      <td>-0.142397</td>\n      <td>-0.389400</td>\n      <td>-0.224074</td>\n      <td>-0.056422</td>\n      <td>-0.552498</td>\n      <td>0.065720</td>\n      <td>-0.176695</td>\n      <td>-0.079383</td>\n      <td>-0.159391</td>\n      <td>-0.431184</td>\n      <td>0.239871</td>\n      <td>-0.236745</td>\n      <td>-0.009576</td>\n      <td>-0.231364</td>\n      <td>0.272019</td>\n      <td>-0.037922</td>\n      <td>0.021827</td>\n      <td>-0.032716</td>\n      <td>-0.068437</td>\n      <td>-0.025033</td>\n      <td>-0.066474</td>\n      <td>0.184263</td>\n      <td>0.019257</td>\n      <td>0.092456</td>\n      <td>-0.381083</td>\n      <td>-0.404728</td>\n      <td>-0.135549</td>\n      <td>-0.092909</td>\n      <td>-0.032019</td>\n      <td>-0.154564</td>\n      <td>0.245694</td>\n      <td>0.074664</td>\n      <td>-0.105019</td>\n      <td>0.138730</td>\n      <td>-0.051127</td>\n      <td>-0.003614</td>\n      <td>-0.033994</td>\n      <td>-0.271795</td>\n      <td>-0.037047</td>\n      <td>0.037355</td>\n      <td>-0.192260</td>\n      <td>0.073082</td>\n      <td>0.057636</td>\n      <td>0.151957</td>\n      <td>0.184729</td>\n      <td>0.029239</td>\n      <td>0.083366</td>\n      <td>-0.029940</td>\n      <td>-0.066711</td>\n      <td>0.070731</td>\n      <td>0.069432</td>\n      <td>-0.227426</td>\n      <td>0.069980</td>\n      <td>-0.165079</td>\n      <td>-0.067444</td>\n      <td>-0.143729</td>\n      <td>0.032275</td>\n      <td>0.372293</td>\n      <td>0.041818</td>\n      <td>0.032854</td>\n      <td>0.146240</td>\n      <td>-0.137018</td>\n      <td>0.190802</td>\n      <td>0.108555</td>\n      <td>0.170401</td>\n      <td>-0.170474</td>\n      <td>0.065800</td>\n      <td>-0.162053</td>\n      <td>-0.241368</td>\n      <td>0.072358</td>\n      <td>-0.170512</td>\n      <td>0.181729</td>\n      <td>0.394393</td>\n      <td>0.065402</td>\n      <td>0.205735</td>\n      <td>-0.103268</td>\n      <td>-0.039133</td>\n      <td>-0.052853</td>\n      <td>-0.421413</td>\n      <td>0.066120</td>\n      <td>-0.036619</td>\n      <td>-0.129683</td>\n      <td>0.152110</td>\n      <td>0.058852</td>\n      <td>-0.200716</td>\n      <td>0.425438</td>\n      <td>-0.633506</td>\n      <td>-0.184883</td>\n      <td>-0.046711</td>\n      <td>-0.277424</td>\n      <td>0.150955</td>\n      <td>0.174308</td>\n      <td>-0.191348</td>\n      <td>0.121687</td>\n      <td>-0.002648</td>\n      <td>-0.149996</td>\n      <td>0.036184</td>\n      <td>-0.095719</td>\n      <td>0.059427</td>\n      <td>0.130815</td>\n      <td>0.155930</td>\n      <td>0.243257</td>\n      <td>0.070662</td>\n      <td>0.079929</td>\n      <td>-0.050331</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "NFOLDS = \n",
    "kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=777)\n",
    "\n",
    "df_train['fold_no'] = [0 for x in range(len(df_train))]\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df_train)):\n",
    "    df_train.loc[val_idx, 'fold_no'] = fold\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_0_cols = ['econ_y', 'eess_y', 'nlin_y', 'physics_y', 'acc-phys_y',\n",
    "       'adap-org_y', 'alg-geom_y', 'ao-sci_y', 'astro-ph_y', 'atom-ph_y',\n",
    "       'bayes-an_y', 'chao-dyn_y', 'chem-ph_y', 'cmp-lg_y', 'comp-gas_y',\n",
    "       'dg-ga_y', 'funct-an_y', 'gr-qc_y', 'math-ph_y', 'mtrl-th_y',\n",
    "       'nucl-ex_y', 'patt-sol_y', 'plasm-ph_y', 'q-alg_y', 'q-fin_y',\n",
    "       'solv-int_y', 'supr-con_y', 'acc_y', 'adap_y', 'alg_y', 'ao_y',\n",
    "       'astro_y', 'atom_y', 'bayes_y', 'chao_y', 'chem_y', 'cmp_y',\n",
    "       'comp_y', 'cond_y', 'dg_y', 'econ', 'eess', 'funct_y', 'gr_y',\n",
    "       'math', 'mtrl_y', 'nlin', 'patt_y', 'physics', 'plasm_y',\n",
    "       'quant_y', 'solv_y', 'stat', 'supr_y', 'astro-ph.ga',\n",
    "       'astro-ph.he', 'astro-ph.sr', 'cond-mat.dis-nn',\n",
    "       'cond-mat.mes-hall', 'cond-mat.other', 'cond-mat.soft',\n",
    "       'cond-mat.stat-mech', 'cs.ai', 'cs.ar', 'cs.cc', 'cs.ce', 'cs.cg',\n",
    "       'cs.cl', 'cs.cr', 'cs.cv', 'cs.cy', 'cs.db', 'cs.dc', 'cs.dl',\n",
    "       'cs.dm', 'cs.et', 'cs.fl', 'cs.gl', 'cs.gr', 'cs.gt', 'cs.hc',\n",
    "       'cs.ir', 'cs.it', 'cs.lo', 'cs.ma', 'cs.mm', 'cs.ms', 'cs.na',\n",
    "       'cs.ne', 'cs.ni', 'cs.oh', 'cs.os', 'cs.pf', 'cs.pl', 'cs.ro',\n",
    "       'cs.sc', 'cs.sd', 'cs.se', 'cs.sy', 'econ.em', 'econ.gn',\n",
    "       'econ.th', 'eess.as', 'eess.iv', 'eess.sp', 'eess.sy', 'math.ac',\n",
    "       'math.ap', 'math.at', 'math.ca', 'math.ct', 'math.cv', 'math.dg',\n",
    "       'math.ds', 'math.fa', 'math.gm', 'math.gn', 'math.gr', 'math.gt',\n",
    "       'math.ho', 'math.it', 'math.kt', 'math.lo', 'math.mg', 'math.mp',\n",
    "       'math.na', 'math.nt', 'math.oa', 'math.oc', 'math.qa', 'math.ra',\n",
    "       'math.rt', 'math.sg', 'math.sp', 'math.st', 'nlin.ao', 'nlin.cd',\n",
    "       'nlin.cg', 'nlin.ps', 'nlin.si', 'physics.acc-ph', 'physics.ao-ph',\n",
    "       'physics.app-ph', 'physics.atm-clus', 'physics.bio-ph',\n",
    "       'physics.chem-ph', 'physics.class-ph', 'physics.comp-ph',\n",
    "       'physics.data-an', 'physics.ed-ph', 'physics.flu-dyn',\n",
    "       'physics.gen-ph', 'physics.geo-ph', 'physics.hist-ph',\n",
    "       'physics.ins-det', 'physics.med-ph', 'physics.optics',\n",
    "       'physics.plasm-ph', 'physics.pop-ph', 'physics.soc-ph', 'q-bio.bm',\n",
    "       'q-bio.cb', 'q-bio.gn', 'q-bio.mn', 'q-bio.nc', 'q-bio.ot',\n",
    "       'q-bio.pe', 'q-bio.qm', 'q-bio.sc', 'q-bio.to', 'q-fin.cp',\n",
    "       'q-fin.ec', 'q-fin.gn', 'q-fin.mf', 'q-fin.pm', 'q-fin.pr',\n",
    "       'q-fin.rm', 'q-fin.st', 'q-fin.tr', 'stat.ap', 'stat.co',\n",
    "       'stat.me', 'stat.ml', 'stat.ot', 'doi_cites_min_doi_id_label',\n",
    "       'doi_cites_min_pub_publisher_label',\n",
    "       'doi_cites_median_pub_publisher_label', 'doi_cites_min_update_ym',\n",
    "       'doi_cites_min_first_created_ym', 'doi_cites_min_license_label',\n",
    "       'doi_cites_max_license_label', 'doi_cites_q10_license_label',\n",
    "       'doi_cites_q75_license_label', 'doi_cites_min_category_main_label',\n",
    "       'doi_cites_q10_category_main_label',\n",
    "       'doi_cites_q25_category_main_label',\n",
    "       'doi_cites_min_category_main_detail_label',\n",
    "       'doi_cites_median_category_main_detail_label',\n",
    "       'doi_cites_q10_category_main_detail_label',\n",
    "       'doi_cites_q25_category_main_detail_label',\n",
    "       'doi_cites_min_category_name_parent_label',\n",
    "       'doi_cites_q10_category_name_parent_label',\n",
    "       'doi_cites_min_category_name_parent_main_label',\n",
    "       'doi_cites_q10_category_name_parent_main_label',\n",
    "       'doi_cites_min_category_name_label',\n",
    "       'pred_doi_cites_min_doi_id_label',\n",
    "       'pred_doi_cites_min_pub_publisher_label',\n",
    "       'pred_doi_cites_median_pub_publisher_label',\n",
    "       'pred_doi_cites_q75_pub_publisher_label',\n",
    "       'pred_doi_cites_min_update_ym', 'pred_doi_cites_q10_update_ym',\n",
    "       'pred_doi_cites_min_first_created_ym',\n",
    "       'pred_doi_cites_q10_first_created_ym',\n",
    "       'pred_doi_cites_mean_license_label',\n",
    "       'pred_doi_cites_count_license_label',\n",
    "       'pred_doi_cites_sum_license_label',\n",
    "       'pred_doi_cites_min_license_label',\n",
    "       'pred_doi_cites_std_license_label',\n",
    "       'pred_doi_cites_q10_license_label',\n",
    "       'pred_doi_cites_q25_license_label',\n",
    "       'pred_doi_cites_q75_license_label',\n",
    "       'pred_doi_cites_mean_category_main_label',\n",
    "       'pred_doi_cites_min_category_main_label',\n",
    "       'pred_doi_cites_median_category_main_label',\n",
    "       'pred_doi_cites_q10_category_main_label',\n",
    "       'pred_doi_cites_q25_category_main_label',\n",
    "       'pred_doi_cites_mean_category_main_detail_label',\n",
    "       'pred_doi_cites_sum_category_main_detail_label',\n",
    "       'pred_doi_cites_min_category_main_detail_label',\n",
    "       'pred_doi_cites_median_category_main_detail_label',\n",
    "       'pred_doi_cites_q10_category_main_detail_label',\n",
    "       'pred_doi_cites_q25_category_main_detail_label',\n",
    "       'pred_doi_cites_q75_category_main_detail_label',\n",
    "       'pred_doi_cites_min_category_name_parent_label',\n",
    "       'pred_doi_cites_median_category_name_parent_label',\n",
    "       'pred_doi_cites_q10_category_name_parent_label',\n",
    "       'pred_doi_cites_q25_category_name_parent_label',\n",
    "       'pred_doi_cites_min_category_name_parent_main_label',\n",
    "       'pred_doi_cites_median_category_name_parent_main_label',\n",
    "       'pred_doi_cites_q10_category_name_parent_main_label',\n",
    "       'pred_doi_cites_q25_category_name_parent_main_label',\n",
    "       'pred_doi_cites_min_category_name_label',\n",
    "       'diff_rate_doi_cites_pred_doi_cites',\n",
    "       'diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_submitter_label',\n",
    "       'diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_pub_publisher_label',\n",
    "       'diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_doi_id_label',\n",
    "       'diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_pub_publisher_label',\n",
    "       'diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_author_first_label',\n",
    "       'diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_pub_publisher_label',\n",
    "       'diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_update_ym',\n",
    "       'diff_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_first_created_ym',\n",
    "       'diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_doi_cites_mean_license_label_pred_doi_cites_mean_license_label',\n",
    "       'diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_license_label',\n",
    "       'diff_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_doi_cites_mean_category_main_label_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_doi_id_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_pub_publisher_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_update_ym',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_license_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_update_ym',\n",
    "       'diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_license_label',\n",
    "       'diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_license_label',\n",
    "       'diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_pub_publisher_label',\n",
    "       'diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_doi_id_label',\n",
    "       'diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_author_first_label',\n",
    "       'diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_pub_publisher_label',\n",
    "       'diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_update_ym',\n",
    "       'diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_first_created_ym',\n",
    "       'diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_license_label',\n",
    "       'diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_pub_publisher_label',\n",
    "       'diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_update_ym',\n",
    "       'diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_update_ym',\n",
    "       'diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_first_created_ym',\n",
    "       'diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_first_created_ym',\n",
    "       'diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_license_label',\n",
    "       'diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_license_label',\n",
    "       'diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_license_label',\n",
    "       'diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_license_label',\n",
    "       'diff_rate_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_rate_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_rate_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_rate_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_rate_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_rate_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_rate_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_rate_pred_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_pred_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_rate_pred_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_label',\n",
    "       'is_null_comments', 'is_null_journal-ref']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_0_cols.extend(['id', 'authors', 'title', 'comments',\n",
    "    'journal-ref', 'doi', 'report-no', 'categories', 'license',\n",
    "    'abstract', 'versions', 'authors_parsed', 'pub_publisher',\n",
    "    'update_date_y', 'first_created_date', 'last_created_date', 'doi_id', 'submitter', 'author_first', 'category_main', 'category_main_detail', 'category_name_parent_main_unique', 'category_name_parent_unique', 'category_name_unique',\n",
    "    'submitter_label','doi_id_label','author_first_label','pub_publisher_label',\n",
    "    'license_label','category_main_label','category_name_parent_label','category_name_parent_main_label', 'category_name_label',\n",
    "    #'diff_pred_doi_cites', 'rate_pred_doi_cites'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "    | 2/20 [00:29<04:38, 15.47s/it]\u001b[A\u001b[32m[I 2021-03-28 04:18:16,261]\u001b[0m Trial 41 finished with value: 0.5115401856119461 and parameters: {'lambda_l1': 5.862010160751429, 'lambda_l2': 0.00019899079419315966}. Best is trial 41 with value: 0.5115401856119461.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  10%|#         | 2/20 [00:29<04:38, 15.47s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  10%|#         | 2/20 [00:44<04:38, 15.47s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270:  15%|#5        | 3/20 [00:44<04:19, 15.29s/it]\u001b[A\u001b[32m[I 2021-03-28 04:18:31,131]\u001b[0m Trial 42 finished with value: 0.5117071008501705 and parameters: {'lambda_l1': 1.0281343117286185e-07, 'lambda_l2': 0.07362007087857975}. Best is trial 41 with value: 0.5115401856119461.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  15%|#5        | 3/20 [00:44<04:19, 15.29s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  15%|#5        | 3/20 [01:04<04:19, 15.29s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270:  20%|##        | 4/20 [01:04<04:25, 16.61s/it]\u001b[A\u001b[32m[I 2021-03-28 04:18:50,822]\u001b[0m Trial 43 finished with value: 0.5115079911613138 and parameters: {'lambda_l1': 9.510533055600156e-06, 'lambda_l2': 0.0045895706910520076}. Best is trial 43 with value: 0.5115079911613138.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  20%|##        | 4/20 [01:04<04:25, 16.61s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  20%|##        | 4/20 [01:26<04:25, 16.61s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270:  25%|##5       | 5/20 [01:26<04:32, 18.19s/it]\u001b[A\u001b[32m[I 2021-03-28 04:19:12,711]\u001b[0m Trial 44 finished with value: 0.5102702912509909 and parameters: {'lambda_l1': 1.2909721308949545e-08, 'lambda_l2': 3.631451323185456e-06}. Best is trial 44 with value: 0.5102702912509909.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  25%|##5       | 5/20 [01:26<04:32, 18.19s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  25%|##5       | 5/20 [01:48<04:32, 18.19s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270:  30%|###       | 6/20 [01:48<04:30, 19.33s/it]\u001b[A\u001b[32m[I 2021-03-28 04:19:34,700]\u001b[0m Trial 45 finished with value: 0.5102701168763357 and parameters: {'lambda_l1': 4.778897020869823e-07, 'lambda_l2': 3.6995312884728906e-08}. Best is trial 45 with value: 0.5102701168763357.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  30%|###       | 6/20 [01:48<04:30, 19.33s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  30%|###       | 6/20 [02:10<04:30, 19.33s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270:  35%|###5      | 7/20 [02:10<04:23, 20.23s/it]\u001b[A\u001b[32m[I 2021-03-28 04:19:57,041]\u001b[0m Trial 46 finished with value: 0.5102702884924208 and parameters: {'lambda_l1': 5.770472606796519e-06, 'lambda_l2': 0.0001327066259211453}. Best is trial 45 with value: 0.5102701168763357.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  35%|###5      | 7/20 [02:10<04:23, 20.23s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  35%|###5      | 7/20 [02:25<04:23, 20.23s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270:  40%|####      | 8/20 [02:25<03:44, 18.74s/it]\u001b[A\u001b[32m[I 2021-03-28 04:20:12,280]\u001b[0m Trial 47 finished with value: 0.5120569904598329 and parameters: {'lambda_l1': 0.00013742817013747454, 'lambda_l2': 6.047616101730652}. Best is trial 45 with value: 0.5102701168763357.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  40%|####      | 8/20 [02:25<03:44, 18.74s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  40%|####      | 8/20 [02:47<03:44, 18.74s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270:  45%|####5     | 9/20 [02:47<03:35, 19.59s/it]\u001b[A\u001b[32m[I 2021-03-28 04:20:33,866]\u001b[0m Trial 48 finished with value: 0.5102702913432219 and parameters: {'lambda_l1': 2.4758364030736657e-07, 'lambda_l2': 8.195728698106591e-08}. Best is trial 45 with value: 0.5102701168763357.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  45%|####5     | 9/20 [02:47<03:35, 19.59s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  45%|####5     | 9/20 [03:07<03:35, 19.59s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270:  50%|#####     | 10/20 [03:07<03:16, 19.62s/it]\u001b[A\u001b[32m[I 2021-03-28 04:20:53,563]\u001b[0m Trial 49 finished with value: 0.5115581185296776 and parameters: {'lambda_l1': 0.029356293914542864, 'lambda_l2': 0.004208454339637081}. Best is trial 45 with value: 0.5102701168763357.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  50%|#####     | 10/20 [03:07<03:16, 19.62s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064938 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  50%|#####     | 10/20 [03:22<03:16, 19.62s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270:  55%|#####5    | 11/20 [03:22<02:45, 18.35s/it]\u001b[A\u001b[32m[I 2021-03-28 04:21:08,958]\u001b[0m Trial 50 finished with value: 0.511547136668019 and parameters: {'lambda_l1': 0.12229317626204782, 'lambda_l2': 1.7470939643055957e-08}. Best is trial 45 with value: 0.5102701168763357.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  55%|#####5    | 11/20 [03:22<02:45, 18.35s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064541 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  55%|#####5    | 11/20 [03:44<02:45, 18.35s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270:  60%|######    | 12/20 [03:44<02:35, 19.49s/it]\u001b[A\u001b[32m[I 2021-03-28 04:21:31,083]\u001b[0m Trial 51 finished with value: 0.5102702913934349 and parameters: {'lambda_l1': 4.233318795582756e-06, 'lambda_l2': 8.092961597556779e-07}. Best is trial 45 with value: 0.5102701168763357.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  60%|######    | 12/20 [03:44<02:35, 19.49s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  60%|######    | 12/20 [04:02<02:35, 19.49s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270:  65%|######5   | 13/20 [04:02<02:12, 18.97s/it]\u001b[A\u001b[32m[I 2021-03-28 04:21:48,834]\u001b[0m Trial 52 finished with value: 0.5116972144718152 and parameters: {'lambda_l1': 3.854362747526809e-06, 'lambda_l2': 4.987204191417331}. Best is trial 45 with value: 0.5102701168763357.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  65%|######5   | 13/20 [04:02<02:12, 18.97s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  65%|######5   | 13/20 [04:24<02:12, 18.97s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270:  70%|#######   | 14/20 [04:24<01:59, 19.92s/it]\u001b[A\u001b[32m[I 2021-03-28 04:22:10,990]\u001b[0m Trial 53 finished with value: 0.5102702913317668 and parameters: {'lambda_l1': 2.0277301248334622e-08, 'lambda_l2': 4.6781989157158397e-07}. Best is trial 45 with value: 0.5102701168763357.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  70%|#######   | 14/20 [04:24<01:59, 19.92s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067388 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  70%|#######   | 14/20 [04:39<01:59, 19.92s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270:  75%|#######5  | 15/20 [04:39<01:32, 18.57s/it]\u001b[A\u001b[32m[I 2021-03-28 04:22:26,397]\u001b[0m Trial 54 finished with value: 0.5112849996816203 and parameters: {'lambda_l1': 9.252223364503636e-05, 'lambda_l2': 1.4416851930421652e-05}. Best is trial 45 with value: 0.5102701168763357.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  75%|#######5  | 15/20 [04:39<01:32, 18.57s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064888 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  75%|#######5  | 15/20 [04:56<01:32, 18.57s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270:  80%|########  | 16/20 [04:56<01:12, 18.10s/it]\u001b[A\u001b[32m[I 2021-03-28 04:22:43,408]\u001b[0m Trial 55 finished with value: 0.5115501922477566 and parameters: {'lambda_l1': 4.6339932601227565e-07, 'lambda_l2': 0.25969230835223883}. Best is trial 45 with value: 0.5102701168763357.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  80%|########  | 16/20 [04:56<01:12, 18.10s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  80%|########  | 16/20 [05:18<01:12, 18.10s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270:  85%|########5 | 17/20 [05:18<00:57, 19.22s/it]\u001b[A\u001b[32m[I 2021-03-28 04:23:05,246]\u001b[0m Trial 56 finished with value: 0.5102702917115325 and parameters: {'lambda_l1': 1.992548158264979e-05, 'lambda_l2': 1.4587180134990865e-08}. Best is trial 45 with value: 0.5102701168763357.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  85%|########5 | 17/20 [05:18<00:57, 19.22s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075845 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  85%|########5 | 17/20 [05:33<00:57, 19.22s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270:  90%|######### | 18/20 [05:33<00:36, 18.01s/it]\u001b[A\u001b[32m[I 2021-03-28 04:23:20,440]\u001b[0m Trial 57 finished with value: 0.5117391622176234 and parameters: {'lambda_l1': 0.004339616066809944, 'lambda_l2': 0.0012111635767084882}. Best is trial 45 with value: 0.5102701168763357.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  90%|######### | 18/20 [05:33<00:36, 18.01s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  90%|######### | 18/20 [05:48<00:36, 18.01s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270:  95%|#########5| 19/20 [05:48<00:16, 16.88s/it]\u001b[A\u001b[32m[I 2021-03-28 04:23:34,672]\u001b[0m Trial 58 finished with value: 0.5121827539640055 and parameters: {'lambda_l1': 8.638576877963167e-07, 'lambda_l2': 0.06636992935968503}. Best is trial 45 with value: 0.5102701168763357.\u001b[0m\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  95%|#########5| 19/20 [05:48<00:16, 16.88s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "regularization_factors, val_score: 0.510270:  95%|#########5| 19/20 [06:10<00:16, 16.88s/it]\u001b[A\n",
      "regularization_factors, val_score: 0.510270: 100%|##########| 20/20 [06:10<00:00, 18.38s/it]\u001b[A\u001b[32m[I 2021-03-28 04:23:56,553]\u001b[0m Trial 59 finished with value: 0.5102702883014907 and parameters: {'lambda_l1': 2.9126630104132885e-08, 'lambda_l2': 0.00013698827955837037}. Best is trial 45 with value: 0.5102701168763357.\u001b[0m\n",
      "regularization_factors, val_score: 0.510270: 100%|##########| 20/20 [06:10<00:00, 18.50s/it]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "min_data_in_leaf, val_score: 0.510270:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063631 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "min_data_in_leaf, val_score: 0.510270:   0%|          | 0/5 [00:14<?, ?it/s]\u001b[A\n",
      "min_data_in_leaf, val_score: 0.510270:  20%|##        | 1/5 [00:14<00:57, 14.31s/it]\u001b[A\u001b[32m[I 2021-03-28 04:24:10,869]\u001b[0m Trial 60 finished with value: 0.5126662168519455 and parameters: {'min_child_samples': 100}. Best is trial 60 with value: 0.5126662168519455.\u001b[0m\n",
      "\n",
      "min_data_in_leaf, val_score: 0.510270:  20%|##        | 1/5 [00:14<00:57, 14.31s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "min_data_in_leaf, val_score: 0.510168:  20%|##        | 1/5 [00:37<00:57, 14.31s/it]\u001b[A\n",
      "min_data_in_leaf, val_score: 0.510168:  40%|####      | 2/5 [00:37<00:50, 16.89s/it]\u001b[A\u001b[32m[I 2021-03-28 04:24:33,771]\u001b[0m Trial 61 finished with value: 0.5101681905733891 and parameters: {'min_child_samples': 10}. Best is trial 61 with value: 0.5101681905733891.\u001b[0m\n",
      "\n",
      "min_data_in_leaf, val_score: 0.510168:  40%|####      | 2/5 [00:37<00:50, 16.89s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062501 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "min_data_in_leaf, val_score: 0.510168:  40%|####      | 2/5 [00:53<00:50, 16.89s/it]\u001b[A\n",
      "min_data_in_leaf, val_score: 0.510168:  60%|######    | 3/5 [00:53<00:33, 16.70s/it]\u001b[A\u001b[32m[I 2021-03-28 04:24:50,030]\u001b[0m Trial 62 finished with value: 0.5108197689317007 and parameters: {'min_child_samples': 25}. Best is trial 61 with value: 0.5101681905733891.\u001b[0m\n",
      "\n",
      "min_data_in_leaf, val_score: 0.510168:  60%|######    | 3/5 [00:53<00:33, 16.70s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065676 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "min_data_in_leaf, val_score: 0.510168:  60%|######    | 3/5 [01:07<00:33, 16.70s/it]\u001b[A\n",
      "min_data_in_leaf, val_score: 0.510168:  80%|########  | 4/5 [01:07<00:16, 16.01s/it]\u001b[A\u001b[32m[I 2021-03-28 04:25:04,417]\u001b[0m Trial 63 finished with value: 0.5127265206538938 and parameters: {'min_child_samples': 5}. Best is trial 61 with value: 0.5101681905733891.\u001b[0m\n",
      "\n",
      "min_data_in_leaf, val_score: 0.510168:  80%|########  | 4/5 [01:07<00:16, 16.01s/it]\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066726 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 355276\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1555\n",
      "[LightGBM] [Info] Start training from score 2.576288\n",
      "\n",
      "min_data_in_leaf, val_score: 0.510168:  80%|########  | 4/5 [01:23<00:16, 16.01s/it]\u001b[A\n",
      "min_data_in_leaf, val_score: 0.510168: 100%|##########| 5/5 [01:23<00:00, 15.81s/it]\u001b[A\u001b[32m[I 2021-03-28 04:25:19,756]\u001b[0m Trial 64 finished with value: 0.5120643253985561 and parameters: {'min_child_samples': 50}. Best is trial 61 with value: 0.5101681905733891.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.510168: 100%|##########| 5/5 [01:23<00:00, 16.64s/it]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'objective': 'regression',\n",
       " 'metric': 'rmse',\n",
       " 'learning_rate': 0.01,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 0.0,\n",
       " 'lambda_l2': 0.0,\n",
       " 'num_leaves': 29,\n",
       " 'feature_fraction': 0.4,\n",
       " 'bagging_fraction': 1.0,\n",
       " 'bagging_freq': 0,\n",
       " 'min_child_samples': 10,\n",
       " 'num_iterations': 10000,\n",
       " 'early_stopping_round': 50}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# \n",
    "result_y = []\n",
    "lgb_result_proba_test = []\n",
    "lgb_result_proba_valid = []\n",
    "\n",
    "fold_no = 2\n",
    "test_fold_no = fold_no\n",
    "valid_fold_no = fold_no + 1\n",
    "if valid_fold_no == NFOLDS:\n",
    "    valid_fold_no = 0\n",
    "\n",
    "# train\n",
    "train = df_train.copy()\n",
    "y_train = train[~train['fold_no'].isin([test_fold_no, valid_fold_no])]['cites'].values\n",
    "y_valid = train[train['fold_no'].isin([test_fold_no, valid_fold_no])]['cites'].values\n",
    "\n",
    "train = train.drop(importance_0_cols, axis=1)\n",
    "x_train = train[~train['fold_no'].isin([test_fold_no, valid_fold_no])]\n",
    "x_valid = train[train['fold_no'].isin([test_fold_no, valid_fold_no])]\n",
    "\n",
    "'''\n",
    "# target encoding\n",
    "target = 'doi_cites'\n",
    "key = 'pred_doi_cites'\n",
    "x_train, x_valid, x_test = make_statics_table(target, key, 'doi_cites', x_train, x_valid, x_test)\n",
    "\n",
    "keys = ['update_ym']\n",
    "target = 'cites'\n",
    "for key in keys:\n",
    "    x_train, x_valid, x_test = make_statics_table(target, key, 'doi_cites', x_train, x_valid, x_test)\n",
    "'''\n",
    "\n",
    "# drop\n",
    "x_train = x_train.drop(['cites', 'fold_no'], axis=1)\n",
    "x_valid = x_valid.drop(['cites', 'fold_no'], axis=1)\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_iterations': 10000,\n",
    "    'early_stopping_rounds': 50\n",
    "}\n",
    "best_params, history = {}, []\n",
    "trains = lgb.Dataset(x_train, y_train)\n",
    "valids = lgb.Dataset(x_valid, y_valid)\n",
    "\n",
    "best = lgbo.train(params, trains, valid_sets=valids,\n",
    "                    verbose_eval=False,\n",
    "                    num_boost_round=100,\n",
    "                    early_stopping_rounds=50)\n",
    "best.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.6584\n",
      "[200]\tvalid_0's rmse: 0.529796\n",
      "[300]\tvalid_0's rmse: 0.500701\n",
      "[400]\tvalid_0's rmse: 0.491146\n",
      "[500]\tvalid_0's rmse: 0.486466\n",
      "[600]\tvalid_0's rmse: 0.483945\n",
      "[700]\tvalid_0's rmse: 0.482658\n",
      "[800]\tvalid_0's rmse: 0.481833\n",
      "[900]\tvalid_0's rmse: 0.481527\n",
      "[1000]\tvalid_0's rmse: 0.481363\n",
      "[1100]\tvalid_0's rmse: 0.481159\n",
      "Early stopping, best iteration is:\n",
      "[1081]\tvalid_0's rmse: 0.481064\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.656115\n",
      "[200]\tvalid_0's rmse: 0.549682\n",
      "[300]\tvalid_0's rmse: 0.530599\n",
      "[400]\tvalid_0's rmse: 0.525011\n",
      "[500]\tvalid_0's rmse: 0.521972\n",
      "[600]\tvalid_0's rmse: 0.520405\n",
      "[700]\tvalid_0's rmse: 0.519453\n",
      "[800]\tvalid_0's rmse: 0.518803\n",
      "[900]\tvalid_0's rmse: 0.518184\n",
      "[1000]\tvalid_0's rmse: 0.517624\n",
      "[1100]\tvalid_0's rmse: 0.517451\n",
      "Early stopping, best iteration is:\n",
      "[1096]\tvalid_0's rmse: 0.517333\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.670592\n",
      "[200]\tvalid_0's rmse: 0.550574\n",
      "[300]\tvalid_0's rmse: 0.525478\n",
      "[400]\tvalid_0's rmse: 0.516852\n",
      "[500]\tvalid_0's rmse: 0.511642\n",
      "[600]\tvalid_0's rmse: 0.509629\n",
      "[700]\tvalid_0's rmse: 0.508388\n",
      "[800]\tvalid_0's rmse: 0.507504\n",
      "[900]\tvalid_0's rmse: 0.50729\n",
      "[1000]\tvalid_0's rmse: 0.507106\n",
      "[1100]\tvalid_0's rmse: 0.507027\n",
      "Early stopping, best iteration is:\n",
      "[1132]\tvalid_0's rmse: 0.506753\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.65466\n",
      "[200]\tvalid_0's rmse: 0.53201\n",
      "[300]\tvalid_0's rmse: 0.506292\n",
      "[400]\tvalid_0's rmse: 0.498174\n",
      "[500]\tvalid_0's rmse: 0.49446\n",
      "[600]\tvalid_0's rmse: 0.49226\n",
      "[700]\tvalid_0's rmse: 0.49159\n",
      "[800]\tvalid_0's rmse: 0.491053\n",
      "[900]\tvalid_0's rmse: 0.49065\n",
      "Early stopping, best iteration is:\n",
      "[888]\tvalid_0's rmse: 0.490599\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.658981\n",
      "[200]\tvalid_0's rmse: 0.535602\n",
      "[300]\tvalid_0's rmse: 0.510509\n",
      "[400]\tvalid_0's rmse: 0.503574\n",
      "[500]\tvalid_0's rmse: 0.499936\n",
      "[600]\tvalid_0's rmse: 0.498499\n",
      "[700]\tvalid_0's rmse: 0.497801\n",
      "[800]\tvalid_0's rmse: 0.497329\n",
      "Early stopping, best iteration is:\n",
      "[822]\tvalid_0's rmse: 0.497288\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.664632\n",
      "[200]\tvalid_0's rmse: 0.550039\n",
      "[300]\tvalid_0's rmse: 0.526657\n",
      "[400]\tvalid_0's rmse: 0.51899\n",
      "[500]\tvalid_0's rmse: 0.514508\n",
      "[600]\tvalid_0's rmse: 0.511861\n",
      "[700]\tvalid_0's rmse: 0.510549\n",
      "[800]\tvalid_0's rmse: 0.51\n",
      "[900]\tvalid_0's rmse: 0.50961\n",
      "[1000]\tvalid_0's rmse: 0.509537\n",
      "Early stopping, best iteration is:\n",
      "[971]\tvalid_0's rmse: 0.50931\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.651093\n",
      "[200]\tvalid_0's rmse: 0.526802\n",
      "[300]\tvalid_0's rmse: 0.502211\n",
      "[400]\tvalid_0's rmse: 0.494607\n",
      "[500]\tvalid_0's rmse: 0.491213\n",
      "[600]\tvalid_0's rmse: 0.48879\n",
      "[700]\tvalid_0's rmse: 0.487994\n",
      "[800]\tvalid_0's rmse: 0.487203\n",
      "[900]\tvalid_0's rmse: 0.486445\n",
      "[1000]\tvalid_0's rmse: 0.486473\n",
      "Early stopping, best iteration is:\n",
      "[971]\tvalid_0's rmse: 0.486246\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.655973\n",
      "[200]\tvalid_0's rmse: 0.527781\n",
      "[300]\tvalid_0's rmse: 0.499239\n",
      "[400]\tvalid_0's rmse: 0.489973\n",
      "[500]\tvalid_0's rmse: 0.485558\n",
      "[600]\tvalid_0's rmse: 0.483337\n",
      "[700]\tvalid_0's rmse: 0.481652\n",
      "[800]\tvalid_0's rmse: 0.480913\n",
      "[900]\tvalid_0's rmse: 0.480074\n",
      "[1000]\tvalid_0's rmse: 0.479825\n",
      "Early stopping, best iteration is:\n",
      "[961]\tvalid_0's rmse: 0.479723\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.661299\n",
      "[200]\tvalid_0's rmse: 0.546275\n",
      "[300]\tvalid_0's rmse: 0.524571\n",
      "[400]\tvalid_0's rmse: 0.51762\n",
      "[500]\tvalid_0's rmse: 0.514701\n",
      "[600]\tvalid_0's rmse: 0.513407\n",
      "[700]\tvalid_0's rmse: 0.512646\n",
      "[800]\tvalid_0's rmse: 0.51193\n",
      "Early stopping, best iteration is:\n",
      "[812]\tvalid_0's rmse: 0.511908\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.644455\n",
      "[200]\tvalid_0's rmse: 0.522039\n",
      "[300]\tvalid_0's rmse: 0.497485\n",
      "[400]\tvalid_0's rmse: 0.490261\n",
      "[500]\tvalid_0's rmse: 0.486568\n",
      "[600]\tvalid_0's rmse: 0.484379\n",
      "[700]\tvalid_0's rmse: 0.482716\n",
      "Early stopping, best iteration is:\n",
      "[748]\tvalid_0's rmse: 0.482062\n",
      "valid 0.49641139891276964\n"
     ]
    }
   ],
   "source": [
    "result_y_test = []\n",
    "result_y_valid = []\n",
    "lgb_result_proba_test = []\n",
    "lgb_result_proba_valid = []\n",
    "\n",
    "'''\n",
    "lgb_params = {'objective': 'regression',\n",
    " 'metric': 'rmse',\n",
    " 'learning_rate': 0.01,\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 8.856362720452578e-07,\n",
    " 'lambda_l2': 0.0002814044321572516,\n",
    " 'num_leaves': 31,\n",
    " 'feature_fraction': 0.42,\n",
    " 'bagging_fraction': 0.8762325382386713,\n",
    " 'bagging_freq': 4,\n",
    " 'min_child_samples': 20,\n",
    " 'num_iterations': 10000,\n",
    " 'early_stopping_round': 50,}\n",
    "'''\n",
    "\n",
    "lgb_params = {'objective': 'regression',\n",
    " 'metric': 'rmse',\n",
    " 'feature_pre_filter': False,\n",
    " 'num_leaves': 31,\n",
    " 'feature_fraction': 0.42,\n",
    " 'bagging_fraction': 0.8762325382386713,\n",
    " 'bagging_freq': 4,\n",
    " 'learning_rate': 0.01,\n",
    " 'lambda_l1': 8.856362720452578e-07,\n",
    " 'lambda_l2': 0.0002814044321572516,\n",
    " 'min_child_samples': 20,\n",
    " 'num_iterations': 10000,\n",
    " 'early_stopping_round': 50}\n",
    "\n",
    "for fold_no in range(NFOLDS):\n",
    "    test_fold_no = fold_no\n",
    "    # train\n",
    "    train = df_train.copy()\n",
    "    y_train = train[~train['fold_no'].isin([valid_fold_no])]['cites'].values # test_fold_no, \n",
    "    y_valid = train[train['fold_no'] == valid_fold_no]['cites'].values\n",
    "    #y_test = train[train['fold_no'] == test_fold_no]['cites'].values\n",
    "\n",
    "    train = train.drop(importance_0_cols, axis=1)\n",
    "\n",
    "    x_train = train[~train['fold_no'].isin([valid_fold_no])] # test_fold_no, \n",
    "    x_valid = train[train['fold_no'] == valid_fold_no]\n",
    "    #x_test = train[train['fold_no'] == test_fold_no]\n",
    "\n",
    "    '''\n",
    "    # target encoding\n",
    "    target = 'doi_cites'\n",
    "    key = 'pred_doi_cites'\n",
    "    x_train, x_valid, x_test = make_statics_table(target, key, 'doi_cites', x_train, x_valid, x_test)\n",
    "\n",
    "    keys = ['update_ym']\n",
    "    target = 'cites'\n",
    "    for key in keys:\n",
    "        x_train, x_valid, x_test = make_statics_table(target, key, 'doi_cites', x_train, x_valid, x_test)\n",
    "    '''\n",
    "\n",
    "    # drop\n",
    "    x_train = x_train.drop(['cites', 'fold_no'], axis=1)\n",
    "    x_valid = x_valid.drop(['cites', 'fold_no'], axis=1)\n",
    "    #x_test = x_test.drop(['cites', 'fold_no'], axis=1)\n",
    "\n",
    "    model = lgb.LGBMRegressor(**lgb_params)\n",
    "    model.fit(x_train, y_train,\n",
    "                eval_set=(x_valid, y_valid),\n",
    "                eval_metric='rmse',\n",
    "                verbose=100,\n",
    "                early_stopping_rounds=50,\n",
    "    )\n",
    "\n",
    "    pickle.dump(model, open(f'../models/lgb_{fold_no}.pickle', 'wb'))\n",
    "\n",
    "    #fold_result_test = model.predict(x_test)\n",
    "    fold_result_valid = model.predict(x_valid)\n",
    "    #result_y_test.extend(y_test)\n",
    "    result_y_valid.extend(y_valid)\n",
    "    #lgb_result_proba_test.extend(fold_result_test)\n",
    "    lgb_result_proba_valid.extend(fold_result_valid)\n",
    "    #rmsle = mean_squared_error(y_test, fold_result_test, squared=False)\n",
    "    #print(f\"fold {fold_no} lgb score: {rmsle}\")\n",
    "\n",
    "\n",
    "print(\"valid\", mean_squared_error(result_y_valid, lgb_result_proba_valid, squared=False))\n",
    "#print(\"test\", mean_squared_error(result_y_test, lgb_result_proba_test, squared=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m[I 2021-03-28 05:02:29,175]\u001b[0m A new study created in memory with name: no-name-1b4f71d6-c821-4936-8e06-9043ee8cc1d3\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:02:37,927]\u001b[0m Trial 0 finished with value: 0.5591247978476644 and parameters: {'bootstrap': True, 'max_depth': 10, 'max_leaf_nodes': 13, 'n_estimators': 47, 'min_samples_split': 11, 'min_samples_leaf': 14}. Best is trial 0 with value: 0.5591247978476644.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:03:34,668]\u001b[0m Trial 1 finished with value: 0.5419898130448385 and parameters: {'bootstrap': True, 'max_depth': 14, 'max_leaf_nodes': 45, 'n_estimators': 220, 'min_samples_split': 20, 'min_samples_leaf': 11}. Best is trial 1 with value: 0.5419898130448385.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:04:33,260]\u001b[0m Trial 2 finished with value: 0.5791650456165397 and parameters: {'bootstrap': False, 'max_depth': 26, 'max_leaf_nodes': 58, 'n_estimators': 134, 'min_samples_split': 92, 'min_samples_leaf': 12}. Best is trial 1 with value: 0.5419898130448385.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:04:59,086]\u001b[0m Trial 3 finished with value: 0.6003334896452377 and parameters: {'bootstrap': False, 'max_depth': 3, 'max_leaf_nodes': 51, 'n_estimators': 128, 'min_samples_split': 64, 'min_samples_leaf': 20}. Best is trial 1 with value: 0.5419898130448385.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:05:03,856]\u001b[0m Trial 4 finished with value: 0.7988727917946279 and parameters: {'bootstrap': True, 'max_depth': 1, 'max_leaf_nodes': 19, 'n_estimators': 107, 'min_samples_split': 100, 'min_samples_leaf': 17}. Best is trial 1 with value: 0.5419898130448385.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:06:02,031]\u001b[0m Trial 5 finished with value: 0.5484460127350754 and parameters: {'bootstrap': True, 'max_depth': 8, 'max_leaf_nodes': 23, 'n_estimators': 267, 'min_samples_split': 87, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.5419898130448385.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:06:07,551]\u001b[0m Trial 6 finished with value: 0.5888106148991205 and parameters: {'bootstrap': False, 'max_depth': 9, 'max_leaf_nodes': 12, 'n_estimators': 6, 'min_samples_split': 32, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.5419898130448385.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:06:44,073]\u001b[0m Trial 7 finished with value: 0.5436222812528677 and parameters: {'bootstrap': True, 'max_depth': 17, 'max_leaf_nodes': 47, 'n_estimators': 140, 'min_samples_split': 63, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.5419898130448385.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:07:26,755]\u001b[0m Trial 8 finished with value: 0.5493872719371954 and parameters: {'bootstrap': True, 'max_depth': 26, 'max_leaf_nodes': 24, 'n_estimators': 193, 'min_samples_split': 60, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.5419898130448385.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:08:32,356]\u001b[0m Trial 9 finished with value: 0.5948716731904524 and parameters: {'bootstrap': False, 'max_depth': 15, 'max_leaf_nodes': 10, 'n_estimators': 261, 'min_samples_split': 74, 'min_samples_leaf': 18}. Best is trial 1 with value: 0.5419898130448385.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:09:23,014]\u001b[0m Trial 10 finished with value: 0.544432117477379 and parameters: {'bootstrap': True, 'max_depth': 20, 'max_leaf_nodes': 38, 'n_estimators': 213, 'min_samples_split': 5, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.5419898130448385.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:10:14,413]\u001b[0m Trial 11 finished with value: 0.5462328022854984 and parameters: {'bootstrap': True, 'max_depth': 18, 'max_leaf_nodes': 42, 'n_estimators': 201, 'min_samples_split': 41, 'min_samples_leaf': 1}. Best is trial 1 with value: 0.5419898130448385.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:10:38,461]\u001b[0m Trial 12 finished with value: 0.5423565353997223 and parameters: {'bootstrap': True, 'max_depth': 21, 'max_leaf_nodes': 48, 'n_estimators': 92, 'min_samples_split': 21, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.5419898130448385.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:11:01,851]\u001b[0m Trial 13 finished with value: 0.5404036489176229 and parameters: {'bootstrap': True, 'max_depth': 22, 'max_leaf_nodes': 58, 'n_estimators': 74, 'min_samples_split': 23, 'min_samples_leaf': 10}. Best is trial 13 with value: 0.5404036489176229.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:11:19,657]\u001b[0m Trial 14 finished with value: 0.5404148368644458 and parameters: {'bootstrap': True, 'max_depth': 23, 'max_leaf_nodes': 60, 'n_estimators': 57, 'min_samples_split': 23, 'min_samples_leaf': 13}. Best is trial 13 with value: 0.5404036489176229.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:11:31,368]\u001b[0m Trial 15 finished with value: 0.5400770781407066 and parameters: {'bootstrap': True, 'max_depth': 23, 'max_leaf_nodes': 59, 'n_estimators': 34, 'min_samples_split': 40, 'min_samples_leaf': 14}. Best is trial 15 with value: 0.5400770781407066.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:11:36,874]\u001b[0m Trial 16 finished with value: 0.54324170077939 and parameters: {'bootstrap': True, 'max_depth': 28, 'max_leaf_nodes': 57, 'n_estimators': 8, 'min_samples_split': 42, 'min_samples_leaf': 15}. Best is trial 15 with value: 0.5400770781407066.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:11:52,353]\u001b[0m Trial 17 finished with value: 0.5448264508039117 and parameters: {'bootstrap': True, 'max_depth': 24, 'max_leaf_nodes': 35, 'n_estimators': 53, 'min_samples_split': 46, 'min_samples_leaf': 8}. Best is trial 15 with value: 0.5400770781407066.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:12:15,599]\u001b[0m Trial 18 finished with value: 0.5400929231130241 and parameters: {'bootstrap': True, 'max_depth': 28, 'max_leaf_nodes': 54, 'n_estimators': 80, 'min_samples_split': 33, 'min_samples_leaf': 16}. Best is trial 15 with value: 0.5400770781407066.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:12:21,502]\u001b[0m Trial 19 finished with value: 0.5447514181606955 and parameters: {'bootstrap': True, 'max_depth': 27, 'max_leaf_nodes': 53, 'n_estimators': 17, 'min_samples_split': 52, 'min_samples_leaf': 16}. Best is trial 15 with value: 0.5400770781407066.\u001b[0m\n",
      "\u001b[32m[I 2021-03-28 05:13:22,729]\u001b[0m Trial 20 finished with value: 0.5667508860539329 and parameters: {'bootstrap': False, 'max_depth': 28, 'max_leaf_nodes': 31, 'n_estimators': 168, 'min_samples_split': 34, 'min_samples_leaf': 20}. Best is trial 15 with value: 0.5400770781407066.\u001b[0m\n",
      "Number of finished trials: 21\n",
      "Best trial:\n",
      "  Value: 0.5400770781407066\n",
      "  Params: \n",
      "    bootstrap: True\n",
      "    max_depth: 23\n",
      "    max_leaf_nodes: 59\n",
      "    n_estimators: 34\n",
      "    min_samples_split: 40\n",
      "    min_samples_leaf: 14\n"
     ]
    }
   ],
   "source": [
    "fold_no = 2\n",
    "test_fold_no = fold_no\n",
    "valid_fold_no = fold_no + 1\n",
    "if valid_fold_no == NFOLDS:\n",
    "    valid_fold_no = 0\n",
    "\n",
    "# train\n",
    "train = df_train.copy()\n",
    "y_train = train[~train['fold_no'].isin([test_fold_no, valid_fold_no])]['cites'].values\n",
    "y_test = train[train['fold_no'].isin([test_fold_no, valid_fold_no])]['cites'].values\n",
    "\n",
    "train = train.drop(importance_0_cols, axis=1)\n",
    "train = train.fillna(0)\n",
    "\n",
    "x_train = train[~train['fold_no'].isin([test_fold_no, valid_fold_no])]\n",
    "x_test = train[train['fold_no'].isin([test_fold_no, valid_fold_no])]\n",
    "\n",
    "x_train = x_train.drop(['cites', 'fold_no'], axis=1)\n",
    "x_test = x_test.drop(['cites', 'fold_no'], axis=1)\n",
    "\n",
    "def objective(trial):\n",
    "    bootstrap = trial.suggest_categorical('bootstrap',[True,False])\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 28)\n",
    "    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 2, 60)\n",
    "    n_estimators =  trial.suggest_int('n_estimators', 1, 300)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split',2, 100)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf',1, 20)\n",
    "\n",
    "    model = RandomForestRegressor(bootstrap = bootstrap, criterion = 'mse',\n",
    "                                 max_depth = max_depth, \n",
    "                                 max_leaf_nodes = max_leaf_nodes,\n",
    "                                 n_estimators = n_estimators,\n",
    "                                 min_samples_split = min_samples_split,min_samples_leaf = min_samples_leaf,\n",
    "                                 n_jobs=-1)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    return mean_squared_error(y_test, model.predict(x_test), squared=False)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=40, timeout=600)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "valid 0.5236471937748199\n"
     ]
    }
   ],
   "source": [
    "result_y_test = []\n",
    "result_y_valid = []\n",
    "#rf_result_proba_test = []\n",
    "rf_result_proba_valid = []\n",
    "\n",
    "for fold_no in range(NFOLDS):\n",
    "    test_fold_no = fold_no\n",
    "    valid_fold_no = fold_no + 1\n",
    "    if valid_fold_no == NFOLDS:\n",
    "        valid_fold_no = 0\n",
    "\n",
    "    # train\n",
    "    train = df_train.copy()\n",
    "    train = train.fillna(0)\n",
    "\n",
    "    y_train = train[~train['fold_no'].isin([valid_fold_no])]['cites'].values # test_fold_no, \n",
    "    y_valid = train[train['fold_no'] == valid_fold_no]['cites'].values\n",
    "    #y_test = train[train['fold_no'] == test_fold_no]['cites'].values\n",
    "\n",
    "    train = train.drop(importance_0_cols, axis=1)\n",
    "\n",
    "    x_train = train[~train['fold_no'].isin([valid_fold_no])]\n",
    "    x_valid = train[train['fold_no'] == valid_fold_no]\n",
    "    #x_test = train[train['fold_no'] == test_fold_no]\n",
    "\n",
    "    # drop\n",
    "    x_train = x_train.drop(['cites', 'fold_no'], axis=1)\n",
    "    x_valid = x_valid.drop(['cites', 'fold_no'], axis=1)\n",
    "    model = RandomForestRegressor(\n",
    "        max_depth=23,\n",
    "        max_leaf_nodes=59,\n",
    "        n_estimators=34,\n",
    "        min_samples_split=40,\n",
    "        min_samples_leaf=14,\n",
    "        criterion='mse',\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1)\n",
    "    model.fit(x_train, y_train)\n",
    "    pickle.dump(model, open(f'../models/rf_{fold_no}.pickle', 'wb'))\n",
    "\n",
    "    #fold_result_test = model.predict(x_test)\n",
    "    fold_result_valid = model.predict(x_valid)\n",
    "    #result_y_test.extend(y_test)\n",
    "    result_y_valid.extend(y_valid)\n",
    "    #rf_result_proba_test.extend(fold_result_test)\n",
    "    rf_result_proba_valid.extend(fold_result_valid)\n",
    "    #rmsle = mean_squared_error(y_test, fold_result_test, squared=False)\n",
    "    print('done...', fold_no)\n",
    "\n",
    "print(\"valid\", mean_squared_error(result_y_valid, rf_result_proba_valid, squared=False))\n",
    "#print(\"test\", mean_squared_error(result_y_test, rf_result_proba_test, squared=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    categorical_features_indices = np.where(x_train.dtypes == 'category')[0]\n",
    "\n",
    "    train_pool = Pool(x_train, y_train, cat_features=categorical_features_indices)\n",
    "    test_pool = Pool(x_test, y_test, cat_features=categorical_features_indices)\n",
    "    \n",
    "    # Parameters\n",
    "    param = {\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\n",
    "            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
    "        ),\n",
    "        \"iterations\": 1000,\n",
    "        \"learning_rate\": 0.03,\n",
    "        \"loss_function\": \"RMSE\",\n",
    "        \"eval_metric\": \"RMSE\",\n",
    "        \"random_seed\": SEED,\n",
    "    }\n",
    "\n",
    "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
    "\n",
    "    model = CatBoostRegressor(**param)\n",
    "    model.fit(train_pool, eval_set=test_pool, verbose=200, early_stopping_rounds=20)\n",
    "    \n",
    "    preds = model.predict(test_pool)\n",
    "    return mean_squared_error(y_test, preds, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m[I 2021-03-27 13:14:24,743]\u001b[0m A new study created in memory with name: no-name-3c4a5d94-a509-479b-bb32-0534db732653\u001b[0m\n",
      "0:\tlearn: 1.1743888\ttest: 1.1671023\tbest: 1.1671023 (0)\ttotal: 8.96ms\tremaining: 8.95s\n",
      "200:\tlearn: 0.5220326\ttest: 0.5447488\tbest: 0.5447488 (200)\ttotal: 1.28s\tremaining: 5.08s\n",
      "400:\tlearn: 0.5033216\ttest: 0.5310702\tbest: 0.5310702 (400)\ttotal: 2.41s\tremaining: 3.6s\n",
      "600:\tlearn: 0.4937408\ttest: 0.5264670\tbest: 0.5264670 (600)\ttotal: 3.5s\tremaining: 2.33s\n",
      "800:\tlearn: 0.4871515\ttest: 0.5240054\tbest: 0.5240054 (800)\ttotal: 4.64s\tremaining: 1.15s\n",
      "\u001b[32m[I 2021-03-27 13:14:31,065]\u001b[0m Trial 0 finished with value: 0.5227816675067647 and parameters: {'colsample_bylevel': 0.09178193753637694, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'subsample': 0.22889708929840308}. Best is trial 0 with value: 0.5227816675067647.\u001b[0m\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5227816692\n",
      "bestIteration = 950\n",
      "\n",
      "Shrink model to first 951 iterations.\n",
      "0:\tlearn: 1.1701072\ttest: 1.1628915\tbest: 1.1628915 (0)\ttotal: 15ms\tremaining: 15s\n",
      "200:\tlearn: 0.5060998\ttest: 0.5355241\tbest: 0.5355241 (200)\ttotal: 2.98s\tremaining: 11.9s\n",
      "400:\tlearn: 0.4866027\ttest: 0.5258138\tbest: 0.5258138 (400)\ttotal: 5.57s\tremaining: 8.32s\n",
      "600:\tlearn: 0.4742182\ttest: 0.5221911\tbest: 0.5221911 (600)\ttotal: 7.9s\tremaining: 5.24s\n",
      "800:\tlearn: 0.4645218\ttest: 0.5204861\tbest: 0.5204524 (793)\ttotal: 10.4s\tremaining: 2.58s\n",
      "\u001b[32m[I 2021-03-27 13:14:44,373]\u001b[0m Trial 1 finished with value: 0.5192035142839727 and parameters: {'colsample_bylevel': 0.03530634402380269, 'depth': 5, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'bagging_temperature': 1.4237092235466464}. Best is trial 1 with value: 0.5192035142839727.\u001b[0m\n",
      "999:\tlearn: 0.4555187\ttest: 0.5192138\tbest: 0.5192035 (997)\ttotal: 12.6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.5192035158\n",
      "bestIteration = 997\n",
      "\n",
      "Shrink model to first 998 iterations.\n",
      "0:\tlearn: 1.1698347\ttest: 1.1625281\tbest: 1.1625281 (0)\ttotal: 15.3ms\tremaining: 15.3s\n",
      "200:\tlearn: 0.4986444\ttest: 0.5320849\tbest: 0.5320849 (200)\ttotal: 2.39s\tremaining: 9.5s\n",
      "400:\tlearn: 0.4761593\ttest: 0.5234304\tbest: 0.5234304 (400)\ttotal: 4.33s\tremaining: 6.47s\n",
      "600:\tlearn: 0.4596755\ttest: 0.5197349\tbest: 0.5197349 (600)\ttotal: 6.37s\tremaining: 4.23s\n",
      "800:\tlearn: 0.4455293\ttest: 0.5172312\tbest: 0.5172312 (800)\ttotal: 8.13s\tremaining: 2.02s\n",
      "\u001b[32m[I 2021-03-27 13:14:54,945]\u001b[0m Trial 2 finished with value: 0.5160497383405246 and parameters: {'colsample_bylevel': 0.06436981813677815, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'bagging_temperature': 1.773816352733344}. Best is trial 2 with value: 0.5160497383405246.\u001b[0m\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.51604974\n",
      "bestIteration = 976\n",
      "\n",
      "Shrink model to first 977 iterations.\n",
      "0:\tlearn: 1.1803660\ttest: 1.1726114\tbest: 1.1726114 (0)\ttotal: 10.4ms\tremaining: 10.4s\n",
      "200:\tlearn: 0.5995501\ttest: 0.6155142\tbest: 0.6155126 (199)\ttotal: 1.42s\tremaining: 5.63s\n",
      "400:\tlearn: 0.5594432\ttest: 0.5802986\tbest: 0.5802986 (400)\ttotal: 2.39s\tremaining: 3.58s\n",
      "600:\tlearn: 0.5405498\ttest: 0.5645676\tbest: 0.5645676 (600)\ttotal: 3.43s\tremaining: 2.27s\n",
      "800:\tlearn: 0.5315189\ttest: 0.5577950\tbest: 0.5577950 (800)\ttotal: 4.39s\tremaining: 1.09s\n",
      "\u001b[32m[I 2021-03-27 13:15:00,685]\u001b[0m Trial 3 finished with value: 0.5533013861815453 and parameters: {'colsample_bylevel': 0.017194952111980265, 'depth': 3, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'bagging_temperature': 8.489018279705345}. Best is trial 2 with value: 0.5160497383405246.\u001b[0m\n",
      "999:\tlearn: 0.5254568\ttest: 0.5533025\tbest: 0.5533014 (998)\ttotal: 5.21s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.5533013872\n",
      "bestIteration = 998\n",
      "\n",
      "Shrink model to first 999 iterations.\n",
      "0:\tlearn: 1.1804561\ttest: 1.1727788\tbest: 1.1727788 (0)\ttotal: 1.89ms\tremaining: 1.89s\n",
      "200:\tlearn: 0.6098123\ttest: 0.6199893\tbest: 0.6199893 (200)\ttotal: 811ms\tremaining: 3.22s\n",
      "400:\tlearn: 0.5628249\ttest: 0.5795178\tbest: 0.5795178 (400)\ttotal: 1.57s\tremaining: 2.35s\n",
      "600:\tlearn: 0.5485430\ttest: 0.5668997\tbest: 0.5668997 (600)\ttotal: 2.3s\tremaining: 1.53s\n",
      "800:\tlearn: 0.5406215\ttest: 0.5596083\tbest: 0.5596083 (800)\ttotal: 2.83s\tremaining: 704ms\n",
      "\u001b[32m[I 2021-03-27 13:15:04,571]\u001b[0m Trial 4 finished with value: 0.5550109967352977 and parameters: {'colsample_bylevel': 0.04906487561232437, 'depth': 1, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'bagging_temperature': 7.0517061840530095}. Best is trial 2 with value: 0.5160497383405246.\u001b[0m\n",
      "999:\tlearn: 0.5352099\ttest: 0.5550110\tbest: 0.5550110 (999)\ttotal: 3.29s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.5550109979\n",
      "bestIteration = 999\n",
      "\n",
      "0:\tlearn: 1.1691391\ttest: 1.1621991\tbest: 1.1621991 (0)\ttotal: 29.8ms\tremaining: 29.7s\n",
      "200:\tlearn: 0.4746109\ttest: 0.5299144\tbest: 0.5299144 (200)\ttotal: 5.39s\tremaining: 21.4s\n",
      "400:\tlearn: 0.4315044\ttest: 0.5202047\tbest: 0.5202047 (400)\ttotal: 10.5s\tremaining: 15.6s\n",
      "600:\tlearn: 0.3958929\ttest: 0.5174034\tbest: 0.5174034 (600)\ttotal: 15.2s\tremaining: 10.1s\n",
      "\u001b[32m[I 2021-03-27 13:15:21,875]\u001b[0m Trial 5 finished with value: 0.5170299998354149 and parameters: {'colsample_bylevel': 0.0780327198793032, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'subsample': 0.47765509235266557}. Best is trial 2 with value: 0.5160497383405246.\u001b[0m\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5170300011\n",
      "bestIteration = 646\n",
      "\n",
      "Shrink model to first 647 iterations.\n",
      "0:\tlearn: 1.1723096\ttest: 1.1650342\tbest: 1.1650342 (0)\ttotal: 15.4ms\tremaining: 15.4s\n",
      "200:\tlearn: 0.5056445\ttest: 0.5344420\tbest: 0.5344420 (200)\ttotal: 2s\tremaining: 7.94s\n",
      "400:\tlearn: 0.4814008\ttest: 0.5216839\tbest: 0.5216631 (399)\ttotal: 3.84s\tremaining: 5.74s\n",
      "600:\tlearn: 0.4642226\ttest: 0.5174658\tbest: 0.5174580 (599)\ttotal: 5.75s\tremaining: 3.81s\n",
      "800:\tlearn: 0.4494347\ttest: 0.5150231\tbest: 0.5150231 (800)\ttotal: 7.38s\tremaining: 1.83s\n",
      "\u001b[32m[I 2021-03-27 13:15:31,628]\u001b[0m Trial 6 finished with value: 0.5133738901947987 and parameters: {'colsample_bylevel': 0.02596296252322562, 'depth': 5, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS'}. Best is trial 6 with value: 0.5133738901947987.\u001b[0m\n",
      "999:\tlearn: 0.4359535\ttest: 0.5134023\tbest: 0.5133739 (997)\ttotal: 9.1s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.5133738917\n",
      "bestIteration = 997\n",
      "\n",
      "Shrink model to first 998 iterations.\n",
      "0:\tlearn: 1.1859909\ttest: 1.1783570\tbest: 1.1783570 (0)\ttotal: 6.34ms\tremaining: 6.34s\n",
      "200:\tlearn: 0.5235839\ttest: 0.5465569\tbest: 0.5465569 (200)\ttotal: 1.14s\tremaining: 4.52s\n",
      "400:\tlearn: 0.5021806\ttest: 0.5296872\tbest: 0.5296872 (400)\ttotal: 1.9s\tremaining: 2.84s\n",
      "600:\tlearn: 0.4918916\ttest: 0.5247606\tbest: 0.5247606 (600)\ttotal: 2.84s\tremaining: 1.89s\n",
      "800:\tlearn: 0.4846214\ttest: 0.5225554\tbest: 0.5225554 (800)\ttotal: 3.76s\tremaining: 933ms\n",
      "\u001b[32m[I 2021-03-27 13:15:36,970]\u001b[0m Trial 7 finished with value: 0.5211465693367602 and parameters: {'colsample_bylevel': 0.06882769576062191, 'depth': 2, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'subsample': 0.9649739202529878}. Best is trial 6 with value: 0.5133738901947987.\u001b[0m\n",
      "999:\tlearn: 0.4785125\ttest: 0.5211560\tbest: 0.5211466 (997)\ttotal: 4.59s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.521146571\n",
      "bestIteration = 997\n",
      "\n",
      "Shrink model to first 998 iterations.\n",
      "0:\tlearn: 1.1711719\ttest: 1.1636533\tbest: 1.1636533 (0)\ttotal: 21.4ms\tremaining: 21.4s\n",
      "200:\tlearn: 0.4735309\ttest: 0.5298970\tbest: 0.5298970 (200)\ttotal: 3.28s\tremaining: 13s\n",
      "400:\tlearn: 0.4256570\ttest: 0.5189129\tbest: 0.5189129 (400)\ttotal: 5.9s\tremaining: 8.81s\n",
      "\u001b[32m[I 2021-03-27 13:15:45,485]\u001b[0m Trial 8 finished with value: 0.5160839152748805 and parameters: {'colsample_bylevel': 0.02507185734367059, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS'}. Best is trial 6 with value: 0.5133738901947987.\u001b[0m\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5160839167\n",
      "bestIteration = 538\n",
      "\n",
      "Shrink model to first 539 iterations.\n",
      "0:\tlearn: 1.1729746\ttest: 1.1653265\tbest: 1.1653265 (0)\ttotal: 4.46ms\tremaining: 4.45s\n",
      "200:\tlearn: 0.5221320\ttest: 0.5418632\tbest: 0.5418632 (200)\ttotal: 1.09s\tremaining: 4.33s\n",
      "400:\tlearn: 0.5015319\ttest: 0.5260923\tbest: 0.5260923 (400)\ttotal: 1.92s\tremaining: 2.87s\n",
      "600:\tlearn: 0.4912131\ttest: 0.5210230\tbest: 0.5210230 (600)\ttotal: 3s\tremaining: 1.99s\n",
      "800:\tlearn: 0.4836954\ttest: 0.5187777\tbest: 0.5187758 (795)\ttotal: 4.31s\tremaining: 1.07s\n",
      "\u001b[32m[I 2021-03-27 13:15:51,230]\u001b[0m Trial 9 finished with value: 0.5171200735109395 and parameters: {'colsample_bylevel': 0.07630173834869354, 'depth': 2, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS'}. Best is trial 6 with value: 0.5133738901947987.\u001b[0m\n",
      "999:\tlearn: 0.4776386\ttest: 0.5171313\tbest: 0.5171201 (996)\ttotal: 5.1s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.5171200751\n",
      "bestIteration = 996\n",
      "\n",
      "Shrink model to first 997 iterations.\n",
      "0:\tlearn: 1.1692356\ttest: 1.1625135\tbest: 1.1625135 (0)\ttotal: 383ms\tremaining: 6m 23s\n",
      "200:\tlearn: 0.4709531\ttest: 0.5348974\tbest: 0.5348974 (200)\ttotal: 1m 7s\tremaining: 4m 28s\n",
      "400:\tlearn: 0.4329989\ttest: 0.5234405\tbest: 0.5234316 (399)\ttotal: 2m 13s\tremaining: 3m 19s\n",
      "600:\tlearn: 0.4018707\ttest: 0.5186515\tbest: 0.5186072 (599)\ttotal: 3m 15s\tremaining: 2m 9s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5172388472\n",
      "bestIteration = 701\n",
      "\n",
      "Shrink model to first 702 iterations.\n",
      "\u001b[32m[I 2021-03-27 13:19:45,963]\u001b[0m Trial 10 finished with value: 0.5172388457105925 and parameters: {'colsample_bylevel': 0.012034276168009485, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS'}. Best is trial 6 with value: 0.5133738901947987.\u001b[0m\n",
      "0:\tlearn: 1.1714604\ttest: 1.1638548\tbest: 1.1638548 (0)\ttotal: 22.9ms\tremaining: 22.9s\n",
      "200:\tlearn: 0.4983571\ttest: 0.5298354\tbest: 0.5298354 (200)\ttotal: 6.64s\tremaining: 26.4s\n",
      "400:\tlearn: 0.4737780\ttest: 0.5188214\tbest: 0.5188214 (400)\ttotal: 13.1s\tremaining: 19.5s\n",
      "600:\tlearn: 0.4523830\ttest: 0.5150113\tbest: 0.5150113 (600)\ttotal: 19.5s\tremaining: 13s\n",
      "800:\tlearn: 0.4328805\ttest: 0.5132298\tbest: 0.5131927 (795)\ttotal: 26s\tremaining: 6.46s\n",
      "\u001b[32m[I 2021-03-27 13:20:14,984]\u001b[0m Trial 11 finished with value: 0.5125028838315051 and parameters: {'colsample_bylevel': 0.052922169991835896, 'depth': 6, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS'}. Best is trial 11 with value: 0.5125028838315051.\u001b[0m\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5125028852\n",
      "bestIteration = 854\n",
      "\n",
      "Shrink model to first 855 iterations.\n",
      "0:\tlearn: 1.1696358\ttest: 1.1624778\tbest: 1.1624778 (0)\ttotal: 71.2ms\tremaining: 1m 11s\n",
      "200:\tlearn: 0.4900229\ttest: 0.5304728\tbest: 0.5304728 (200)\ttotal: 10.5s\tremaining: 41.9s\n",
      "400:\tlearn: 0.4622376\ttest: 0.5199645\tbest: 0.5199645 (400)\ttotal: 20.6s\tremaining: 30.8s\n",
      "600:\tlearn: 0.4355969\ttest: 0.5162238\tbest: 0.5162238 (600)\ttotal: 31s\tremaining: 20.6s\n",
      "\u001b[32m[I 2021-03-27 13:20:48,791]\u001b[0m Trial 12 finished with value: 0.5157985708284907 and parameters: {'colsample_bylevel': 0.03967199268584552, 'depth': 8, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS'}. Best is trial 11 with value: 0.5125028838315051.\u001b[0m\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5157985722\n",
      "bestIteration = 622\n",
      "\n",
      "Shrink model to first 623 iterations.\n",
      "0:\tlearn: 1.1702937\ttest: 1.1635999\tbest: 1.1635999 (0)\ttotal: 10.5ms\tremaining: 10.5s\n",
      "200:\tlearn: 0.5023370\ttest: 0.5321857\tbest: 0.5321857 (200)\ttotal: 2.67s\tremaining: 10.6s\n",
      "400:\tlearn: 0.4796644\ttest: 0.5207440\tbest: 0.5207440 (400)\ttotal: 5.13s\tremaining: 7.66s\n",
      "600:\tlearn: 0.4622057\ttest: 0.5162725\tbest: 0.5162725 (600)\ttotal: 7.58s\tremaining: 5.04s\n",
      "800:\tlearn: 0.4456892\ttest: 0.5141238\tbest: 0.5140614 (794)\ttotal: 10.1s\tremaining: 2.51s\n",
      "\u001b[32m[I 2021-03-27 13:21:00,923]\u001b[0m Trial 13 finished with value: 0.5133350317146614 and parameters: {'colsample_bylevel': 0.04972781796748116, 'depth': 5, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS'}. Best is trial 11 with value: 0.5125028838315051.\u001b[0m\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5133350333\n",
      "bestIteration = 906\n",
      "\n",
      "Shrink model to first 907 iterations.\n",
      "0:\tlearn: 1.1679935\ttest: 1.1608362\tbest: 1.1608362 (0)\ttotal: 728ms\tremaining: 12m 7s\n",
      "200:\tlearn: 0.4780723\ttest: 0.5292360\tbest: 0.5292360 (200)\ttotal: 1m 48s\tremaining: 7m 10s\n",
      "400:\tlearn: 0.4472430\ttest: 0.5199779\tbest: 0.5199779 (400)\ttotal: 3m 32s\tremaining: 5m 17s\n",
      "600:\tlearn: 0.4133899\ttest: 0.5158366\tbest: 0.5158226 (591)\ttotal: 5m 20s\tremaining: 3m 33s\n",
      "\u001b[32m[I 2021-03-27 13:26:59,646]\u001b[0m Trial 14 finished with value: 0.5151047308021028 and parameters: {'colsample_bylevel': 0.05242934212629483, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS'}. Best is trial 11 with value: 0.5125028838315051.\u001b[0m\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5151047323\n",
      "bestIteration = 648\n",
      "\n",
      "Shrink model to first 649 iterations.\n"
     ]
    }
   ],
   "source": [
    "fold_no = 2\n",
    "test_fold_no = fold_no\n",
    "valid_fold_no = fold_no + 1\n",
    "if valid_fold_no == NFOLDS:\n",
    "    valid_fold_no = 0\n",
    "\n",
    "# train\n",
    "train = df_train.copy()\n",
    "y_train = train[~train['fold_no'].isin([test_fold_no, valid_fold_no])]['cites'].values\n",
    "y_test = train[train['fold_no'].isin([test_fold_no, valid_fold_no])]['cites'].values\n",
    "\n",
    "train = train.drop(importance_0_cols, axis=1)\n",
    "train['diff_pred_doi_cites'] = train['pred_doi_cites'] - train['doi_cites']\n",
    "train['rate_pred_doi_cites'] = train['pred_doi_cites'] / (train['doi_cites'] + 1)\n",
    "\n",
    "x_train = train[~train['fold_no'].isin([test_fold_no, valid_fold_no])]\n",
    "x_test = train[train['fold_no'].isin([test_fold_no, valid_fold_no])]\n",
    "\n",
    "x_train = x_train.drop(['cites', 'fold_no'], axis=1)\n",
    "x_test = x_test.drop(['cites', 'fold_no'], axis=1)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30, timeout=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of finished trials: 15\nBest trial:\n  Value: 0.5125028838315051\n  Params: \n    colsample_bylevel: 0.052922169991835896\n    depth: 6\n    boosting_type: Ordered\n    bootstrap_type: MVS\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning rate set to 0.017979\n",
      "0:\tlearn: 1.1757940\ttest: 1.1736326\tbest: 1.1736326 (0)\ttotal: 88.7ms\tremaining: 14m 47s\n",
      "100:\tlearn: 0.5777559\ttest: 0.5750382\tbest: 0.5750382 (100)\ttotal: 5.63s\tremaining: 9m 11s\n",
      "200:\tlearn: 0.5216260\ttest: 0.5130720\tbest: 0.5130720 (200)\ttotal: 11.2s\tremaining: 9m 3s\n",
      "300:\tlearn: 0.5070068\ttest: 0.4993360\tbest: 0.4993360 (300)\ttotal: 16.7s\tremaining: 8m 58s\n",
      "400:\tlearn: 0.4984863\ttest: 0.4934983\tbest: 0.4934983 (400)\ttotal: 22.3s\tremaining: 8m 54s\n",
      "500:\tlearn: 0.4917994\ttest: 0.4894988\tbest: 0.4894988 (500)\ttotal: 27.9s\tremaining: 8m 48s\n",
      "600:\tlearn: 0.4857147\ttest: 0.4868635\tbest: 0.4868635 (600)\ttotal: 33.5s\tremaining: 8m 43s\n",
      "700:\tlearn: 0.4789177\ttest: 0.4850007\tbest: 0.4849902 (698)\ttotal: 39.1s\tremaining: 8m 39s\n",
      "800:\tlearn: 0.4727986\ttest: 0.4835293\tbest: 0.4834929 (796)\ttotal: 44.8s\tremaining: 8m 34s\n",
      "900:\tlearn: 0.4668707\ttest: 0.4827211\tbest: 0.4827211 (900)\ttotal: 50.4s\tremaining: 8m 28s\n",
      "1000:\tlearn: 0.4612808\ttest: 0.4819116\tbest: 0.4819116 (1000)\ttotal: 56s\tremaining: 8m 23s\n",
      "1100:\tlearn: 0.4553130\ttest: 0.4813351\tbest: 0.4813210 (1098)\ttotal: 1m 1s\tremaining: 8m 18s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.4811544028\n",
      "bestIteration = 1135\n",
      "\n",
      "Shrink model to first 1136 iterations.\n",
      "Learning rate set to 0.017979\n",
      "0:\tlearn: 1.1780525\ttest: 1.1534814\tbest: 1.1534814 (0)\ttotal: 37.6ms\tremaining: 6m 15s\n",
      "100:\tlearn: 0.5742797\ttest: 0.5844479\tbest: 0.5844479 (100)\ttotal: 5.72s\tremaining: 9m 20s\n",
      "200:\tlearn: 0.5170053\ttest: 0.5411751\tbest: 0.5411751 (200)\ttotal: 11.4s\tremaining: 9m 13s\n",
      "300:\tlearn: 0.5022512\ttest: 0.5335797\tbest: 0.5335797 (300)\ttotal: 17s\tremaining: 9m 8s\n",
      "400:\tlearn: 0.4934415\ttest: 0.5295341\tbest: 0.5295341 (400)\ttotal: 22.7s\tremaining: 9m 3s\n",
      "500:\tlearn: 0.4869909\ttest: 0.5268579\tbest: 0.5268579 (500)\ttotal: 28.3s\tremaining: 8m 56s\n",
      "600:\tlearn: 0.4807234\ttest: 0.5249557\tbest: 0.5249557 (600)\ttotal: 33.9s\tremaining: 8m 50s\n",
      "700:\tlearn: 0.4746080\ttest: 0.5232905\tbest: 0.5232809 (697)\ttotal: 39.6s\tremaining: 8m 45s\n",
      "800:\tlearn: 0.4680963\ttest: 0.5218825\tbest: 0.5218825 (800)\ttotal: 45.3s\tremaining: 8m 40s\n",
      "900:\tlearn: 0.4619642\ttest: 0.5203124\tbest: 0.5202784 (897)\ttotal: 51s\tremaining: 8m 35s\n",
      "1000:\tlearn: 0.4563103\ttest: 0.5196444\tbest: 0.5196091 (995)\ttotal: 56.7s\tremaining: 8m 29s\n",
      "1100:\tlearn: 0.4508152\ttest: 0.5190812\tbest: 0.5190576 (1094)\ttotal: 1m 2s\tremaining: 8m 23s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5187844679\n",
      "bestIteration = 1143\n",
      "\n",
      "Shrink model to first 1144 iterations.\n",
      "Learning rate set to 0.017979\n",
      "0:\tlearn: 1.1744441\ttest: 1.1857495\tbest: 1.1857495 (0)\ttotal: 40.4ms\tremaining: 6m 43s\n",
      "100:\tlearn: 0.5750548\ttest: 0.5954820\tbest: 0.5954820 (100)\ttotal: 5.79s\tremaining: 9m 27s\n",
      "200:\tlearn: 0.5186862\ttest: 0.5389901\tbest: 0.5389901 (200)\ttotal: 11.5s\tremaining: 9m 19s\n",
      "300:\tlearn: 0.5048997\ttest: 0.5270239\tbest: 0.5270239 (300)\ttotal: 17.1s\tremaining: 9m 12s\n",
      "400:\tlearn: 0.4958165\ttest: 0.5201155\tbest: 0.5201155 (400)\ttotal: 22.8s\tremaining: 9m 6s\n",
      "500:\tlearn: 0.4890227\ttest: 0.5164068\tbest: 0.5164068 (500)\ttotal: 28.4s\tremaining: 8m 59s\n",
      "600:\tlearn: 0.4830291\ttest: 0.5136705\tbest: 0.5136705 (600)\ttotal: 34.1s\tremaining: 8m 53s\n",
      "700:\tlearn: 0.4762777\ttest: 0.5111010\tbest: 0.5111010 (700)\ttotal: 39.8s\tremaining: 8m 48s\n",
      "800:\tlearn: 0.4697066\ttest: 0.5091104\tbest: 0.5091104 (800)\ttotal: 45.5s\tremaining: 8m 42s\n",
      "900:\tlearn: 0.4630915\ttest: 0.5076076\tbest: 0.5075879 (899)\ttotal: 51.1s\tremaining: 8m 36s\n",
      "1000:\tlearn: 0.4569709\ttest: 0.5062959\tbest: 0.5062959 (1000)\ttotal: 56.7s\tremaining: 8m 30s\n",
      "1100:\tlearn: 0.4507044\ttest: 0.5055559\tbest: 0.5054780 (1080)\ttotal: 1m 2s\tremaining: 8m 24s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.505477956\n",
      "bestIteration = 1080\n",
      "\n",
      "Shrink model to first 1081 iterations.\n",
      "Learning rate set to 0.017979\n",
      "0:\tlearn: 1.1766969\ttest: 1.1692696\tbest: 1.1692696 (0)\ttotal: 39ms\tremaining: 6m 29s\n",
      "100:\tlearn: 0.5771049\ttest: 0.5776161\tbest: 0.5776161 (100)\ttotal: 5.75s\tremaining: 9m 23s\n",
      "200:\tlearn: 0.5202934\ttest: 0.5228023\tbest: 0.5228023 (200)\ttotal: 11.4s\tremaining: 9m 17s\n",
      "300:\tlearn: 0.5062247\ttest: 0.5120495\tbest: 0.5120495 (300)\ttotal: 17.1s\tremaining: 9m 12s\n",
      "400:\tlearn: 0.4969013\ttest: 0.5056189\tbest: 0.5056189 (400)\ttotal: 22.9s\tremaining: 9m 8s\n",
      "500:\tlearn: 0.4901759\ttest: 0.5015122\tbest: 0.5015095 (499)\ttotal: 28.5s\tremaining: 9m 1s\n",
      "600:\tlearn: 0.4833521\ttest: 0.4981722\tbest: 0.4981722 (600)\ttotal: 34.3s\tremaining: 8m 56s\n",
      "700:\tlearn: 0.4769164\ttest: 0.4959811\tbest: 0.4959811 (700)\ttotal: 40s\tremaining: 8m 50s\n",
      "800:\tlearn: 0.4701324\ttest: 0.4940903\tbest: 0.4940903 (800)\ttotal: 45.7s\tremaining: 8m 44s\n",
      "900:\tlearn: 0.4646887\ttest: 0.4924168\tbest: 0.4923953 (899)\ttotal: 51.3s\tremaining: 8m 38s\n",
      "1000:\tlearn: 0.4590253\ttest: 0.4915254\tbest: 0.4915178 (985)\ttotal: 57s\tremaining: 8m 32s\n",
      "1100:\tlearn: 0.4533527\ttest: 0.4906471\tbest: 0.4906373 (1099)\ttotal: 1m 2s\tremaining: 8m 26s\n",
      "1200:\tlearn: 0.4481471\ttest: 0.4897937\tbest: 0.4897680 (1199)\ttotal: 1m 8s\tremaining: 8m 21s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.4895487433\n",
      "bestIteration = 1219\n",
      "\n",
      "Shrink model to first 1220 iterations.\n",
      "Learning rate set to 0.017979\n",
      "0:\tlearn: 1.1733126\ttest: 1.1923851\tbest: 1.1923851 (0)\ttotal: 41.3ms\tremaining: 6m 52s\n",
      "100:\tlearn: 0.5746573\ttest: 0.5863696\tbest: 0.5863696 (100)\ttotal: 5.71s\tremaining: 9m 19s\n",
      "200:\tlearn: 0.5196393\ttest: 0.5308522\tbest: 0.5308522 (200)\ttotal: 11.3s\tremaining: 9m 10s\n",
      "300:\tlearn: 0.5054287\ttest: 0.5184315\tbest: 0.5184315 (300)\ttotal: 17s\tremaining: 9m 7s\n",
      "400:\tlearn: 0.4964665\ttest: 0.5116572\tbest: 0.5116572 (400)\ttotal: 22.6s\tremaining: 9m 1s\n",
      "500:\tlearn: 0.4895017\ttest: 0.5074811\tbest: 0.5074811 (500)\ttotal: 28.2s\tremaining: 8m 55s\n",
      "600:\tlearn: 0.4832850\ttest: 0.5047129\tbest: 0.5046828 (597)\ttotal: 33.9s\tremaining: 8m 49s\n",
      "700:\tlearn: 0.4760211\ttest: 0.5019031\tbest: 0.5019031 (700)\ttotal: 39.6s\tremaining: 8m 45s\n",
      "800:\tlearn: 0.4691436\ttest: 0.4998859\tbest: 0.4998859 (800)\ttotal: 45.4s\tremaining: 8m 41s\n",
      "900:\tlearn: 0.4631813\ttest: 0.4987379\tbest: 0.4987277 (899)\ttotal: 51.2s\tremaining: 8m 37s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.4986046565\n",
      "bestIteration = 910\n",
      "\n",
      "Shrink model to first 911 iterations.\n",
      "Learning rate set to 0.017979\n",
      "0:\tlearn: 1.1761396\ttest: 1.1698179\tbest: 1.1698179 (0)\ttotal: 38.4ms\tremaining: 6m 23s\n",
      "100:\tlearn: 0.5742837\ttest: 0.5868108\tbest: 0.5868108 (100)\ttotal: 5.73s\tremaining: 9m 21s\n",
      "200:\tlearn: 0.5187860\ttest: 0.5368169\tbest: 0.5368169 (200)\ttotal: 11.3s\tremaining: 9m 9s\n",
      "300:\tlearn: 0.5048490\ttest: 0.5254280\tbest: 0.5254280 (300)\ttotal: 16.8s\tremaining: 9m 2s\n",
      "400:\tlearn: 0.4952738\ttest: 0.5193202\tbest: 0.5193202 (400)\ttotal: 22.4s\tremaining: 8m 55s\n",
      "500:\tlearn: 0.4883004\ttest: 0.5158125\tbest: 0.5158125 (500)\ttotal: 28s\tremaining: 8m 50s\n",
      "600:\tlearn: 0.4815671\ttest: 0.5131652\tbest: 0.5131652 (600)\ttotal: 33.5s\tremaining: 8m 44s\n",
      "700:\tlearn: 0.4746164\ttest: 0.5110298\tbest: 0.5110298 (700)\ttotal: 39.1s\tremaining: 8m 39s\n",
      "800:\tlearn: 0.4674529\ttest: 0.5095800\tbest: 0.5095698 (798)\ttotal: 44.7s\tremaining: 8m 33s\n",
      "900:\tlearn: 0.4611335\ttest: 0.5082487\tbest: 0.5082487 (900)\ttotal: 50.4s\tremaining: 8m 28s\n",
      "1000:\tlearn: 0.4551296\ttest: 0.5073137\tbest: 0.5073137 (1000)\ttotal: 56s\tremaining: 8m 23s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5072462381\n",
      "bestIteration = 1014\n",
      "\n",
      "Shrink model to first 1015 iterations.\n",
      "Learning rate set to 0.017979\n",
      "0:\tlearn: 1.1751948\ttest: 1.1781667\tbest: 1.1781667 (0)\ttotal: 42.3ms\tremaining: 7m 2s\n",
      "100:\tlearn: 0.5767516\ttest: 0.5660606\tbest: 0.5660606 (100)\ttotal: 5.69s\tremaining: 9m 17s\n",
      "200:\tlearn: 0.5202640\ttest: 0.5139361\tbest: 0.5139361 (200)\ttotal: 11.3s\tremaining: 9m 11s\n",
      "300:\tlearn: 0.5063327\ttest: 0.5054555\tbest: 0.5054555 (300)\ttotal: 16.9s\tremaining: 9m 5s\n",
      "400:\tlearn: 0.4973336\ttest: 0.5013566\tbest: 0.5013369 (399)\ttotal: 22.6s\tremaining: 9m\n",
      "500:\tlearn: 0.4909058\ttest: 0.4993818\tbest: 0.4993818 (500)\ttotal: 28.2s\tremaining: 8m 54s\n",
      "600:\tlearn: 0.4844933\ttest: 0.4973681\tbest: 0.4973681 (600)\ttotal: 33.8s\tremaining: 8m 49s\n",
      "700:\tlearn: 0.4785466\ttest: 0.4958673\tbest: 0.4958538 (699)\ttotal: 39.5s\tremaining: 8m 44s\n",
      "800:\tlearn: 0.4721802\ttest: 0.4945666\tbest: 0.4945462 (797)\ttotal: 45.1s\tremaining: 8m 38s\n",
      "900:\tlearn: 0.4664015\ttest: 0.4936673\tbest: 0.4936673 (900)\ttotal: 50.8s\tremaining: 8m 32s\n",
      "1000:\tlearn: 0.4605823\ttest: 0.4929570\tbest: 0.4929058 (999)\ttotal: 56.4s\tremaining: 8m 27s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.4929057529\n",
      "bestIteration = 999\n",
      "\n",
      "Shrink model to first 1000 iterations.\n",
      "Learning rate set to 0.017979\n",
      "0:\tlearn: 1.1745588\ttest: 1.1837889\tbest: 1.1837889 (0)\ttotal: 39.3ms\tremaining: 6m 33s\n",
      "100:\tlearn: 0.5779041\ttest: 0.5685203\tbest: 0.5685203 (100)\ttotal: 5.74s\tremaining: 9m 22s\n",
      "200:\tlearn: 0.5217031\ttest: 0.5121762\tbest: 0.5121762 (200)\ttotal: 11.4s\tremaining: 9m 14s\n",
      "300:\tlearn: 0.5074997\ttest: 0.5024212\tbest: 0.5024212 (300)\ttotal: 17.1s\tremaining: 9m 10s\n",
      "400:\tlearn: 0.4987982\ttest: 0.4971329\tbest: 0.4971329 (400)\ttotal: 22.9s\tremaining: 9m 7s\n",
      "500:\tlearn: 0.4917821\ttest: 0.4939610\tbest: 0.4939610 (500)\ttotal: 28.6s\tremaining: 9m 2s\n",
      "600:\tlearn: 0.4851355\ttest: 0.4906442\tbest: 0.4906442 (600)\ttotal: 34.3s\tremaining: 8m 56s\n",
      "700:\tlearn: 0.4787160\ttest: 0.4886142\tbest: 0.4886142 (700)\ttotal: 39.9s\tremaining: 8m 49s\n",
      "800:\tlearn: 0.4720714\ttest: 0.4869297\tbest: 0.4869297 (800)\ttotal: 45.8s\tremaining: 8m 45s\n",
      "900:\tlearn: 0.4656510\ttest: 0.4857975\tbest: 0.4857873 (899)\ttotal: 51.5s\tremaining: 8m 39s\n",
      "1000:\tlearn: 0.4596172\ttest: 0.4845268\tbest: 0.4845268 (1000)\ttotal: 57.2s\tremaining: 8m 33s\n",
      "1100:\tlearn: 0.4536797\ttest: 0.4836525\tbest: 0.4836525 (1100)\ttotal: 1m 2s\tremaining: 8m 28s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.4831745068\n",
      "bestIteration = 1151\n",
      "\n",
      "Shrink model to first 1152 iterations.\n",
      "Learning rate set to 0.017979\n",
      "0:\tlearn: 1.1756257\ttest: 1.1804679\tbest: 1.1804679 (0)\ttotal: 34.9ms\tremaining: 5m 48s\n",
      "100:\tlearn: 0.5736057\ttest: 0.5884614\tbest: 0.5884614 (100)\ttotal: 5.64s\tremaining: 9m 12s\n",
      "200:\tlearn: 0.5169119\ttest: 0.5396432\tbest: 0.5396432 (200)\ttotal: 10.8s\tremaining: 8m 48s\n",
      "300:\tlearn: 0.5031724\ttest: 0.5307052\tbest: 0.5307052 (300)\ttotal: 16.2s\tremaining: 8m 42s\n",
      "400:\tlearn: 0.4948073\ttest: 0.5256684\tbest: 0.5256684 (400)\ttotal: 21.3s\tremaining: 8m 29s\n",
      "500:\tlearn: 0.4881601\ttest: 0.5226555\tbest: 0.5226555 (500)\ttotal: 26.5s\tremaining: 8m 22s\n",
      "600:\tlearn: 0.4823692\ttest: 0.5206496\tbest: 0.5206496 (600)\ttotal: 31.7s\tremaining: 8m 15s\n",
      "700:\tlearn: 0.4763883\ttest: 0.5192269\tbest: 0.5192269 (700)\ttotal: 36.8s\tremaining: 8m 7s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5187253943\n",
      "bestIteration = 743\n",
      "\n",
      "Shrink model to first 744 iterations.\n",
      "Learning rate set to 0.017979\n",
      "0:\tlearn: 1.1761659\ttest: 1.1694475\tbest: 1.1694475 (0)\ttotal: 39.2ms\tremaining: 6m 31s\n",
      "100:\tlearn: 0.5775021\ttest: 0.5630318\tbest: 0.5630318 (100)\ttotal: 5.3s\tremaining: 8m 39s\n",
      "200:\tlearn: 0.5213708\ttest: 0.5122343\tbest: 0.5122343 (200)\ttotal: 10.5s\tremaining: 8m 29s\n",
      "300:\tlearn: 0.5072248\ttest: 0.5027602\tbest: 0.5027602 (300)\ttotal: 15.6s\tremaining: 8m 23s\n",
      "400:\tlearn: 0.4984994\ttest: 0.4976223\tbest: 0.4976223 (400)\ttotal: 20.8s\tremaining: 8m 18s\n",
      "500:\tlearn: 0.4926132\ttest: 0.4947583\tbest: 0.4947572 (499)\ttotal: 26.2s\tremaining: 8m 16s\n",
      "600:\tlearn: 0.4872650\ttest: 0.4919753\tbest: 0.4919753 (600)\ttotal: 31.4s\tremaining: 8m 10s\n",
      "700:\tlearn: 0.4817852\ttest: 0.4899498\tbest: 0.4899498 (700)\ttotal: 36.5s\tremaining: 8m 4s\n",
      "800:\tlearn: 0.4763145\ttest: 0.4882975\tbest: 0.4882975 (800)\ttotal: 42.2s\tremaining: 8m 4s\n",
      "900:\tlearn: 0.4709088\ttest: 0.4867931\tbest: 0.4867920 (897)\ttotal: 47.7s\tremaining: 8m 1s\n",
      "1000:\tlearn: 0.4658611\ttest: 0.4857435\tbest: 0.4857435 (1000)\ttotal: 52.9s\tremaining: 7m 55s\n",
      "1100:\tlearn: 0.4607959\ttest: 0.4847586\tbest: 0.4847586 (1100)\ttotal: 58.2s\tremaining: 7m 50s\n",
      "1200:\tlearn: 0.4557848\ttest: 0.4841776\tbest: 0.4841776 (1200)\ttotal: 1m 3s\tremaining: 7m 43s\n",
      "1300:\tlearn: 0.4513688\ttest: 0.4834651\tbest: 0.4834513 (1299)\ttotal: 1m 8s\tremaining: 7m 37s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.4832649204\n",
      "bestIteration = 1331\n",
      "\n",
      "Shrink model to first 1332 iterations.\n",
      "valid 0.4980714825166237\n"
     ]
    }
   ],
   "source": [
    "from catboost import Pool\n",
    "SEED = 777\n",
    "result_y_test = []\n",
    "result_y_valid = []\n",
    "cat_result_proba_test = []\n",
    "cat_result_proba_valid = []\n",
    "\n",
    "for fold_no in range(NFOLDS):\n",
    "    test_fold_no = fold_no\n",
    "    valid_fold_no = fold_no + 1\n",
    "    if valid_fold_no == NFOLDS:\n",
    "        valid_fold_no = 0\n",
    "\n",
    "    # train\n",
    "    train = df_train.copy()\n",
    "    y_train = train[~train['fold_no'].isin([valid_fold_no])]['cites'].values\n",
    "    y_valid = train[train['fold_no'] == valid_fold_no]['cites'].values\n",
    "    #y_test = train[train['fold_no'] == test_fold_no]['cites'].values\n",
    "\n",
    "    train = train.drop(importance_0_cols, axis=1)\n",
    "\n",
    "    x_train = train[~train['fold_no'].isin([valid_fold_no])]\n",
    "    x_valid = train[train['fold_no'] == valid_fold_no]\n",
    "    #x_test = train[train['fold_no'] == test_fold_no]\n",
    "\n",
    "    # drop\n",
    "    x_train = x_train.drop(['cites', 'fold_no'], axis=1)\n",
    "    x_valid = x_valid.drop(['cites', 'fold_no'], axis=1)\n",
    "    #x_test = x_test.drop(['cites', 'fold_no'], axis=1)\n",
    "\n",
    "    #cat_features = np.where(x_train.dtypes == 'category')[0]\n",
    "    train_pool = Pool(x_train, y_train)\n",
    "    validate_pool = Pool(x_valid, y_valid)\n",
    "    #test_pool = Pool(x_test, y_test, cat_features=cat_features)\n",
    "    \n",
    "    model = CatBoostRegressor(\n",
    "        loss_function='RMSE',\n",
    "        eval_metric=\"RMSE\",\n",
    "        colsample_bylevel=0.052922169991835896,\n",
    "        depth=6,\n",
    "        boosting_type='Ordered',\n",
    "        bootstrap_type='MVS',\n",
    "        iterations=10000,\n",
    "        random_seed=SEED,\n",
    "    )\n",
    "    #model = lgb.LGBMRegressor(**lgb_params)\n",
    "    model.fit(train_pool,\n",
    "                eval_set=validate_pool,\n",
    "                verbose=100,\n",
    "                early_stopping_rounds=20,\n",
    "    )\n",
    "\n",
    "    pickle.dump(model, open(f'../models/cat_{fold_no}.pickle', 'wb'))\n",
    "\n",
    "    #fold_result_test = model.predict(x_test)\n",
    "    fold_result_valid = model.predict(x_valid)\n",
    "    #result_y_test.extend(y_test)\n",
    "    result_y_valid.extend(y_valid)\n",
    "    #cat_result_proba_test.extend(fold_result_test)\n",
    "    cat_result_proba_valid.extend(fold_result_valid)\n",
    "\n",
    "    #rmsle = mean_squared_error(y_test, fold_result_test, squared=False)\n",
    "    #print(f\"fold {fold_no} lgb score: {rmsle}\")\n",
    "\n",
    "print(\"valid\", mean_squared_error(result_y_valid, cat_result_proba_valid, squared=False))\n",
    "#print(\"test\", mean_squared_error(result_y_test, cat_result_proba_test, squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "done... fold 0\n",
      "done... fold 1\n",
      "done... fold 2\n",
      "done... fold 3\n",
      "done... fold 4\n",
      "done... fold 5\n",
      "done... fold 6\n",
      "done... fold 7\n",
      "done... fold 8\n",
      "done... fold 9\n",
      "valid 0.5191902200700491\n"
     ]
    }
   ],
   "source": [
    "result_y_valid = []\n",
    "reg_result_proba_valid = []\n",
    "\n",
    "for fold_no in range(NFOLDS):\n",
    "    test_fold_no = fold_no\n",
    "    valid_fold_no = fold_no + 1\n",
    "    if valid_fold_no == NFOLDS:\n",
    "        valid_fold_no = 0\n",
    "\n",
    "    # train\n",
    "    train = df_train.copy()\n",
    "    y_train = train[~train['fold_no'].isin([valid_fold_no])]['cites'].values\n",
    "    y_valid = train[train['fold_no'] == valid_fold_no]['cites'].values\n",
    "    #y_test = train[train['fold_no'] == test_fold_no]['cites'].values\n",
    "\n",
    "    train = train.drop(importance_0_cols, axis=1)\n",
    "    train = train.fillna(0)\n",
    "\n",
    "    x_train = train[~train['fold_no'].isin([valid_fold_no])]\n",
    "    x_valid = train[train['fold_no'] == valid_fold_no]\n",
    "    #x_test = train[train['fold_no'] == test_fold_no]\n",
    "\n",
    "    # drop\n",
    "    x_train = x_train.drop(['cites', 'fold_no'], axis=1)\n",
    "    x_valid = x_valid.drop(['cites', 'fold_no'], axis=1)\n",
    "    #x_test = x_test.drop(['cites', 'fold_no'], axis=1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_valid = scaler.fit_transform(x_valid)\n",
    "\n",
    "    model = linear_model.Ridge()\n",
    "    model.fit(x_train, y_train)\n",
    "    pickle.dump(model, open(f'../models/reg_{fold_no}.pickle', 'wb'))\n",
    "\n",
    "    #fold_result_test = model.predict(x_test)\n",
    "    fold_result_valid = model.predict(x_valid)\n",
    "    #result_y_test.extend(y_test)\n",
    "    result_y_valid.extend(y_valid)\n",
    "    #cat_result_proba_test.extend(fold_result_test)\n",
    "    reg_result_proba_valid.extend(fold_result_valid)\n",
    "\n",
    "    #rmsle = mean_squared_error(y_test, fold_result_test, squared=False)\n",
    "    print(f\"done... fold {fold_no}\")\n",
    "\n",
    "\n",
    "print(\"valid\", mean_squared_error(result_y_valid, reg_result_proba_valid, squared=False))\n",
    "#print(\"test\", mean_squared_error(result_y_test, cat_result_proba_test, squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_53\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput (InputLayer)           [(None, 1555)]            0         \n_________________________________________________________________\ndense_74 (Dense)             (None, 256)               398336    \n_________________________________________________________________\ndropout_24 (Dropout)         (None, 256)               0         \n_________________________________________________________________\ndense_75 (Dense)             (None, 128)               32896     \n_________________________________________________________________\ndropout_25 (Dropout)         (None, 128)               0         \n_________________________________________________________________\ndense_76 (Dense)             (None, 128)               16512     \n_________________________________________________________________\noutputs (Dense)              (None, 1)                 129       \n=================================================================\nTotal params: 447,873\nTrainable params: 447,873\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "  input = tf.keras.layers.Input(x_train.shape[1], name=\"input\")\n",
    "  x = tf.keras.layers.Dense(256, activation='relu')(input)\n",
    "  x = tf.keras.layers.Dropout(0.5)(x)\n",
    "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "  x = tf.keras.layers.Dropout(0.1)(x)\n",
    "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "  outputs = tf.keras.layers.Dense(1, name='outputs')(x)\n",
    "  optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "  model = tf.keras.Model(inputs=input, outputs=outputs)\n",
    "  model.compile(\n",
    "    loss='mse',\n",
    "    metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "    optimizer=optimizer)\n",
    "  return model\n",
    "  \n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFOLDS  = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.8304 - root_mean_squared_error: 0.9113 - val_loss: 0.4810 - val_root_mean_squared_error: 0.6935\n",
      "Epoch 2/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.4936 - root_mean_squared_error: 0.7026 - val_loss: 0.3901 - val_root_mean_squared_error: 0.6246\n",
      "Epoch 3/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3987 - root_mean_squared_error: 0.6314 - val_loss: 0.5783 - val_root_mean_squared_error: 0.7605\n",
      "Epoch 4/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3746 - root_mean_squared_error: 0.6120 - val_loss: 0.5138 - val_root_mean_squared_error: 0.7168\n",
      "Epoch 5/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3532 - root_mean_squared_error: 0.5943 - val_loss: 0.4326 - val_root_mean_squared_error: 0.6577\n",
      "Epoch 6/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3308 - root_mean_squared_error: 0.5752 - val_loss: 0.2851 - val_root_mean_squared_error: 0.5339\n",
      "Epoch 7/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3204 - root_mean_squared_error: 0.5661 - val_loss: 0.2731 - val_root_mean_squared_error: 0.5226\n",
      "Epoch 8/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3130 - root_mean_squared_error: 0.5595 - val_loss: 0.3153 - val_root_mean_squared_error: 0.5615\n",
      "Epoch 9/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3135 - root_mean_squared_error: 0.5599 - val_loss: 0.2885 - val_root_mean_squared_error: 0.5371\n",
      "Epoch 10/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2973 - root_mean_squared_error: 0.5453 - val_loss: 0.2807 - val_root_mean_squared_error: 0.5298\n",
      "Epoch 11/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3002 - root_mean_squared_error: 0.5479 - val_loss: 0.2906 - val_root_mean_squared_error: 0.5391\n",
      "Epoch 12/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2900 - root_mean_squared_error: 0.5385 - val_loss: 0.2623 - val_root_mean_squared_error: 0.5122\n",
      "Epoch 13/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2871 - root_mean_squared_error: 0.5358 - val_loss: 0.2772 - val_root_mean_squared_error: 0.5265\n",
      "Epoch 14/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2842 - root_mean_squared_error: 0.5331 - val_loss: 0.3161 - val_root_mean_squared_error: 0.5622\n",
      "Epoch 15/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2804 - root_mean_squared_error: 0.5296 - val_loss: 0.2538 - val_root_mean_squared_error: 0.5038\n",
      "Epoch 16/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2795 - root_mean_squared_error: 0.5287 - val_loss: 0.3016 - val_root_mean_squared_error: 0.5492\n",
      "Epoch 17/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2775 - root_mean_squared_error: 0.5268 - val_loss: 0.2687 - val_root_mean_squared_error: 0.5184\n",
      "Epoch 18/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2796 - root_mean_squared_error: 0.5288 - val_loss: 0.2693 - val_root_mean_squared_error: 0.5190\n",
      "Epoch 19/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2701 - root_mean_squared_error: 0.5197 - val_loss: 0.2512 - val_root_mean_squared_error: 0.5012\n",
      "Epoch 20/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2696 - root_mean_squared_error: 0.5192 - val_loss: 0.2536 - val_root_mean_squared_error: 0.5036\n",
      "Epoch 21/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2686 - root_mean_squared_error: 0.5183 - val_loss: 0.2613 - val_root_mean_squared_error: 0.5112\n",
      "Epoch 22/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2774 - root_mean_squared_error: 0.5267 - val_loss: 0.2524 - val_root_mean_squared_error: 0.5024\n",
      "Epoch 23/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2726 - root_mean_squared_error: 0.5221 - val_loss: 0.2488 - val_root_mean_squared_error: 0.4988\n",
      "Epoch 24/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2636 - root_mean_squared_error: 0.5134 - val_loss: 0.2627 - val_root_mean_squared_error: 0.5125\n",
      "Epoch 25/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2605 - root_mean_squared_error: 0.5104 - val_loss: 0.2451 - val_root_mean_squared_error: 0.4951\n",
      "Epoch 26/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2643 - root_mean_squared_error: 0.5141 - val_loss: 0.2686 - val_root_mean_squared_error: 0.5182\n",
      "Epoch 27/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2612 - root_mean_squared_error: 0.5111 - val_loss: 0.2488 - val_root_mean_squared_error: 0.4988\n",
      "Epoch 28/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2582 - root_mean_squared_error: 0.5081 - val_loss: 0.2675 - val_root_mean_squared_error: 0.5172\n",
      "Epoch 29/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2571 - root_mean_squared_error: 0.5071 - val_loss: 0.2569 - val_root_mean_squared_error: 0.5069\n",
      "Epoch 30/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2567 - root_mean_squared_error: 0.5066 - val_loss: 0.2460 - val_root_mean_squared_error: 0.4960\n",
      "Epoch 31/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2625 - root_mean_squared_error: 0.5124 - val_loss: 0.2779 - val_root_mean_squared_error: 0.5271\n",
      "Epoch 32/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2531 - root_mean_squared_error: 0.5031 - val_loss: 0.2521 - val_root_mean_squared_error: 0.5021\n",
      "Epoch 33/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2570 - root_mean_squared_error: 0.5069 - val_loss: 0.3284 - val_root_mean_squared_error: 0.5730\n",
      "Epoch 34/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2570 - root_mean_squared_error: 0.5070 - val_loss: 0.2788 - val_root_mean_squared_error: 0.5280\n",
      "Epoch 35/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2502 - root_mean_squared_error: 0.5002 - val_loss: 0.2567 - val_root_mean_squared_error: 0.5066\n",
      "Epoch 36/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2501 - root_mean_squared_error: 0.5001 - val_loss: 0.2937 - val_root_mean_squared_error: 0.5420\n",
      "Epoch 37/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2459 - root_mean_squared_error: 0.4958 - val_loss: 0.2716 - val_root_mean_squared_error: 0.5211\n",
      "Epoch 38/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2510 - root_mean_squared_error: 0.5010 - val_loss: 0.2578 - val_root_mean_squared_error: 0.5077\n",
      "Epoch 39/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2470 - root_mean_squared_error: 0.4970 - val_loss: 0.2951 - val_root_mean_squared_error: 0.5433\n",
      "Epoch 40/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2458 - root_mean_squared_error: 0.4958 - val_loss: 0.2539 - val_root_mean_squared_error: 0.5039\n",
      "Epoch 41/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2425 - root_mean_squared_error: 0.4924 - val_loss: 0.2687 - val_root_mean_squared_error: 0.5184\n",
      "Epoch 42/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2479 - root_mean_squared_error: 0.4979 - val_loss: 0.2497 - val_root_mean_squared_error: 0.4997\n",
      "Epoch 43/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2477 - root_mean_squared_error: 0.4977 - val_loss: 0.2511 - val_root_mean_squared_error: 0.5011\n",
      "Epoch 44/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2521 - root_mean_squared_error: 0.5021 - val_loss: 0.2676 - val_root_mean_squared_error: 0.5173\n",
      "Epoch 45/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2441 - root_mean_squared_error: 0.4941 - val_loss: 0.2508 - val_root_mean_squared_error: 0.5008\n",
      "done... fold 0\n",
      "Epoch 1/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.8228 - root_mean_squared_error: 0.9071 - val_loss: 0.4510 - val_root_mean_squared_error: 0.6716\n",
      "Epoch 2/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.4695 - root_mean_squared_error: 0.6852 - val_loss: 0.5638 - val_root_mean_squared_error: 0.7508\n",
      "Epoch 3/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3883 - root_mean_squared_error: 0.6231 - val_loss: 0.3522 - val_root_mean_squared_error: 0.5934\n",
      "Epoch 4/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3595 - root_mean_squared_error: 0.5996 - val_loss: 0.4826 - val_root_mean_squared_error: 0.6947\n",
      "Epoch 5/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3376 - root_mean_squared_error: 0.5811 - val_loss: 0.4236 - val_root_mean_squared_error: 0.6508\n",
      "Epoch 6/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3391 - root_mean_squared_error: 0.5823 - val_loss: 0.6443 - val_root_mean_squared_error: 0.8027\n",
      "Epoch 7/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3163 - root_mean_squared_error: 0.5624 - val_loss: 0.2760 - val_root_mean_squared_error: 0.5254\n",
      "Epoch 8/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3045 - root_mean_squared_error: 0.5518 - val_loss: 0.3670 - val_root_mean_squared_error: 0.6058\n",
      "Epoch 9/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2980 - root_mean_squared_error: 0.5459 - val_loss: 0.3080 - val_root_mean_squared_error: 0.5550\n",
      "Epoch 10/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2904 - root_mean_squared_error: 0.5389 - val_loss: 0.2821 - val_root_mean_squared_error: 0.5311\n",
      "Epoch 11/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2889 - root_mean_squared_error: 0.5375 - val_loss: 0.3451 - val_root_mean_squared_error: 0.5875\n",
      "Epoch 12/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2846 - root_mean_squared_error: 0.5335 - val_loss: 0.3638 - val_root_mean_squared_error: 0.6031\n",
      "Epoch 13/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2804 - root_mean_squared_error: 0.5295 - val_loss: 0.2970 - val_root_mean_squared_error: 0.5450\n",
      "Epoch 14/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2832 - root_mean_squared_error: 0.5322 - val_loss: 0.2934 - val_root_mean_squared_error: 0.5416\n",
      "Epoch 15/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2750 - root_mean_squared_error: 0.5244 - val_loss: 0.2656 - val_root_mean_squared_error: 0.5153\n",
      "Epoch 16/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2830 - root_mean_squared_error: 0.5320 - val_loss: 0.2608 - val_root_mean_squared_error: 0.5107\n",
      "Epoch 17/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2726 - root_mean_squared_error: 0.5221 - val_loss: 0.2895 - val_root_mean_squared_error: 0.5381\n",
      "Epoch 18/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2756 - root_mean_squared_error: 0.5250 - val_loss: 0.2608 - val_root_mean_squared_error: 0.5107\n",
      "Epoch 19/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2694 - root_mean_squared_error: 0.5190 - val_loss: 0.2580 - val_root_mean_squared_error: 0.5080\n",
      "Epoch 20/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2729 - root_mean_squared_error: 0.5224 - val_loss: 0.3093 - val_root_mean_squared_error: 0.5562\n",
      "Epoch 21/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2646 - root_mean_squared_error: 0.5144 - val_loss: 0.2606 - val_root_mean_squared_error: 0.5104\n",
      "Epoch 22/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2677 - root_mean_squared_error: 0.5174 - val_loss: 0.2557 - val_root_mean_squared_error: 0.5057\n",
      "Epoch 23/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2619 - root_mean_squared_error: 0.5118 - val_loss: 0.2572 - val_root_mean_squared_error: 0.5071\n",
      "Epoch 24/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2647 - root_mean_squared_error: 0.5145 - val_loss: 0.2874 - val_root_mean_squared_error: 0.5361\n",
      "Epoch 25/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2634 - root_mean_squared_error: 0.5132 - val_loss: 0.2774 - val_root_mean_squared_error: 0.5267\n",
      "Epoch 26/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2595 - root_mean_squared_error: 0.5094 - val_loss: 0.3039 - val_root_mean_squared_error: 0.5513\n",
      "Epoch 27/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2600 - root_mean_squared_error: 0.5099 - val_loss: 0.2721 - val_root_mean_squared_error: 0.5217\n",
      "Epoch 28/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2555 - root_mean_squared_error: 0.5054 - val_loss: 0.2759 - val_root_mean_squared_error: 0.5253\n",
      "Epoch 29/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2546 - root_mean_squared_error: 0.5046 - val_loss: 0.2623 - val_root_mean_squared_error: 0.5122\n",
      "Epoch 30/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2582 - root_mean_squared_error: 0.5082 - val_loss: 0.2845 - val_root_mean_squared_error: 0.5334\n",
      "Epoch 31/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2657 - root_mean_squared_error: 0.5154 - val_loss: 0.2984 - val_root_mean_squared_error: 0.5463\n",
      "Epoch 32/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2548 - root_mean_squared_error: 0.5048 - val_loss: 0.2844 - val_root_mean_squared_error: 0.5333\n",
      "Epoch 33/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2511 - root_mean_squared_error: 0.5011 - val_loss: 0.2644 - val_root_mean_squared_error: 0.5142\n",
      "Epoch 34/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2503 - root_mean_squared_error: 0.5003 - val_loss: 0.2570 - val_root_mean_squared_error: 0.5069\n",
      "Epoch 35/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2435 - root_mean_squared_error: 0.4935 - val_loss: 0.2593 - val_root_mean_squared_error: 0.5092\n",
      "Epoch 36/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2498 - root_mean_squared_error: 0.4998 - val_loss: 0.2853 - val_root_mean_squared_error: 0.5341\n",
      "Epoch 37/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2519 - root_mean_squared_error: 0.5019 - val_loss: 0.2571 - val_root_mean_squared_error: 0.5071\n",
      "Epoch 38/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2433 - root_mean_squared_error: 0.4932 - val_loss: 0.2936 - val_root_mean_squared_error: 0.5419\n",
      "Epoch 39/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2416 - root_mean_squared_error: 0.4916 - val_loss: 0.2599 - val_root_mean_squared_error: 0.5098\n",
      "Epoch 40/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2433 - root_mean_squared_error: 0.4932 - val_loss: 0.2590 - val_root_mean_squared_error: 0.5089\n",
      "Epoch 41/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2407 - root_mean_squared_error: 0.4907 - val_loss: 0.2617 - val_root_mean_squared_error: 0.5116\n",
      "Epoch 42/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2408 - root_mean_squared_error: 0.4907 - val_loss: 0.3549 - val_root_mean_squared_error: 0.5958\n",
      "done... fold 1\n",
      "Epoch 1/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.7562 - root_mean_squared_error: 0.8696 - val_loss: 0.4842 - val_root_mean_squared_error: 0.6959\n",
      "Epoch 2/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.4389 - root_mean_squared_error: 0.6625 - val_loss: 0.6696 - val_root_mean_squared_error: 0.8183\n",
      "Epoch 3/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3696 - root_mean_squared_error: 0.6079 - val_loss: 0.5443 - val_root_mean_squared_error: 0.7378\n",
      "Epoch 4/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3462 - root_mean_squared_error: 0.5884 - val_loss: 0.4584 - val_root_mean_squared_error: 0.6770\n",
      "Epoch 5/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3185 - root_mean_squared_error: 0.5643 - val_loss: 0.4056 - val_root_mean_squared_error: 0.6369\n",
      "Epoch 6/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.3094 - root_mean_squared_error: 0.5562 - val_loss: 0.4666 - val_root_mean_squared_error: 0.6831\n",
      "Epoch 7/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2931 - root_mean_squared_error: 0.5414 - val_loss: 0.3265 - val_root_mean_squared_error: 0.5714\n",
      "Epoch 8/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2932 - root_mean_squared_error: 0.5414 - val_loss: 0.3249 - val_root_mean_squared_error: 0.5700\n",
      "Epoch 9/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2922 - root_mean_squared_error: 0.5406 - val_loss: 0.2902 - val_root_mean_squared_error: 0.5387\n",
      "Epoch 10/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2834 - root_mean_squared_error: 0.5324 - val_loss: 0.4326 - val_root_mean_squared_error: 0.6577\n",
      "Epoch 11/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2834 - root_mean_squared_error: 0.5324 - val_loss: 0.3753 - val_root_mean_squared_error: 0.6126\n",
      "Epoch 12/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2890 - root_mean_squared_error: 0.5376 - val_loss: 0.3518 - val_root_mean_squared_error: 0.5931\n",
      "Epoch 13/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2744 - root_mean_squared_error: 0.5239 - val_loss: 0.3301 - val_root_mean_squared_error: 0.5746\n",
      "Epoch 14/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2685 - root_mean_squared_error: 0.5182 - val_loss: 0.2861 - val_root_mean_squared_error: 0.5349\n",
      "Epoch 15/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2702 - root_mean_squared_error: 0.5198 - val_loss: 0.2828 - val_root_mean_squared_error: 0.5318\n",
      "Epoch 16/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2653 - root_mean_squared_error: 0.5151 - val_loss: 0.3912 - val_root_mean_squared_error: 0.6255\n",
      "Epoch 17/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2694 - root_mean_squared_error: 0.5191 - val_loss: 0.2920 - val_root_mean_squared_error: 0.5404\n",
      "Epoch 18/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2647 - root_mean_squared_error: 0.5145 - val_loss: 0.2771 - val_root_mean_squared_error: 0.5264\n",
      "Epoch 19/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2678 - root_mean_squared_error: 0.5175 - val_loss: 0.2762 - val_root_mean_squared_error: 0.5256\n",
      "Epoch 20/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2550 - root_mean_squared_error: 0.5049 - val_loss: 0.3097 - val_root_mean_squared_error: 0.5565\n",
      "Epoch 21/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2610 - root_mean_squared_error: 0.5109 - val_loss: 0.3040 - val_root_mean_squared_error: 0.5514\n",
      "Epoch 22/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2591 - root_mean_squared_error: 0.5090 - val_loss: 0.3321 - val_root_mean_squared_error: 0.5763\n",
      "Epoch 23/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2579 - root_mean_squared_error: 0.5079 - val_loss: 0.2769 - val_root_mean_squared_error: 0.5262\n",
      "Epoch 24/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2520 - root_mean_squared_error: 0.5020 - val_loss: 0.2782 - val_root_mean_squared_error: 0.5274\n",
      "Epoch 25/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2562 - root_mean_squared_error: 0.5062 - val_loss: 0.3193 - val_root_mean_squared_error: 0.5650\n",
      "Epoch 26/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2526 - root_mean_squared_error: 0.5026 - val_loss: 0.2849 - val_root_mean_squared_error: 0.5338\n",
      "Epoch 27/100\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2513 - root_mean_squared_error: 0.5013 - val_loss: 0.2893 - val_root_mean_squared_error: 0.5378\n",
      "Epoch 28/100\n",
      "  1/378 [..............................] - ETA: 0s - loss: 0.1936 - root_mean_squared_error: 0.4400"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-a0267d2d0213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     37\u001b[0m     \u001b[0mearly_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     39\u001b[0m          \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m          \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \"\"\"\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    531\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \"\"\"\n\u001b[1;32m   1062\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result_y_valid = []\n",
    "nn_result_proba_valid = []\n",
    "for fold_no in range(NFOLDS):\n",
    "    # train\n",
    "    train = df_train.copy()\n",
    "    y_train = train[~train['fold_no'] != fold_no)]['cites'].values\n",
    "    y_valid = train[train['fold_no'] == fold_no]['cites'].values\n",
    "    #y_test = train[train['fold_no'] == test_fold_no]['cites'].values\n",
    "\n",
    "    train = train.drop(importance_0_cols, axis=1)\n",
    "    train = train.fillna(0)\n",
    "\n",
    "    x_train = train[~train['fold_no'] != fold_no]\n",
    "    x_valid = train[train['fold_no'] == fold_no]\n",
    "    #x_test = train[train['fold_no'] == test_fold_no]\n",
    "\n",
    "    # drop\n",
    "    x_train = x_train.drop(['cites', 'fold_no'], axis=1)\n",
    "    x_valid = x_valid.drop(['cites', 'fold_no'], axis=1)\n",
    "    #x_test = x_test.drop(['cites', 'fold_no'], axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_valid = scaler.fit_transform(x_valid)\n",
    "\n",
    "    model = build_model()\n",
    "    checkpoint_filepath = f'./models/nn_{fold_no}_weights.hdf5'\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True\n",
    "    )\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "    history = model.fit(\n",
    "         x=x_train,\n",
    "         y=y_train,\n",
    "         validation_data=(x_valid, y_valid),\n",
    "         epochs=100,\n",
    "         verbose=1,\n",
    "         callbacks=[early_stop, model_checkpoint_callback])\n",
    "\n",
    "    model = model.load_weights(checkpoint_filepath)\n",
    "    result_y_valid.extend(y_valid)\n",
    "    #cat_result_proba_test.extend(fold_result_test)\n",
    "    nn_result_proba_valid.extend(model.predict(x_valid))\n",
    "\n",
    "    #rmsle = mean_squared_error(y_test, fold_result_test, squared=False)\n",
    "    print(f\"done... fold {fold_no}\")\n",
    "\n",
    "print(\"valid\", mean_squared_error(result_y_valid, nn_result_proba_valid, squared=False))\n",
    "#print(\"test\", mean_squared_error(result_y_test, cat_result_proba_test, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "valid_fold_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "15117"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "ids = []\n",
    "\n",
    "for fold_no in range(NFOLDS):\n",
    "    test_fold_no = fold_no\n",
    "    valid_fold_no = fold_no + 1\n",
    "    if valid_fold_no == NFOLDS:\n",
    "        valid_fold_no = 0\n",
    "\n",
    "    # train\n",
    "    train = df_train.copy()\n",
    "    ids.extend(train[train['fold_no'] == valid_fold_no]['id'].values)\n",
    "\n",
    "    #y_test = train[train['fold_no'] == test_fold_no]['cites'].values\n",
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    2.197225\n",
       "1    0.693147\n",
       "2    3.044522\n",
       "3    3.178054\n",
       "4    2.833213\n",
       "5    0.693147\n",
       "6    3.465736\n",
       "7     3.89182\n",
       "8    1.098612\n",
       "9    0.693147\n",
       "Name: y, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "df_result['y'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                id       lgb       cat        rf       reg         y\n",
       "0       1601.03743  1.904554  1.944293  2.110282  1.965835  2.197225\n",
       "1       1903.05430  1.276475  1.190545  1.212217  1.248960  0.693147\n",
       "2  hep-lat/0602030  2.836751  2.757892  2.955130  2.896053  3.044522"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>lgb</th>\n      <th>cat</th>\n      <th>rf</th>\n      <th>reg</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1601.03743</td>\n      <td>1.904554</td>\n      <td>1.944293</td>\n      <td>2.110282</td>\n      <td>1.965835</td>\n      <td>2.197225</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1903.05430</td>\n      <td>1.276475</td>\n      <td>1.190545</td>\n      <td>1.212217</td>\n      <td>1.248960</td>\n      <td>0.693147</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>hep-lat/0602030</td>\n      <td>2.836751</td>\n      <td>2.757892</td>\n      <td>2.955130</td>\n      <td>2.896053</td>\n      <td>3.044522</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df_result = pd.DataFrame(ids, columns=['id'])\n",
    "df_result = pd.concat([df_result, pd.DataFrame(lgb_result_proba_valid, columns=['lgb'])], axis=1)\n",
    "df_result = pd.concat([df_result, pd.DataFrame(cat_result_proba_valid, columns=['cat'])], axis=1)\n",
    "df_result = pd.concat([df_result, pd.DataFrame(rf_result_proba_valid, columns=['rf'])], axis=1)\n",
    "df_result = pd.concat([df_result, pd.DataFrame(reg_result_proba_valid, columns=['reg'])], axis=1)\n",
    "df_result = pd.concat([df_result, pd.DataFrame(result_y_valid, columns=['y'])], axis=1)\n",
    "df_result.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_pickle('./first_layer_proba.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.read_pickle('./first_layer_proba.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(15117, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "df_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                id       lgb       cat        rf       reg         y  fold_no\n",
       "0       1601.03743  1.904554  1.944293  2.110282  1.965835  2.197225        2\n",
       "1       1903.05430  1.276475  1.190545  1.212217  1.248960  0.693147        3\n",
       "2  hep-lat/0602030  2.836751  2.757892  2.955130  2.896053  3.044522        1\n",
       "3        0908.1669  3.189939  3.265711  3.344978  3.268603  3.178054        1\n",
       "4        0802.4265  2.859747  2.786780  2.684017  2.530880  2.833213        1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>lgb</th>\n      <th>cat</th>\n      <th>rf</th>\n      <th>reg</th>\n      <th>y</th>\n      <th>fold_no</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1601.03743</td>\n      <td>1.904554</td>\n      <td>1.944293</td>\n      <td>2.110282</td>\n      <td>1.965835</td>\n      <td>2.197225</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1903.05430</td>\n      <td>1.276475</td>\n      <td>1.190545</td>\n      <td>1.212217</td>\n      <td>1.248960</td>\n      <td>0.693147</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>hep-lat/0602030</td>\n      <td>2.836751</td>\n      <td>2.757892</td>\n      <td>2.955130</td>\n      <td>2.896053</td>\n      <td>3.044522</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0908.1669</td>\n      <td>3.189939</td>\n      <td>3.265711</td>\n      <td>3.344978</td>\n      <td>3.268603</td>\n      <td>3.178054</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0802.4265</td>\n      <td>2.859747</td>\n      <td>2.786780</td>\n      <td>2.684017</td>\n      <td>2.530880</td>\n      <td>2.833213</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "NFOLDS = 5\n",
    "kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=777)\n",
    "df_result['fold_no'] = [0 for x in range(len(df_result))]\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df_result)):\n",
    "    df_result.loc[val_idx, 'fold_no'] = fold\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           id   lgb   cat    rf   reg     y\n",
       "fold_no                                    \n",
       "0        3024  3024  3024  3024  3024  3024\n",
       "1        3024  3024  3024  3024  3024  3024\n",
       "2        3023  3023  3023  3023  3023  3023\n",
       "3        3023  3023  3023  3023  3023  3023\n",
       "4        3023  3023  3023  3023  3023  3023"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>lgb</th>\n      <th>cat</th>\n      <th>rf</th>\n      <th>reg</th>\n      <th>y</th>\n    </tr>\n    <tr>\n      <th>fold_no</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3024</td>\n      <td>3024</td>\n      <td>3024</td>\n      <td>3024</td>\n      <td>3024</td>\n      <td>3024</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3024</td>\n      <td>3024</td>\n      <td>3024</td>\n      <td>3024</td>\n      <td>3024</td>\n      <td>3024</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3023</td>\n      <td>3023</td>\n      <td>3023</td>\n      <td>3023</td>\n      <td>3023</td>\n      <td>3023</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3023</td>\n      <td>3023</td>\n      <td>3023</td>\n      <td>3023</td>\n      <td>3023</td>\n      <td>3023</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3023</td>\n      <td>3023</td>\n      <td>3023</td>\n      <td>3023</td>\n      <td>3023</td>\n      <td>3023</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "df_result.groupby('fold_no').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.49331739186014323"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "\n",
    "result_pred = []\n",
    "result_y = []\n",
    "for fold_no in range(5):\n",
    "    X = df_result.copy()\n",
    "    x_train = X[X['fold_no'] != fold_no]\n",
    "    x_valid = X[X['fold_no'] == fold_no]\n",
    "    y_train = x_train['y']\n",
    "    y_valid = x_valid['y']\n",
    "    x_train = x_train.drop(['id', 'fold_no', 'y'], axis=1)\n",
    "    x_valid = x_valid.drop(['id', 'fold_no', 'y'], axis=1)\n",
    "\n",
    "\n",
    "    model = SVR(kernel=\"linear\", C=0.72, epsilon=0.465)\n",
    "    model.fit(x_train, y_train)\n",
    "    pred = model.predict(x_valid)\n",
    "    result_y.extend(y_valid)\n",
    "    result_pred.extend(pred)\n",
    "    pickle.dump(model, open(f'../models/2nd_svr_{fold_no}.pickle', 'wb'))\n",
    "\n",
    "mean_squared_error(result_y, result_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4932670372385315"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "\n",
    "result_pred_r = []\n",
    "result_y = []\n",
    "for fold_no in range(5):\n",
    "    X = df_result.copy()\n",
    "    x_train = X[X['fold_no'] != fold_no]\n",
    "    x_valid = X[X['fold_no'] == fold_no]\n",
    "    y_train = x_train['y']\n",
    "    y_valid = x_valid['y']\n",
    "    x_train = x_train.drop(['id', 'fold_no', 'y'], axis=1)\n",
    "    x_valid = x_valid.drop(['id', 'fold_no', 'y'], axis=1)\n",
    "\n",
    "    model = linear_model.Ridge()\n",
    "    model.fit(x_train, y_train)\n",
    "    pred = model.predict(x_valid)\n",
    "    result_y.extend(y_valid)\n",
    "    result_pred_r.extend(pred)\n",
    "    pickle.dump(model, open(f'../models/2nd_ridge_{fold_no}.pickle', 'wb'))\n",
    "\n",
    "mean_squared_error(result_y, result_pred_r, squared=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4932649098344016"
      ]
     },
     "metadata": {},
     "execution_count": 301
    }
   ],
   "source": [
    "a = (np.array(result_pred)  + np.array(result_pred_r)) /2\n",
    "mean_squared_error(result_y, a, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              0         0\n",
       "0      1.735784  1.746114\n",
       "1      2.973110  2.985532\n",
       "2      3.725636  3.723628\n",
       "3      1.986350  1.995129\n",
       "4      1.795368  1.806691\n",
       "...         ...       ...\n",
       "15112  4.614348  4.599155\n",
       "15113  1.730040  1.726163\n",
       "15114  3.989737  3.995231\n",
       "15115  2.168759  2.172269\n",
       "15116  1.813352  1.802511\n",
       "\n",
       "[15117 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.735784</td>\n      <td>1.746114</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.973110</td>\n      <td>2.985532</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.725636</td>\n      <td>3.723628</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.986350</td>\n      <td>1.995129</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.795368</td>\n      <td>1.806691</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15112</th>\n      <td>4.614348</td>\n      <td>4.599155</td>\n    </tr>\n    <tr>\n      <th>15113</th>\n      <td>1.730040</td>\n      <td>1.726163</td>\n    </tr>\n    <tr>\n      <th>15114</th>\n      <td>3.989737</td>\n      <td>3.995231</td>\n    </tr>\n    <tr>\n      <th>15115</th>\n      <td>2.168759</td>\n      <td>2.172269</td>\n    </tr>\n    <tr>\n      <th>15116</th>\n      <td>1.813352</td>\n      <td>1.802511</td>\n    </tr>\n  </tbody>\n</table>\n<p>15117 rows  2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 298
    }
   ],
   "source": [
    "pd.concat([pd.DataFrame(result_pred), pd.DataFrame(result_pred_r)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4932649098344016"
      ]
     },
     "metadata": {},
     "execution_count": 295
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4934494796935973"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5  roberta_vec_226  \\\n",
       "0         0.038293        -0.076999         0.035965         -0.00164   \n",
       "\n",
       "   roberta_vec_227  roberta_vec_228  roberta_vec_229  roberta_vec_230  \\\n",
       "0        -0.067477          0.20682        -0.090941         0.011562   \n",
       "\n",
       "   roberta_vec_231  roberta_vec_232  roberta_vec_233  roberta_vec_234  \\\n",
       "0         0.523355         0.205579         0.027943         0.256336   \n",
       "\n",
       "   roberta_vec_235  roberta_vec_236  roberta_vec_237  roberta_vec_238  \\\n",
       "0        -0.408816        -0.036333        -0.039951        -0.067919   \n",
       "\n",
       "   roberta_vec_239  roberta_vec_240  roberta_vec_241  roberta_vec_242  \\\n",
       "0        -0.153039        -0.669205         0.106416         0.140609   \n",
       "\n",
       "   roberta_vec_243  roberta_vec_244  roberta_vec_245  roberta_vec_246  \\\n",
       "0         0.073663        -0.095072        -0.029422        -0.021315   \n",
       "\n",
       "   roberta_vec_247  roberta_vec_248  roberta_vec_249  roberta_vec_250  \\\n",
       "0         0.024537         0.019863           0.1113        -0.087268   \n",
       "\n",
       "   roberta_vec_251  roberta_vec_252  roberta_vec_253  roberta_vec_254  \\\n",
       "0         0.033974          -0.0413        -0.002832         0.191807   \n",
       "\n",
       "   roberta_vec_255  roberta_vec_256  roberta_vec_257  roberta_vec_258  \\\n",
       "0        -0.201867         0.066451        -0.023439         -0.33664   \n",
       "\n",
       "   roberta_vec_259  roberta_vec_260  roberta_vec_261  roberta_vec_262  \\\n",
       "0         0.714376         0.090345        -0.191597         0.216644   \n",
       "\n",
       "   roberta_vec_263  roberta_vec_264  roberta_vec_265  roberta_vec_266  \\\n",
       "0        -0.025642        -0.009782        -0.170917        -0.068642   \n",
       "\n",
       "   roberta_vec_267  roberta_vec_268  roberta_vec_269  roberta_vec_270  \\\n",
       "0        -0.000975         0.141942        -0.020858         -0.09397   \n",
       "\n",
       "   roberta_vec_271  roberta_vec_272  roberta_vec_273  roberta_vec_274  \\\n",
       "0         0.041412         0.061818        -0.048434        -0.040401   \n",
       "\n",
       "   roberta_vec_275  roberta_vec_276  roberta_vec_277  roberta_vec_278  \\\n",
       "0         0.484307         0.199968        -0.079365        -0.282826   \n",
       "\n",
       "   roberta_vec_279  roberta_vec_280  roberta_vec_281  roberta_vec_282  \\\n",
       "0         0.018812         0.142837        -0.237518         0.177559   \n",
       "\n",
       "   roberta_vec_283  roberta_vec_284  roberta_vec_285  roberta_vec_286  \\\n",
       "0         0.108307         0.015854        -0.030182        -0.045865   \n",
       "\n",
       "   roberta_vec_287  roberta_vec_288  roberta_vec_289  roberta_vec_290  \\\n",
       "0        -0.018895        -0.130665         0.158553        -0.120776   \n",
       "\n",
       "   roberta_vec_291  roberta_vec_292  roberta_vec_293  roberta_vec_294  \\\n",
       "0        -0.060394        -0.245842        -0.171578         0.217652   \n",
       "\n",
       "   roberta_vec_295  roberta_vec_296  roberta_vec_297  roberta_vec_298  \\\n",
       "0        -0.069561         0.218517        -0.073254        -0.029057   \n",
       "\n",
       "   roberta_vec_299  roberta_vec_300  roberta_vec_301  roberta_vec_302  \\\n",
       "0        -0.045591         0.122068         0.059425         0.108973   \n",
       "\n",
       "   roberta_vec_303  roberta_vec_304  roberta_vec_305  roberta_vec_306  \\\n",
       "0         0.037294        -0.031758         0.092954        -0.298806   \n",
       "\n",
       "   roberta_vec_307  roberta_vec_308  roberta_vec_309  roberta_vec_310  \\\n",
       "0         0.093344         0.245105         -0.03089        -0.109563   \n",
       "\n",
       "   roberta_vec_311  roberta_vec_312  roberta_vec_313  roberta_vec_314  \\\n",
       "0         0.203928        -0.128601         -0.15751         0.035108   \n",
       "\n",
       "   roberta_vec_315  roberta_vec_316  roberta_vec_317  roberta_vec_318  \\\n",
       "0         0.130013        -0.028601        -0.050875        -0.020912   \n",
       "\n",
       "   roberta_vec_319  roberta_vec_320  roberta_vec_321  roberta_vec_322  \\\n",
       "0        -0.041617        -0.225915        -0.147597        -0.346384   \n",
       "\n",
       "   roberta_vec_323  roberta_vec_324  roberta_vec_325  roberta_vec_326  \\\n",
       "0        -0.230136         -0.45081         0.134377        -0.004222   \n",
       "\n",
       "   roberta_vec_327  roberta_vec_328  roberta_vec_329  roberta_vec_330  \\\n",
       "0         0.075372         0.246987         0.189751         1.171893   \n",
       "\n",
       "   roberta_vec_331  roberta_vec_332  roberta_vec_333  roberta_vec_334  \\\n",
       "0         0.423259         0.129495         0.047323         0.245403   \n",
       "\n",
       "   roberta_vec_335  roberta_vec_336  roberta_vec_337  roberta_vec_338  \\\n",
       "0         0.009631        -0.154849         0.136841         0.310707   \n",
       "\n",
       "   roberta_vec_339  roberta_vec_340  roberta_vec_341  roberta_vec_342  \\\n",
       "0         0.053092        -0.118639          0.13245         0.005078   \n",
       "\n",
       "   roberta_vec_343  roberta_vec_344  roberta_vec_345  roberta_vec_346  \\\n",
       "0         0.145224         0.315543        -0.036221         0.078925   \n",
       "\n",
       "   roberta_vec_347  roberta_vec_348  roberta_vec_349  roberta_vec_350  \\\n",
       "0         0.070699         0.323266        -0.204535         -0.19691   \n",
       "\n",
       "   roberta_vec_351  roberta_vec_352  roberta_vec_353  roberta_vec_354  \\\n",
       "0         -0.25279        -0.010706         -0.16057         0.057044   \n",
       "\n",
       "   roberta_vec_355  roberta_vec_356  roberta_vec_357  roberta_vec_358  \\\n",
       "0         0.159736         -0.11282        -0.079733        -0.325507   \n",
       "\n",
       "   roberta_vec_359  roberta_vec_360  roberta_vec_361  roberta_vec_362  \\\n",
       "0        -0.188891        -0.037424         0.284523         0.069744   \n",
       "\n",
       "   roberta_vec_363  roberta_vec_364  roberta_vec_365  roberta_vec_366  \\\n",
       "0         0.039888         0.027148        -0.180135         0.270566   \n",
       "\n",
       "   roberta_vec_367  roberta_vec_368  roberta_vec_369  roberta_vec_370  \\\n",
       "0         0.013305         0.162727        -0.054204         0.033835   \n",
       "\n",
       "   roberta_vec_371  roberta_vec_372  roberta_vec_373  roberta_vec_374  \\\n",
       "0        -0.054199        -0.152402        -0.004757         0.042156   \n",
       "\n",
       "   roberta_vec_375  roberta_vec_376  roberta_vec_377  roberta_vec_378  \\\n",
       "0        -0.095999         0.147516        -0.211751        -0.189056   \n",
       "\n",
       "   roberta_vec_379  roberta_vec_380  roberta_vec_381  roberta_vec_382  \\\n",
       "0         0.024325         0.074092         0.093858        -0.106786   \n",
       "\n",
       "   roberta_vec_383  roberta_vec_384  roberta_vec_385  roberta_vec_386  \\\n",
       "0        -0.024229         0.361901        -0.034408         0.173214   \n",
       "\n",
       "   roberta_vec_387  roberta_vec_388  roberta_vec_389  roberta_vec_390  \\\n",
       "0         0.111462         0.031852         -0.09153         0.117739   \n",
       "\n",
       "   roberta_vec_391  roberta_vec_392  roberta_vec_393  roberta_vec_394  \\\n",
       "0        -0.177065        -0.025684        -0.056748         0.097014   \n",
       "\n",
       "   roberta_vec_395  roberta_vec_396  roberta_vec_397  roberta_vec_398  \\\n",
       "0        -0.064555         0.157628         -0.84213         0.027633   \n",
       "\n",
       "   roberta_vec_399  roberta_vec_400  roberta_vec_401  roberta_vec_402  \\\n",
       "0        -0.110726        -0.012435         0.236703        -0.001034   \n",
       "\n",
       "   roberta_vec_403  roberta_vec_404  roberta_vec_405  roberta_vec_406  \\\n",
       "0        -0.164259        -0.005667         0.162612        -0.022558   \n",
       "\n",
       "   roberta_vec_407  roberta_vec_408  roberta_vec_409  roberta_vec_410  \\\n",
       "0        -0.138819        -0.067203         0.209579         0.166321   \n",
       "\n",
       "   roberta_vec_411  roberta_vec_412  roberta_vec_413  roberta_vec_414  \\\n",
       "0          0.14238         0.005088        -0.166263         0.056212   \n",
       "\n",
       "   roberta_vec_415  roberta_vec_416  roberta_vec_417  roberta_vec_418  \\\n",
       "0         0.058532        -0.172003        -0.141869         -0.30124   \n",
       "\n",
       "   roberta_vec_419  roberta_vec_420  roberta_vec_421  roberta_vec_422  \\\n",
       "0        -0.030682        -0.228836        -0.144127        -0.019697   \n",
       "\n",
       "   roberta_vec_423  roberta_vec_424  roberta_vec_425  roberta_vec_426  \\\n",
       "0        -0.055351        -0.520402        -0.117021          0.30517   \n",
       "\n",
       "   roberta_vec_427  roberta_vec_428  roberta_vec_429  roberta_vec_430  \\\n",
       "0        -0.069852         0.036632          0.16741         0.169146   \n",
       "\n",
       "   roberta_vec_431  roberta_vec_432  roberta_vec_433  roberta_vec_434  \\\n",
       "0         0.017619        -0.193883        -0.067916        -0.273727   \n",
       "\n",
       "   roberta_vec_435  roberta_vec_436  roberta_vec_437  roberta_vec_438  \\\n",
       "0        -0.106562          0.09762        -0.108914        -0.194292   \n",
       "\n",
       "   roberta_vec_439  roberta_vec_440  roberta_vec_441  roberta_vec_442  \\\n",
       "0        -0.050219        -0.271828          0.08072        -0.100197   \n",
       "\n",
       "   roberta_vec_443  roberta_vec_444  roberta_vec_445  roberta_vec_446  \\\n",
       "0         0.024852         -0.16531        -0.122422         0.084847   \n",
       "\n",
       "   roberta_vec_447  roberta_vec_448  roberta_vec_449  roberta_vec_450  \\\n",
       "0        -0.124941        -0.003572        -0.107315        -0.084064   \n",
       "\n",
       "   roberta_vec_451  roberta_vec_452  roberta_vec_453  roberta_vec_454  \\\n",
       "0        -0.034131        -0.471144        -1.553909         0.427042   \n",
       "\n",
       "   roberta_vec_455  roberta_vec_456  roberta_vec_457  roberta_vec_458  \\\n",
       "0         0.133481        -0.149348         0.085869        -0.026381   \n",
       "\n",
       "   roberta_vec_459  roberta_vec_460  roberta_vec_461  roberta_vec_462  \\\n",
       "0        -0.150482          0.11949        -0.034511         0.125529   \n",
       "\n",
       "   roberta_vec_463  roberta_vec_464  roberta_vec_465  roberta_vec_466  \\\n",
       "0         -0.22076         0.051272        -0.168972         0.112018   \n",
       "\n",
       "   roberta_vec_467  roberta_vec_468  roberta_vec_469  roberta_vec_470  \\\n",
       "0        -0.060398         0.114152        -0.003922         0.152598   \n",
       "\n",
       "   roberta_vec_471  roberta_vec_472  roberta_vec_473  roberta_vec_474  \\\n",
       "0        -0.037112        -0.117118        -0.172868        -0.065486   \n",
       "\n",
       "   roberta_vec_475  roberta_vec_476  roberta_vec_477  roberta_vec_478  \\\n",
       "0        -0.066613        -0.090606         0.447557         0.253628   \n",
       "\n",
       "   roberta_vec_479  roberta_vec_480  roberta_vec_481  roberta_vec_482  \\\n",
       "0         0.099496         -0.02372         0.066293         0.149939   \n",
       "\n",
       "   roberta_vec_483  roberta_vec_484  roberta_vec_485  roberta_vec_486  \\\n",
       "0        -0.021434         0.059437          -0.1489         0.022031   \n",
       "\n",
       "   roberta_vec_487  roberta_vec_488  roberta_vec_489  roberta_vec_490  \\\n",
       "0         0.026973         0.262776        -0.259509        -0.190479   \n",
       "\n",
       "   roberta_vec_491  roberta_vec_492  roberta_vec_493  roberta_vec_494  \\\n",
       "0        -0.141458         0.170991         0.259469         0.175812   \n",
       "\n",
       "   roberta_vec_495  roberta_vec_496  roberta_vec_497  roberta_vec_498  \\\n",
       "0         0.248569          0.09565         -0.20874         -0.20845   \n",
       "\n",
       "   roberta_vec_499  roberta_vec_500  roberta_vec_501  roberta_vec_502  \\\n",
       "0         0.113769         0.068898         0.112756        -0.066803   \n",
       "\n",
       "   roberta_vec_503  roberta_vec_504  roberta_vec_505  roberta_vec_506  \\\n",
       "0         0.130623        -0.038461        -0.045274         -0.22113   \n",
       "\n",
       "   roberta_vec_507  roberta_vec_508  roberta_vec_509  roberta_vec_510  \\\n",
       "0         0.057998         0.130247         0.042116        -0.316556   \n",
       "\n",
       "   roberta_vec_511  roberta_vec_512  roberta_vec_513  roberta_vec_514  \\\n",
       "0         0.075787         0.082944         0.086548         0.098292   \n",
       "\n",
       "   roberta_vec_515  roberta_vec_516  roberta_vec_517  roberta_vec_518  \\\n",
       "0         0.178536         0.093488        -0.212282         0.030044   \n",
       "\n",
       "   roberta_vec_519  roberta_vec_520  roberta_vec_521  roberta_vec_522  \\\n",
       "0         0.373361         0.072374         0.007171         0.050314   \n",
       "\n",
       "   roberta_vec_523  roberta_vec_524  roberta_vec_525  roberta_vec_526  \\\n",
       "0         0.078846        -0.125926         0.117607        -0.182267   \n",
       "\n",
       "   roberta_vec_527  roberta_vec_528  roberta_vec_529  roberta_vec_530  \\\n",
       "0        -0.018583         0.010673        -0.101328         0.451349   \n",
       "\n",
       "   roberta_vec_531  roberta_vec_532  roberta_vec_533  roberta_vec_534  \\\n",
       "0        -0.107005         0.303963          0.10985        -0.300183   \n",
       "\n",
       "   roberta_vec_535  roberta_vec_536  roberta_vec_537  roberta_vec_538  \\\n",
       "0        -0.026005         0.120307        -0.143743        -0.053908   \n",
       "\n",
       "   roberta_vec_539  roberta_vec_540  roberta_vec_541  roberta_vec_542  \\\n",
       "0         0.038879        -0.207219         0.136312         0.055207   \n",
       "\n",
       "   roberta_vec_543  roberta_vec_544  roberta_vec_545  roberta_vec_546  \\\n",
       "0         0.080172        -0.067663        -0.183928         0.204984   \n",
       "\n",
       "   roberta_vec_547  roberta_vec_548  roberta_vec_549  roberta_vec_550  \\\n",
       "0        -0.001481         0.160431         0.064906        -0.046666   \n",
       "\n",
       "   roberta_vec_551  roberta_vec_552  roberta_vec_553  roberta_vec_554  \\\n",
       "0         0.114594         0.185505         0.041588        -0.106086   \n",
       "\n",
       "   roberta_vec_555  roberta_vec_556  roberta_vec_557  roberta_vec_558  \\\n",
       "0         0.324065        -0.014526        -0.138719        -0.217771   \n",
       "\n",
       "   roberta_vec_559  roberta_vec_560  roberta_vec_561  roberta_vec_562  \\\n",
       "0         0.388382         0.036732         0.290681        -0.118576   \n",
       "\n",
       "   roberta_vec_563  roberta_vec_564  roberta_vec_565  roberta_vec_566  \\\n",
       "0        -0.056008         0.174637        -0.108908         0.021874   \n",
       "\n",
       "   roberta_vec_567  roberta_vec_568  roberta_vec_569  roberta_vec_570  \\\n",
       "0         0.055263         0.050176         0.018494         1.638244   \n",
       "\n",
       "   roberta_vec_571  roberta_vec_572  roberta_vec_573  roberta_vec_574  \\\n",
       "0         0.292534        -0.157802        -0.064801         0.262028   \n",
       "\n",
       "   roberta_vec_575  roberta_vec_576  roberta_vec_577  roberta_vec_578  \\\n",
       "0         0.053501        -0.101853         0.026971         0.278975   \n",
       "\n",
       "   roberta_vec_579  roberta_vec_580  roberta_vec_581  roberta_vec_582  \\\n",
       "0          0.12391         0.080051        -0.116013         -0.18412   \n",
       "\n",
       "   roberta_vec_583  roberta_vec_584  roberta_vec_585  roberta_vec_586  \\\n",
       "0         0.175828        -0.047083        -0.045451        -0.185876   \n",
       "\n",
       "   roberta_vec_587  roberta_vec_588  roberta_vec_589  roberta_vec_590  \\\n",
       "0          0.04159        12.282816         0.079741        -0.060993   \n",
       "\n",
       "   roberta_vec_591  roberta_vec_592  roberta_vec_593  roberta_vec_594  \\\n",
       "0          0.06344        -0.268049         -0.09487         0.024665   \n",
       "\n",
       "   roberta_vec_595  roberta_vec_596  roberta_vec_597  roberta_vec_598  \\\n",
       "0          0.00188          0.18718         0.040667         0.006068   \n",
       "\n",
       "   roberta_vec_599  roberta_vec_600  roberta_vec_601  roberta_vec_602  \\\n",
       "0         0.023697         0.187934        -0.221596         0.068942   \n",
       "\n",
       "   roberta_vec_603  roberta_vec_604  roberta_vec_605  roberta_vec_606  \\\n",
       "0         0.005736         0.098046         0.084538         0.091436   \n",
       "\n",
       "   roberta_vec_607  roberta_vec_608  roberta_vec_609  roberta_vec_610  \\\n",
       "0        -0.074916         0.082979         0.174427        -0.010615   \n",
       "\n",
       "   roberta_vec_611  roberta_vec_612  roberta_vec_613  roberta_vec_614  \\\n",
       "0         0.956542        -0.157765         0.202592        -0.065107   \n",
       "\n",
       "   roberta_vec_615  roberta_vec_616  roberta_vec_617  roberta_vec_618  \\\n",
       "0         0.086034         0.082509         0.116933        -0.130661   \n",
       "\n",
       "   roberta_vec_619  roberta_vec_620  roberta_vec_621  roberta_vec_622  \\\n",
       "0         0.056648        -0.001675        -0.010225         0.085153   \n",
       "\n",
       "   roberta_vec_623  roberta_vec_624  roberta_vec_625  roberta_vec_626  \\\n",
       "0        -0.174518        -0.064504         0.161261         0.148312   \n",
       "\n",
       "   roberta_vec_627  roberta_vec_628  roberta_vec_629  roberta_vec_630  \\\n",
       "0        -0.075701         -0.12233        -0.085408         0.294268   \n",
       "\n",
       "   roberta_vec_631  roberta_vec_632  roberta_vec_633  roberta_vec_634  \\\n",
       "0        -0.416573         0.074603        -0.209788         0.263318   \n",
       "\n",
       "   roberta_vec_635  roberta_vec_636  roberta_vec_637  roberta_vec_638  \\\n",
       "0          0.31023         0.106331        -0.020113        -0.048917   \n",
       "\n",
       "   roberta_vec_639  roberta_vec_640  roberta_vec_641  roberta_vec_642  \\\n",
       "0        -0.070626         0.096161        -0.188461        -0.075923   \n",
       "\n",
       "   roberta_vec_643  roberta_vec_644  roberta_vec_645  roberta_vec_646  \\\n",
       "0        -0.234436        -0.020224        -0.090146         0.016027   \n",
       "\n",
       "   roberta_vec_647  roberta_vec_648  roberta_vec_649  roberta_vec_650  \\\n",
       "0        -0.018889          -0.1145        -0.154983         0.169091   \n",
       "\n",
       "   roberta_vec_651  roberta_vec_652  roberta_vec_653  roberta_vec_654  \\\n",
       "0         0.098314        -0.164059         0.183723         0.187806   \n",
       "\n",
       "   roberta_vec_655  roberta_vec_656  roberta_vec_657  roberta_vec_658  \\\n",
       "0        -0.131551        -0.050215        -0.003589         0.114386   \n",
       "\n",
       "   roberta_vec_659  roberta_vec_660  roberta_vec_661  roberta_vec_662  \\\n",
       "0        -0.010999        -0.022542         0.019642         0.036398   \n",
       "\n",
       "   roberta_vec_663  roberta_vec_664  roberta_vec_665  roberta_vec_666  \\\n",
       "0        -0.196691        -0.317724         -0.19068        -0.218587   \n",
       "\n",
       "   roberta_vec_667  roberta_vec_668  roberta_vec_669  roberta_vec_670  \\\n",
       "0        -0.481349         0.132161        -0.064054         -0.13544   \n",
       "\n",
       "   roberta_vec_671  roberta_vec_672  roberta_vec_673  roberta_vec_674  \\\n",
       "0        -0.064407        -0.314367          0.12035        -0.229907   \n",
       "\n",
       "   roberta_vec_675  roberta_vec_676  roberta_vec_677  roberta_vec_678  \\\n",
       "0        -0.007436        -0.172259         0.153599         0.202984   \n",
       "\n",
       "   roberta_vec_679  roberta_vec_680  roberta_vec_681  roberta_vec_682  \\\n",
       "0         0.076411         0.012433        -0.207095         0.033421   \n",
       "\n",
       "   roberta_vec_683  roberta_vec_684  roberta_vec_685  roberta_vec_686  \\\n",
       "0        -0.124092         0.215223        -0.086132         0.223904   \n",
       "\n",
       "   roberta_vec_687  roberta_vec_688  roberta_vec_689  roberta_vec_690  \\\n",
       "0         -0.15867        -0.441802        -0.005265        -0.076523   \n",
       "\n",
       "   roberta_vec_691  roberta_vec_692  roberta_vec_693  roberta_vec_694  \\\n",
       "0         0.007779        -0.222167         0.287094         0.008407   \n",
       "\n",
       "   roberta_vec_695  roberta_vec_696  roberta_vec_697  roberta_vec_698  \\\n",
       "0          -0.1184         0.111839         0.096921        -0.109888   \n",
       "\n",
       "   roberta_vec_699  roberta_vec_700  roberta_vec_701  roberta_vec_702  \\\n",
       "0        -0.032591        -0.262088        -0.143762         0.001745   \n",
       "\n",
       "   roberta_vec_703  roberta_vec_704  roberta_vec_705  roberta_vec_706  \\\n",
       "0        -0.218345         0.092003         0.029601         0.020191   \n",
       "\n",
       "   roberta_vec_707  roberta_vec_708  roberta_vec_709  roberta_vec_710  \\\n",
       "0         0.111559         0.005574         0.073102         0.047232   \n",
       "\n",
       "   roberta_vec_711  roberta_vec_712  roberta_vec_713  roberta_vec_714  \\\n",
       "0          0.08606        -0.080468        -0.066836        -0.307097   \n",
       "\n",
       "   roberta_vec_715  roberta_vec_716  roberta_vec_717  roberta_vec_718  \\\n",
       "0        -0.099523        -0.264944          -0.1652        -0.270445   \n",
       "\n",
       "   roberta_vec_719  roberta_vec_720  roberta_vec_721  roberta_vec_722  \\\n",
       "0         0.170967         0.298262         0.029693         0.077656   \n",
       "\n",
       "   roberta_vec_723  roberta_vec_724  roberta_vec_725  roberta_vec_726  \\\n",
       "0         0.060142        -0.168183         0.077958         0.168891   \n",
       "\n",
       "   roberta_vec_727  roberta_vec_728  roberta_vec_729  roberta_vec_730  \\\n",
       "0         0.268703        -0.149408         0.017111        -0.243692   \n",
       "\n",
       "   roberta_vec_731  roberta_vec_732  roberta_vec_733  roberta_vec_734  \\\n",
       "0        -0.190973         0.018555        -0.244975         0.347429   \n",
       "\n",
       "   roberta_vec_735  roberta_vec_736  roberta_vec_737  roberta_vec_738  \\\n",
       "0         0.465995         0.145426         0.111209        -0.079083   \n",
       "\n",
       "   roberta_vec_739  roberta_vec_740  roberta_vec_741  roberta_vec_742  \\\n",
       "0        -0.170695        -0.197767        -0.445161         0.029228   \n",
       "\n",
       "   roberta_vec_743  roberta_vec_744  roberta_vec_745  roberta_vec_746  \\\n",
       "0         0.070849        -0.132407         0.278908         0.070082   \n",
       "\n",
       "   roberta_vec_747  roberta_vec_748  roberta_vec_749  roberta_vec_750  \\\n",
       "0        -0.276633         0.595741        -0.456886        -0.110131   \n",
       "\n",
       "   roberta_vec_751  roberta_vec_752  roberta_vec_753  roberta_vec_754  \\\n",
       "0         0.033729        -0.337985         0.117239         0.234729   \n",
       "\n",
       "   roberta_vec_755  roberta_vec_756  roberta_vec_757  roberta_vec_758  \\\n",
       "0        -0.072439          0.11365         0.048002        -0.158772   \n",
       "\n",
       "   roberta_vec_759  roberta_vec_760  roberta_vec_761  roberta_vec_762  \\\n",
       "0        -0.019317         0.086273         0.033125         0.099122   \n",
       "\n",
       "   roberta_vec_763  roberta_vec_764  roberta_vec_765  roberta_vec_766  \\\n",
       "0         0.154706         0.245593         0.108821        -0.001333   \n",
       "\n",
       "   roberta_vec_767  fold_no  \n",
       "0         0.027004        4  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>submitter</th>\n      <th>authors</th>\n      <th>title</th>\n      <th>comments</th>\n      <th>journal-ref</th>\n      <th>doi</th>\n      <th>report-no</th>\n      <th>categories</th>\n      <th>license</th>\n      <th>abstract</th>\n      <th>versions</th>\n      <th>authors_parsed</th>\n      <th>doi_cites</th>\n      <th>cites</th>\n      <th>doi_id</th>\n      <th>pub_publisher</th>\n      <th>pub_journals</th>\n      <th>pub_dois</th>\n      <th>update_date_y</th>\n      <th>first_created_date</th>\n      <th>last_created_date</th>\n      <th>update_year</th>\n      <th>first_created_year</th>\n      <th>last_created_year</th>\n      <th>update_month</th>\n      <th>first_created_month</th>\n      <th>last_created_month</th>\n      <th>update_ym</th>\n      <th>first_created_ym</th>\n      <th>last_created_ym</th>\n      <th>update_day</th>\n      <th>first_created_day</th>\n      <th>last_created_day</th>\n      <th>update_date_unixtime</th>\n      <th>first_created_unixtime</th>\n      <th>last_created_unixtime</th>\n      <th>diff_update_date_unixtime</th>\n      <th>diff_created_unixtime</th>\n      <th>num_created</th>\n      <th>update_date_days</th>\n      <th>first_created_days</th>\n      <th>last_created_days</th>\n      <th>diff_created_days</th>\n      <th>rate_created_days</th>\n      <th>author_first</th>\n      <th>author_num</th>\n      <th>pred_doi_cites</th>\n      <th>category_main_detail</th>\n      <th>category_main</th>\n      <th>cs_y</th>\n      <th>econ_y</th>\n      <th>eess_y</th>\n      <th>math_y</th>\n      <th>nlin_y</th>\n      <th>physics_y</th>\n      <th>stat_y</th>\n      <th>category_name_parent_main_unique</th>\n      <th>category_name_parent_unique</th>\n      <th>category_name_unique</th>\n      <th>acc-phys_y</th>\n      <th>adap-org_y</th>\n      <th>alg-geom_y</th>\n      <th>ao-sci_y</th>\n      <th>astro-ph_y</th>\n      <th>atom-ph_y</th>\n      <th>bayes-an_y</th>\n      <th>chao-dyn_y</th>\n      <th>chem-ph_y</th>\n      <th>cmp-lg_y</th>\n      <th>comp-gas_y</th>\n      <th>cond-mat_y</th>\n      <th>dg-ga_y</th>\n      <th>funct-an_y</th>\n      <th>gr-qc_y</th>\n      <th>hep-ex_y</th>\n      <th>hep-lat_y</th>\n      <th>hep-ph_y</th>\n      <th>hep-th_y</th>\n      <th>math-ph_y</th>\n      <th>mtrl-th_y</th>\n      <th>nucl-ex_y</th>\n      <th>nucl-th_y</th>\n      <th>patt-sol_y</th>\n      <th>plasm-ph_y</th>\n      <th>q-alg_y</th>\n      <th>q-bio_y</th>\n      <th>q-fin_y</th>\n      <th>quant-ph_y</th>\n      <th>solv-int_y</th>\n      <th>supr-con_y</th>\n      <th>acc_y</th>\n      <th>adap_y</th>\n      <th>alg_y</th>\n      <th>ao_y</th>\n      <th>astro_y</th>\n      <th>atom_y</th>\n      <th>bayes_y</th>\n      <th>chao_y</th>\n      <th>chem_y</th>\n      <th>cmp_y</th>\n      <th>comp_y</th>\n      <th>cond_y</th>\n      <th>cs</th>\n      <th>dg_y</th>\n      <th>econ</th>\n      <th>eess</th>\n      <th>funct_y</th>\n      <th>gr_y</th>\n      <th>hep_y</th>\n      <th>math</th>\n      <th>mtrl_y</th>\n      <th>nlin</th>\n      <th>nucl_y</th>\n      <th>patt_y</th>\n      <th>physics</th>\n      <th>plasm_y</th>\n      <th>q_y</th>\n      <th>quant_y</th>\n      <th>solv_y</th>\n      <th>stat</th>\n      <th>supr_y</th>\n      <th>astro-ph.co</th>\n      <th>astro-ph.ep</th>\n      <th>astro-ph.ga</th>\n      <th>astro-ph.he</th>\n      <th>astro-ph.im</th>\n      <th>astro-ph.sr</th>\n      <th>cond-mat.dis-nn</th>\n      <th>cond-mat.mes-hall</th>\n      <th>cond-mat.mtrl-sci</th>\n      <th>cond-mat.other</th>\n      <th>cond-mat.quant-gas</th>\n      <th>cond-mat.soft</th>\n      <th>cond-mat.stat-mech</th>\n      <th>cond-mat.str-el</th>\n      <th>cond-mat.supr-con</th>\n      <th>cs.ai</th>\n      <th>cs.ar</th>\n      <th>cs.cc</th>\n      <th>cs.ce</th>\n      <th>cs.cg</th>\n      <th>cs.cl</th>\n      <th>cs.cr</th>\n      <th>cs.cv</th>\n      <th>cs.cy</th>\n      <th>cs.db</th>\n      <th>cs.dc</th>\n      <th>cs.dl</th>\n      <th>cs.dm</th>\n      <th>cs.ds</th>\n      <th>cs.et</th>\n      <th>cs.fl</th>\n      <th>cs.gl</th>\n      <th>cs.gr</th>\n      <th>cs.gt</th>\n      <th>cs.hc</th>\n      <th>cs.ir</th>\n      <th>cs.it</th>\n      <th>cs.lg</th>\n      <th>cs.lo</th>\n      <th>cs.ma</th>\n      <th>cs.mm</th>\n      <th>cs.ms</th>\n      <th>cs.na</th>\n      <th>cs.ne</th>\n      <th>cs.ni</th>\n      <th>cs.oh</th>\n      <th>cs.os</th>\n      <th>cs.pf</th>\n      <th>cs.pl</th>\n      <th>cs.ro</th>\n      <th>cs.sc</th>\n      <th>cs.sd</th>\n      <th>cs.se</th>\n      <th>cs.si</th>\n      <th>cs.sy</th>\n      <th>econ.em</th>\n      <th>econ.gn</th>\n      <th>econ.th</th>\n      <th>eess.as</th>\n      <th>eess.iv</th>\n      <th>eess.sp</th>\n      <th>eess.sy</th>\n      <th>math.ac</th>\n      <th>math.ag</th>\n      <th>math.ap</th>\n      <th>math.at</th>\n      <th>math.ca</th>\n      <th>math.co</th>\n      <th>math.ct</th>\n      <th>math.cv</th>\n      <th>math.dg</th>\n      <th>math.ds</th>\n      <th>math.fa</th>\n      <th>math.gm</th>\n      <th>math.gn</th>\n      <th>math.gr</th>\n      <th>math.gt</th>\n      <th>math.ho</th>\n      <th>math.it</th>\n      <th>math.kt</th>\n      <th>math.lo</th>\n      <th>math.mg</th>\n      <th>math.mp</th>\n      <th>math.na</th>\n      <th>math.nt</th>\n      <th>math.oa</th>\n      <th>math.oc</th>\n      <th>math.pr</th>\n      <th>math.qa</th>\n      <th>math.ra</th>\n      <th>math.rt</th>\n      <th>math.sg</th>\n      <th>math.sp</th>\n      <th>math.st</th>\n      <th>nlin.ao</th>\n      <th>nlin.cd</th>\n      <th>nlin.cg</th>\n      <th>nlin.ps</th>\n      <th>nlin.si</th>\n      <th>physics.acc-ph</th>\n      <th>physics.ao-ph</th>\n      <th>physics.app-ph</th>\n      <th>physics.atm-clus</th>\n      <th>physics.atom-ph</th>\n      <th>physics.bio-ph</th>\n      <th>physics.chem-ph</th>\n      <th>physics.class-ph</th>\n      <th>physics.comp-ph</th>\n      <th>physics.data-an</th>\n      <th>physics.ed-ph</th>\n      <th>physics.flu-dyn</th>\n      <th>physics.gen-ph</th>\n      <th>physics.geo-ph</th>\n      <th>physics.hist-ph</th>\n      <th>physics.ins-det</th>\n      <th>physics.med-ph</th>\n      <th>physics.optics</th>\n      <th>physics.plasm-ph</th>\n      <th>physics.pop-ph</th>\n      <th>physics.soc-ph</th>\n      <th>physics.space-ph</th>\n      <th>q-bio.bm</th>\n      <th>q-bio.cb</th>\n      <th>q-bio.gn</th>\n      <th>q-bio.mn</th>\n      <th>q-bio.nc</th>\n      <th>q-bio.ot</th>\n      <th>q-bio.pe</th>\n      <th>q-bio.qm</th>\n      <th>q-bio.sc</th>\n      <th>q-bio.to</th>\n      <th>q-fin.cp</th>\n      <th>q-fin.ec</th>\n      <th>q-fin.gn</th>\n      <th>q-fin.mf</th>\n      <th>q-fin.pm</th>\n      <th>q-fin.pr</th>\n      <th>q-fin.rm</th>\n      <th>q-fin.st</th>\n      <th>q-fin.tr</th>\n      <th>stat.ap</th>\n      <th>stat.co</th>\n      <th>stat.me</th>\n      <th>stat.ml</th>\n      <th>stat.ot</th>\n      <th>stat.th</th>\n      <th>submitter_label</th>\n      <th>doi_id_label</th>\n      <th>author_first_label</th>\n      <th>pub_publisher_label</th>\n      <th>license_label</th>\n      <th>category_main_label</th>\n      <th>category_main_detail_label</th>\n      <th>category_name_parent_label</th>\n      <th>category_name_parent_main_label</th>\n      <th>category_name_label</th>\n      <th>doi_cites_mean_author_first_label</th>\n      <th>doi_cites_count_author_first_label</th>\n      <th>doi_cites_sum_author_first_label</th>\n      <th>doi_cites_min_author_first_label</th>\n      <th>doi_cites_max_author_first_label</th>\n      <th>doi_cites_median_author_first_label</th>\n      <th>doi_cites_std_author_first_label</th>\n      <th>doi_cites_q10_author_first_label</th>\n      <th>doi_cites_q25_author_first_label</th>\n      <th>doi_cites_q75_author_first_label</th>\n      <th>doi_cites_mean_doi_id_label</th>\n      <th>doi_cites_count_doi_id_label</th>\n      <th>doi_cites_sum_doi_id_label</th>\n      <th>doi_cites_min_doi_id_label</th>\n      <th>doi_cites_max_doi_id_label</th>\n      <th>doi_cites_median_doi_id_label</th>\n      <th>doi_cites_std_doi_id_label</th>\n      <th>doi_cites_q10_doi_id_label</th>\n      <th>doi_cites_q25_doi_id_label</th>\n      <th>doi_cites_q75_doi_id_label</th>\n      <th>doi_cites_mean_pub_publisher_label</th>\n      <th>doi_cites_count_pub_publisher_label</th>\n      <th>doi_cites_sum_pub_publisher_label</th>\n      <th>doi_cites_min_pub_publisher_label</th>\n      <th>doi_cites_max_pub_publisher_label</th>\n      <th>doi_cites_median_pub_publisher_label</th>\n      <th>doi_cites_std_pub_publisher_label</th>\n      <th>doi_cites_q10_pub_publisher_label</th>\n      <th>doi_cites_q25_pub_publisher_label</th>\n      <th>doi_cites_q75_pub_publisher_label</th>\n      <th>doi_cites_mean_submitter_label</th>\n      <th>doi_cites_count_submitter_label</th>\n      <th>doi_cites_sum_submitter_label</th>\n      <th>doi_cites_min_submitter_label</th>\n      <th>doi_cites_max_submitter_label</th>\n      <th>doi_cites_median_submitter_label</th>\n      <th>doi_cites_std_submitter_label</th>\n      <th>doi_cites_q10_submitter_label</th>\n      <th>doi_cites_q25_submitter_label</th>\n      <th>doi_cites_q75_submitter_label</th>\n      <th>doi_cites_mean_update_ym</th>\n      <th>doi_cites_count_update_ym</th>\n      <th>doi_cites_sum_update_ym</th>\n      <th>doi_cites_min_update_ym</th>\n      <th>doi_cites_max_update_ym</th>\n      <th>doi_cites_median_update_ym</th>\n      <th>doi_cites_std_update_ym</th>\n      <th>doi_cites_q10_update_ym</th>\n      <th>doi_cites_q25_update_ym</th>\n      <th>doi_cites_q75_update_ym</th>\n      <th>doi_cites_mean_first_created_ym</th>\n      <th>doi_cites_count_first_created_ym</th>\n      <th>doi_cites_sum_first_created_ym</th>\n      <th>doi_cites_min_first_created_ym</th>\n      <th>doi_cites_max_first_created_ym</th>\n      <th>doi_cites_median_first_created_ym</th>\n      <th>doi_cites_std_first_created_ym</th>\n      <th>doi_cites_q10_first_created_ym</th>\n      <th>doi_cites_q25_first_created_ym</th>\n      <th>doi_cites_q75_first_created_ym</th>\n      <th>doi_cites_mean_license_label</th>\n      <th>doi_cites_count_license_label</th>\n      <th>doi_cites_sum_license_label</th>\n      <th>doi_cites_min_license_label</th>\n      <th>doi_cites_max_license_label</th>\n      <th>doi_cites_median_license_label</th>\n      <th>doi_cites_std_license_label</th>\n      <th>doi_cites_q10_license_label</th>\n      <th>doi_cites_q25_license_label</th>\n      <th>doi_cites_q75_license_label</th>\n      <th>doi_cites_mean_category_main_label</th>\n      <th>doi_cites_count_category_main_label</th>\n      <th>doi_cites_sum_category_main_label</th>\n      <th>doi_cites_min_category_main_label</th>\n      <th>doi_cites_max_category_main_label</th>\n      <th>doi_cites_median_category_main_label</th>\n      <th>doi_cites_std_category_main_label</th>\n      <th>doi_cites_q10_category_main_label</th>\n      <th>doi_cites_q25_category_main_label</th>\n      <th>doi_cites_q75_category_main_label</th>\n      <th>doi_cites_mean_category_main_detail_label</th>\n      <th>doi_cites_count_category_main_detail_label</th>\n      <th>doi_cites_sum_category_main_detail_label</th>\n      <th>doi_cites_min_category_main_detail_label</th>\n      <th>doi_cites_max_category_main_detail_label</th>\n      <th>doi_cites_median_category_main_detail_label</th>\n      <th>doi_cites_std_category_main_detail_label</th>\n      <th>doi_cites_q10_category_main_detail_label</th>\n      <th>doi_cites_q25_category_main_detail_label</th>\n      <th>doi_cites_q75_category_main_detail_label</th>\n      <th>doi_cites_mean_category_name_parent_label</th>\n      <th>doi_cites_count_category_name_parent_label</th>\n      <th>doi_cites_sum_category_name_parent_label</th>\n      <th>doi_cites_min_category_name_parent_label</th>\n      <th>doi_cites_max_category_name_parent_label</th>\n      <th>doi_cites_median_category_name_parent_label</th>\n      <th>doi_cites_std_category_name_parent_label</th>\n      <th>doi_cites_q10_category_name_parent_label</th>\n      <th>doi_cites_q25_category_name_parent_label</th>\n      <th>doi_cites_q75_category_name_parent_label</th>\n      <th>doi_cites_mean_category_name_parent_main_label</th>\n      <th>doi_cites_count_category_name_parent_main_label</th>\n      <th>doi_cites_sum_category_name_parent_main_label</th>\n      <th>doi_cites_min_category_name_parent_main_label</th>\n      <th>doi_cites_max_category_name_parent_main_label</th>\n      <th>doi_cites_median_category_name_parent_main_label</th>\n      <th>doi_cites_std_category_name_parent_main_label</th>\n      <th>doi_cites_q10_category_name_parent_main_label</th>\n      <th>doi_cites_q25_category_name_parent_main_label</th>\n      <th>doi_cites_q75_category_name_parent_main_label</th>\n      <th>doi_cites_mean_category_name_label</th>\n      <th>doi_cites_count_category_name_label</th>\n      <th>doi_cites_sum_category_name_label</th>\n      <th>doi_cites_min_category_name_label</th>\n      <th>doi_cites_max_category_name_label</th>\n      <th>doi_cites_median_category_name_label</th>\n      <th>doi_cites_std_category_name_label</th>\n      <th>doi_cites_q10_category_name_label</th>\n      <th>doi_cites_q25_category_name_label</th>\n      <th>doi_cites_q75_category_name_label</th>\n      <th>pred_doi_cites_mean_author_first_label</th>\n      <th>pred_doi_cites_count_author_first_label</th>\n      <th>pred_doi_cites_sum_author_first_label</th>\n      <th>pred_doi_cites_min_author_first_label</th>\n      <th>pred_doi_cites_max_author_first_label</th>\n      <th>pred_doi_cites_median_author_first_label</th>\n      <th>pred_doi_cites_std_author_first_label</th>\n      <th>pred_doi_cites_q10_author_first_label</th>\n      <th>pred_doi_cites_q25_author_first_label</th>\n      <th>pred_doi_cites_q75_author_first_label</th>\n      <th>pred_doi_cites_mean_doi_id_label</th>\n      <th>pred_doi_cites_count_doi_id_label</th>\n      <th>pred_doi_cites_sum_doi_id_label</th>\n      <th>pred_doi_cites_min_doi_id_label</th>\n      <th>pred_doi_cites_max_doi_id_label</th>\n      <th>pred_doi_cites_median_doi_id_label</th>\n      <th>pred_doi_cites_std_doi_id_label</th>\n      <th>pred_doi_cites_q10_doi_id_label</th>\n      <th>pred_doi_cites_q25_doi_id_label</th>\n      <th>pred_doi_cites_q75_doi_id_label</th>\n      <th>pred_doi_cites_mean_pub_publisher_label</th>\n      <th>pred_doi_cites_count_pub_publisher_label</th>\n      <th>pred_doi_cites_sum_pub_publisher_label</th>\n      <th>pred_doi_cites_min_pub_publisher_label</th>\n      <th>pred_doi_cites_max_pub_publisher_label</th>\n      <th>pred_doi_cites_median_pub_publisher_label</th>\n      <th>pred_doi_cites_std_pub_publisher_label</th>\n      <th>pred_doi_cites_q10_pub_publisher_label</th>\n      <th>pred_doi_cites_q25_pub_publisher_label</th>\n      <th>pred_doi_cites_q75_pub_publisher_label</th>\n      <th>pred_doi_cites_mean_submitter_label</th>\n      <th>pred_doi_cites_count_submitter_label</th>\n      <th>pred_doi_cites_sum_submitter_label</th>\n      <th>pred_doi_cites_min_submitter_label</th>\n      <th>pred_doi_cites_max_submitter_label</th>\n      <th>pred_doi_cites_median_submitter_label</th>\n      <th>pred_doi_cites_std_submitter_label</th>\n      <th>pred_doi_cites_q10_submitter_label</th>\n      <th>pred_doi_cites_q25_submitter_label</th>\n      <th>pred_doi_cites_q75_submitter_label</th>\n      <th>pred_doi_cites_mean_update_ym</th>\n      <th>pred_doi_cites_count_update_ym</th>\n      <th>pred_doi_cites_sum_update_ym</th>\n      <th>pred_doi_cites_min_update_ym</th>\n      <th>pred_doi_cites_max_update_ym</th>\n      <th>pred_doi_cites_median_update_ym</th>\n      <th>pred_doi_cites_std_update_ym</th>\n      <th>pred_doi_cites_q10_update_ym</th>\n      <th>pred_doi_cites_q25_update_ym</th>\n      <th>pred_doi_cites_q75_update_ym</th>\n      <th>pred_doi_cites_mean_first_created_ym</th>\n      <th>pred_doi_cites_count_first_created_ym</th>\n      <th>pred_doi_cites_sum_first_created_ym</th>\n      <th>pred_doi_cites_min_first_created_ym</th>\n      <th>pred_doi_cites_max_first_created_ym</th>\n      <th>pred_doi_cites_median_first_created_ym</th>\n      <th>pred_doi_cites_std_first_created_ym</th>\n      <th>pred_doi_cites_q10_first_created_ym</th>\n      <th>pred_doi_cites_q25_first_created_ym</th>\n      <th>pred_doi_cites_q75_first_created_ym</th>\n      <th>pred_doi_cites_mean_license_label</th>\n      <th>pred_doi_cites_count_license_label</th>\n      <th>pred_doi_cites_sum_license_label</th>\n      <th>pred_doi_cites_min_license_label</th>\n      <th>pred_doi_cites_max_license_label</th>\n      <th>pred_doi_cites_median_license_label</th>\n      <th>pred_doi_cites_std_license_label</th>\n      <th>pred_doi_cites_q10_license_label</th>\n      <th>pred_doi_cites_q25_license_label</th>\n      <th>pred_doi_cites_q75_license_label</th>\n      <th>pred_doi_cites_mean_category_main_label</th>\n      <th>pred_doi_cites_count_category_main_label</th>\n      <th>pred_doi_cites_sum_category_main_label</th>\n      <th>pred_doi_cites_min_category_main_label</th>\n      <th>pred_doi_cites_max_category_main_label</th>\n      <th>pred_doi_cites_median_category_main_label</th>\n      <th>pred_doi_cites_std_category_main_label</th>\n      <th>pred_doi_cites_q10_category_main_label</th>\n      <th>pred_doi_cites_q25_category_main_label</th>\n      <th>pred_doi_cites_q75_category_main_label</th>\n      <th>pred_doi_cites_mean_category_main_detail_label</th>\n      <th>pred_doi_cites_count_category_main_detail_label</th>\n      <th>pred_doi_cites_sum_category_main_detail_label</th>\n      <th>pred_doi_cites_min_category_main_detail_label</th>\n      <th>pred_doi_cites_max_category_main_detail_label</th>\n      <th>pred_doi_cites_median_category_main_detail_label</th>\n      <th>pred_doi_cites_std_category_main_detail_label</th>\n      <th>pred_doi_cites_q10_category_main_detail_label</th>\n      <th>pred_doi_cites_q25_category_main_detail_label</th>\n      <th>pred_doi_cites_q75_category_main_detail_label</th>\n      <th>pred_doi_cites_mean_category_name_parent_label</th>\n      <th>pred_doi_cites_count_category_name_parent_label</th>\n      <th>pred_doi_cites_sum_category_name_parent_label</th>\n      <th>pred_doi_cites_min_category_name_parent_label</th>\n      <th>pred_doi_cites_max_category_name_parent_label</th>\n      <th>pred_doi_cites_median_category_name_parent_label</th>\n      <th>pred_doi_cites_std_category_name_parent_label</th>\n      <th>pred_doi_cites_q10_category_name_parent_label</th>\n      <th>pred_doi_cites_q25_category_name_parent_label</th>\n      <th>pred_doi_cites_q75_category_name_parent_label</th>\n      <th>pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>pred_doi_cites_count_category_name_parent_main_label</th>\n      <th>pred_doi_cites_sum_category_name_parent_main_label</th>\n      <th>pred_doi_cites_min_category_name_parent_main_label</th>\n      <th>pred_doi_cites_max_category_name_parent_main_label</th>\n      <th>pred_doi_cites_median_category_name_parent_main_label</th>\n      <th>pred_doi_cites_std_category_name_parent_main_label</th>\n      <th>pred_doi_cites_q10_category_name_parent_main_label</th>\n      <th>pred_doi_cites_q25_category_name_parent_main_label</th>\n      <th>pred_doi_cites_q75_category_name_parent_main_label</th>\n      <th>pred_doi_cites_mean_category_name_label</th>\n      <th>pred_doi_cites_count_category_name_label</th>\n      <th>pred_doi_cites_sum_category_name_label</th>\n      <th>pred_doi_cites_min_category_name_label</th>\n      <th>pred_doi_cites_max_category_name_label</th>\n      <th>pred_doi_cites_median_category_name_label</th>\n      <th>pred_doi_cites_std_category_name_label</th>\n      <th>pred_doi_cites_q10_category_name_label</th>\n      <th>pred_doi_cites_q25_category_name_label</th>\n      <th>pred_doi_cites_q75_category_name_label</th>\n      <th>diff_doi_cites_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_pred_doi_cites</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_main_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_category_main_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_main_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_main_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_submitter_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_update_ym</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_license_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_doi_id_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_author_first_label</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_update_ym</th>\n      <th>diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_license_label</th>\n      <th>diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_first_created_ym</th>\n      <th>diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_license_label</th>\n      <th>diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_license_label</th>\n      <th>diff_rate_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_license_label</th>\n      <th>diff_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_label</th>\n      <th>diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_pred_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_pred_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_pred_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_label</th>\n      <th>is_null_comments</th>\n      <th>is_null_report-no</th>\n      <th>is_null_journal-ref</th>\n      <th>roberta_vec_0</th>\n      <th>roberta_vec_1</th>\n      <th>roberta_vec_2</th>\n      <th>roberta_vec_3</th>\n      <th>roberta_vec_4</th>\n      <th>roberta_vec_5</th>\n      <th>roberta_vec_6</th>\n      <th>roberta_vec_7</th>\n      <th>roberta_vec_8</th>\n      <th>roberta_vec_9</th>\n      <th>roberta_vec_10</th>\n      <th>roberta_vec_11</th>\n      <th>roberta_vec_12</th>\n      <th>roberta_vec_13</th>\n      <th>roberta_vec_14</th>\n      <th>roberta_vec_15</th>\n      <th>roberta_vec_16</th>\n      <th>roberta_vec_17</th>\n      <th>roberta_vec_18</th>\n      <th>roberta_vec_19</th>\n      <th>roberta_vec_20</th>\n      <th>roberta_vec_21</th>\n      <th>roberta_vec_22</th>\n      <th>roberta_vec_23</th>\n      <th>roberta_vec_24</th>\n      <th>roberta_vec_25</th>\n      <th>roberta_vec_26</th>\n      <th>roberta_vec_27</th>\n      <th>roberta_vec_28</th>\n      <th>roberta_vec_29</th>\n      <th>roberta_vec_30</th>\n      <th>roberta_vec_31</th>\n      <th>roberta_vec_32</th>\n      <th>roberta_vec_33</th>\n      <th>roberta_vec_34</th>\n      <th>roberta_vec_35</th>\n      <th>roberta_vec_36</th>\n      <th>roberta_vec_37</th>\n      <th>roberta_vec_38</th>\n      <th>roberta_vec_39</th>\n      <th>roberta_vec_40</th>\n      <th>roberta_vec_41</th>\n      <th>roberta_vec_42</th>\n      <th>roberta_vec_43</th>\n      <th>roberta_vec_44</th>\n      <th>roberta_vec_45</th>\n      <th>roberta_vec_46</th>\n      <th>roberta_vec_47</th>\n      <th>roberta_vec_48</th>\n      <th>roberta_vec_49</th>\n      <th>roberta_vec_50</th>\n      <th>roberta_vec_51</th>\n      <th>roberta_vec_52</th>\n      <th>roberta_vec_53</th>\n      <th>roberta_vec_54</th>\n      <th>roberta_vec_55</th>\n      <th>roberta_vec_56</th>\n      <th>roberta_vec_57</th>\n      <th>roberta_vec_58</th>\n      <th>roberta_vec_59</th>\n      <th>roberta_vec_60</th>\n      <th>roberta_vec_61</th>\n      <th>roberta_vec_62</th>\n      <th>roberta_vec_63</th>\n      <th>roberta_vec_64</th>\n      <th>roberta_vec_65</th>\n      <th>roberta_vec_66</th>\n      <th>roberta_vec_67</th>\n      <th>roberta_vec_68</th>\n      <th>roberta_vec_69</th>\n      <th>roberta_vec_70</th>\n      <th>roberta_vec_71</th>\n      <th>roberta_vec_72</th>\n      <th>roberta_vec_73</th>\n      <th>roberta_vec_74</th>\n      <th>roberta_vec_75</th>\n      <th>roberta_vec_76</th>\n      <th>roberta_vec_77</th>\n      <th>roberta_vec_78</th>\n      <th>roberta_vec_79</th>\n      <th>roberta_vec_80</th>\n      <th>roberta_vec_81</th>\n      <th>roberta_vec_82</th>\n      <th>roberta_vec_83</th>\n      <th>roberta_vec_84</th>\n      <th>roberta_vec_85</th>\n      <th>roberta_vec_86</th>\n      <th>roberta_vec_87</th>\n      <th>roberta_vec_88</th>\n      <th>roberta_vec_89</th>\n      <th>roberta_vec_90</th>\n      <th>roberta_vec_91</th>\n      <th>roberta_vec_92</th>\n      <th>roberta_vec_93</th>\n      <th>roberta_vec_94</th>\n      <th>roberta_vec_95</th>\n      <th>roberta_vec_96</th>\n      <th>roberta_vec_97</th>\n      <th>roberta_vec_98</th>\n      <th>roberta_vec_99</th>\n      <th>roberta_vec_100</th>\n      <th>roberta_vec_101</th>\n      <th>roberta_vec_102</th>\n      <th>roberta_vec_103</th>\n      <th>roberta_vec_104</th>\n      <th>roberta_vec_105</th>\n      <th>roberta_vec_106</th>\n      <th>roberta_vec_107</th>\n      <th>roberta_vec_108</th>\n      <th>roberta_vec_109</th>\n      <th>roberta_vec_110</th>\n      <th>roberta_vec_111</th>\n      <th>roberta_vec_112</th>\n      <th>roberta_vec_113</th>\n      <th>roberta_vec_114</th>\n      <th>roberta_vec_115</th>\n      <th>roberta_vec_116</th>\n      <th>roberta_vec_117</th>\n      <th>roberta_vec_118</th>\n      <th>roberta_vec_119</th>\n      <th>roberta_vec_120</th>\n      <th>roberta_vec_121</th>\n      <th>roberta_vec_122</th>\n      <th>roberta_vec_123</th>\n      <th>roberta_vec_124</th>\n      <th>roberta_vec_125</th>\n      <th>roberta_vec_126</th>\n      <th>roberta_vec_127</th>\n      <th>roberta_vec_128</th>\n      <th>roberta_vec_129</th>\n      <th>roberta_vec_130</th>\n      <th>roberta_vec_131</th>\n      <th>roberta_vec_132</th>\n      <th>roberta_vec_133</th>\n      <th>roberta_vec_134</th>\n      <th>roberta_vec_135</th>\n      <th>roberta_vec_136</th>\n      <th>roberta_vec_137</th>\n      <th>roberta_vec_138</th>\n      <th>roberta_vec_139</th>\n      <th>roberta_vec_140</th>\n      <th>roberta_vec_141</th>\n      <th>roberta_vec_142</th>\n      <th>roberta_vec_143</th>\n      <th>roberta_vec_144</th>\n      <th>roberta_vec_145</th>\n      <th>roberta_vec_146</th>\n      <th>roberta_vec_147</th>\n      <th>roberta_vec_148</th>\n      <th>roberta_vec_149</th>\n      <th>roberta_vec_150</th>\n      <th>roberta_vec_151</th>\n      <th>roberta_vec_152</th>\n      <th>roberta_vec_153</th>\n      <th>roberta_vec_154</th>\n      <th>roberta_vec_155</th>\n      <th>roberta_vec_156</th>\n      <th>roberta_vec_157</th>\n      <th>roberta_vec_158</th>\n      <th>roberta_vec_159</th>\n      <th>roberta_vec_160</th>\n      <th>roberta_vec_161</th>\n      <th>roberta_vec_162</th>\n      <th>roberta_vec_163</th>\n      <th>roberta_vec_164</th>\n      <th>roberta_vec_165</th>\n      <th>roberta_vec_166</th>\n      <th>roberta_vec_167</th>\n      <th>roberta_vec_168</th>\n      <th>roberta_vec_169</th>\n      <th>roberta_vec_170</th>\n      <th>roberta_vec_171</th>\n      <th>roberta_vec_172</th>\n      <th>roberta_vec_173</th>\n      <th>roberta_vec_174</th>\n      <th>roberta_vec_175</th>\n      <th>roberta_vec_176</th>\n      <th>roberta_vec_177</th>\n      <th>roberta_vec_178</th>\n      <th>roberta_vec_179</th>\n      <th>roberta_vec_180</th>\n      <th>roberta_vec_181</th>\n      <th>roberta_vec_182</th>\n      <th>roberta_vec_183</th>\n      <th>roberta_vec_184</th>\n      <th>roberta_vec_185</th>\n      <th>roberta_vec_186</th>\n      <th>roberta_vec_187</th>\n      <th>roberta_vec_188</th>\n      <th>roberta_vec_189</th>\n      <th>roberta_vec_190</th>\n      <th>roberta_vec_191</th>\n      <th>roberta_vec_192</th>\n      <th>roberta_vec_193</th>\n      <th>roberta_vec_194</th>\n      <th>roberta_vec_195</th>\n      <th>roberta_vec_196</th>\n      <th>roberta_vec_197</th>\n      <th>roberta_vec_198</th>\n      <th>roberta_vec_199</th>\n      <th>roberta_vec_200</th>\n      <th>roberta_vec_201</th>\n      <th>roberta_vec_202</th>\n      <th>roberta_vec_203</th>\n      <th>roberta_vec_204</th>\n      <th>roberta_vec_205</th>\n      <th>roberta_vec_206</th>\n      <th>roberta_vec_207</th>\n      <th>roberta_vec_208</th>\n      <th>roberta_vec_209</th>\n      <th>roberta_vec_210</th>\n      <th>roberta_vec_211</th>\n      <th>roberta_vec_212</th>\n      <th>roberta_vec_213</th>\n      <th>roberta_vec_214</th>\n      <th>roberta_vec_215</th>\n      <th>roberta_vec_216</th>\n      <th>roberta_vec_217</th>\n      <th>roberta_vec_218</th>\n      <th>roberta_vec_219</th>\n      <th>roberta_vec_220</th>\n      <th>roberta_vec_221</th>\n      <th>roberta_vec_222</th>\n      <th>roberta_vec_223</th>\n      <th>roberta_vec_224</th>\n      <th>roberta_vec_225</th>\n      <th>roberta_vec_226</th>\n      <th>roberta_vec_227</th>\n      <th>roberta_vec_228</th>\n      <th>roberta_vec_229</th>\n      <th>roberta_vec_230</th>\n      <th>roberta_vec_231</th>\n      <th>roberta_vec_232</th>\n      <th>roberta_vec_233</th>\n      <th>roberta_vec_234</th>\n      <th>roberta_vec_235</th>\n      <th>roberta_vec_236</th>\n      <th>roberta_vec_237</th>\n      <th>roberta_vec_238</th>\n      <th>roberta_vec_239</th>\n      <th>roberta_vec_240</th>\n      <th>roberta_vec_241</th>\n      <th>roberta_vec_242</th>\n      <th>roberta_vec_243</th>\n      <th>roberta_vec_244</th>\n      <th>roberta_vec_245</th>\n      <th>roberta_vec_246</th>\n      <th>roberta_vec_247</th>\n      <th>roberta_vec_248</th>\n      <th>roberta_vec_249</th>\n      <th>roberta_vec_250</th>\n      <th>roberta_vec_251</th>\n      <th>roberta_vec_252</th>\n      <th>roberta_vec_253</th>\n      <th>roberta_vec_254</th>\n      <th>roberta_vec_255</th>\n      <th>roberta_vec_256</th>\n      <th>roberta_vec_257</th>\n      <th>roberta_vec_258</th>\n      <th>roberta_vec_259</th>\n      <th>roberta_vec_260</th>\n      <th>roberta_vec_261</th>\n      <th>roberta_vec_262</th>\n      <th>roberta_vec_263</th>\n      <th>roberta_vec_264</th>\n      <th>roberta_vec_265</th>\n      <th>roberta_vec_266</th>\n      <th>roberta_vec_267</th>\n      <th>roberta_vec_268</th>\n      <th>roberta_vec_269</th>\n      <th>roberta_vec_270</th>\n      <th>roberta_vec_271</th>\n      <th>roberta_vec_272</th>\n      <th>roberta_vec_273</th>\n      <th>roberta_vec_274</th>\n      <th>roberta_vec_275</th>\n      <th>roberta_vec_276</th>\n      <th>roberta_vec_277</th>\n      <th>roberta_vec_278</th>\n      <th>roberta_vec_279</th>\n      <th>roberta_vec_280</th>\n      <th>roberta_vec_281</th>\n      <th>roberta_vec_282</th>\n      <th>roberta_vec_283</th>\n      <th>roberta_vec_284</th>\n      <th>roberta_vec_285</th>\n      <th>roberta_vec_286</th>\n      <th>roberta_vec_287</th>\n      <th>roberta_vec_288</th>\n      <th>roberta_vec_289</th>\n      <th>roberta_vec_290</th>\n      <th>roberta_vec_291</th>\n      <th>roberta_vec_292</th>\n      <th>roberta_vec_293</th>\n      <th>roberta_vec_294</th>\n      <th>roberta_vec_295</th>\n      <th>roberta_vec_296</th>\n      <th>roberta_vec_297</th>\n      <th>roberta_vec_298</th>\n      <th>roberta_vec_299</th>\n      <th>roberta_vec_300</th>\n      <th>roberta_vec_301</th>\n      <th>roberta_vec_302</th>\n      <th>roberta_vec_303</th>\n      <th>roberta_vec_304</th>\n      <th>roberta_vec_305</th>\n      <th>roberta_vec_306</th>\n      <th>roberta_vec_307</th>\n      <th>roberta_vec_308</th>\n      <th>roberta_vec_309</th>\n      <th>roberta_vec_310</th>\n      <th>roberta_vec_311</th>\n      <th>roberta_vec_312</th>\n      <th>roberta_vec_313</th>\n      <th>roberta_vec_314</th>\n      <th>roberta_vec_315</th>\n      <th>roberta_vec_316</th>\n      <th>roberta_vec_317</th>\n      <th>roberta_vec_318</th>\n      <th>roberta_vec_319</th>\n      <th>roberta_vec_320</th>\n      <th>roberta_vec_321</th>\n      <th>roberta_vec_322</th>\n      <th>roberta_vec_323</th>\n      <th>roberta_vec_324</th>\n      <th>roberta_vec_325</th>\n      <th>roberta_vec_326</th>\n      <th>roberta_vec_327</th>\n      <th>roberta_vec_328</th>\n      <th>roberta_vec_329</th>\n      <th>roberta_vec_330</th>\n      <th>roberta_vec_331</th>\n      <th>roberta_vec_332</th>\n      <th>roberta_vec_333</th>\n      <th>roberta_vec_334</th>\n      <th>roberta_vec_335</th>\n      <th>roberta_vec_336</th>\n      <th>roberta_vec_337</th>\n      <th>roberta_vec_338</th>\n      <th>roberta_vec_339</th>\n      <th>roberta_vec_340</th>\n      <th>roberta_vec_341</th>\n      <th>roberta_vec_342</th>\n      <th>roberta_vec_343</th>\n      <th>roberta_vec_344</th>\n      <th>roberta_vec_345</th>\n      <th>roberta_vec_346</th>\n      <th>roberta_vec_347</th>\n      <th>roberta_vec_348</th>\n      <th>roberta_vec_349</th>\n      <th>roberta_vec_350</th>\n      <th>roberta_vec_351</th>\n      <th>roberta_vec_352</th>\n      <th>roberta_vec_353</th>\n      <th>roberta_vec_354</th>\n      <th>roberta_vec_355</th>\n      <th>roberta_vec_356</th>\n      <th>roberta_vec_357</th>\n      <th>roberta_vec_358</th>\n      <th>roberta_vec_359</th>\n      <th>roberta_vec_360</th>\n      <th>roberta_vec_361</th>\n      <th>roberta_vec_362</th>\n      <th>roberta_vec_363</th>\n      <th>roberta_vec_364</th>\n      <th>roberta_vec_365</th>\n      <th>roberta_vec_366</th>\n      <th>roberta_vec_367</th>\n      <th>roberta_vec_368</th>\n      <th>roberta_vec_369</th>\n      <th>roberta_vec_370</th>\n      <th>roberta_vec_371</th>\n      <th>roberta_vec_372</th>\n      <th>roberta_vec_373</th>\n      <th>roberta_vec_374</th>\n      <th>roberta_vec_375</th>\n      <th>roberta_vec_376</th>\n      <th>roberta_vec_377</th>\n      <th>roberta_vec_378</th>\n      <th>roberta_vec_379</th>\n      <th>roberta_vec_380</th>\n      <th>roberta_vec_381</th>\n      <th>roberta_vec_382</th>\n      <th>roberta_vec_383</th>\n      <th>roberta_vec_384</th>\n      <th>roberta_vec_385</th>\n      <th>roberta_vec_386</th>\n      <th>roberta_vec_387</th>\n      <th>roberta_vec_388</th>\n      <th>roberta_vec_389</th>\n      <th>roberta_vec_390</th>\n      <th>roberta_vec_391</th>\n      <th>roberta_vec_392</th>\n      <th>roberta_vec_393</th>\n      <th>roberta_vec_394</th>\n      <th>roberta_vec_395</th>\n      <th>roberta_vec_396</th>\n      <th>roberta_vec_397</th>\n      <th>roberta_vec_398</th>\n      <th>roberta_vec_399</th>\n      <th>roberta_vec_400</th>\n      <th>roberta_vec_401</th>\n      <th>roberta_vec_402</th>\n      <th>roberta_vec_403</th>\n      <th>roberta_vec_404</th>\n      <th>roberta_vec_405</th>\n      <th>roberta_vec_406</th>\n      <th>roberta_vec_407</th>\n      <th>roberta_vec_408</th>\n      <th>roberta_vec_409</th>\n      <th>roberta_vec_410</th>\n      <th>roberta_vec_411</th>\n      <th>roberta_vec_412</th>\n      <th>roberta_vec_413</th>\n      <th>roberta_vec_414</th>\n      <th>roberta_vec_415</th>\n      <th>roberta_vec_416</th>\n      <th>roberta_vec_417</th>\n      <th>roberta_vec_418</th>\n      <th>roberta_vec_419</th>\n      <th>roberta_vec_420</th>\n      <th>roberta_vec_421</th>\n      <th>roberta_vec_422</th>\n      <th>roberta_vec_423</th>\n      <th>roberta_vec_424</th>\n      <th>roberta_vec_425</th>\n      <th>roberta_vec_426</th>\n      <th>roberta_vec_427</th>\n      <th>roberta_vec_428</th>\n      <th>roberta_vec_429</th>\n      <th>roberta_vec_430</th>\n      <th>roberta_vec_431</th>\n      <th>roberta_vec_432</th>\n      <th>roberta_vec_433</th>\n      <th>roberta_vec_434</th>\n      <th>roberta_vec_435</th>\n      <th>roberta_vec_436</th>\n      <th>roberta_vec_437</th>\n      <th>roberta_vec_438</th>\n      <th>roberta_vec_439</th>\n      <th>roberta_vec_440</th>\n      <th>roberta_vec_441</th>\n      <th>roberta_vec_442</th>\n      <th>roberta_vec_443</th>\n      <th>roberta_vec_444</th>\n      <th>roberta_vec_445</th>\n      <th>roberta_vec_446</th>\n      <th>roberta_vec_447</th>\n      <th>roberta_vec_448</th>\n      <th>roberta_vec_449</th>\n      <th>roberta_vec_450</th>\n      <th>roberta_vec_451</th>\n      <th>roberta_vec_452</th>\n      <th>roberta_vec_453</th>\n      <th>roberta_vec_454</th>\n      <th>roberta_vec_455</th>\n      <th>roberta_vec_456</th>\n      <th>roberta_vec_457</th>\n      <th>roberta_vec_458</th>\n      <th>roberta_vec_459</th>\n      <th>roberta_vec_460</th>\n      <th>roberta_vec_461</th>\n      <th>roberta_vec_462</th>\n      <th>roberta_vec_463</th>\n      <th>roberta_vec_464</th>\n      <th>roberta_vec_465</th>\n      <th>roberta_vec_466</th>\n      <th>roberta_vec_467</th>\n      <th>roberta_vec_468</th>\n      <th>roberta_vec_469</th>\n      <th>roberta_vec_470</th>\n      <th>roberta_vec_471</th>\n      <th>roberta_vec_472</th>\n      <th>roberta_vec_473</th>\n      <th>roberta_vec_474</th>\n      <th>roberta_vec_475</th>\n      <th>roberta_vec_476</th>\n      <th>roberta_vec_477</th>\n      <th>roberta_vec_478</th>\n      <th>roberta_vec_479</th>\n      <th>roberta_vec_480</th>\n      <th>roberta_vec_481</th>\n      <th>roberta_vec_482</th>\n      <th>roberta_vec_483</th>\n      <th>roberta_vec_484</th>\n      <th>roberta_vec_485</th>\n      <th>roberta_vec_486</th>\n      <th>roberta_vec_487</th>\n      <th>roberta_vec_488</th>\n      <th>roberta_vec_489</th>\n      <th>roberta_vec_490</th>\n      <th>roberta_vec_491</th>\n      <th>roberta_vec_492</th>\n      <th>roberta_vec_493</th>\n      <th>roberta_vec_494</th>\n      <th>roberta_vec_495</th>\n      <th>roberta_vec_496</th>\n      <th>roberta_vec_497</th>\n      <th>roberta_vec_498</th>\n      <th>roberta_vec_499</th>\n      <th>roberta_vec_500</th>\n      <th>roberta_vec_501</th>\n      <th>roberta_vec_502</th>\n      <th>roberta_vec_503</th>\n      <th>roberta_vec_504</th>\n      <th>roberta_vec_505</th>\n      <th>roberta_vec_506</th>\n      <th>roberta_vec_507</th>\n      <th>roberta_vec_508</th>\n      <th>roberta_vec_509</th>\n      <th>roberta_vec_510</th>\n      <th>roberta_vec_511</th>\n      <th>roberta_vec_512</th>\n      <th>roberta_vec_513</th>\n      <th>roberta_vec_514</th>\n      <th>roberta_vec_515</th>\n      <th>roberta_vec_516</th>\n      <th>roberta_vec_517</th>\n      <th>roberta_vec_518</th>\n      <th>roberta_vec_519</th>\n      <th>roberta_vec_520</th>\n      <th>roberta_vec_521</th>\n      <th>roberta_vec_522</th>\n      <th>roberta_vec_523</th>\n      <th>roberta_vec_524</th>\n      <th>roberta_vec_525</th>\n      <th>roberta_vec_526</th>\n      <th>roberta_vec_527</th>\n      <th>roberta_vec_528</th>\n      <th>roberta_vec_529</th>\n      <th>roberta_vec_530</th>\n      <th>roberta_vec_531</th>\n      <th>roberta_vec_532</th>\n      <th>roberta_vec_533</th>\n      <th>roberta_vec_534</th>\n      <th>roberta_vec_535</th>\n      <th>roberta_vec_536</th>\n      <th>roberta_vec_537</th>\n      <th>roberta_vec_538</th>\n      <th>roberta_vec_539</th>\n      <th>roberta_vec_540</th>\n      <th>roberta_vec_541</th>\n      <th>roberta_vec_542</th>\n      <th>roberta_vec_543</th>\n      <th>roberta_vec_544</th>\n      <th>roberta_vec_545</th>\n      <th>roberta_vec_546</th>\n      <th>roberta_vec_547</th>\n      <th>roberta_vec_548</th>\n      <th>roberta_vec_549</th>\n      <th>roberta_vec_550</th>\n      <th>roberta_vec_551</th>\n      <th>roberta_vec_552</th>\n      <th>roberta_vec_553</th>\n      <th>roberta_vec_554</th>\n      <th>roberta_vec_555</th>\n      <th>roberta_vec_556</th>\n      <th>roberta_vec_557</th>\n      <th>roberta_vec_558</th>\n      <th>roberta_vec_559</th>\n      <th>roberta_vec_560</th>\n      <th>roberta_vec_561</th>\n      <th>roberta_vec_562</th>\n      <th>roberta_vec_563</th>\n      <th>roberta_vec_564</th>\n      <th>roberta_vec_565</th>\n      <th>roberta_vec_566</th>\n      <th>roberta_vec_567</th>\n      <th>roberta_vec_568</th>\n      <th>roberta_vec_569</th>\n      <th>roberta_vec_570</th>\n      <th>roberta_vec_571</th>\n      <th>roberta_vec_572</th>\n      <th>roberta_vec_573</th>\n      <th>roberta_vec_574</th>\n      <th>roberta_vec_575</th>\n      <th>roberta_vec_576</th>\n      <th>roberta_vec_577</th>\n      <th>roberta_vec_578</th>\n      <th>roberta_vec_579</th>\n      <th>roberta_vec_580</th>\n      <th>roberta_vec_581</th>\n      <th>roberta_vec_582</th>\n      <th>roberta_vec_583</th>\n      <th>roberta_vec_584</th>\n      <th>roberta_vec_585</th>\n      <th>roberta_vec_586</th>\n      <th>roberta_vec_587</th>\n      <th>roberta_vec_588</th>\n      <th>roberta_vec_589</th>\n      <th>roberta_vec_590</th>\n      <th>roberta_vec_591</th>\n      <th>roberta_vec_592</th>\n      <th>roberta_vec_593</th>\n      <th>roberta_vec_594</th>\n      <th>roberta_vec_595</th>\n      <th>roberta_vec_596</th>\n      <th>roberta_vec_597</th>\n      <th>roberta_vec_598</th>\n      <th>roberta_vec_599</th>\n      <th>roberta_vec_600</th>\n      <th>roberta_vec_601</th>\n      <th>roberta_vec_602</th>\n      <th>roberta_vec_603</th>\n      <th>roberta_vec_604</th>\n      <th>roberta_vec_605</th>\n      <th>roberta_vec_606</th>\n      <th>roberta_vec_607</th>\n      <th>roberta_vec_608</th>\n      <th>roberta_vec_609</th>\n      <th>roberta_vec_610</th>\n      <th>roberta_vec_611</th>\n      <th>roberta_vec_612</th>\n      <th>roberta_vec_613</th>\n      <th>roberta_vec_614</th>\n      <th>roberta_vec_615</th>\n      <th>roberta_vec_616</th>\n      <th>roberta_vec_617</th>\n      <th>roberta_vec_618</th>\n      <th>roberta_vec_619</th>\n      <th>roberta_vec_620</th>\n      <th>roberta_vec_621</th>\n      <th>roberta_vec_622</th>\n      <th>roberta_vec_623</th>\n      <th>roberta_vec_624</th>\n      <th>roberta_vec_625</th>\n      <th>roberta_vec_626</th>\n      <th>roberta_vec_627</th>\n      <th>roberta_vec_628</th>\n      <th>roberta_vec_629</th>\n      <th>roberta_vec_630</th>\n      <th>roberta_vec_631</th>\n      <th>roberta_vec_632</th>\n      <th>roberta_vec_633</th>\n      <th>roberta_vec_634</th>\n      <th>roberta_vec_635</th>\n      <th>roberta_vec_636</th>\n      <th>roberta_vec_637</th>\n      <th>roberta_vec_638</th>\n      <th>roberta_vec_639</th>\n      <th>roberta_vec_640</th>\n      <th>roberta_vec_641</th>\n      <th>roberta_vec_642</th>\n      <th>roberta_vec_643</th>\n      <th>roberta_vec_644</th>\n      <th>roberta_vec_645</th>\n      <th>roberta_vec_646</th>\n      <th>roberta_vec_647</th>\n      <th>roberta_vec_648</th>\n      <th>roberta_vec_649</th>\n      <th>roberta_vec_650</th>\n      <th>roberta_vec_651</th>\n      <th>roberta_vec_652</th>\n      <th>roberta_vec_653</th>\n      <th>roberta_vec_654</th>\n      <th>roberta_vec_655</th>\n      <th>roberta_vec_656</th>\n      <th>roberta_vec_657</th>\n      <th>roberta_vec_658</th>\n      <th>roberta_vec_659</th>\n      <th>roberta_vec_660</th>\n      <th>roberta_vec_661</th>\n      <th>roberta_vec_662</th>\n      <th>roberta_vec_663</th>\n      <th>roberta_vec_664</th>\n      <th>roberta_vec_665</th>\n      <th>roberta_vec_666</th>\n      <th>roberta_vec_667</th>\n      <th>roberta_vec_668</th>\n      <th>roberta_vec_669</th>\n      <th>roberta_vec_670</th>\n      <th>roberta_vec_671</th>\n      <th>roberta_vec_672</th>\n      <th>roberta_vec_673</th>\n      <th>roberta_vec_674</th>\n      <th>roberta_vec_675</th>\n      <th>roberta_vec_676</th>\n      <th>roberta_vec_677</th>\n      <th>roberta_vec_678</th>\n      <th>roberta_vec_679</th>\n      <th>roberta_vec_680</th>\n      <th>roberta_vec_681</th>\n      <th>roberta_vec_682</th>\n      <th>roberta_vec_683</th>\n      <th>roberta_vec_684</th>\n      <th>roberta_vec_685</th>\n      <th>roberta_vec_686</th>\n      <th>roberta_vec_687</th>\n      <th>roberta_vec_688</th>\n      <th>roberta_vec_689</th>\n      <th>roberta_vec_690</th>\n      <th>roberta_vec_691</th>\n      <th>roberta_vec_692</th>\n      <th>roberta_vec_693</th>\n      <th>roberta_vec_694</th>\n      <th>roberta_vec_695</th>\n      <th>roberta_vec_696</th>\n      <th>roberta_vec_697</th>\n      <th>roberta_vec_698</th>\n      <th>roberta_vec_699</th>\n      <th>roberta_vec_700</th>\n      <th>roberta_vec_701</th>\n      <th>roberta_vec_702</th>\n      <th>roberta_vec_703</th>\n      <th>roberta_vec_704</th>\n      <th>roberta_vec_705</th>\n      <th>roberta_vec_706</th>\n      <th>roberta_vec_707</th>\n      <th>roberta_vec_708</th>\n      <th>roberta_vec_709</th>\n      <th>roberta_vec_710</th>\n      <th>roberta_vec_711</th>\n      <th>roberta_vec_712</th>\n      <th>roberta_vec_713</th>\n      <th>roberta_vec_714</th>\n      <th>roberta_vec_715</th>\n      <th>roberta_vec_716</th>\n      <th>roberta_vec_717</th>\n      <th>roberta_vec_718</th>\n      <th>roberta_vec_719</th>\n      <th>roberta_vec_720</th>\n      <th>roberta_vec_721</th>\n      <th>roberta_vec_722</th>\n      <th>roberta_vec_723</th>\n      <th>roberta_vec_724</th>\n      <th>roberta_vec_725</th>\n      <th>roberta_vec_726</th>\n      <th>roberta_vec_727</th>\n      <th>roberta_vec_728</th>\n      <th>roberta_vec_729</th>\n      <th>roberta_vec_730</th>\n      <th>roberta_vec_731</th>\n      <th>roberta_vec_732</th>\n      <th>roberta_vec_733</th>\n      <th>roberta_vec_734</th>\n      <th>roberta_vec_735</th>\n      <th>roberta_vec_736</th>\n      <th>roberta_vec_737</th>\n      <th>roberta_vec_738</th>\n      <th>roberta_vec_739</th>\n      <th>roberta_vec_740</th>\n      <th>roberta_vec_741</th>\n      <th>roberta_vec_742</th>\n      <th>roberta_vec_743</th>\n      <th>roberta_vec_744</th>\n      <th>roberta_vec_745</th>\n      <th>roberta_vec_746</th>\n      <th>roberta_vec_747</th>\n      <th>roberta_vec_748</th>\n      <th>roberta_vec_749</th>\n      <th>roberta_vec_750</th>\n      <th>roberta_vec_751</th>\n      <th>roberta_vec_752</th>\n      <th>roberta_vec_753</th>\n      <th>roberta_vec_754</th>\n      <th>roberta_vec_755</th>\n      <th>roberta_vec_756</th>\n      <th>roberta_vec_757</th>\n      <th>roberta_vec_758</th>\n      <th>roberta_vec_759</th>\n      <th>roberta_vec_760</th>\n      <th>roberta_vec_761</th>\n      <th>roberta_vec_762</th>\n      <th>roberta_vec_763</th>\n      <th>roberta_vec_764</th>\n      <th>roberta_vec_765</th>\n      <th>roberta_vec_766</th>\n      <th>roberta_vec_767</th>\n      <th>fold_no</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1403.7138</td>\n      <td>Aigen Li</td>\n      <td>Qi Li, S.L. Liang, Aigen Li (University of Mis...</td>\n      <td>Spectropolarimetric Constraints on the Nature ...</td>\n      <td>5 pages, 2 figures; accepted for publication i...</td>\n      <td>None</td>\n      <td>10.1093/mnrasl/slu021</td>\n      <td>None</td>\n      <td>astro-ph.GA</td>\n      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n      <td>While it is well recognized that interstella...</td>\n      <td>[{'version': 'v1', 'created': 'Thu, 27 Mar 201...</td>\n      <td>[[Li, Qi, , University of Missouri], [Liang, S...</td>\n      <td>2.197266</td>\n      <td>2.079442</td>\n      <td>10.1093</td>\n      <td>Oxford University Press</td>\n      <td>5.832031</td>\n      <td>1091568.0</td>\n      <td>2015-06-19</td>\n      <td>2014-03-27 17:25:40+00:00</td>\n      <td>2014-03-27 17:25:40+00:00</td>\n      <td>2015</td>\n      <td>2014</td>\n      <td>2014</td>\n      <td>6</td>\n      <td>3</td>\n      <td>3</td>\n      <td>201506</td>\n      <td>201403</td>\n      <td>201403</td>\n      <td>19</td>\n      <td>27</td>\n      <td>27</td>\n      <td>1.434672e+09</td>\n      <td>1.395941e+09</td>\n      <td>1.395941e+09</td>\n      <td>38730860</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>16605</td>\n      <td>16156</td>\n      <td>16156</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>Li</td>\n      <td>3</td>\n      <td>2.197266</td>\n      <td>astro-ph</td>\n      <td>astro-ph</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>astro</td>\n      <td>astro-ph</td>\n      <td>astro-ph.ga</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3033</td>\n      <td>53</td>\n      <td>60233</td>\n      <td>344</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>92</td>\n      <td>88</td>\n      <td>1965</td>\n      <td>1.975586</td>\n      <td>6310</td>\n      <td>12464.0</td>\n      <td>0.0</td>\n      <td>8.492188</td>\n      <td>1.946289</td>\n      <td>1.362305</td>\n      <td>0.0</td>\n      <td>0.693359</td>\n      <td>2.890625</td>\n      <td>2.072266</td>\n      <td>29728</td>\n      <td>61618.074219</td>\n      <td>0.0</td>\n      <td>8.078125</td>\n      <td>2.080078</td>\n      <td>1.298828</td>\n      <td>0.0</td>\n      <td>1.098633</td>\n      <td>3.044922</td>\n      <td>2.072266</td>\n      <td>29728</td>\n      <td>61618.074219</td>\n      <td>0.0</td>\n      <td>8.078125</td>\n      <td>2.080078</td>\n      <td>1.298828</td>\n      <td>0.0</td>\n      <td>1.098633</td>\n      <td>3.044922</td>\n      <td>2.28125</td>\n      <td>43</td>\n      <td>98.0625</td>\n      <td>0.0</td>\n      <td>4.09375</td>\n      <td>2.302734</td>\n      <td>0.999023</td>\n      <td>0.774414</td>\n      <td>1.946289</td>\n      <td>2.917969</td>\n      <td>2.416016</td>\n      <td>75054</td>\n      <td>181269.65625</td>\n      <td>0.0</td>\n      <td>9.53125</td>\n      <td>2.484375</td>\n      <td>1.274414</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.294922</td>\n      <td>2.304688</td>\n      <td>4588</td>\n      <td>10568.0</td>\n      <td>0.0</td>\n      <td>6.882812</td>\n      <td>2.302734</td>\n      <td>1.259766</td>\n      <td>0.693359</td>\n      <td>1.386719</td>\n      <td>3.177734</td>\n      <td>1.94043</td>\n      <td>599833</td>\n      <td>1163680.25</td>\n      <td>0.0</td>\n      <td>9.101562</td>\n      <td>1.946289</td>\n      <td>1.349609</td>\n      <td>0.0</td>\n      <td>0.693359</td>\n      <td>2.890625</td>\n      <td>2.640625</td>\n      <td>195890</td>\n      <td>517164.90625</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.773438</td>\n      <td>1.436523</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.664062</td>\n      <td>2.640625</td>\n      <td>195890</td>\n      <td>517164.90625</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.773438</td>\n      <td>1.436523</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.664062</td>\n      <td>2.714844</td>\n      <td>145677</td>\n      <td>395449.46875</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.833984</td>\n      <td>1.429688</td>\n      <td>0.693359</td>\n      <td>1.791992</td>\n      <td>3.712891</td>\n      <td>2.714844</td>\n      <td>145677</td>\n      <td>395449.46875</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.833984</td>\n      <td>1.429688</td>\n      <td>0.693359</td>\n      <td>1.791992</td>\n      <td>3.712891</td>\n      <td>2.189453</td>\n      <td>18375</td>\n      <td>40248.957031</td>\n      <td>0.0</td>\n      <td>7.996094</td>\n      <td>2.302734</td>\n      <td>1.303711</td>\n      <td>0.0</td>\n      <td>1.386719</td>\n      <td>3.134766</td>\n      <td>1.975586</td>\n      <td>6310</td>\n      <td>12472.0</td>\n      <td>0.0</td>\n      <td>8.492188</td>\n      <td>1.946289</td>\n      <td>1.362305</td>\n      <td>0.0</td>\n      <td>0.693359</td>\n      <td>2.890625</td>\n      <td>2.072266</td>\n      <td>29728</td>\n      <td>61621.222656</td>\n      <td>0.0</td>\n      <td>8.078125</td>\n      <td>2.080078</td>\n      <td>1.298828</td>\n      <td>0.0</td>\n      <td>1.098633</td>\n      <td>3.044922</td>\n      <td>2.072266</td>\n      <td>29728</td>\n      <td>61621.222656</td>\n      <td>0.0</td>\n      <td>8.078125</td>\n      <td>2.080078</td>\n      <td>1.298828</td>\n      <td>0.0</td>\n      <td>1.098633</td>\n      <td>3.044922</td>\n      <td>2.28125</td>\n      <td>43</td>\n      <td>98.0625</td>\n      <td>0.0</td>\n      <td>4.09375</td>\n      <td>2.302734</td>\n      <td>0.999023</td>\n      <td>0.774414</td>\n      <td>1.946289</td>\n      <td>2.917969</td>\n      <td>2.416016</td>\n      <td>75054</td>\n      <td>181276.96875</td>\n      <td>0.0</td>\n      <td>9.53125</td>\n      <td>2.484375</td>\n      <td>1.274414</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.294922</td>\n      <td>2.304688</td>\n      <td>4588</td>\n      <td>10568.0</td>\n      <td>0.0</td>\n      <td>6.882812</td>\n      <td>2.302734</td>\n      <td>1.259766</td>\n      <td>0.693359</td>\n      <td>1.386719</td>\n      <td>3.177734</td>\n      <td>1.94043</td>\n      <td>599833</td>\n      <td>1163743.0</td>\n      <td>0.0</td>\n      <td>9.101562</td>\n      <td>1.946289</td>\n      <td>1.349609</td>\n      <td>0.0</td>\n      <td>0.693359</td>\n      <td>2.890625</td>\n      <td>2.640625</td>\n      <td>195890</td>\n      <td>517176.90625</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.773438</td>\n      <td>1.436523</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.664062</td>\n      <td>2.640625</td>\n      <td>195890</td>\n      <td>517176.90625</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.773438</td>\n      <td>1.436523</td>\n      <td>0.693359</td>\n      <td>1.609375</td>\n      <td>3.664062</td>\n      <td>2.714844</td>\n      <td>145677</td>\n      <td>395457.6875</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.833984</td>\n      <td>1.429688</td>\n      <td>0.693359</td>\n      <td>1.791992</td>\n      <td>3.712891</td>\n      <td>2.714844</td>\n      <td>145677</td>\n      <td>395457.6875</td>\n      <td>0.0</td>\n      <td>9.242188</td>\n      <td>2.833984</td>\n      <td>1.429688</td>\n      <td>0.693359</td>\n      <td>1.791992</td>\n      <td>3.712891</td>\n      <td>2.191406</td>\n      <td>18375</td>\n      <td>40250.773438</td>\n      <td>0.0</td>\n      <td>7.996094</td>\n      <td>2.302734</td>\n      <td>1.303711</td>\n      <td>0.0</td>\n      <td>1.386719</td>\n      <td>3.134766</td>\n      <td>-0.083679</td>\n      <td>0.974609</td>\n      <td>0.124512</td>\n      <td>1.040039</td>\n      <td>0.221313</td>\n      <td>1.074219</td>\n      <td>0.124512</td>\n      <td>1.040039</td>\n      <td>-0.218018</td>\n      <td>0.936035</td>\n      <td>-0.106506</td>\n      <td>0.967773</td>\n      <td>0.257324</td>\n      <td>1.087891</td>\n      <td>-0.442871</td>\n      <td>0.878418</td>\n      <td>-0.442871</td>\n      <td>0.878418</td>\n      <td>-0.517578</td>\n      <td>0.86084</td>\n      <td>-0.517578</td>\n      <td>0.86084</td>\n      <td>0.006805</td>\n      <td>1.001953</td>\n      <td>-0.000041</td>\n      <td>1.0</td>\n      <td>-0.083923</td>\n      <td>0.974609</td>\n      <td>0.12439</td>\n      <td>1.040039</td>\n      <td>0.221313</td>\n      <td>1.074219</td>\n      <td>0.12439</td>\n      <td>1.040039</td>\n      <td>-0.218018</td>\n      <td>0.936035</td>\n      <td>-0.106628</td>\n      <td>0.967773</td>\n      <td>0.25708</td>\n      <td>1.087891</td>\n      <td>-0.442871</td>\n      <td>0.878418</td>\n      <td>-0.442871</td>\n      <td>0.878418</td>\n      <td>-0.517578</td>\n      <td>0.86084</td>\n      <td>-0.517578</td>\n      <td>0.86084</td>\n      <td>0.006706</td>\n      <td>1.001953</td>\n      <td>0.20813</td>\n      <td>1.067383</td>\n      <td>0.304932</td>\n      <td>1.102539</td>\n      <td>0.20813</td>\n      <td>1.067383</td>\n      <td>-0.134277</td>\n      <td>0.960449</td>\n      <td>-0.022827</td>\n      <td>0.993164</td>\n      <td>0.34082</td>\n      <td>1.116211</td>\n      <td>-0.359131</td>\n      <td>0.901367</td>\n      <td>-0.359131</td>\n      <td>0.901367</td>\n      <td>-0.433594</td>\n      <td>0.883301</td>\n      <td>-0.433594</td>\n      <td>0.883301</td>\n      <td>0.090515</td>\n      <td>1.02832</td>\n      <td>0.083618</td>\n      <td>1.026367</td>\n      <td>-0.000227</td>\n      <td>1.0</td>\n      <td>0.20813</td>\n      <td>1.067383</td>\n      <td>0.304932</td>\n      <td>1.102539</td>\n      <td>0.20813</td>\n      <td>1.067383</td>\n      <td>-0.134399</td>\n      <td>0.960449</td>\n      <td>-0.022934</td>\n      <td>0.993164</td>\n      <td>0.34082</td>\n      <td>1.116211</td>\n      <td>-0.359131</td>\n      <td>0.901367</td>\n      <td>-0.359131</td>\n      <td>0.901367</td>\n      <td>-0.433594</td>\n      <td>0.883301</td>\n      <td>-0.433594</td>\n      <td>0.883301</td>\n      <td>0.090393</td>\n      <td>1.02832</td>\n      <td>0.096863</td>\n      <td>1.032227</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.342529</td>\n      <td>0.899902</td>\n      <td>-0.230957</td>\n      <td>0.930176</td>\n      <td>0.13269</td>\n      <td>1.044922</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.641602</td>\n      <td>0.827148</td>\n      <td>-0.641602</td>\n      <td>0.827148</td>\n      <td>-0.117676</td>\n      <td>0.962891</td>\n      <td>-0.124512</td>\n      <td>0.960938</td>\n      <td>-0.208374</td>\n      <td>0.936523</td>\n      <td>-0.000106</td>\n      <td>1.0</td>\n      <td>0.096802</td>\n      <td>1.032227</td>\n      <td>-0.000106</td>\n      <td>1.0</td>\n      <td>-0.342529</td>\n      <td>0.899902</td>\n      <td>-0.231079</td>\n      <td>0.930176</td>\n      <td>0.132568</td>\n      <td>1.044922</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.64209</td>\n      <td>0.827148</td>\n      <td>-0.64209</td>\n      <td>0.827148</td>\n      <td>-0.117798</td>\n      <td>0.962891</td>\n      <td>-0.096863</td>\n      <td>0.968262</td>\n      <td>-0.439453</td>\n      <td>0.871582</td>\n      <td>-0.327881</td>\n      <td>0.900879</td>\n      <td>0.035858</td>\n      <td>1.011719</td>\n      <td>-0.664062</td>\n      <td>0.817383</td>\n      <td>-0.664062</td>\n      <td>0.817383</td>\n      <td>-0.73877</td>\n      <td>0.80127</td>\n      <td>-0.73877</td>\n      <td>0.80127</td>\n      <td>-0.2146</td>\n      <td>0.932617</td>\n      <td>-0.221436</td>\n      <td>0.930664</td>\n      <td>-0.305176</td>\n      <td>0.906738</td>\n      <td>-0.096985</td>\n      <td>0.968262</td>\n      <td>-0.000098</td>\n      <td>1.0</td>\n      <td>-0.096985</td>\n      <td>0.968262</td>\n      <td>-0.439453</td>\n      <td>0.871094</td>\n      <td>-0.327881</td>\n      <td>0.900879</td>\n      <td>0.035736</td>\n      <td>1.011719</td>\n      <td>-0.664062</td>\n      <td>0.817383</td>\n      <td>-0.664062</td>\n      <td>0.817383</td>\n      <td>-0.73877</td>\n      <td>0.80127</td>\n      <td>-0.73877</td>\n      <td>0.80127</td>\n      <td>-0.214722</td>\n      <td>0.932617</td>\n      <td>-0.342529</td>\n      <td>0.899902</td>\n      <td>-0.230957</td>\n      <td>0.930176</td>\n      <td>0.13269</td>\n      <td>1.044922</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.641602</td>\n      <td>0.827148</td>\n      <td>-0.641602</td>\n      <td>0.827148</td>\n      <td>-0.117676</td>\n      <td>0.962891</td>\n      <td>-0.124512</td>\n      <td>0.960938</td>\n      <td>-0.208374</td>\n      <td>0.936523</td>\n      <td>-0.000106</td>\n      <td>1.0</td>\n      <td>0.096802</td>\n      <td>1.032227</td>\n      <td>-0.000106</td>\n      <td>1.0</td>\n      <td>-0.342529</td>\n      <td>0.899902</td>\n      <td>-0.231079</td>\n      <td>0.930176</td>\n      <td>0.132568</td>\n      <td>1.044922</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.64209</td>\n      <td>0.827148</td>\n      <td>-0.64209</td>\n      <td>0.827148</td>\n      <td>-0.117798</td>\n      <td>0.962891</td>\n      <td>0.11145</td>\n      <td>1.03418</td>\n      <td>0.475098</td>\n      <td>1.162109</td>\n      <td>-0.224854</td>\n      <td>0.937988</td>\n      <td>-0.224854</td>\n      <td>0.937988</td>\n      <td>-0.299316</td>\n      <td>0.919434</td>\n      <td>-0.299316</td>\n      <td>0.919434</td>\n      <td>0.224731</td>\n      <td>1.070312</td>\n      <td>0.217896</td>\n      <td>1.068359</td>\n      <td>0.134033</td>\n      <td>1.041016</td>\n      <td>0.342285</td>\n      <td>1.111328</td>\n      <td>0.439209</td>\n      <td>1.147461</td>\n      <td>0.342285</td>\n      <td>1.111328</td>\n      <td>-0.000097</td>\n      <td>1.0</td>\n      <td>0.111328</td>\n      <td>1.03418</td>\n      <td>0.475098</td>\n      <td>1.161133</td>\n      <td>-0.224976</td>\n      <td>0.937988</td>\n      <td>-0.224976</td>\n      <td>0.937988</td>\n      <td>-0.299316</td>\n      <td>0.919434</td>\n      <td>-0.299316</td>\n      <td>0.919434</td>\n      <td>0.224731</td>\n      <td>1.070312</td>\n      <td>0.36377</td>\n      <td>1.124023</td>\n      <td>-0.336426</td>\n      <td>0.907715</td>\n      <td>-0.336426</td>\n      <td>0.907715</td>\n      <td>-0.410889</td>\n      <td>0.88916</td>\n      <td>-0.410889</td>\n      <td>0.88916</td>\n      <td>0.113342</td>\n      <td>1.035156</td>\n      <td>0.106445</td>\n      <td>1.033203</td>\n      <td>0.022598</td>\n      <td>1.006836</td>\n      <td>0.230957</td>\n      <td>1.075195</td>\n      <td>0.327881</td>\n      <td>1.110352</td>\n      <td>0.230957</td>\n      <td>1.075195</td>\n      <td>-0.111572</td>\n      <td>0.967285</td>\n      <td>-0.000104</td>\n      <td>1.0</td>\n      <td>0.363525</td>\n      <td>1.124023</td>\n      <td>-0.336426</td>\n      <td>0.907715</td>\n      <td>-0.336426</td>\n      <td>0.907715</td>\n      <td>-0.410889</td>\n      <td>0.88916</td>\n      <td>-0.410889</td>\n      <td>0.88916</td>\n      <td>0.11322</td>\n      <td>1.035156</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.774414</td>\n      <td>0.791504</td>\n      <td>-0.774414</td>\n      <td>0.791504</td>\n      <td>-0.250488</td>\n      <td>0.921387</td>\n      <td>-0.257324</td>\n      <td>0.919434</td>\n      <td>-0.341064</td>\n      <td>0.895996</td>\n      <td>-0.132812</td>\n      <td>0.956543</td>\n      <td>-0.03595</td>\n      <td>0.987793</td>\n      <td>-0.132812</td>\n      <td>0.956543</td>\n      <td>-0.475342</td>\n      <td>0.86084</td>\n      <td>-0.36377</td>\n      <td>0.889648</td>\n      <td>-0.000105</td>\n      <td>1.0</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.774414</td>\n      <td>0.791504</td>\n      <td>-0.774414</td>\n      <td>0.791504</td>\n      <td>-0.250488</td>\n      <td>0.921387</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.074463</td>\n      <td>0.97998</td>\n      <td>-0.074463</td>\n      <td>0.97998</td>\n      <td>0.449707</td>\n      <td>1.140625</td>\n      <td>0.442871</td>\n      <td>1.138672</td>\n      <td>0.358887</td>\n      <td>1.109375</td>\n      <td>0.567383</td>\n      <td>1.18457</td>\n      <td>0.664062</td>\n      <td>1.223633</td>\n      <td>0.567383</td>\n      <td>1.18457</td>\n      <td>0.224731</td>\n      <td>1.06543</td>\n      <td>0.336182</td>\n      <td>1.101562</td>\n      <td>0.700195</td>\n      <td>1.238281</td>\n      <td>-0.000061</td>\n      <td>1.0</td>\n      <td>-0.000061</td>\n      <td>1.0</td>\n      <td>-0.074524</td>\n      <td>0.97998</td>\n      <td>-0.074524</td>\n      <td>0.97998</td>\n      <td>0.449463</td>\n      <td>1.140625</td>\n      <td>-0.074463</td>\n      <td>0.97998</td>\n      <td>-0.074463</td>\n      <td>0.97998</td>\n      <td>0.449707</td>\n      <td>1.140625</td>\n      <td>0.442871</td>\n      <td>1.138672</td>\n      <td>0.358887</td>\n      <td>1.109375</td>\n      <td>0.567383</td>\n      <td>1.18457</td>\n      <td>0.664062</td>\n      <td>1.223633</td>\n      <td>0.567383</td>\n      <td>1.18457</td>\n      <td>0.224731</td>\n      <td>1.06543</td>\n      <td>0.336182</td>\n      <td>1.101562</td>\n      <td>0.700195</td>\n      <td>1.238281</td>\n      <td>-0.000061</td>\n      <td>1.0</td>\n      <td>-0.000061</td>\n      <td>1.0</td>\n      <td>-0.074524</td>\n      <td>0.97998</td>\n      <td>-0.074524</td>\n      <td>0.97998</td>\n      <td>0.449463</td>\n      <td>1.140625</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.523926</td>\n      <td>1.164062</td>\n      <td>0.51709</td>\n      <td>1.162109</td>\n      <td>0.43335</td>\n      <td>1.131836</td>\n      <td>0.641602</td>\n      <td>1.208984</td>\n      <td>0.73877</td>\n      <td>1.248047</td>\n      <td>0.641602</td>\n      <td>1.208984</td>\n      <td>0.299316</td>\n      <td>1.087891</td>\n      <td>0.410645</td>\n      <td>1.124023</td>\n      <td>0.774414</td>\n      <td>1.263672</td>\n      <td>0.074402</td>\n      <td>1.020508</td>\n      <td>0.074402</td>\n      <td>1.020508</td>\n      <td>-0.000057</td>\n      <td>1.0</td>\n      <td>-0.000057</td>\n      <td>1.0</td>\n      <td>0.523926</td>\n      <td>1.164062</td>\n      <td>0.523926</td>\n      <td>1.164062</td>\n      <td>0.51709</td>\n      <td>1.162109</td>\n      <td>0.43335</td>\n      <td>1.131836</td>\n      <td>0.641602</td>\n      <td>1.208984</td>\n      <td>0.73877</td>\n      <td>1.248047</td>\n      <td>0.641602</td>\n      <td>1.208984</td>\n      <td>0.299316</td>\n      <td>1.087891</td>\n      <td>0.410645</td>\n      <td>1.124023</td>\n      <td>0.774414</td>\n      <td>1.263672</td>\n      <td>0.074402</td>\n      <td>1.020508</td>\n      <td>0.074402</td>\n      <td>1.020508</td>\n      <td>-0.000057</td>\n      <td>1.0</td>\n      <td>-0.000057</td>\n      <td>1.0</td>\n      <td>0.523926</td>\n      <td>1.164062</td>\n      <td>-0.006847</td>\n      <td>0.998047</td>\n      <td>-0.090698</td>\n      <td>0.972168</td>\n      <td>0.117615</td>\n      <td>1.038086</td>\n      <td>0.214478</td>\n      <td>1.072266</td>\n      <td>0.117615</td>\n      <td>1.038086</td>\n      <td>-0.224854</td>\n      <td>0.934082</td>\n      <td>-0.113403</td>\n      <td>0.96582</td>\n      <td>0.250244</td>\n      <td>1.084961</td>\n      <td>-0.449707</td>\n      <td>0.876465</td>\n      <td>-0.449707</td>\n      <td>0.876465</td>\n      <td>-0.524414</td>\n      <td>0.858887</td>\n      <td>-0.524414</td>\n      <td>0.858887</td>\n      <td>-0.000099</td>\n      <td>1.0</td>\n      <td>-0.083862</td>\n      <td>0.974609</td>\n      <td>0.124451</td>\n      <td>1.040039</td>\n      <td>0.221313</td>\n      <td>1.074219</td>\n      <td>0.124451</td>\n      <td>1.040039</td>\n      <td>-0.218018</td>\n      <td>0.936035</td>\n      <td>-0.106567</td>\n      <td>0.967773</td>\n      <td>0.25708</td>\n      <td>1.087891</td>\n      <td>-0.442871</td>\n      <td>0.878418</td>\n      <td>-0.442871</td>\n      <td>0.878418</td>\n      <td>-0.517578</td>\n      <td>0.86084</td>\n      <td>-0.517578</td>\n      <td>0.86084</td>\n      <td>0.006748</td>\n      <td>1.001953</td>\n      <td>0.208252</td>\n      <td>1.067383</td>\n      <td>0.305176</td>\n      <td>1.102539</td>\n      <td>0.208252</td>\n      <td>1.067383</td>\n      <td>-0.134155</td>\n      <td>0.960938</td>\n      <td>-0.022705</td>\n      <td>0.993164</td>\n      <td>0.341064</td>\n      <td>1.116211</td>\n      <td>-0.358887</td>\n      <td>0.901367</td>\n      <td>-0.358887</td>\n      <td>0.901367</td>\n      <td>-0.433594</td>\n      <td>0.883301</td>\n      <td>-0.433594</td>\n      <td>0.883301</td>\n      <td>0.090637</td>\n      <td>1.02832</td>\n      <td>0.096863</td>\n      <td>1.032227</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.342529</td>\n      <td>0.899902</td>\n      <td>-0.230957</td>\n      <td>0.930176</td>\n      <td>0.13269</td>\n      <td>1.044922</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.641602</td>\n      <td>0.827148</td>\n      <td>-0.641602</td>\n      <td>0.827148</td>\n      <td>-0.117676</td>\n      <td>0.962891</td>\n      <td>-0.096863</td>\n      <td>0.968262</td>\n      <td>-0.439453</td>\n      <td>0.871582</td>\n      <td>-0.327881</td>\n      <td>0.900879</td>\n      <td>0.035858</td>\n      <td>1.011719</td>\n      <td>-0.664062</td>\n      <td>0.817383</td>\n      <td>-0.664062</td>\n      <td>0.817383</td>\n      <td>-0.73877</td>\n      <td>0.80127</td>\n      <td>-0.73877</td>\n      <td>0.80127</td>\n      <td>-0.2146</td>\n      <td>0.932617</td>\n      <td>-0.342529</td>\n      <td>0.899902</td>\n      <td>-0.230957</td>\n      <td>0.930176</td>\n      <td>0.13269</td>\n      <td>1.044922</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.567383</td>\n      <td>0.844238</td>\n      <td>-0.641602</td>\n      <td>0.827148</td>\n      <td>-0.641602</td>\n      <td>0.827148</td>\n      <td>-0.117676</td>\n      <td>0.962891</td>\n      <td>0.11145</td>\n      <td>1.03418</td>\n      <td>0.475098</td>\n      <td>1.161133</td>\n      <td>-0.224854</td>\n      <td>0.937988</td>\n      <td>-0.224854</td>\n      <td>0.937988</td>\n      <td>-0.299316</td>\n      <td>0.919434</td>\n      <td>-0.299316</td>\n      <td>0.919434</td>\n      <td>0.224731</td>\n      <td>1.070312</td>\n      <td>0.36377</td>\n      <td>1.124023</td>\n      <td>-0.336182</td>\n      <td>0.907715</td>\n      <td>-0.336182</td>\n      <td>0.907715</td>\n      <td>-0.410889</td>\n      <td>0.889648</td>\n      <td>-0.410889</td>\n      <td>0.889648</td>\n      <td>0.113342</td>\n      <td>1.035156</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.700195</td>\n      <td>0.807617</td>\n      <td>-0.774414</td>\n      <td>0.791504</td>\n      <td>-0.774414</td>\n      <td>0.791504</td>\n      <td>-0.250488</td>\n      <td>0.921387</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.074463</td>\n      <td>0.97998</td>\n      <td>-0.074463</td>\n      <td>0.97998</td>\n      <td>0.449707</td>\n      <td>1.140625</td>\n      <td>-0.074463</td>\n      <td>0.97998</td>\n      <td>-0.074463</td>\n      <td>0.97998</td>\n      <td>0.449707</td>\n      <td>1.140625</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.523926</td>\n      <td>1.164062</td>\n      <td>0.523926</td>\n      <td>1.164062</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.029891</td>\n      <td>0.073973</td>\n      <td>-0.094534</td>\n      <td>0.126522</td>\n      <td>0.181991</td>\n      <td>0.180853</td>\n      <td>0.266522</td>\n      <td>0.097094</td>\n      <td>-0.123095</td>\n      <td>-0.09531</td>\n      <td>-0.179767</td>\n      <td>-0.457392</td>\n      <td>-0.08062</td>\n      <td>0.089512</td>\n      <td>0.082543</td>\n      <td>0.058552</td>\n      <td>0.027953</td>\n      <td>0.203759</td>\n      <td>0.072223</td>\n      <td>0.050499</td>\n      <td>-0.263804</td>\n      <td>0.327592</td>\n      <td>-0.1034</td>\n      <td>-0.121484</td>\n      <td>0.250285</td>\n      <td>0.074155</td>\n      <td>0.14488</td>\n      <td>0.259375</td>\n      <td>-0.288159</td>\n      <td>0.185025</td>\n      <td>-0.030508</td>\n      <td>0.070924</td>\n      <td>0.074667</td>\n      <td>0.03675</td>\n      <td>-0.061805</td>\n      <td>0.060579</td>\n      <td>-0.036916</td>\n      <td>0.039057</td>\n      <td>-0.296855</td>\n      <td>0.009289</td>\n      <td>-0.116685</td>\n      <td>0.71559</td>\n      <td>0.101258</td>\n      <td>-0.220261</td>\n      <td>-0.076486</td>\n      <td>-0.188758</td>\n      <td>-0.01706</td>\n      <td>0.037125</td>\n      <td>0.057709</td>\n      <td>0.166822</td>\n      <td>-0.043452</td>\n      <td>-0.019516</td>\n      <td>-0.173006</td>\n      <td>0.089757</td>\n      <td>-0.190869</td>\n      <td>0.095406</td>\n      <td>0.060052</td>\n      <td>-0.132407</td>\n      <td>0.005342</td>\n      <td>0.03332</td>\n      <td>0.024427</td>\n      <td>-0.118014</td>\n      <td>0.251357</td>\n      <td>0.040674</td>\n      <td>-0.027957</td>\n      <td>-0.026394</td>\n      <td>0.024263</td>\n      <td>0.372732</td>\n      <td>-0.025801</td>\n      <td>-0.246002</td>\n      <td>0.109832</td>\n      <td>-0.0175</td>\n      <td>-0.001231</td>\n      <td>0.143727</td>\n      <td>0.264198</td>\n      <td>0.107633</td>\n      <td>0.06034</td>\n      <td>-0.318682</td>\n      <td>0.006398</td>\n      <td>0.09976</td>\n      <td>-0.036413</td>\n      <td>0.055242</td>\n      <td>1.750735</td>\n      <td>0.118613</td>\n      <td>0.177189</td>\n      <td>-0.080642</td>\n      <td>0.044355</td>\n      <td>0.100218</td>\n      <td>-0.051002</td>\n      <td>-0.008707</td>\n      <td>-0.134474</td>\n      <td>-0.004757</td>\n      <td>-0.011541</td>\n      <td>-0.217713</td>\n      <td>-0.042322</td>\n      <td>0.052909</td>\n      <td>0.071231</td>\n      <td>-1.152318</td>\n      <td>-0.056206</td>\n      <td>-0.060179</td>\n      <td>0.18923</td>\n      <td>-0.190384</td>\n      <td>-0.025456</td>\n      <td>-0.110303</td>\n      <td>-0.197674</td>\n      <td>0.367068</td>\n      <td>0.155162</td>\n      <td>0.074811</td>\n      <td>0.09964</td>\n      <td>-0.164789</td>\n      <td>-0.005851</td>\n      <td>0.0465</td>\n      <td>-0.082355</td>\n      <td>0.04549</td>\n      <td>0.089042</td>\n      <td>-0.19725</td>\n      <td>0.304634</td>\n      <td>0.225055</td>\n      <td>-0.028008</td>\n      <td>0.400766</td>\n      <td>0.061376</td>\n      <td>0.030082</td>\n      <td>0.118571</td>\n      <td>-0.081431</td>\n      <td>0.120853</td>\n      <td>0.292195</td>\n      <td>-0.141171</td>\n      <td>0.152237</td>\n      <td>-0.059104</td>\n      <td>0.141703</td>\n      <td>0.194321</td>\n      <td>-0.700524</td>\n      <td>-0.223517</td>\n      <td>-0.19895</td>\n      <td>-0.048475</td>\n      <td>0.04559</td>\n      <td>0.032415</td>\n      <td>0.00568</td>\n      <td>-0.050341</td>\n      <td>0.018153</td>\n      <td>0.211495</td>\n      <td>0.210816</td>\n      <td>-0.227017</td>\n      <td>-0.245958</td>\n      <td>0.129851</td>\n      <td>0.194608</td>\n      <td>-0.074843</td>\n      <td>-0.347806</td>\n      <td>0.008898</td>\n      <td>0.330879</td>\n      <td>0.291852</td>\n      <td>0.063475</td>\n      <td>-0.278245</td>\n      <td>-0.31251</td>\n      <td>-0.021102</td>\n      <td>0.703451</td>\n      <td>0.140124</td>\n      <td>-0.089052</td>\n      <td>-0.132394</td>\n      <td>0.020074</td>\n      <td>-0.022465</td>\n      <td>-0.017278</td>\n      <td>0.210464</td>\n      <td>-0.011987</td>\n      <td>0.079176</td>\n      <td>0.043043</td>\n      <td>-0.10348</td>\n      <td>0.110626</td>\n      <td>-0.036202</td>\n      <td>0.118242</td>\n      <td>-0.063492</td>\n      <td>-0.068646</td>\n      <td>0.113279</td>\n      <td>0.071068</td>\n      <td>-0.098169</td>\n      <td>-0.019378</td>\n      <td>-0.040457</td>\n      <td>-0.086607</td>\n      <td>0.030731</td>\n      <td>0.072026</td>\n      <td>-0.039192</td>\n      <td>0.114556</td>\n      <td>-0.050776</td>\n      <td>-0.167901</td>\n      <td>0.117924</td>\n      <td>0.215771</td>\n      <td>-0.063886</td>\n      <td>0.23364</td>\n      <td>0.017298</td>\n      <td>-0.082844</td>\n      <td>-0.067025</td>\n      <td>-0.149464</td>\n      <td>-0.188189</td>\n      <td>-0.23059</td>\n      <td>-0.033488</td>\n      <td>-0.188438</td>\n      <td>0.027001</td>\n      <td>0.040944</td>\n      <td>0.488956</td>\n      <td>-0.082526</td>\n      <td>0.17299</td>\n      <td>-0.29941</td>\n      <td>0.126077</td>\n      <td>-0.0539</td>\n      <td>0.195646</td>\n      <td>0.008763</td>\n      <td>-0.449042</td>\n      <td>0.146812</td>\n      <td>0.036107</td>\n      <td>-0.134739</td>\n      <td>0.037157</td>\n      <td>-0.022641</td>\n      <td>-0.343432</td>\n      <td>0.162106</td>\n      <td>-0.195311</td>\n      <td>0.222396</td>\n      <td>0.097842</td>\n      <td>-0.307882</td>\n      <td>-0.325424</td>\n      <td>-0.210876</td>\n      <td>0.034118</td>\n      <td>0.114727</td>\n      <td>-0.092626</td>\n      <td>0.038293</td>\n      <td>-0.076999</td>\n      <td>0.035965</td>\n      <td>-0.00164</td>\n      <td>-0.067477</td>\n      <td>0.20682</td>\n      <td>-0.090941</td>\n      <td>0.011562</td>\n      <td>0.523355</td>\n      <td>0.205579</td>\n      <td>0.027943</td>\n      <td>0.256336</td>\n      <td>-0.408816</td>\n      <td>-0.036333</td>\n      <td>-0.039951</td>\n      <td>-0.067919</td>\n      <td>-0.153039</td>\n      <td>-0.669205</td>\n      <td>0.106416</td>\n      <td>0.140609</td>\n      <td>0.073663</td>\n      <td>-0.095072</td>\n      <td>-0.029422</td>\n      <td>-0.021315</td>\n      <td>0.024537</td>\n      <td>0.019863</td>\n      <td>0.1113</td>\n      <td>-0.087268</td>\n      <td>0.033974</td>\n      <td>-0.0413</td>\n      <td>-0.002832</td>\n      <td>0.191807</td>\n      <td>-0.201867</td>\n      <td>0.066451</td>\n      <td>-0.023439</td>\n      <td>-0.33664</td>\n      <td>0.714376</td>\n      <td>0.090345</td>\n      <td>-0.191597</td>\n      <td>0.216644</td>\n      <td>-0.025642</td>\n      <td>-0.009782</td>\n      <td>-0.170917</td>\n      <td>-0.068642</td>\n      <td>-0.000975</td>\n      <td>0.141942</td>\n      <td>-0.020858</td>\n      <td>-0.09397</td>\n      <td>0.041412</td>\n      <td>0.061818</td>\n      <td>-0.048434</td>\n      <td>-0.040401</td>\n      <td>0.484307</td>\n      <td>0.199968</td>\n      <td>-0.079365</td>\n      <td>-0.282826</td>\n      <td>0.018812</td>\n      <td>0.142837</td>\n      <td>-0.237518</td>\n      <td>0.177559</td>\n      <td>0.108307</td>\n      <td>0.015854</td>\n      <td>-0.030182</td>\n      <td>-0.045865</td>\n      <td>-0.018895</td>\n      <td>-0.130665</td>\n      <td>0.158553</td>\n      <td>-0.120776</td>\n      <td>-0.060394</td>\n      <td>-0.245842</td>\n      <td>-0.171578</td>\n      <td>0.217652</td>\n      <td>-0.069561</td>\n      <td>0.218517</td>\n      <td>-0.073254</td>\n      <td>-0.029057</td>\n      <td>-0.045591</td>\n      <td>0.122068</td>\n      <td>0.059425</td>\n      <td>0.108973</td>\n      <td>0.037294</td>\n      <td>-0.031758</td>\n      <td>0.092954</td>\n      <td>-0.298806</td>\n      <td>0.093344</td>\n      <td>0.245105</td>\n      <td>-0.03089</td>\n      <td>-0.109563</td>\n      <td>0.203928</td>\n      <td>-0.128601</td>\n      <td>-0.15751</td>\n      <td>0.035108</td>\n      <td>0.130013</td>\n      <td>-0.028601</td>\n      <td>-0.050875</td>\n      <td>-0.020912</td>\n      <td>-0.041617</td>\n      <td>-0.225915</td>\n      <td>-0.147597</td>\n      <td>-0.346384</td>\n      <td>-0.230136</td>\n      <td>-0.45081</td>\n      <td>0.134377</td>\n      <td>-0.004222</td>\n      <td>0.075372</td>\n      <td>0.246987</td>\n      <td>0.189751</td>\n      <td>1.171893</td>\n      <td>0.423259</td>\n      <td>0.129495</td>\n      <td>0.047323</td>\n      <td>0.245403</td>\n      <td>0.009631</td>\n      <td>-0.154849</td>\n      <td>0.136841</td>\n      <td>0.310707</td>\n      <td>0.053092</td>\n      <td>-0.118639</td>\n      <td>0.13245</td>\n      <td>0.005078</td>\n      <td>0.145224</td>\n      <td>0.315543</td>\n      <td>-0.036221</td>\n      <td>0.078925</td>\n      <td>0.070699</td>\n      <td>0.323266</td>\n      <td>-0.204535</td>\n      <td>-0.19691</td>\n      <td>-0.25279</td>\n      <td>-0.010706</td>\n      <td>-0.16057</td>\n      <td>0.057044</td>\n      <td>0.159736</td>\n      <td>-0.11282</td>\n      <td>-0.079733</td>\n      <td>-0.325507</td>\n      <td>-0.188891</td>\n      <td>-0.037424</td>\n      <td>0.284523</td>\n      <td>0.069744</td>\n      <td>0.039888</td>\n      <td>0.027148</td>\n      <td>-0.180135</td>\n      <td>0.270566</td>\n      <td>0.013305</td>\n      <td>0.162727</td>\n      <td>-0.054204</td>\n      <td>0.033835</td>\n      <td>-0.054199</td>\n      <td>-0.152402</td>\n      <td>-0.004757</td>\n      <td>0.042156</td>\n      <td>-0.095999</td>\n      <td>0.147516</td>\n      <td>-0.211751</td>\n      <td>-0.189056</td>\n      <td>0.024325</td>\n      <td>0.074092</td>\n      <td>0.093858</td>\n      <td>-0.106786</td>\n      <td>-0.024229</td>\n      <td>0.361901</td>\n      <td>-0.034408</td>\n      <td>0.173214</td>\n      <td>0.111462</td>\n      <td>0.031852</td>\n      <td>-0.09153</td>\n      <td>0.117739</td>\n      <td>-0.177065</td>\n      <td>-0.025684</td>\n      <td>-0.056748</td>\n      <td>0.097014</td>\n      <td>-0.064555</td>\n      <td>0.157628</td>\n      <td>-0.84213</td>\n      <td>0.027633</td>\n      <td>-0.110726</td>\n      <td>-0.012435</td>\n      <td>0.236703</td>\n      <td>-0.001034</td>\n      <td>-0.164259</td>\n      <td>-0.005667</td>\n      <td>0.162612</td>\n      <td>-0.022558</td>\n      <td>-0.138819</td>\n      <td>-0.067203</td>\n      <td>0.209579</td>\n      <td>0.166321</td>\n      <td>0.14238</td>\n      <td>0.005088</td>\n      <td>-0.166263</td>\n      <td>0.056212</td>\n      <td>0.058532</td>\n      <td>-0.172003</td>\n      <td>-0.141869</td>\n      <td>-0.30124</td>\n      <td>-0.030682</td>\n      <td>-0.228836</td>\n      <td>-0.144127</td>\n      <td>-0.019697</td>\n      <td>-0.055351</td>\n      <td>-0.520402</td>\n      <td>-0.117021</td>\n      <td>0.30517</td>\n      <td>-0.069852</td>\n      <td>0.036632</td>\n      <td>0.16741</td>\n      <td>0.169146</td>\n      <td>0.017619</td>\n      <td>-0.193883</td>\n      <td>-0.067916</td>\n      <td>-0.273727</td>\n      <td>-0.106562</td>\n      <td>0.09762</td>\n      <td>-0.108914</td>\n      <td>-0.194292</td>\n      <td>-0.050219</td>\n      <td>-0.271828</td>\n      <td>0.08072</td>\n      <td>-0.100197</td>\n      <td>0.024852</td>\n      <td>-0.16531</td>\n      <td>-0.122422</td>\n      <td>0.084847</td>\n      <td>-0.124941</td>\n      <td>-0.003572</td>\n      <td>-0.107315</td>\n      <td>-0.084064</td>\n      <td>-0.034131</td>\n      <td>-0.471144</td>\n      <td>-1.553909</td>\n      <td>0.427042</td>\n      <td>0.133481</td>\n      <td>-0.149348</td>\n      <td>0.085869</td>\n      <td>-0.026381</td>\n      <td>-0.150482</td>\n      <td>0.11949</td>\n      <td>-0.034511</td>\n      <td>0.125529</td>\n      <td>-0.22076</td>\n      <td>0.051272</td>\n      <td>-0.168972</td>\n      <td>0.112018</td>\n      <td>-0.060398</td>\n      <td>0.114152</td>\n      <td>-0.003922</td>\n      <td>0.152598</td>\n      <td>-0.037112</td>\n      <td>-0.117118</td>\n      <td>-0.172868</td>\n      <td>-0.065486</td>\n      <td>-0.066613</td>\n      <td>-0.090606</td>\n      <td>0.447557</td>\n      <td>0.253628</td>\n      <td>0.099496</td>\n      <td>-0.02372</td>\n      <td>0.066293</td>\n      <td>0.149939</td>\n      <td>-0.021434</td>\n      <td>0.059437</td>\n      <td>-0.1489</td>\n      <td>0.022031</td>\n      <td>0.026973</td>\n      <td>0.262776</td>\n      <td>-0.259509</td>\n      <td>-0.190479</td>\n      <td>-0.141458</td>\n      <td>0.170991</td>\n      <td>0.259469</td>\n      <td>0.175812</td>\n      <td>0.248569</td>\n      <td>0.09565</td>\n      <td>-0.20874</td>\n      <td>-0.20845</td>\n      <td>0.113769</td>\n      <td>0.068898</td>\n      <td>0.112756</td>\n      <td>-0.066803</td>\n      <td>0.130623</td>\n      <td>-0.038461</td>\n      <td>-0.045274</td>\n      <td>-0.22113</td>\n      <td>0.057998</td>\n      <td>0.130247</td>\n      <td>0.042116</td>\n      <td>-0.316556</td>\n      <td>0.075787</td>\n      <td>0.082944</td>\n      <td>0.086548</td>\n      <td>0.098292</td>\n      <td>0.178536</td>\n      <td>0.093488</td>\n      <td>-0.212282</td>\n      <td>0.030044</td>\n      <td>0.373361</td>\n      <td>0.072374</td>\n      <td>0.007171</td>\n      <td>0.050314</td>\n      <td>0.078846</td>\n      <td>-0.125926</td>\n      <td>0.117607</td>\n      <td>-0.182267</td>\n      <td>-0.018583</td>\n      <td>0.010673</td>\n      <td>-0.101328</td>\n      <td>0.451349</td>\n      <td>-0.107005</td>\n      <td>0.303963</td>\n      <td>0.10985</td>\n      <td>-0.300183</td>\n      <td>-0.026005</td>\n      <td>0.120307</td>\n      <td>-0.143743</td>\n      <td>-0.053908</td>\n      <td>0.038879</td>\n      <td>-0.207219</td>\n      <td>0.136312</td>\n      <td>0.055207</td>\n      <td>0.080172</td>\n      <td>-0.067663</td>\n      <td>-0.183928</td>\n      <td>0.204984</td>\n      <td>-0.001481</td>\n      <td>0.160431</td>\n      <td>0.064906</td>\n      <td>-0.046666</td>\n      <td>0.114594</td>\n      <td>0.185505</td>\n      <td>0.041588</td>\n      <td>-0.106086</td>\n      <td>0.324065</td>\n      <td>-0.014526</td>\n      <td>-0.138719</td>\n      <td>-0.217771</td>\n      <td>0.388382</td>\n      <td>0.036732</td>\n      <td>0.290681</td>\n      <td>-0.118576</td>\n      <td>-0.056008</td>\n      <td>0.174637</td>\n      <td>-0.108908</td>\n      <td>0.021874</td>\n      <td>0.055263</td>\n      <td>0.050176</td>\n      <td>0.018494</td>\n      <td>1.638244</td>\n      <td>0.292534</td>\n      <td>-0.157802</td>\n      <td>-0.064801</td>\n      <td>0.262028</td>\n      <td>0.053501</td>\n      <td>-0.101853</td>\n      <td>0.026971</td>\n      <td>0.278975</td>\n      <td>0.12391</td>\n      <td>0.080051</td>\n      <td>-0.116013</td>\n      <td>-0.18412</td>\n      <td>0.175828</td>\n      <td>-0.047083</td>\n      <td>-0.045451</td>\n      <td>-0.185876</td>\n      <td>0.04159</td>\n      <td>12.282816</td>\n      <td>0.079741</td>\n      <td>-0.060993</td>\n      <td>0.06344</td>\n      <td>-0.268049</td>\n      <td>-0.09487</td>\n      <td>0.024665</td>\n      <td>0.00188</td>\n      <td>0.18718</td>\n      <td>0.040667</td>\n      <td>0.006068</td>\n      <td>0.023697</td>\n      <td>0.187934</td>\n      <td>-0.221596</td>\n      <td>0.068942</td>\n      <td>0.005736</td>\n      <td>0.098046</td>\n      <td>0.084538</td>\n      <td>0.091436</td>\n      <td>-0.074916</td>\n      <td>0.082979</td>\n      <td>0.174427</td>\n      <td>-0.010615</td>\n      <td>0.956542</td>\n      <td>-0.157765</td>\n      <td>0.202592</td>\n      <td>-0.065107</td>\n      <td>0.086034</td>\n      <td>0.082509</td>\n      <td>0.116933</td>\n      <td>-0.130661</td>\n      <td>0.056648</td>\n      <td>-0.001675</td>\n      <td>-0.010225</td>\n      <td>0.085153</td>\n      <td>-0.174518</td>\n      <td>-0.064504</td>\n      <td>0.161261</td>\n      <td>0.148312</td>\n      <td>-0.075701</td>\n      <td>-0.12233</td>\n      <td>-0.085408</td>\n      <td>0.294268</td>\n      <td>-0.416573</td>\n      <td>0.074603</td>\n      <td>-0.209788</td>\n      <td>0.263318</td>\n      <td>0.31023</td>\n      <td>0.106331</td>\n      <td>-0.020113</td>\n      <td>-0.048917</td>\n      <td>-0.070626</td>\n      <td>0.096161</td>\n      <td>-0.188461</td>\n      <td>-0.075923</td>\n      <td>-0.234436</td>\n      <td>-0.020224</td>\n      <td>-0.090146</td>\n      <td>0.016027</td>\n      <td>-0.018889</td>\n      <td>-0.1145</td>\n      <td>-0.154983</td>\n      <td>0.169091</td>\n      <td>0.098314</td>\n      <td>-0.164059</td>\n      <td>0.183723</td>\n      <td>0.187806</td>\n      <td>-0.131551</td>\n      <td>-0.050215</td>\n      <td>-0.003589</td>\n      <td>0.114386</td>\n      <td>-0.010999</td>\n      <td>-0.022542</td>\n      <td>0.019642</td>\n      <td>0.036398</td>\n      <td>-0.196691</td>\n      <td>-0.317724</td>\n      <td>-0.19068</td>\n      <td>-0.218587</td>\n      <td>-0.481349</td>\n      <td>0.132161</td>\n      <td>-0.064054</td>\n      <td>-0.13544</td>\n      <td>-0.064407</td>\n      <td>-0.314367</td>\n      <td>0.12035</td>\n      <td>-0.229907</td>\n      <td>-0.007436</td>\n      <td>-0.172259</td>\n      <td>0.153599</td>\n      <td>0.202984</td>\n      <td>0.076411</td>\n      <td>0.012433</td>\n      <td>-0.207095</td>\n      <td>0.033421</td>\n      <td>-0.124092</td>\n      <td>0.215223</td>\n      <td>-0.086132</td>\n      <td>0.223904</td>\n      <td>-0.15867</td>\n      <td>-0.441802</td>\n      <td>-0.005265</td>\n      <td>-0.076523</td>\n      <td>0.007779</td>\n      <td>-0.222167</td>\n      <td>0.287094</td>\n      <td>0.008407</td>\n      <td>-0.1184</td>\n      <td>0.111839</td>\n      <td>0.096921</td>\n      <td>-0.109888</td>\n      <td>-0.032591</td>\n      <td>-0.262088</td>\n      <td>-0.143762</td>\n      <td>0.001745</td>\n      <td>-0.218345</td>\n      <td>0.092003</td>\n      <td>0.029601</td>\n      <td>0.020191</td>\n      <td>0.111559</td>\n      <td>0.005574</td>\n      <td>0.073102</td>\n      <td>0.047232</td>\n      <td>0.08606</td>\n      <td>-0.080468</td>\n      <td>-0.066836</td>\n      <td>-0.307097</td>\n      <td>-0.099523</td>\n      <td>-0.264944</td>\n      <td>-0.1652</td>\n      <td>-0.270445</td>\n      <td>0.170967</td>\n      <td>0.298262</td>\n      <td>0.029693</td>\n      <td>0.077656</td>\n      <td>0.060142</td>\n      <td>-0.168183</td>\n      <td>0.077958</td>\n      <td>0.168891</td>\n      <td>0.268703</td>\n      <td>-0.149408</td>\n      <td>0.017111</td>\n      <td>-0.243692</td>\n      <td>-0.190973</td>\n      <td>0.018555</td>\n      <td>-0.244975</td>\n      <td>0.347429</td>\n      <td>0.465995</td>\n      <td>0.145426</td>\n      <td>0.111209</td>\n      <td>-0.079083</td>\n      <td>-0.170695</td>\n      <td>-0.197767</td>\n      <td>-0.445161</td>\n      <td>0.029228</td>\n      <td>0.070849</td>\n      <td>-0.132407</td>\n      <td>0.278908</td>\n      <td>0.070082</td>\n      <td>-0.276633</td>\n      <td>0.595741</td>\n      <td>-0.456886</td>\n      <td>-0.110131</td>\n      <td>0.033729</td>\n      <td>-0.337985</td>\n      <td>0.117239</td>\n      <td>0.234729</td>\n      <td>-0.072439</td>\n      <td>0.11365</td>\n      <td>0.048002</td>\n      <td>-0.158772</td>\n      <td>-0.019317</td>\n      <td>0.086273</td>\n      <td>0.033125</td>\n      <td>0.099122</td>\n      <td>0.154706</td>\n      <td>0.245593</td>\n      <td>0.108821</td>\n      <td>-0.001333</td>\n      <td>0.027004</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           id       lgb       cat        rf       reg         y\n",
       "0  1601.03743  1.904554  1.944293  2.110282  1.965835  2.197225"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>lgb</th>\n      <th>cat</th>\n      <th>rf</th>\n      <th>reg</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1601.03743</td>\n      <td>1.904554</td>\n      <td>1.944293</td>\n      <td>2.110282</td>\n      <td>1.965835</td>\n      <td>2.197225</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "df_result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_0_cols2 = ['econ_y', 'eess_y', 'nlin_y', 'physics_y', 'acc-phys_y',\n",
    "       'adap-org_y', 'alg-geom_y', 'ao-sci_y', 'astro-ph_y', 'atom-ph_y',\n",
    "       'bayes-an_y', 'chao-dyn_y', 'chem-ph_y', 'cmp-lg_y', 'comp-gas_y',\n",
    "       'dg-ga_y', 'funct-an_y', 'gr-qc_y', 'math-ph_y', 'mtrl-th_y',\n",
    "       'nucl-ex_y', 'patt-sol_y', 'plasm-ph_y', 'q-alg_y', 'q-fin_y',\n",
    "       'solv-int_y', 'supr-con_y', 'acc_y', 'adap_y', 'alg_y', 'ao_y',\n",
    "       'astro_y', 'atom_y', 'bayes_y', 'chao_y', 'chem_y', 'cmp_y',\n",
    "       'comp_y', 'cond_y', 'dg_y', 'econ', 'eess', 'funct_y', 'gr_y',\n",
    "       'math', 'mtrl_y', 'nlin', 'patt_y', 'physics', 'plasm_y',\n",
    "       'quant_y', 'solv_y', 'stat', 'supr_y', 'astro-ph.ga',\n",
    "       'astro-ph.he', 'astro-ph.sr', 'cond-mat.dis-nn',\n",
    "       'cond-mat.mes-hall', 'cond-mat.other', 'cond-mat.soft',\n",
    "       'cond-mat.stat-mech', 'cs.ai', 'cs.ar', 'cs.cc', 'cs.ce', 'cs.cg',\n",
    "       'cs.cl', 'cs.cr', 'cs.cv', 'cs.cy', 'cs.db', 'cs.dc', 'cs.dl',\n",
    "       'cs.dm', 'cs.et', 'cs.fl', 'cs.gl', 'cs.gr', 'cs.gt', 'cs.hc',\n",
    "       'cs.ir', 'cs.it', 'cs.lo', 'cs.ma', 'cs.mm', 'cs.ms', 'cs.na',\n",
    "       'cs.ne', 'cs.ni', 'cs.oh', 'cs.os', 'cs.pf', 'cs.pl', 'cs.ro',\n",
    "       'cs.sc', 'cs.sd', 'cs.se', 'cs.sy', 'econ.em', 'econ.gn',\n",
    "       'econ.th', 'eess.as', 'eess.iv', 'eess.sp', 'eess.sy', 'math.ac',\n",
    "       'math.ap', 'math.at', 'math.ca', 'math.ct', 'math.cv', 'math.dg',\n",
    "       'math.ds', 'math.fa', 'math.gm', 'math.gn', 'math.gr', 'math.gt',\n",
    "       'math.ho', 'math.it', 'math.kt', 'math.lo', 'math.mg', 'math.mp',\n",
    "       'math.na', 'math.nt', 'math.oa', 'math.oc', 'math.qa', 'math.ra',\n",
    "       'math.rt', 'math.sg', 'math.sp', 'math.st', 'nlin.ao', 'nlin.cd',\n",
    "       'nlin.cg', 'nlin.ps', 'nlin.si', 'physics.acc-ph', 'physics.ao-ph',\n",
    "       'physics.app-ph', 'physics.atm-clus', 'physics.bio-ph',\n",
    "       'physics.chem-ph', 'physics.class-ph', 'physics.comp-ph',\n",
    "       'physics.data-an', 'physics.ed-ph', 'physics.flu-dyn',\n",
    "       'physics.gen-ph', 'physics.geo-ph', 'physics.hist-ph',\n",
    "       'physics.ins-det', 'physics.med-ph', 'physics.optics',\n",
    "       'physics.plasm-ph', 'physics.pop-ph', 'physics.soc-ph', 'q-bio.bm',\n",
    "       'q-bio.cb', 'q-bio.gn', 'q-bio.mn', 'q-bio.nc', 'q-bio.ot',\n",
    "       'q-bio.pe', 'q-bio.qm', 'q-bio.sc', 'q-bio.to', 'q-fin.cp',\n",
    "       'q-fin.ec', 'q-fin.gn', 'q-fin.mf', 'q-fin.pm', 'q-fin.pr',\n",
    "       'q-fin.rm', 'q-fin.st', 'q-fin.tr', 'stat.ap', 'stat.co',\n",
    "       'stat.me', 'stat.ml', 'stat.ot', 'doi_cites_min_doi_id_label',\n",
    "       'doi_cites_min_pub_publisher_label',\n",
    "       'doi_cites_median_pub_publisher_label', 'doi_cites_min_update_ym',\n",
    "       'doi_cites_min_first_created_ym', 'doi_cites_min_license_label',\n",
    "       'doi_cites_max_license_label', 'doi_cites_q10_license_label',\n",
    "       'doi_cites_q75_license_label', 'doi_cites_min_category_main_label',\n",
    "       'doi_cites_q10_category_main_label',\n",
    "       'doi_cites_q25_category_main_label',\n",
    "       'doi_cites_min_category_main_detail_label',\n",
    "       'doi_cites_median_category_main_detail_label',\n",
    "       'doi_cites_q10_category_main_detail_label',\n",
    "       'doi_cites_q25_category_main_detail_label',\n",
    "       'doi_cites_min_category_name_parent_label',\n",
    "       'doi_cites_q10_category_name_parent_label',\n",
    "       'doi_cites_min_category_name_parent_main_label',\n",
    "       'doi_cites_q10_category_name_parent_main_label',\n",
    "       'doi_cites_min_category_name_label',\n",
    "       'pred_doi_cites_min_doi_id_label',\n",
    "       'pred_doi_cites_min_pub_publisher_label',\n",
    "       'pred_doi_cites_median_pub_publisher_label',\n",
    "       'pred_doi_cites_q75_pub_publisher_label',\n",
    "       'pred_doi_cites_min_update_ym', 'pred_doi_cites_q10_update_ym',\n",
    "       'pred_doi_cites_min_first_created_ym',\n",
    "       'pred_doi_cites_q10_first_created_ym',\n",
    "       'pred_doi_cites_mean_license_label',\n",
    "       'pred_doi_cites_count_license_label',\n",
    "       'pred_doi_cites_sum_license_label',\n",
    "       'pred_doi_cites_min_license_label',\n",
    "       'pred_doi_cites_std_license_label',\n",
    "       'pred_doi_cites_q10_license_label',\n",
    "       'pred_doi_cites_q25_license_label',\n",
    "       'pred_doi_cites_q75_license_label',\n",
    "       'pred_doi_cites_mean_category_main_label',\n",
    "       'pred_doi_cites_min_category_main_label',\n",
    "       'pred_doi_cites_median_category_main_label',\n",
    "       'pred_doi_cites_q10_category_main_label',\n",
    "       'pred_doi_cites_q25_category_main_label',\n",
    "       'pred_doi_cites_mean_category_main_detail_label',\n",
    "       'pred_doi_cites_sum_category_main_detail_label',\n",
    "       'pred_doi_cites_min_category_main_detail_label',\n",
    "       'pred_doi_cites_median_category_main_detail_label',\n",
    "       'pred_doi_cites_q10_category_main_detail_label',\n",
    "       'pred_doi_cites_q25_category_main_detail_label',\n",
    "       'pred_doi_cites_q75_category_main_detail_label',\n",
    "       'pred_doi_cites_min_category_name_parent_label',\n",
    "       'pred_doi_cites_median_category_name_parent_label',\n",
    "       'pred_doi_cites_q10_category_name_parent_label',\n",
    "       'pred_doi_cites_q25_category_name_parent_label',\n",
    "       'pred_doi_cites_min_category_name_parent_main_label',\n",
    "       'pred_doi_cites_median_category_name_parent_main_label',\n",
    "       'pred_doi_cites_q10_category_name_parent_main_label',\n",
    "       'pred_doi_cites_q25_category_name_parent_main_label',\n",
    "       'pred_doi_cites_min_category_name_label',\n",
    "       'diff_rate_doi_cites_pred_doi_cites',\n",
    "       'diff_rate_doi_cites_mean_submitter_label_pred_doi_cites_mean_submitter_label',\n",
    "       'diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_pub_publisher_label',\n",
    "       'diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_doi_id_label',\n",
    "       'diff_rate_doi_cites_mean_doi_id_label_pred_doi_cites_mean_pub_publisher_label',\n",
    "       'diff_rate_doi_cites_mean_author_first_label_pred_doi_cites_mean_author_first_label',\n",
    "       'diff_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_pub_publisher_label',\n",
    "       'diff_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_update_ym_pred_doi_cites_mean_update_ym',\n",
    "       'diff_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_rate_doi_cites_mean_first_created_ym_pred_doi_cites_mean_first_created_ym',\n",
    "       'diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_doi_cites_mean_license_label_pred_doi_cites_mean_license_label',\n",
    "       'diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_license_label',\n",
    "       'diff_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_doi_cites_mean_category_main_label_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_doi_id_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_pub_publisher_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_update_ym',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_license_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_update_ym',\n",
    "       'diff_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_license_label',\n",
    "       'diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_license_label',\n",
    "       'diff_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_pub_publisher_label',\n",
    "       'diff_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_category_name_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_doi_id_label',\n",
    "       'diff_rate_pred_doi_cites_mean_submitter_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_author_first_label',\n",
    "       'diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_pub_publisher_label',\n",
    "       'diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_update_ym',\n",
    "       'diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_first_created_ym',\n",
    "       'diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_license_label',\n",
    "       'diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_pred_doi_cites_mean_doi_id_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_pub_publisher_label',\n",
    "       'diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_update_ym',\n",
    "       'diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_rate_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_pred_doi_cites_mean_author_first_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_update_ym',\n",
    "       'diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_first_created_ym',\n",
    "       'diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_first_created_ym',\n",
    "       'diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_license_label',\n",
    "       'diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_license_label',\n",
    "       'diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_pred_doi_cites_mean_pub_publisher_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_license_label',\n",
    "       'diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_pred_doi_cites_mean_update_ym_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_license_label',\n",
    "       'diff_rate_pred_doi_cites_mean_first_created_ym_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_rate_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_label',\n",
    "       'diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_rate_pred_doi_cites_mean_license_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_rate_pred_doi_cites_mean_category_main_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_rate_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_label',\n",
    "       'diff_rate_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_parent_main_label',\n",
    "       'diff_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_rate_pred_doi_cites_mean_category_main_detail_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_rate_pred_doi_cites_mean_category_name_parent_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_pred_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_label',\n",
    "       'diff_rate_pred_doi_cites_mean_category_name_parent_main_label_pred_doi_cites_mean_category_name_label',\n",
    "       'is_null_comments', 'is_null_journal-ref']\n",
    "importance_0_cols2.extend(['authors', 'title', 'comments',\n",
    "    'journal-ref', 'doi', 'report-no', 'categories', 'license',\n",
    "    'abstract', 'versions', 'authors_parsed', 'pub_publisher',\n",
    "    'update_date_y', 'first_created_date', 'last_created_date', 'doi_id', 'submitter', 'author_first', 'category_main', 'category_main_detail', 'category_name_parent_main_unique', 'category_name_parent_unique', 'category_name_unique',\n",
    "    'submitter_label','doi_id_label','author_first_label','pub_publisher_label',\n",
    "    'license_label','category_main_label','category_name_parent_label','category_name_parent_main_label', 'category_name_label', 'cites'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.495677909118905"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "\n",
    "X = df_result.copy()\n",
    "y = X['y'].values\n",
    "X = X.drop(['id', 'y'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.1, random_state=0)\n",
    "mean_squared_error(y_test, X_test.mean(axis=1), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.645232\n",
      "[200]\tvalid_0's rmse: 0.533321\n",
      "[300]\tvalid_0's rmse: 0.519698\n",
      "Early stopping, best iteration is:\n",
      "[325]\tvalid_0's rmse: 0.5194\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5194004739849023"
      ]
     },
     "metadata": {},
     "execution_count": 168
    }
   ],
   "source": [
    "X = df_result.copy()\n",
    "# train\n",
    "#train = df_train.copy()\n",
    "#train = train.drop(importance_0_cols2, axis=1)\n",
    "#X = pd.merge(X, train, on='id', how='left')\n",
    "\n",
    "y = X['y'].values\n",
    "X = X.drop(['id', 'y'], axis=1)\n",
    "X['mean'] = X.mean(axis=1)\n",
    "#X = X.drop(['rf', 'lgb', 'reg', 'cat'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, np.array(y, dtype=np.float64), train_size=0.1, random_state=0)\n",
    "#model = linear_model.ElasticNet(alpha=0.7, l1_ratio=0.7, max_iter=5000)\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'random_state': SEED,\n",
    "    'learning_rate': 0.01,\n",
    "    'min_child_samples': 5,\n",
    "    'num_iterations': 10000,\n",
    "    'early_stopping_round': 50\n",
    "}\n",
    "model = lgb.LGBMRegressor(**lgb_params)\n",
    "model.fit(X_train, y_train,\n",
    "    eval_set=(X_test, y_test),\n",
    "    eval_metric='rmse',\n",
    "    verbose=100,\n",
    ")\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "mean_squared_error(y_test, pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([49.87204595, 10.43947197,  5.77703491, ..., 21.71168656,\n",
       "        3.04065107, 11.57012645])"
      ]
     },
     "metadata": {},
     "execution_count": 172
    }
   ],
   "source": [
    "np.expm1(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      ":15:31,769]\u001b[0m Trial 38 finished with value: 0.5057563885089879 and parameters: {'feature_fraction': 0.48000000000000004}. Best is trial 37 with value: 0.5057563885089877.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.505756:  67%|######6   | 2/3 [00:02<00:01,  1.25s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006654 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "feature_fraction_stage2, val_score: 0.505756: 100%|##########| 3/3 [00:03<00:00,  1.21s/it]\u001b[32m[I 2021-03-28 09:15:32,898]\u001b[0m Trial 39 finished with value: 0.5057563885089877 and parameters: {'feature_fraction': 0.44800000000000006}. Best is trial 37 with value: 0.5057563885089877.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.505756: 100%|##########| 3/3 [00:03<00:00,  1.19s/it]\n",
      "regularization_factors, val_score: 0.505756:   0%|          | 0/20 [00:00<?, ?it/s][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505756:   5%|5         | 1/20 [00:01<00:25,  1.33s/it]\u001b[32m[I 2021-03-28 09:15:34,226]\u001b[0m Trial 40 finished with value: 0.505933256462116 and parameters: {'lambda_l1': 2.113833273905981, 'lambda_l2': 2.1440010380679546e-05}. Best is trial 40 with value: 0.505933256462116.\u001b[0m\n",
      "regularization_factors, val_score: 0.505756:   5%|5         | 1/20 [00:01<00:25,  1.33s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505756:  10%|#         | 2/20 [00:02<00:23,  1.28s/it]\u001b[32m[I 2021-03-28 09:15:35,410]\u001b[0m Trial 41 finished with value: 0.5057563786346002 and parameters: {'lambda_l1': 7.235948258699531e-05, 'lambda_l2': 4.751139710967606e-05}. Best is trial 41 with value: 0.5057563786346002.\u001b[0m\n",
      "regularization_factors, val_score: 0.505756:  10%|#         | 2/20 [00:02<00:23,  1.28s/it][LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505756:  15%|#5        | 3/20 [00:03<00:22,  1.31s/it]\u001b[32m[I 2021-03-28 09:15:36,798]\u001b[0m Trial 42 finished with value: 0.5057563836127295 and parameters: {'lambda_l1': 5.67132482050943e-07, 'lambda_l2': 0.00017702139620258647}. Best is trial 41 with value: 0.5057563786346002.\u001b[0m\n",
      "regularization_factors, val_score: 0.505756:  15%|#5        | 3/20 [00:03<00:22,  1.31s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505756:  20%|##        | 4/20 [00:05<00:20,  1.30s/it]\u001b[32m[I 2021-03-28 09:15:38,059]\u001b[0m Trial 43 finished with value: 0.5088722945693902 and parameters: {'lambda_l1': 7.515969449023385, 'lambda_l2': 6.309325310060373e-08}. Best is trial 41 with value: 0.5057563786346002.\u001b[0m\n",
      "regularization_factors, val_score: 0.505756:  20%|##        | 4/20 [00:05<00:20,  1.30s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505756:  25%|##5       | 5/20 [00:06<00:19,  1.31s/it]\u001b[32m[I 2021-03-28 09:15:39,393]\u001b[0m Trial 44 finished with value: 0.5059880376263082 and parameters: {'lambda_l1': 1.934739453956337e-06, 'lambda_l2': 0.054229794183446256}. Best is trial 41 with value: 0.5057563786346002.\u001b[0m\n",
      "regularization_factors, val_score: 0.505756:  25%|##5       | 5/20 [00:06<00:19,  1.31s/it][LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505756:  30%|###       | 6/20 [00:07<00:18,  1.32s/it]\u001b[32m[I 2021-03-28 09:15:40,723]\u001b[0m Trial 45 finished with value: 0.506123845638258 and parameters: {'lambda_l1': 0.6460875251848148, 'lambda_l2': 0.03416027326772376}. Best is trial 41 with value: 0.5057563786346002.\u001b[0m\n",
      "regularization_factors, val_score: 0.505756:  30%|###       | 6/20 [00:07<00:18,  1.32s/it][LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505756:  35%|###5      | 7/20 [00:09<00:16,  1.29s/it]\u001b[32m[I 2021-03-28 09:15:41,954]\u001b[0m Trial 46 finished with value: 0.5059505731921092 and parameters: {'lambda_l1': 0.036758644379422306, 'lambda_l2': 0.00021339170334467968}. Best is trial 41 with value: 0.5057563786346002.\u001b[0m\n",
      "regularization_factors, val_score: 0.505756:  35%|###5      | 7/20 [00:09<00:16,  1.29s/it][LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505756:  40%|####      | 8/20 [00:10<00:15,  1.29s/it]\u001b[32m[I 2021-03-28 09:15:43,240]\u001b[0m Trial 47 finished with value: 0.5058376085273764 and parameters: {'lambda_l1': 0.015883203633259537, 'lambda_l2': 0.17854485432443148}. Best is trial 41 with value: 0.5057563786346002.\u001b[0m\n",
      "regularization_factors, val_score: 0.505756:  40%|####      | 8/20 [00:10<00:15,  1.29s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055919 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505756:  45%|####5     | 9/20 [00:11<00:13,  1.26s/it]\u001b[32m[I 2021-03-28 09:15:44,438]\u001b[0m Trial 48 finished with value: 0.505903969617391 and parameters: {'lambda_l1': 0.4098247624926821, 'lambda_l2': 0.4847446341152764}. Best is trial 41 with value: 0.5057563786346002.\u001b[0m\n",
      "regularization_factors, val_score: 0.505756:  45%|####5     | 9/20 [00:11<00:13,  1.26s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042884 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505756:  50%|#####     | 10/20 [00:12<00:12,  1.24s/it]\u001b[32m[I 2021-03-28 09:15:45,634]\u001b[0m Trial 49 finished with value: 0.5057563885089688 and parameters: {'lambda_l1': 4.597388749751311e-08, 'lambda_l2': 7.778797233181253e-08}. Best is trial 41 with value: 0.5057563786346002.\u001b[0m\n",
      "regularization_factors, val_score: 0.505756:  50%|#####     | 10/20 [00:12<00:12,  1.24s/it][LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505756:  55%|#####5    | 11/20 [00:14<00:11,  1.28s/it]\u001b[32m[I 2021-03-28 09:15:47,010]\u001b[0m Trial 50 finished with value: 0.5057563804053908 and parameters: {'lambda_l1': 6.788640016789893e-05, 'lambda_l2': 2.433736514992568e-06}. Best is trial 41 with value: 0.5057563786346002.\u001b[0m\n",
      "regularization_factors, val_score: 0.505756:  55%|#####5    | 11/20 [00:14<00:11,  1.28s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505756:  60%|######    | 12/20 [00:15<00:09,  1.24s/it]\u001b[32m[I 2021-03-28 09:15:48,156]\u001b[0m Trial 51 finished with value: 0.505756377368318 and parameters: {'lambda_l1': 9.248382274331713e-05, 'lambda_l2': 6.803491961877153e-06}. Best is trial 51 with value: 0.505756377368318.\u001b[0m\n",
      "regularization_factors, val_score: 0.505756:  60%|######    | 12/20 [00:15<00:09,  1.24s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003497 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505756:  65%|######5   | 13/20 [00:16<00:08,  1.23s/it]\u001b[32m[I 2021-03-28 09:15:49,352]\u001b[0m Trial 52 finished with value: 0.5057563624243836 and parameters: {'lambda_l1': 0.00021976241358545877, 'lambda_l2': 2.2599308355255584e-06}. Best is trial 52 with value: 0.5057563624243836.\u001b[0m\n",
      "regularization_factors, val_score: 0.505756:  65%|######5   | 13/20 [00:16<00:08,  1.23s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070811 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505756:  70%|#######   | 14/20 [00:17<00:07,  1.22s/it]\u001b[32m[I 2021-03-28 09:15:50,547]\u001b[0m Trial 53 finished with value: 0.5057561959851612 and parameters: {'lambda_l1': 0.001626871010348158, 'lambda_l2': 1.9544895549783147e-06}. Best is trial 53 with value: 0.5057561959851612.\u001b[0m\n",
      "regularization_factors, val_score: 0.505756:  70%|#######   | 14/20 [00:17<00:07,  1.22s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045866 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505737:  75%|#######5  | 15/20 [00:18<00:06,  1.23s/it]\u001b[32m[I 2021-03-28 09:15:51,799]\u001b[0m Trial 54 finished with value: 0.5057365271009708 and parameters: {'lambda_l1': 0.0034528733059771117, 'lambda_l2': 5.70174262528642e-07}. Best is trial 54 with value: 0.5057365271009708.\u001b[0m\n",
      "regularization_factors, val_score: 0.505737:  75%|#######5  | 15/20 [00:18<00:06,  1.23s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505737:  80%|########  | 16/20 [00:20<00:05,  1.28s/it]\u001b[32m[I 2021-03-28 09:15:53,196]\u001b[0m Trial 55 finished with value: 0.5057365265142667 and parameters: {'lambda_l1': 0.00345950495765581, 'lambda_l2': 1.0575308851308875e-08}. Best is trial 55 with value: 0.5057365265142667.\u001b[0m\n",
      "regularization_factors, val_score: 0.505737:  80%|########  | 16/20 [00:20<00:05,  1.28s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505737:  85%|########5 | 17/20 [00:21<00:03,  1.27s/it]\u001b[32m[I 2021-03-28 09:15:54,462]\u001b[0m Trial 56 finished with value: 0.5057560340687715 and parameters: {'lambda_l1': 0.0029981531924197498, 'lambda_l2': 1.0373450898686271e-08}. Best is trial 55 with value: 0.5057365265142667.\u001b[0m\n",
      "regularization_factors, val_score: 0.505737:  85%|########5 | 17/20 [00:21<00:03,  1.27s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505737:  90%|######### | 18/20 [00:22<00:02,  1.28s/it]\u001b[32m[I 2021-03-28 09:15:55,768]\u001b[0m Trial 57 finished with value: 0.5060478587150116 and parameters: {'lambda_l1': 0.05840345106391805, 'lambda_l2': 1.0953196505165718e-08}. Best is trial 55 with value: 0.5057365265142667.\u001b[0m\n",
      "regularization_factors, val_score: 0.505737:  90%|######### | 18/20 [00:22<00:02,  1.28s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505737:  95%|#########5| 19/20 [00:24<00:01,  1.26s/it]\u001b[32m[I 2021-03-28 09:15:56,987]\u001b[0m Trial 58 finished with value: 0.5057558128493043 and parameters: {'lambda_l1': 0.00389080444800426, 'lambda_l2': 0.004264202116075403}. Best is trial 55 with value: 0.5057365265142667.\u001b[0m\n",
      "regularization_factors, val_score: 0.505737:  95%|#########5| 19/20 [00:24<00:01,  1.26s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "regularization_factors, val_score: 0.505737: 100%|##########| 20/20 [00:25<00:00,  1.26s/it]\u001b[32m[I 2021-03-28 09:15:58,237]\u001b[0m Trial 59 finished with value: 0.5059937111858867 and parameters: {'lambda_l1': 6.098028315553903e-06, 'lambda_l2': 6.129369823971338}. Best is trial 55 with value: 0.5057365265142667.\u001b[0m\n",
      "regularization_factors, val_score: 0.505737: 100%|##########| 20/20 [00:25<00:00,  1.27s/it]\n",
      "min_data_in_leaf, val_score: 0.505737:   0%|          | 0/5 [00:00<?, ?it/s][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "min_data_in_leaf, val_score: 0.505737:  20%|##        | 1/5 [00:01<00:05,  1.41s/it]\u001b[32m[I 2021-03-28 09:15:59,648]\u001b[0m Trial 60 finished with value: 0.512169003175232 and parameters: {'min_child_samples': 50}. Best is trial 60 with value: 0.512169003175232.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.505737:  20%|##        | 1/5 [00:01<00:05,  1.41s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "min_data_in_leaf, val_score: 0.505737:  40%|####      | 2/5 [00:02<00:04,  1.39s/it]\u001b[32m[I 2021-03-28 09:16:00,993]\u001b[0m Trial 61 finished with value: 0.5232532239864336 and parameters: {'min_child_samples': 100}. Best is trial 60 with value: 0.512169003175232.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.505737:  40%|####      | 2/5 [00:02<00:04,  1.39s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027612 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "min_data_in_leaf, val_score: 0.505737:  60%|######    | 3/5 [00:04<00:03,  1.54s/it]\u001b[32m[I 2021-03-28 09:16:02,888]\u001b[0m Trial 62 finished with value: 0.5075514925462659 and parameters: {'min_child_samples': 25}. Best is trial 62 with value: 0.5075514925462659.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.505737:  60%|######    | 3/5 [00:04<00:03,  1.54s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "min_data_in_leaf, val_score: 0.503366:  80%|########  | 4/5 [00:06<00:01,  1.70s/it]\u001b[32m[I 2021-03-28 09:16:04,945]\u001b[0m Trial 63 finished with value: 0.503366011757998 and parameters: {'min_child_samples': 10}. Best is trial 63 with value: 0.503366011757998.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.503366:  80%|########  | 4/5 [00:06<00:01,  1.70s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 2.583781\n",
      "min_data_in_leaf, val_score: 0.502972: 100%|##########| 5/5 [00:08<00:00,  1.83s/it]\u001b[32m[I 2021-03-28 09:16:07,104]\u001b[0m Trial 64 finished with value: 0.5029724031429625 and parameters: {'min_child_samples': 5}. Best is trial 64 with value: 0.5029724031429625.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.502972: 100%|##########| 5/5 [00:08<00:00,  1.77s/it]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'objective': 'regression',\n",
       " 'metric': 'rmse',\n",
       " 'boosting': 'dart',\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 0.00345950495765581,\n",
       " 'lambda_l2': 1.0575308851308875e-08,\n",
       " 'num_leaves': 2,\n",
       " 'feature_fraction': 0.41600000000000004,\n",
       " 'bagging_fraction': 0.8953021991684762,\n",
       " 'bagging_freq': 5,\n",
       " 'min_child_samples': 5,\n",
       " 'num_iterations': 1000,\n",
       " 'early_stopping_round': None}"
      ]
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting': 'dart',\n",
    "}\n",
    "best_params, history = {}, []\n",
    "trains = lgb.Dataset(X_train, y_train)\n",
    "valids = lgb.Dataset(X_test, y_test)\n",
    "\n",
    "best = lgbo.train(params, trains, valid_sets=valids,\n",
    "                    verbose_eval=False,)\n",
    "best.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           mean         y     proba\n",
       "0      4.228942  4.634729  4.100452\n",
       "1      2.564596  2.484907  2.440408\n",
       "2      1.934328  1.609438  1.942481\n",
       "3      3.224041  3.178054  3.153585\n",
       "4      2.131268  2.639057  2.200531\n",
       "...         ...       ...       ...\n",
       "13601  2.830482  3.583519  2.916153\n",
       "13602  0.976814  1.098612  0.799299\n",
       "13603  3.156468  2.944439  2.962118\n",
       "13604  1.317303  2.708050  1.374299\n",
       "13605  2.289848  1.609438  2.424729\n",
       "\n",
       "[13606 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>y</th>\n      <th>proba</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.228942</td>\n      <td>4.634729</td>\n      <td>4.100452</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.564596</td>\n      <td>2.484907</td>\n      <td>2.440408</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.934328</td>\n      <td>1.609438</td>\n      <td>1.942481</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.224041</td>\n      <td>3.178054</td>\n      <td>3.153585</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.131268</td>\n      <td>2.639057</td>\n      <td>2.200531</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13601</th>\n      <td>2.830482</td>\n      <td>3.583519</td>\n      <td>2.916153</td>\n    </tr>\n    <tr>\n      <th>13602</th>\n      <td>0.976814</td>\n      <td>1.098612</td>\n      <td>0.799299</td>\n    </tr>\n    <tr>\n      <th>13603</th>\n      <td>3.156468</td>\n      <td>2.944439</td>\n      <td>2.962118</td>\n    </tr>\n    <tr>\n      <th>13604</th>\n      <td>1.317303</td>\n      <td>2.708050</td>\n      <td>1.374299</td>\n    </tr>\n    <tr>\n      <th>13605</th>\n      <td>2.289848</td>\n      <td>1.609438</td>\n      <td>2.424729</td>\n    </tr>\n  </tbody>\n</table>\n<p>13606 rows  3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 159
    }
   ],
   "source": [
    "pd.concat([X_test.reset_index(drop=True), pd.DataFrame(y_test).rename(columns={0:'y'}), pd.DataFrame(pred).rename(columns={0:'proba'})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "mean    float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "source": [
    "X_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(13606, 13606, 13606)"
      ]
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "source": [
    "len(X_test), len(y_test), len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([3.98169748, 2.50585187, 1.97973932, ..., 3.16836493, 1.47949482,\n",
       "       2.46380513])"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "pred"
   ]
  }
 ]
}