{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import Pool\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error,mean_squared_log_error\n",
    "import optuna.integration.lightgbm as lgbo\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "import optuna\n",
    "import umap\n",
    "pd.set_option('display.max_columns', 2000)\n",
    "\n",
    "SEED = 777\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(851524, 1387)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df_train = pd.read_pickle('../data/train/full_comp.pickle')\n",
    "df_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dorop_cols = []\n",
    "for col in df_train.columns:\n",
    "    if '_x' in col:\n",
    "        dorop_cols.append(col)\n",
    "len(dorop_cols), dorop_cols\n",
    "df_train = df_train.drop(dorop_cols, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(851524, 1324)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((334563, 1324), (15117, 1324))"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df_pre_train = df_train[df_train['cites'].isnull()].reset_index(drop=True).sample(frac=0.4, random_state=777)\n",
    "df_pre_valid = df_train[df_train['cites'].isnull()==False].reset_index(drop=True)\n",
    "df_pre_train.shape, df_pre_valid.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_col = []\n",
    "for col in df_train.columns:\n",
    "    if 'doi_cites' in col:\n",
    "        del_col.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "24  roberta_vec_225  roberta_vec_226  \\\n",
       "0         0.008933        -0.082399         0.111505        -0.004161   \n",
       "\n",
       "   roberta_vec_227  roberta_vec_228  roberta_vec_229  roberta_vec_230  \\\n",
       "0        -0.036977         0.049074        -0.154588        -0.005269   \n",
       "\n",
       "   roberta_vec_231  roberta_vec_232  roberta_vec_233  roberta_vec_234  \\\n",
       "0          0.33097         0.027778         0.042298         0.154414   \n",
       "\n",
       "   roberta_vec_235  roberta_vec_236  roberta_vec_237  roberta_vec_238  \\\n",
       "0        -0.188538          0.03051         -0.01406        -0.051922   \n",
       "\n",
       "   roberta_vec_239  roberta_vec_240  roberta_vec_241  roberta_vec_242  \\\n",
       "0        -0.096339        -0.274143         0.021812         0.053583   \n",
       "\n",
       "   roberta_vec_243  roberta_vec_244  roberta_vec_245  roberta_vec_246  \\\n",
       "0           0.0859        -0.060149         0.047214        -0.007236   \n",
       "\n",
       "   roberta_vec_247  roberta_vec_248  roberta_vec_249  roberta_vec_250  \\\n",
       "0        -0.075574        -0.051089        -0.089726         0.009813   \n",
       "\n",
       "   roberta_vec_251  roberta_vec_252  roberta_vec_253  roberta_vec_254  \\\n",
       "0         -0.01094        -0.001022        -0.015114         0.256334   \n",
       "\n",
       "   roberta_vec_255  roberta_vec_256  roberta_vec_257  roberta_vec_258  \\\n",
       "0        -0.118728        -0.047792         0.032125        -0.162749   \n",
       "\n",
       "   roberta_vec_259  roberta_vec_260  roberta_vec_261  roberta_vec_262  \\\n",
       "0         0.318361        -0.035677         -0.04819         0.147017   \n",
       "\n",
       "   roberta_vec_263  roberta_vec_264  roberta_vec_265  roberta_vec_266  \\\n",
       "0        -0.047834        -0.039544        -0.104349        -0.109669   \n",
       "\n",
       "   roberta_vec_267  roberta_vec_268  roberta_vec_269  roberta_vec_270  \\\n",
       "0        -0.027195         0.132058         0.014702        -0.055527   \n",
       "\n",
       "   roberta_vec_271  roberta_vec_272  roberta_vec_273  roberta_vec_274  \\\n",
       "0         0.026971        -0.054908        -0.061237        -0.063213   \n",
       "\n",
       "   roberta_vec_275  roberta_vec_276  roberta_vec_277  roberta_vec_278  \\\n",
       "0          0.29448         0.358346        -0.009177         -0.02539   \n",
       "\n",
       "   roberta_vec_279  roberta_vec_280  roberta_vec_281  roberta_vec_282  \\\n",
       "0        -0.037266         0.214616        -0.145413          0.11474   \n",
       "\n",
       "   roberta_vec_283  roberta_vec_284  roberta_vec_285  roberta_vec_286  \\\n",
       "0        -0.017633         0.018637         0.047162         0.059369   \n",
       "\n",
       "   roberta_vec_287  roberta_vec_288  roberta_vec_289  roberta_vec_290  \\\n",
       "0         0.021426        -0.033502         0.075583        -0.066334   \n",
       "\n",
       "   roberta_vec_291  roberta_vec_292  roberta_vec_293  roberta_vec_294  \\\n",
       "0         0.062335         -0.09216        -0.021661         0.096643   \n",
       "\n",
       "   roberta_vec_295  roberta_vec_296  roberta_vec_297  roberta_vec_298  \\\n",
       "0         0.090622         0.158539        -0.025748        -0.072213   \n",
       "\n",
       "   roberta_vec_299  roberta_vec_300  roberta_vec_301  roberta_vec_302  \\\n",
       "0        -0.108713          0.16286        -0.022591        -0.037101   \n",
       "\n",
       "   roberta_vec_303  roberta_vec_304  roberta_vec_305  roberta_vec_306  \\\n",
       "0        -0.014994        -0.008607         0.071167        -0.183716   \n",
       "\n",
       "   roberta_vec_307  roberta_vec_308  roberta_vec_309  roberta_vec_310  \\\n",
       "0        -0.007629         0.134016         0.082213        -0.111802   \n",
       "\n",
       "   roberta_vec_311  roberta_vec_312  roberta_vec_313  roberta_vec_314  \\\n",
       "0        -0.008389         -0.06662         -0.08155         0.029572   \n",
       "\n",
       "   roberta_vec_315  roberta_vec_316  roberta_vec_317  roberta_vec_318  \\\n",
       "0         0.189175        -0.084198        -0.055908        -0.009681   \n",
       "\n",
       "   roberta_vec_319  roberta_vec_320  roberta_vec_321  roberta_vec_322  \\\n",
       "0        -0.078716        -0.174606        -0.072681        -0.138039   \n",
       "\n",
       "   roberta_vec_323  roberta_vec_324  roberta_vec_325  roberta_vec_326  \\\n",
       "0        -0.089501        -0.231091           0.0599        -0.156146   \n",
       "\n",
       "   roberta_vec_327  roberta_vec_328  roberta_vec_329  roberta_vec_330  \\\n",
       "0         0.057016         0.162935         0.182202          0.62401   \n",
       "\n",
       "   roberta_vec_331  roberta_vec_332  roberta_vec_333  roberta_vec_334  \\\n",
       "0         0.425136         0.100822         0.002127         0.141032   \n",
       "\n",
       "   roberta_vec_335  roberta_vec_336  roberta_vec_337  roberta_vec_338  \\\n",
       "0        -0.136996        -0.059905         0.155125         0.170771   \n",
       "\n",
       "   roberta_vec_339  roberta_vec_340  roberta_vec_341  roberta_vec_342  \\\n",
       "0         0.056534        -0.136751         0.006972        -0.012448   \n",
       "\n",
       "   roberta_vec_343  roberta_vec_344  roberta_vec_345  roberta_vec_346  \\\n",
       "0          0.01365         0.193015         0.014073         0.092219   \n",
       "\n",
       "   roberta_vec_347  roberta_vec_348  roberta_vec_349  roberta_vec_350  \\\n",
       "0        -0.004052         0.277142        -0.150453         -0.17078   \n",
       "\n",
       "   roberta_vec_351  roberta_vec_352  roberta_vec_353  roberta_vec_354  \\\n",
       "0        -0.110611          0.01494        -0.320114        -0.028465   \n",
       "\n",
       "   roberta_vec_355  roberta_vec_356  roberta_vec_357  roberta_vec_358  \\\n",
       "0          0.23229        -0.095811          0.04421        -0.213199   \n",
       "\n",
       "   roberta_vec_359  roberta_vec_360  roberta_vec_361  roberta_vec_362  \\\n",
       "0         0.068205         0.076826         0.200793         0.021035   \n",
       "\n",
       "   roberta_vec_363  roberta_vec_364  roberta_vec_365  roberta_vec_366  \\\n",
       "0         0.129505         0.104688        -0.140625         0.072766   \n",
       "\n",
       "   roberta_vec_367  roberta_vec_368  roberta_vec_369  roberta_vec_370  \\\n",
       "0         0.004655         0.176511        -0.004922        -0.034623   \n",
       "\n",
       "   roberta_vec_371  roberta_vec_372  roberta_vec_373  roberta_vec_374  \\\n",
       "0         0.010512        -0.051318          -0.0018         0.047339   \n",
       "\n",
       "   roberta_vec_375  roberta_vec_376  roberta_vec_377  roberta_vec_378  \\\n",
       "0        -0.068834          0.19413        -0.076413         0.028653   \n",
       "\n",
       "   roberta_vec_379  roberta_vec_380  roberta_vec_381  roberta_vec_382  \\\n",
       "0         0.007937         0.094334         0.051268         0.020364   \n",
       "\n",
       "   roberta_vec_383  roberta_vec_384  roberta_vec_385  roberta_vec_386  \\\n",
       "0        -0.026033          0.30898        -0.094115         0.308518   \n",
       "\n",
       "   roberta_vec_387  roberta_vec_388  roberta_vec_389  roberta_vec_390  \\\n",
       "0         0.193439        -0.095692         -0.05352         0.002016   \n",
       "\n",
       "   roberta_vec_391  roberta_vec_392  roberta_vec_393  roberta_vec_394  \\\n",
       "0        -0.064693         0.122262        -0.120535         0.041815   \n",
       "\n",
       "   roberta_vec_395  roberta_vec_396  roberta_vec_397  roberta_vec_398  \\\n",
       "0        -0.054836         0.007822        -0.426448         0.047687   \n",
       "\n",
       "   roberta_vec_399  roberta_vec_400  roberta_vec_401  roberta_vec_402  \\\n",
       "0         0.015008        -0.052534         0.140038          0.02585   \n",
       "\n",
       "   roberta_vec_403  roberta_vec_404  roberta_vec_405  roberta_vec_406  \\\n",
       "0         -0.11479         0.053117         0.153543        -0.095239   \n",
       "\n",
       "   roberta_vec_407  roberta_vec_408  roberta_vec_409  roberta_vec_410  \\\n",
       "0        -0.163483        -0.026048         0.162395         0.047188   \n",
       "\n",
       "   roberta_vec_411  roberta_vec_412  roberta_vec_413  roberta_vec_414  \\\n",
       "0         0.073149        -0.008878        -0.032284        -0.084181   \n",
       "\n",
       "   roberta_vec_415  roberta_vec_416  roberta_vec_417  roberta_vec_418  \\\n",
       "0         0.008437         0.006305        -0.028463        -0.281973   \n",
       "\n",
       "   roberta_vec_419  roberta_vec_420  roberta_vec_421  roberta_vec_422  \\\n",
       "0        -0.162848        -0.154905        -0.123429         0.074748   \n",
       "\n",
       "   roberta_vec_423  roberta_vec_424  roberta_vec_425  roberta_vec_426  \\\n",
       "0        -0.053285        -0.440077          -0.0793         0.107435   \n",
       "\n",
       "   roberta_vec_427  roberta_vec_428  roberta_vec_429  roberta_vec_430  \\\n",
       "0        -0.146091         -0.00265         0.058089         0.160322   \n",
       "\n",
       "   roberta_vec_431  roberta_vec_432  roberta_vec_433  roberta_vec_434  \\\n",
       "0         0.056062        -0.024009         -0.04205        -0.139201   \n",
       "\n",
       "   roberta_vec_435  roberta_vec_436  roberta_vec_437  roberta_vec_438  \\\n",
       "0        -0.181151         0.117982        -0.014689        -0.114674   \n",
       "\n",
       "   roberta_vec_439  roberta_vec_440  roberta_vec_441  roberta_vec_442  \\\n",
       "0        -0.019894         -0.40249         0.001017        -0.149398   \n",
       "\n",
       "   roberta_vec_443  roberta_vec_444  roberta_vec_445  roberta_vec_446  \\\n",
       "0        -0.033341        -0.108509         -0.11537         0.057975   \n",
       "\n",
       "   roberta_vec_447  roberta_vec_448  roberta_vec_449  roberta_vec_450  \\\n",
       "0        -0.065308        -0.108287          0.03235        -0.104107   \n",
       "\n",
       "   roberta_vec_451  roberta_vec_452  roberta_vec_453  roberta_vec_454  \\\n",
       "0         0.015036        -0.196914        -1.347596         0.212447   \n",
       "\n",
       "   roberta_vec_455  roberta_vec_456  roberta_vec_457  roberta_vec_458  \\\n",
       "0         0.166778        -0.031664         0.022859        -0.037949   \n",
       "\n",
       "   roberta_vec_459  roberta_vec_460  roberta_vec_461  roberta_vec_462  \\\n",
       "0        -0.144512         0.182012        -0.054833         0.081778   \n",
       "\n",
       "   roberta_vec_463  roberta_vec_464  roberta_vec_465  roberta_vec_466  \\\n",
       "0        -0.187795         0.076714        -0.174939           0.2275   \n",
       "\n",
       "   roberta_vec_467  roberta_vec_468  roberta_vec_469  roberta_vec_470  \\\n",
       "0        -0.015552        -0.100666         0.072337         0.144437   \n",
       "\n",
       "   roberta_vec_471  roberta_vec_472  roberta_vec_473  roberta_vec_474  \\\n",
       "0        -0.052423        -0.183578        -0.171214        -0.049964   \n",
       "\n",
       "   roberta_vec_475  roberta_vec_476  roberta_vec_477  roberta_vec_478  \\\n",
       "0        -0.044868        -0.018483         0.296564         0.072446   \n",
       "\n",
       "   roberta_vec_479  roberta_vec_480  roberta_vec_481  roberta_vec_482  \\\n",
       "0        -0.022746        -0.033217        -0.068958         0.101734   \n",
       "\n",
       "   roberta_vec_483  roberta_vec_484  roberta_vec_485  roberta_vec_486  \\\n",
       "0         0.099974         0.015855        -0.128012        -0.034076   \n",
       "\n",
       "   roberta_vec_487  roberta_vec_488  roberta_vec_489  roberta_vec_490  \\\n",
       "0          0.05335         0.158108        -0.105041        -0.104255   \n",
       "\n",
       "   roberta_vec_491  roberta_vec_492  roberta_vec_493  roberta_vec_494  \\\n",
       "0        -0.083309         0.177937         0.150279         0.201614   \n",
       "\n",
       "   roberta_vec_495  roberta_vec_496  roberta_vec_497  roberta_vec_498  \\\n",
       "0         0.027929          0.37666        -0.152545        -0.155271   \n",
       "\n",
       "   roberta_vec_499  roberta_vec_500  roberta_vec_501  roberta_vec_502  \\\n",
       "0         0.043854        -0.028725         0.061899         0.019833   \n",
       "\n",
       "   roberta_vec_503  roberta_vec_504  roberta_vec_505  roberta_vec_506  \\\n",
       "0         0.082157        -0.035213        -0.019426         -0.14763   \n",
       "\n",
       "   roberta_vec_507  roberta_vec_508  roberta_vec_509  roberta_vec_510  \\\n",
       "0         0.056121         0.073331        -0.023424        -0.227543   \n",
       "\n",
       "   roberta_vec_511  roberta_vec_512  roberta_vec_513  roberta_vec_514  \\\n",
       "0        -0.000602         0.121111         0.038206        -0.008902   \n",
       "\n",
       "   roberta_vec_515  roberta_vec_516  roberta_vec_517  roberta_vec_518  \\\n",
       "0         0.024064        -0.037369        -0.074584         0.066258   \n",
       "\n",
       "   roberta_vec_519  roberta_vec_520  roberta_vec_521  roberta_vec_522  \\\n",
       "0         0.241208        -0.019344         0.078863         0.062034   \n",
       "\n",
       "   roberta_vec_523  roberta_vec_524  roberta_vec_525  roberta_vec_526  \\\n",
       "0         0.013586        -0.020986         0.024147        -0.012329   \n",
       "\n",
       "   roberta_vec_527  roberta_vec_528  roberta_vec_529  roberta_vec_530  \\\n",
       "0         0.041916          0.01863         0.103666         0.267286   \n",
       "\n",
       "   roberta_vec_531  roberta_vec_532  roberta_vec_533  roberta_vec_534  \\\n",
       "0        -0.004326         0.175795        -0.048771        -0.111534   \n",
       "\n",
       "   roberta_vec_535  roberta_vec_536  roberta_vec_537  roberta_vec_538  \\\n",
       "0         0.042488         0.043012        -0.139547        -0.041013   \n",
       "\n",
       "   roberta_vec_539  roberta_vec_540  roberta_vec_541  roberta_vec_542  \\\n",
       "0        -0.129658        -0.101868         0.148641        -0.034011   \n",
       "\n",
       "   roberta_vec_543  roberta_vec_544  roberta_vec_545  roberta_vec_546  \\\n",
       "0         0.026582         0.050715        -0.080829         0.164848   \n",
       "\n",
       "   roberta_vec_547  roberta_vec_548  roberta_vec_549  roberta_vec_550  \\\n",
       "0         0.020371           0.0302         0.065473         0.011906   \n",
       "\n",
       "   roberta_vec_551  roberta_vec_552  roberta_vec_553  roberta_vec_554  \\\n",
       "0         -0.23679         0.132766         0.097148         0.023719   \n",
       "\n",
       "   roberta_vec_555  roberta_vec_556  roberta_vec_557  roberta_vec_558  \\\n",
       "0         0.192919         0.194037        -0.051655        -0.142407   \n",
       "\n",
       "   roberta_vec_559  roberta_vec_560  roberta_vec_561  roberta_vec_562  \\\n",
       "0         0.245611        -0.047618        -0.016797        -0.046147   \n",
       "\n",
       "   roberta_vec_563  roberta_vec_564  roberta_vec_565  roberta_vec_566  \\\n",
       "0         0.004564         0.025534        -0.070372         0.030484   \n",
       "\n",
       "   roberta_vec_567  roberta_vec_568  roberta_vec_569  roberta_vec_570  \\\n",
       "0        -0.007294        -0.008978        -0.083971         1.304693   \n",
       "\n",
       "   roberta_vec_571  roberta_vec_572  roberta_vec_573  roberta_vec_574  \\\n",
       "0         0.116037         0.014827        -0.145521         0.185063   \n",
       "\n",
       "   roberta_vec_575  roberta_vec_576  roberta_vec_577  roberta_vec_578  \\\n",
       "0         0.063887         0.015271        -0.126791         0.113292   \n",
       "\n",
       "   roberta_vec_579  roberta_vec_580  roberta_vec_581  roberta_vec_582  \\\n",
       "0         0.000877         0.260103        -0.021803        -0.005241   \n",
       "\n",
       "   roberta_vec_583  roberta_vec_584  roberta_vec_585  roberta_vec_586  \\\n",
       "0         0.012979        -0.040793        -0.197163        -0.093093   \n",
       "\n",
       "   roberta_vec_587  roberta_vec_588  roberta_vec_589  roberta_vec_590  \\\n",
       "0         0.019604        11.406745        -0.001576         0.009103   \n",
       "\n",
       "   roberta_vec_591  roberta_vec_592  roberta_vec_593  roberta_vec_594  \\\n",
       "0         0.069717         -0.11638          0.02271        -0.009541   \n",
       "\n",
       "   roberta_vec_595  roberta_vec_596  roberta_vec_597  roberta_vec_598  \\\n",
       "0         0.030983         0.143954         0.074972         0.060721   \n",
       "\n",
       "   roberta_vec_599  roberta_vec_600  roberta_vec_601  roberta_vec_602  \\\n",
       "0        -0.046683         0.148974        -0.080296         0.060301   \n",
       "\n",
       "   roberta_vec_603  roberta_vec_604  roberta_vec_605  roberta_vec_606  \\\n",
       "0         0.032032         0.081368         0.086436         0.099291   \n",
       "\n",
       "   roberta_vec_607  roberta_vec_608  roberta_vec_609  roberta_vec_610  \\\n",
       "0        -0.153126         0.092218         0.076372         0.109919   \n",
       "\n",
       "   roberta_vec_611  roberta_vec_612  roberta_vec_613  roberta_vec_614  \\\n",
       "0         0.777324        -0.112435         0.144448        -0.006802   \n",
       "\n",
       "   roberta_vec_615  roberta_vec_616  roberta_vec_617  roberta_vec_618  \\\n",
       "0        -0.016929        -0.025625         0.100593         0.054845   \n",
       "\n",
       "   roberta_vec_619  roberta_vec_620  roberta_vec_621  roberta_vec_622  \\\n",
       "0         0.227658         0.099402        -0.053498         0.096838   \n",
       "\n",
       "   roberta_vec_623  roberta_vec_624  roberta_vec_625  roberta_vec_626  \\\n",
       "0        -0.122846        -0.213329          0.13427          0.05979   \n",
       "\n",
       "   roberta_vec_627  roberta_vec_628  roberta_vec_629  roberta_vec_630  \\\n",
       "0         0.052141         -0.00279         -0.05567         0.182667   \n",
       "\n",
       "   roberta_vec_631  roberta_vec_632  roberta_vec_633  roberta_vec_634  \\\n",
       "0        -0.222613         0.039218        -0.059391         0.290423   \n",
       "\n",
       "   roberta_vec_635  roberta_vec_636  roberta_vec_637  roberta_vec_638  \\\n",
       "0         0.122479         0.007779         0.019957         -0.07321   \n",
       "\n",
       "   roberta_vec_639  roberta_vec_640  roberta_vec_641  roberta_vec_642  \\\n",
       "0         0.019736         0.088344        -0.171627        -0.050579   \n",
       "\n",
       "   roberta_vec_643  roberta_vec_644  roberta_vec_645  roberta_vec_646  \\\n",
       "0        -0.108241         0.032379        -0.132383        -0.021107   \n",
       "\n",
       "   roberta_vec_647  roberta_vec_648  roberta_vec_649  roberta_vec_650  \\\n",
       "0        -0.026656        -0.066542         -0.14062         0.068314   \n",
       "\n",
       "   roberta_vec_651  roberta_vec_652  roberta_vec_653  roberta_vec_654  \\\n",
       "0         0.004812        -0.016412         0.185605          0.10553   \n",
       "\n",
       "   roberta_vec_655  roberta_vec_656  roberta_vec_657  roberta_vec_658  \\\n",
       "0         0.001893        -0.111575        -0.084433        -0.002476   \n",
       "\n",
       "   roberta_vec_659  roberta_vec_660  roberta_vec_661  roberta_vec_662  \\\n",
       "0        -0.057709          0.01483        -0.138764        -0.222441   \n",
       "\n",
       "   roberta_vec_663  roberta_vec_664  roberta_vec_665  roberta_vec_666  \\\n",
       "0        -0.170947        -0.499743        -0.191339        -0.078408   \n",
       "\n",
       "   roberta_vec_667  roberta_vec_668  roberta_vec_669  roberta_vec_670  \\\n",
       "0        -0.378901         0.087936        -0.070182        -0.114958   \n",
       "\n",
       "   roberta_vec_671  roberta_vec_672  roberta_vec_673  roberta_vec_674  \\\n",
       "0         0.073736        -0.395247         0.114391        -0.142501   \n",
       "\n",
       "   roberta_vec_675  roberta_vec_676  roberta_vec_677  roberta_vec_678  \\\n",
       "0         0.029733        -0.127189         0.196349         0.111586   \n",
       "\n",
       "   roberta_vec_679  roberta_vec_680  roberta_vec_681  roberta_vec_682  \\\n",
       "0         0.101117        -0.010446        -0.116452         0.077068   \n",
       "\n",
       "   roberta_vec_683  roberta_vec_684  roberta_vec_685  roberta_vec_686  \\\n",
       "0        -0.051625         0.050996        -0.055594         0.124248   \n",
       "\n",
       "   roberta_vec_687  roberta_vec_688  roberta_vec_689  roberta_vec_690  \\\n",
       "0        -0.249727        -0.315202        -0.031932        -0.047831   \n",
       "\n",
       "   roberta_vec_691  roberta_vec_692  roberta_vec_693  roberta_vec_694  \\\n",
       "0         0.133403        -0.121233         0.188389         0.022408   \n",
       "\n",
       "   roberta_vec_695  roberta_vec_696  roberta_vec_697  roberta_vec_698  \\\n",
       "0        -0.072894          0.01557         0.019908         0.000764   \n",
       "\n",
       "   roberta_vec_699  roberta_vec_700  roberta_vec_701  roberta_vec_702  \\\n",
       "0         0.043054        -0.211885          0.02381        -0.010709   \n",
       "\n",
       "   roberta_vec_703  roberta_vec_704  roberta_vec_705  roberta_vec_706  \\\n",
       "0        -0.141905         0.087001         0.026661         0.035089   \n",
       "\n",
       "   roberta_vec_707  roberta_vec_708  roberta_vec_709  roberta_vec_710  \\\n",
       "0         0.132267        -0.009111         0.065618        -0.040556   \n",
       "\n",
       "   roberta_vec_711  roberta_vec_712  roberta_vec_713  roberta_vec_714  \\\n",
       "0        -0.068752          -0.0509         0.023299         -0.22655   \n",
       "\n",
       "   roberta_vec_715  roberta_vec_716  roberta_vec_717  roberta_vec_718  \\\n",
       "0        -0.060595        -0.111451        -0.038315         -0.22186   \n",
       "\n",
       "   roberta_vec_719  roberta_vec_720  roberta_vec_721  roberta_vec_722  \\\n",
       "0         0.064387         0.237368         0.070414        -0.043413   \n",
       "\n",
       "   roberta_vec_723  roberta_vec_724  roberta_vec_725  roberta_vec_726  \\\n",
       "0         0.049146        -0.108834         0.060299         0.236724   \n",
       "\n",
       "   roberta_vec_727  roberta_vec_728  roberta_vec_729  roberta_vec_730  \\\n",
       "0         0.153273        -0.048493         0.071393         -0.13742   \n",
       "\n",
       "   roberta_vec_731  roberta_vec_732  roberta_vec_733  roberta_vec_734  \\\n",
       "0        -0.253319         0.117817        -0.173621         0.163909   \n",
       "\n",
       "   roberta_vec_735  roberta_vec_736  roberta_vec_737  roberta_vec_738  \\\n",
       "0         0.327391         0.077823         0.129044        -0.030003   \n",
       "\n",
       "   roberta_vec_739  roberta_vec_740  roberta_vec_741  roberta_vec_742  \\\n",
       "0        -0.139961        -0.198641        -0.338671         0.100887   \n",
       "\n",
       "   roberta_vec_743  roberta_vec_744  roberta_vec_745  roberta_vec_746  \\\n",
       "0         0.011717        -0.131644         0.089834         0.063055   \n",
       "\n",
       "   roberta_vec_747  roberta_vec_748  roberta_vec_749  roberta_vec_750  \\\n",
       "0        -0.182048         0.451386        -0.434589         -0.08692   \n",
       "\n",
       "   roberta_vec_751  roberta_vec_752  roberta_vec_753  roberta_vec_754  \\\n",
       "0        -0.031759        -0.051711         0.093028         0.203682   \n",
       "\n",
       "   roberta_vec_755  roberta_vec_756  roberta_vec_757  roberta_vec_758  \\\n",
       "0         0.021781         0.025653         0.132081        -0.090815   \n",
       "\n",
       "   roberta_vec_759  roberta_vec_760  roberta_vec_761  roberta_vec_762  \\\n",
       "0         0.027593        -0.006494         0.045004         0.046405   \n",
       "\n",
       "   roberta_vec_763  roberta_vec_764  roberta_vec_765  roberta_vec_766  \\\n",
       "0           0.1948         0.237505         0.184351         0.028982   \n",
       "\n",
       "   roberta_vec_767  \n",
       "0        -0.053141  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>submitter</th>\n      <th>authors</th>\n      <th>title</th>\n      <th>comments</th>\n      <th>journal-ref</th>\n      <th>doi</th>\n      <th>report-no</th>\n      <th>categories</th>\n      <th>license</th>\n      <th>abstract</th>\n      <th>versions</th>\n      <th>authors_parsed</th>\n      <th>doi_cites</th>\n      <th>cites</th>\n      <th>doi_id</th>\n      <th>pub_publisher</th>\n      <th>pub_journals</th>\n      <th>pub_dois</th>\n      <th>update_date_y</th>\n      <th>first_created_date</th>\n      <th>last_created_date</th>\n      <th>update_year</th>\n      <th>first_created_year</th>\n      <th>last_created_year</th>\n      <th>update_month</th>\n      <th>first_created_month</th>\n      <th>last_created_month</th>\n      <th>update_ym</th>\n      <th>first_created_ym</th>\n      <th>last_created_ym</th>\n      <th>update_day</th>\n      <th>first_created_day</th>\n      <th>last_created_day</th>\n      <th>update_date_unixtime</th>\n      <th>first_created_unixtime</th>\n      <th>last_created_unixtime</th>\n      <th>diff_update_date_unixtime</th>\n      <th>diff_created_unixtime</th>\n      <th>num_created</th>\n      <th>update_date_days</th>\n      <th>first_created_days</th>\n      <th>last_created_days</th>\n      <th>diff_created_days</th>\n      <th>rate_created_days</th>\n      <th>author_first</th>\n      <th>author_num</th>\n      <th>category_main_detail</th>\n      <th>category_main</th>\n      <th>cs_y</th>\n      <th>econ_y</th>\n      <th>eess_y</th>\n      <th>math_y</th>\n      <th>nlin_y</th>\n      <th>physics_y</th>\n      <th>stat_y</th>\n      <th>category_name_parent_main_unique</th>\n      <th>category_name_parent_unique</th>\n      <th>category_name_unique</th>\n      <th>acc-phys_y</th>\n      <th>adap-org_y</th>\n      <th>alg-geom_y</th>\n      <th>ao-sci_y</th>\n      <th>astro-ph_y</th>\n      <th>atom-ph_y</th>\n      <th>bayes-an_y</th>\n      <th>chao-dyn_y</th>\n      <th>chem-ph_y</th>\n      <th>cmp-lg_y</th>\n      <th>comp-gas_y</th>\n      <th>cond-mat_y</th>\n      <th>dg-ga_y</th>\n      <th>funct-an_y</th>\n      <th>gr-qc_y</th>\n      <th>hep-ex_y</th>\n      <th>hep-lat_y</th>\n      <th>hep-ph_y</th>\n      <th>hep-th_y</th>\n      <th>math-ph_y</th>\n      <th>mtrl-th_y</th>\n      <th>nucl-ex_y</th>\n      <th>nucl-th_y</th>\n      <th>patt-sol_y</th>\n      <th>plasm-ph_y</th>\n      <th>q-alg_y</th>\n      <th>q-bio_y</th>\n      <th>q-fin_y</th>\n      <th>quant-ph_y</th>\n      <th>solv-int_y</th>\n      <th>supr-con_y</th>\n      <th>acc_y</th>\n      <th>adap_y</th>\n      <th>alg_y</th>\n      <th>ao_y</th>\n      <th>astro_y</th>\n      <th>atom_y</th>\n      <th>bayes_y</th>\n      <th>chao_y</th>\n      <th>chem_y</th>\n      <th>cmp_y</th>\n      <th>comp_y</th>\n      <th>cond_y</th>\n      <th>cs</th>\n      <th>dg_y</th>\n      <th>econ</th>\n      <th>eess</th>\n      <th>funct_y</th>\n      <th>gr_y</th>\n      <th>hep_y</th>\n      <th>math</th>\n      <th>mtrl_y</th>\n      <th>nlin</th>\n      <th>nucl_y</th>\n      <th>patt_y</th>\n      <th>physics</th>\n      <th>plasm_y</th>\n      <th>q_y</th>\n      <th>quant_y</th>\n      <th>solv_y</th>\n      <th>stat</th>\n      <th>supr_y</th>\n      <th>astro-ph.co</th>\n      <th>astro-ph.ep</th>\n      <th>astro-ph.ga</th>\n      <th>astro-ph.he</th>\n      <th>astro-ph.im</th>\n      <th>astro-ph.sr</th>\n      <th>cond-mat.dis-nn</th>\n      <th>cond-mat.mes-hall</th>\n      <th>cond-mat.mtrl-sci</th>\n      <th>cond-mat.other</th>\n      <th>cond-mat.quant-gas</th>\n      <th>cond-mat.soft</th>\n      <th>cond-mat.stat-mech</th>\n      <th>cond-mat.str-el</th>\n      <th>cond-mat.supr-con</th>\n      <th>cs.ai</th>\n      <th>cs.ar</th>\n      <th>cs.cc</th>\n      <th>cs.ce</th>\n      <th>cs.cg</th>\n      <th>cs.cl</th>\n      <th>cs.cr</th>\n      <th>cs.cv</th>\n      <th>cs.cy</th>\n      <th>cs.db</th>\n      <th>cs.dc</th>\n      <th>cs.dl</th>\n      <th>cs.dm</th>\n      <th>cs.ds</th>\n      <th>cs.et</th>\n      <th>cs.fl</th>\n      <th>cs.gl</th>\n      <th>cs.gr</th>\n      <th>cs.gt</th>\n      <th>cs.hc</th>\n      <th>cs.ir</th>\n      <th>cs.it</th>\n      <th>cs.lg</th>\n      <th>cs.lo</th>\n      <th>cs.ma</th>\n      <th>cs.mm</th>\n      <th>cs.ms</th>\n      <th>cs.na</th>\n      <th>cs.ne</th>\n      <th>cs.ni</th>\n      <th>cs.oh</th>\n      <th>cs.os</th>\n      <th>cs.pf</th>\n      <th>cs.pl</th>\n      <th>cs.ro</th>\n      <th>cs.sc</th>\n      <th>cs.sd</th>\n      <th>cs.se</th>\n      <th>cs.si</th>\n      <th>cs.sy</th>\n      <th>econ.em</th>\n      <th>econ.gn</th>\n      <th>econ.th</th>\n      <th>eess.as</th>\n      <th>eess.iv</th>\n      <th>eess.sp</th>\n      <th>eess.sy</th>\n      <th>math.ac</th>\n      <th>math.ag</th>\n      <th>math.ap</th>\n      <th>math.at</th>\n      <th>math.ca</th>\n      <th>math.co</th>\n      <th>math.ct</th>\n      <th>math.cv</th>\n      <th>math.dg</th>\n      <th>math.ds</th>\n      <th>math.fa</th>\n      <th>math.gm</th>\n      <th>math.gn</th>\n      <th>math.gr</th>\n      <th>math.gt</th>\n      <th>math.ho</th>\n      <th>math.it</th>\n      <th>math.kt</th>\n      <th>math.lo</th>\n      <th>math.mg</th>\n      <th>math.mp</th>\n      <th>math.na</th>\n      <th>math.nt</th>\n      <th>math.oa</th>\n      <th>math.oc</th>\n      <th>math.pr</th>\n      <th>math.qa</th>\n      <th>math.ra</th>\n      <th>math.rt</th>\n      <th>math.sg</th>\n      <th>math.sp</th>\n      <th>math.st</th>\n      <th>nlin.ao</th>\n      <th>nlin.cd</th>\n      <th>nlin.cg</th>\n      <th>nlin.ps</th>\n      <th>nlin.si</th>\n      <th>physics.acc-ph</th>\n      <th>physics.ao-ph</th>\n      <th>physics.app-ph</th>\n      <th>physics.atm-clus</th>\n      <th>physics.atom-ph</th>\n      <th>physics.bio-ph</th>\n      <th>physics.chem-ph</th>\n      <th>physics.class-ph</th>\n      <th>physics.comp-ph</th>\n      <th>physics.data-an</th>\n      <th>physics.ed-ph</th>\n      <th>physics.flu-dyn</th>\n      <th>physics.gen-ph</th>\n      <th>physics.geo-ph</th>\n      <th>physics.hist-ph</th>\n      <th>physics.ins-det</th>\n      <th>physics.med-ph</th>\n      <th>physics.optics</th>\n      <th>physics.plasm-ph</th>\n      <th>physics.pop-ph</th>\n      <th>physics.soc-ph</th>\n      <th>physics.space-ph</th>\n      <th>q-bio.bm</th>\n      <th>q-bio.cb</th>\n      <th>q-bio.gn</th>\n      <th>q-bio.mn</th>\n      <th>q-bio.nc</th>\n      <th>q-bio.ot</th>\n      <th>q-bio.pe</th>\n      <th>q-bio.qm</th>\n      <th>q-bio.sc</th>\n      <th>q-bio.to</th>\n      <th>q-fin.cp</th>\n      <th>q-fin.ec</th>\n      <th>q-fin.gn</th>\n      <th>q-fin.mf</th>\n      <th>q-fin.pm</th>\n      <th>q-fin.pr</th>\n      <th>q-fin.rm</th>\n      <th>q-fin.st</th>\n      <th>q-fin.tr</th>\n      <th>stat.ap</th>\n      <th>stat.co</th>\n      <th>stat.me</th>\n      <th>stat.ml</th>\n      <th>stat.ot</th>\n      <th>stat.th</th>\n      <th>submitter_label</th>\n      <th>doi_id_label</th>\n      <th>author_first_label</th>\n      <th>pub_publisher_label</th>\n      <th>license_label</th>\n      <th>category_main_label</th>\n      <th>category_main_detail_label</th>\n      <th>category_name_parent_label</th>\n      <th>category_name_parent_main_label</th>\n      <th>category_name_label</th>\n      <th>doi_cites_mean_author_first_label</th>\n      <th>doi_cites_count_author_first_label</th>\n      <th>doi_cites_sum_author_first_label</th>\n      <th>doi_cites_min_author_first_label</th>\n      <th>doi_cites_max_author_first_label</th>\n      <th>doi_cites_median_author_first_label</th>\n      <th>doi_cites_std_author_first_label</th>\n      <th>doi_cites_q10_author_first_label</th>\n      <th>doi_cites_q25_author_first_label</th>\n      <th>doi_cites_q75_author_first_label</th>\n      <th>doi_cites_mean_doi_id_label</th>\n      <th>doi_cites_count_doi_id_label</th>\n      <th>doi_cites_sum_doi_id_label</th>\n      <th>doi_cites_min_doi_id_label</th>\n      <th>doi_cites_max_doi_id_label</th>\n      <th>doi_cites_median_doi_id_label</th>\n      <th>doi_cites_std_doi_id_label</th>\n      <th>doi_cites_q10_doi_id_label</th>\n      <th>doi_cites_q25_doi_id_label</th>\n      <th>doi_cites_q75_doi_id_label</th>\n      <th>doi_cites_mean_pub_publisher_label</th>\n      <th>doi_cites_count_pub_publisher_label</th>\n      <th>doi_cites_sum_pub_publisher_label</th>\n      <th>doi_cites_min_pub_publisher_label</th>\n      <th>doi_cites_max_pub_publisher_label</th>\n      <th>doi_cites_median_pub_publisher_label</th>\n      <th>doi_cites_std_pub_publisher_label</th>\n      <th>doi_cites_q10_pub_publisher_label</th>\n      <th>doi_cites_q25_pub_publisher_label</th>\n      <th>doi_cites_q75_pub_publisher_label</th>\n      <th>doi_cites_mean_submitter_label</th>\n      <th>doi_cites_count_submitter_label</th>\n      <th>doi_cites_sum_submitter_label</th>\n      <th>doi_cites_min_submitter_label</th>\n      <th>doi_cites_max_submitter_label</th>\n      <th>doi_cites_median_submitter_label</th>\n      <th>doi_cites_std_submitter_label</th>\n      <th>doi_cites_q10_submitter_label</th>\n      <th>doi_cites_q25_submitter_label</th>\n      <th>doi_cites_q75_submitter_label</th>\n      <th>doi_cites_mean_update_ym</th>\n      <th>doi_cites_count_update_ym</th>\n      <th>doi_cites_sum_update_ym</th>\n      <th>doi_cites_min_update_ym</th>\n      <th>doi_cites_max_update_ym</th>\n      <th>doi_cites_median_update_ym</th>\n      <th>doi_cites_std_update_ym</th>\n      <th>doi_cites_q10_update_ym</th>\n      <th>doi_cites_q25_update_ym</th>\n      <th>doi_cites_q75_update_ym</th>\n      <th>doi_cites_mean_first_created_ym</th>\n      <th>doi_cites_count_first_created_ym</th>\n      <th>doi_cites_sum_first_created_ym</th>\n      <th>doi_cites_min_first_created_ym</th>\n      <th>doi_cites_max_first_created_ym</th>\n      <th>doi_cites_median_first_created_ym</th>\n      <th>doi_cites_std_first_created_ym</th>\n      <th>doi_cites_q10_first_created_ym</th>\n      <th>doi_cites_q25_first_created_ym</th>\n      <th>doi_cites_q75_first_created_ym</th>\n      <th>doi_cites_mean_license_label</th>\n      <th>doi_cites_count_license_label</th>\n      <th>doi_cites_sum_license_label</th>\n      <th>doi_cites_min_license_label</th>\n      <th>doi_cites_max_license_label</th>\n      <th>doi_cites_median_license_label</th>\n      <th>doi_cites_std_license_label</th>\n      <th>doi_cites_q10_license_label</th>\n      <th>doi_cites_q25_license_label</th>\n      <th>doi_cites_q75_license_label</th>\n      <th>doi_cites_mean_category_main_label</th>\n      <th>doi_cites_count_category_main_label</th>\n      <th>doi_cites_sum_category_main_label</th>\n      <th>doi_cites_min_category_main_label</th>\n      <th>doi_cites_max_category_main_label</th>\n      <th>doi_cites_median_category_main_label</th>\n      <th>doi_cites_std_category_main_label</th>\n      <th>doi_cites_q10_category_main_label</th>\n      <th>doi_cites_q25_category_main_label</th>\n      <th>doi_cites_q75_category_main_label</th>\n      <th>doi_cites_mean_category_main_detail_label</th>\n      <th>doi_cites_count_category_main_detail_label</th>\n      <th>doi_cites_sum_category_main_detail_label</th>\n      <th>doi_cites_min_category_main_detail_label</th>\n      <th>doi_cites_max_category_main_detail_label</th>\n      <th>doi_cites_median_category_main_detail_label</th>\n      <th>doi_cites_std_category_main_detail_label</th>\n      <th>doi_cites_q10_category_main_detail_label</th>\n      <th>doi_cites_q25_category_main_detail_label</th>\n      <th>doi_cites_q75_category_main_detail_label</th>\n      <th>doi_cites_mean_category_name_parent_label</th>\n      <th>doi_cites_count_category_name_parent_label</th>\n      <th>doi_cites_sum_category_name_parent_label</th>\n      <th>doi_cites_min_category_name_parent_label</th>\n      <th>doi_cites_max_category_name_parent_label</th>\n      <th>doi_cites_median_category_name_parent_label</th>\n      <th>doi_cites_std_category_name_parent_label</th>\n      <th>doi_cites_q10_category_name_parent_label</th>\n      <th>doi_cites_q25_category_name_parent_label</th>\n      <th>doi_cites_q75_category_name_parent_label</th>\n      <th>doi_cites_mean_category_name_parent_main_label</th>\n      <th>doi_cites_count_category_name_parent_main_label</th>\n      <th>doi_cites_sum_category_name_parent_main_label</th>\n      <th>doi_cites_min_category_name_parent_main_label</th>\n      <th>doi_cites_max_category_name_parent_main_label</th>\n      <th>doi_cites_median_category_name_parent_main_label</th>\n      <th>doi_cites_std_category_name_parent_main_label</th>\n      <th>doi_cites_q10_category_name_parent_main_label</th>\n      <th>doi_cites_q25_category_name_parent_main_label</th>\n      <th>doi_cites_q75_category_name_parent_main_label</th>\n      <th>doi_cites_mean_category_name_label</th>\n      <th>doi_cites_count_category_name_label</th>\n      <th>doi_cites_sum_category_name_label</th>\n      <th>doi_cites_min_category_name_label</th>\n      <th>doi_cites_max_category_name_label</th>\n      <th>doi_cites_median_category_name_label</th>\n      <th>doi_cites_std_category_name_label</th>\n      <th>doi_cites_q10_category_name_label</th>\n      <th>doi_cites_q25_category_name_label</th>\n      <th>doi_cites_q75_category_name_label</th>\n      <th>diff_doi_cites_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_main_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_category_main_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_main_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_main_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_doi_cites_mean_category_name_label</th>\n      <th>is_null_comments</th>\n      <th>is_null_report-no</th>\n      <th>is_null_journal-ref</th>\n      <th>roberta_vec_0</th>\n      <th>roberta_vec_1</th>\n      <th>roberta_vec_2</th>\n      <th>roberta_vec_3</th>\n      <th>roberta_vec_4</th>\n      <th>roberta_vec_5</th>\n      <th>roberta_vec_6</th>\n      <th>roberta_vec_7</th>\n      <th>roberta_vec_8</th>\n      <th>roberta_vec_9</th>\n      <th>roberta_vec_10</th>\n      <th>roberta_vec_11</th>\n      <th>roberta_vec_12</th>\n      <th>roberta_vec_13</th>\n      <th>roberta_vec_14</th>\n      <th>roberta_vec_15</th>\n      <th>roberta_vec_16</th>\n      <th>roberta_vec_17</th>\n      <th>roberta_vec_18</th>\n      <th>roberta_vec_19</th>\n      <th>roberta_vec_20</th>\n      <th>roberta_vec_21</th>\n      <th>roberta_vec_22</th>\n      <th>roberta_vec_23</th>\n      <th>roberta_vec_24</th>\n      <th>roberta_vec_25</th>\n      <th>roberta_vec_26</th>\n      <th>roberta_vec_27</th>\n      <th>roberta_vec_28</th>\n      <th>roberta_vec_29</th>\n      <th>roberta_vec_30</th>\n      <th>roberta_vec_31</th>\n      <th>roberta_vec_32</th>\n      <th>roberta_vec_33</th>\n      <th>roberta_vec_34</th>\n      <th>roberta_vec_35</th>\n      <th>roberta_vec_36</th>\n      <th>roberta_vec_37</th>\n      <th>roberta_vec_38</th>\n      <th>roberta_vec_39</th>\n      <th>roberta_vec_40</th>\n      <th>roberta_vec_41</th>\n      <th>roberta_vec_42</th>\n      <th>roberta_vec_43</th>\n      <th>roberta_vec_44</th>\n      <th>roberta_vec_45</th>\n      <th>roberta_vec_46</th>\n      <th>roberta_vec_47</th>\n      <th>roberta_vec_48</th>\n      <th>roberta_vec_49</th>\n      <th>roberta_vec_50</th>\n      <th>roberta_vec_51</th>\n      <th>roberta_vec_52</th>\n      <th>roberta_vec_53</th>\n      <th>roberta_vec_54</th>\n      <th>roberta_vec_55</th>\n      <th>roberta_vec_56</th>\n      <th>roberta_vec_57</th>\n      <th>roberta_vec_58</th>\n      <th>roberta_vec_59</th>\n      <th>roberta_vec_60</th>\n      <th>roberta_vec_61</th>\n      <th>roberta_vec_62</th>\n      <th>roberta_vec_63</th>\n      <th>roberta_vec_64</th>\n      <th>roberta_vec_65</th>\n      <th>roberta_vec_66</th>\n      <th>roberta_vec_67</th>\n      <th>roberta_vec_68</th>\n      <th>roberta_vec_69</th>\n      <th>roberta_vec_70</th>\n      <th>roberta_vec_71</th>\n      <th>roberta_vec_72</th>\n      <th>roberta_vec_73</th>\n      <th>roberta_vec_74</th>\n      <th>roberta_vec_75</th>\n      <th>roberta_vec_76</th>\n      <th>roberta_vec_77</th>\n      <th>roberta_vec_78</th>\n      <th>roberta_vec_79</th>\n      <th>roberta_vec_80</th>\n      <th>roberta_vec_81</th>\n      <th>roberta_vec_82</th>\n      <th>roberta_vec_83</th>\n      <th>roberta_vec_84</th>\n      <th>roberta_vec_85</th>\n      <th>roberta_vec_86</th>\n      <th>roberta_vec_87</th>\n      <th>roberta_vec_88</th>\n      <th>roberta_vec_89</th>\n      <th>roberta_vec_90</th>\n      <th>roberta_vec_91</th>\n      <th>roberta_vec_92</th>\n      <th>roberta_vec_93</th>\n      <th>roberta_vec_94</th>\n      <th>roberta_vec_95</th>\n      <th>roberta_vec_96</th>\n      <th>roberta_vec_97</th>\n      <th>roberta_vec_98</th>\n      <th>roberta_vec_99</th>\n      <th>roberta_vec_100</th>\n      <th>roberta_vec_101</th>\n      <th>roberta_vec_102</th>\n      <th>roberta_vec_103</th>\n      <th>roberta_vec_104</th>\n      <th>roberta_vec_105</th>\n      <th>roberta_vec_106</th>\n      <th>roberta_vec_107</th>\n      <th>roberta_vec_108</th>\n      <th>roberta_vec_109</th>\n      <th>roberta_vec_110</th>\n      <th>roberta_vec_111</th>\n      <th>roberta_vec_112</th>\n      <th>roberta_vec_113</th>\n      <th>roberta_vec_114</th>\n      <th>roberta_vec_115</th>\n      <th>roberta_vec_116</th>\n      <th>roberta_vec_117</th>\n      <th>roberta_vec_118</th>\n      <th>roberta_vec_119</th>\n      <th>roberta_vec_120</th>\n      <th>roberta_vec_121</th>\n      <th>roberta_vec_122</th>\n      <th>roberta_vec_123</th>\n      <th>roberta_vec_124</th>\n      <th>roberta_vec_125</th>\n      <th>roberta_vec_126</th>\n      <th>roberta_vec_127</th>\n      <th>roberta_vec_128</th>\n      <th>roberta_vec_129</th>\n      <th>roberta_vec_130</th>\n      <th>roberta_vec_131</th>\n      <th>roberta_vec_132</th>\n      <th>roberta_vec_133</th>\n      <th>roberta_vec_134</th>\n      <th>roberta_vec_135</th>\n      <th>roberta_vec_136</th>\n      <th>roberta_vec_137</th>\n      <th>roberta_vec_138</th>\n      <th>roberta_vec_139</th>\n      <th>roberta_vec_140</th>\n      <th>roberta_vec_141</th>\n      <th>roberta_vec_142</th>\n      <th>roberta_vec_143</th>\n      <th>roberta_vec_144</th>\n      <th>roberta_vec_145</th>\n      <th>roberta_vec_146</th>\n      <th>roberta_vec_147</th>\n      <th>roberta_vec_148</th>\n      <th>roberta_vec_149</th>\n      <th>roberta_vec_150</th>\n      <th>roberta_vec_151</th>\n      <th>roberta_vec_152</th>\n      <th>roberta_vec_153</th>\n      <th>roberta_vec_154</th>\n      <th>roberta_vec_155</th>\n      <th>roberta_vec_156</th>\n      <th>roberta_vec_157</th>\n      <th>roberta_vec_158</th>\n      <th>roberta_vec_159</th>\n      <th>roberta_vec_160</th>\n      <th>roberta_vec_161</th>\n      <th>roberta_vec_162</th>\n      <th>roberta_vec_163</th>\n      <th>roberta_vec_164</th>\n      <th>roberta_vec_165</th>\n      <th>roberta_vec_166</th>\n      <th>roberta_vec_167</th>\n      <th>roberta_vec_168</th>\n      <th>roberta_vec_169</th>\n      <th>roberta_vec_170</th>\n      <th>roberta_vec_171</th>\n      <th>roberta_vec_172</th>\n      <th>roberta_vec_173</th>\n      <th>roberta_vec_174</th>\n      <th>roberta_vec_175</th>\n      <th>roberta_vec_176</th>\n      <th>roberta_vec_177</th>\n      <th>roberta_vec_178</th>\n      <th>roberta_vec_179</th>\n      <th>roberta_vec_180</th>\n      <th>roberta_vec_181</th>\n      <th>roberta_vec_182</th>\n      <th>roberta_vec_183</th>\n      <th>roberta_vec_184</th>\n      <th>roberta_vec_185</th>\n      <th>roberta_vec_186</th>\n      <th>roberta_vec_187</th>\n      <th>roberta_vec_188</th>\n      <th>roberta_vec_189</th>\n      <th>roberta_vec_190</th>\n      <th>roberta_vec_191</th>\n      <th>roberta_vec_192</th>\n      <th>roberta_vec_193</th>\n      <th>roberta_vec_194</th>\n      <th>roberta_vec_195</th>\n      <th>roberta_vec_196</th>\n      <th>roberta_vec_197</th>\n      <th>roberta_vec_198</th>\n      <th>roberta_vec_199</th>\n      <th>roberta_vec_200</th>\n      <th>roberta_vec_201</th>\n      <th>roberta_vec_202</th>\n      <th>roberta_vec_203</th>\n      <th>roberta_vec_204</th>\n      <th>roberta_vec_205</th>\n      <th>roberta_vec_206</th>\n      <th>roberta_vec_207</th>\n      <th>roberta_vec_208</th>\n      <th>roberta_vec_209</th>\n      <th>roberta_vec_210</th>\n      <th>roberta_vec_211</th>\n      <th>roberta_vec_212</th>\n      <th>roberta_vec_213</th>\n      <th>roberta_vec_214</th>\n      <th>roberta_vec_215</th>\n      <th>roberta_vec_216</th>\n      <th>roberta_vec_217</th>\n      <th>roberta_vec_218</th>\n      <th>roberta_vec_219</th>\n      <th>roberta_vec_220</th>\n      <th>roberta_vec_221</th>\n      <th>roberta_vec_222</th>\n      <th>roberta_vec_223</th>\n      <th>roberta_vec_224</th>\n      <th>roberta_vec_225</th>\n      <th>roberta_vec_226</th>\n      <th>roberta_vec_227</th>\n      <th>roberta_vec_228</th>\n      <th>roberta_vec_229</th>\n      <th>roberta_vec_230</th>\n      <th>roberta_vec_231</th>\n      <th>roberta_vec_232</th>\n      <th>roberta_vec_233</th>\n      <th>roberta_vec_234</th>\n      <th>roberta_vec_235</th>\n      <th>roberta_vec_236</th>\n      <th>roberta_vec_237</th>\n      <th>roberta_vec_238</th>\n      <th>roberta_vec_239</th>\n      <th>roberta_vec_240</th>\n      <th>roberta_vec_241</th>\n      <th>roberta_vec_242</th>\n      <th>roberta_vec_243</th>\n      <th>roberta_vec_244</th>\n      <th>roberta_vec_245</th>\n      <th>roberta_vec_246</th>\n      <th>roberta_vec_247</th>\n      <th>roberta_vec_248</th>\n      <th>roberta_vec_249</th>\n      <th>roberta_vec_250</th>\n      <th>roberta_vec_251</th>\n      <th>roberta_vec_252</th>\n      <th>roberta_vec_253</th>\n      <th>roberta_vec_254</th>\n      <th>roberta_vec_255</th>\n      <th>roberta_vec_256</th>\n      <th>roberta_vec_257</th>\n      <th>roberta_vec_258</th>\n      <th>roberta_vec_259</th>\n      <th>roberta_vec_260</th>\n      <th>roberta_vec_261</th>\n      <th>roberta_vec_262</th>\n      <th>roberta_vec_263</th>\n      <th>roberta_vec_264</th>\n      <th>roberta_vec_265</th>\n      <th>roberta_vec_266</th>\n      <th>roberta_vec_267</th>\n      <th>roberta_vec_268</th>\n      <th>roberta_vec_269</th>\n      <th>roberta_vec_270</th>\n      <th>roberta_vec_271</th>\n      <th>roberta_vec_272</th>\n      <th>roberta_vec_273</th>\n      <th>roberta_vec_274</th>\n      <th>roberta_vec_275</th>\n      <th>roberta_vec_276</th>\n      <th>roberta_vec_277</th>\n      <th>roberta_vec_278</th>\n      <th>roberta_vec_279</th>\n      <th>roberta_vec_280</th>\n      <th>roberta_vec_281</th>\n      <th>roberta_vec_282</th>\n      <th>roberta_vec_283</th>\n      <th>roberta_vec_284</th>\n      <th>roberta_vec_285</th>\n      <th>roberta_vec_286</th>\n      <th>roberta_vec_287</th>\n      <th>roberta_vec_288</th>\n      <th>roberta_vec_289</th>\n      <th>roberta_vec_290</th>\n      <th>roberta_vec_291</th>\n      <th>roberta_vec_292</th>\n      <th>roberta_vec_293</th>\n      <th>roberta_vec_294</th>\n      <th>roberta_vec_295</th>\n      <th>roberta_vec_296</th>\n      <th>roberta_vec_297</th>\n      <th>roberta_vec_298</th>\n      <th>roberta_vec_299</th>\n      <th>roberta_vec_300</th>\n      <th>roberta_vec_301</th>\n      <th>roberta_vec_302</th>\n      <th>roberta_vec_303</th>\n      <th>roberta_vec_304</th>\n      <th>roberta_vec_305</th>\n      <th>roberta_vec_306</th>\n      <th>roberta_vec_307</th>\n      <th>roberta_vec_308</th>\n      <th>roberta_vec_309</th>\n      <th>roberta_vec_310</th>\n      <th>roberta_vec_311</th>\n      <th>roberta_vec_312</th>\n      <th>roberta_vec_313</th>\n      <th>roberta_vec_314</th>\n      <th>roberta_vec_315</th>\n      <th>roberta_vec_316</th>\n      <th>roberta_vec_317</th>\n      <th>roberta_vec_318</th>\n      <th>roberta_vec_319</th>\n      <th>roberta_vec_320</th>\n      <th>roberta_vec_321</th>\n      <th>roberta_vec_322</th>\n      <th>roberta_vec_323</th>\n      <th>roberta_vec_324</th>\n      <th>roberta_vec_325</th>\n      <th>roberta_vec_326</th>\n      <th>roberta_vec_327</th>\n      <th>roberta_vec_328</th>\n      <th>roberta_vec_329</th>\n      <th>roberta_vec_330</th>\n      <th>roberta_vec_331</th>\n      <th>roberta_vec_332</th>\n      <th>roberta_vec_333</th>\n      <th>roberta_vec_334</th>\n      <th>roberta_vec_335</th>\n      <th>roberta_vec_336</th>\n      <th>roberta_vec_337</th>\n      <th>roberta_vec_338</th>\n      <th>roberta_vec_339</th>\n      <th>roberta_vec_340</th>\n      <th>roberta_vec_341</th>\n      <th>roberta_vec_342</th>\n      <th>roberta_vec_343</th>\n      <th>roberta_vec_344</th>\n      <th>roberta_vec_345</th>\n      <th>roberta_vec_346</th>\n      <th>roberta_vec_347</th>\n      <th>roberta_vec_348</th>\n      <th>roberta_vec_349</th>\n      <th>roberta_vec_350</th>\n      <th>roberta_vec_351</th>\n      <th>roberta_vec_352</th>\n      <th>roberta_vec_353</th>\n      <th>roberta_vec_354</th>\n      <th>roberta_vec_355</th>\n      <th>roberta_vec_356</th>\n      <th>roberta_vec_357</th>\n      <th>roberta_vec_358</th>\n      <th>roberta_vec_359</th>\n      <th>roberta_vec_360</th>\n      <th>roberta_vec_361</th>\n      <th>roberta_vec_362</th>\n      <th>roberta_vec_363</th>\n      <th>roberta_vec_364</th>\n      <th>roberta_vec_365</th>\n      <th>roberta_vec_366</th>\n      <th>roberta_vec_367</th>\n      <th>roberta_vec_368</th>\n      <th>roberta_vec_369</th>\n      <th>roberta_vec_370</th>\n      <th>roberta_vec_371</th>\n      <th>roberta_vec_372</th>\n      <th>roberta_vec_373</th>\n      <th>roberta_vec_374</th>\n      <th>roberta_vec_375</th>\n      <th>roberta_vec_376</th>\n      <th>roberta_vec_377</th>\n      <th>roberta_vec_378</th>\n      <th>roberta_vec_379</th>\n      <th>roberta_vec_380</th>\n      <th>roberta_vec_381</th>\n      <th>roberta_vec_382</th>\n      <th>roberta_vec_383</th>\n      <th>roberta_vec_384</th>\n      <th>roberta_vec_385</th>\n      <th>roberta_vec_386</th>\n      <th>roberta_vec_387</th>\n      <th>roberta_vec_388</th>\n      <th>roberta_vec_389</th>\n      <th>roberta_vec_390</th>\n      <th>roberta_vec_391</th>\n      <th>roberta_vec_392</th>\n      <th>roberta_vec_393</th>\n      <th>roberta_vec_394</th>\n      <th>roberta_vec_395</th>\n      <th>roberta_vec_396</th>\n      <th>roberta_vec_397</th>\n      <th>roberta_vec_398</th>\n      <th>roberta_vec_399</th>\n      <th>roberta_vec_400</th>\n      <th>roberta_vec_401</th>\n      <th>roberta_vec_402</th>\n      <th>roberta_vec_403</th>\n      <th>roberta_vec_404</th>\n      <th>roberta_vec_405</th>\n      <th>roberta_vec_406</th>\n      <th>roberta_vec_407</th>\n      <th>roberta_vec_408</th>\n      <th>roberta_vec_409</th>\n      <th>roberta_vec_410</th>\n      <th>roberta_vec_411</th>\n      <th>roberta_vec_412</th>\n      <th>roberta_vec_413</th>\n      <th>roberta_vec_414</th>\n      <th>roberta_vec_415</th>\n      <th>roberta_vec_416</th>\n      <th>roberta_vec_417</th>\n      <th>roberta_vec_418</th>\n      <th>roberta_vec_419</th>\n      <th>roberta_vec_420</th>\n      <th>roberta_vec_421</th>\n      <th>roberta_vec_422</th>\n      <th>roberta_vec_423</th>\n      <th>roberta_vec_424</th>\n      <th>roberta_vec_425</th>\n      <th>roberta_vec_426</th>\n      <th>roberta_vec_427</th>\n      <th>roberta_vec_428</th>\n      <th>roberta_vec_429</th>\n      <th>roberta_vec_430</th>\n      <th>roberta_vec_431</th>\n      <th>roberta_vec_432</th>\n      <th>roberta_vec_433</th>\n      <th>roberta_vec_434</th>\n      <th>roberta_vec_435</th>\n      <th>roberta_vec_436</th>\n      <th>roberta_vec_437</th>\n      <th>roberta_vec_438</th>\n      <th>roberta_vec_439</th>\n      <th>roberta_vec_440</th>\n      <th>roberta_vec_441</th>\n      <th>roberta_vec_442</th>\n      <th>roberta_vec_443</th>\n      <th>roberta_vec_444</th>\n      <th>roberta_vec_445</th>\n      <th>roberta_vec_446</th>\n      <th>roberta_vec_447</th>\n      <th>roberta_vec_448</th>\n      <th>roberta_vec_449</th>\n      <th>roberta_vec_450</th>\n      <th>roberta_vec_451</th>\n      <th>roberta_vec_452</th>\n      <th>roberta_vec_453</th>\n      <th>roberta_vec_454</th>\n      <th>roberta_vec_455</th>\n      <th>roberta_vec_456</th>\n      <th>roberta_vec_457</th>\n      <th>roberta_vec_458</th>\n      <th>roberta_vec_459</th>\n      <th>roberta_vec_460</th>\n      <th>roberta_vec_461</th>\n      <th>roberta_vec_462</th>\n      <th>roberta_vec_463</th>\n      <th>roberta_vec_464</th>\n      <th>roberta_vec_465</th>\n      <th>roberta_vec_466</th>\n      <th>roberta_vec_467</th>\n      <th>roberta_vec_468</th>\n      <th>roberta_vec_469</th>\n      <th>roberta_vec_470</th>\n      <th>roberta_vec_471</th>\n      <th>roberta_vec_472</th>\n      <th>roberta_vec_473</th>\n      <th>roberta_vec_474</th>\n      <th>roberta_vec_475</th>\n      <th>roberta_vec_476</th>\n      <th>roberta_vec_477</th>\n      <th>roberta_vec_478</th>\n      <th>roberta_vec_479</th>\n      <th>roberta_vec_480</th>\n      <th>roberta_vec_481</th>\n      <th>roberta_vec_482</th>\n      <th>roberta_vec_483</th>\n      <th>roberta_vec_484</th>\n      <th>roberta_vec_485</th>\n      <th>roberta_vec_486</th>\n      <th>roberta_vec_487</th>\n      <th>roberta_vec_488</th>\n      <th>roberta_vec_489</th>\n      <th>roberta_vec_490</th>\n      <th>roberta_vec_491</th>\n      <th>roberta_vec_492</th>\n      <th>roberta_vec_493</th>\n      <th>roberta_vec_494</th>\n      <th>roberta_vec_495</th>\n      <th>roberta_vec_496</th>\n      <th>roberta_vec_497</th>\n      <th>roberta_vec_498</th>\n      <th>roberta_vec_499</th>\n      <th>roberta_vec_500</th>\n      <th>roberta_vec_501</th>\n      <th>roberta_vec_502</th>\n      <th>roberta_vec_503</th>\n      <th>roberta_vec_504</th>\n      <th>roberta_vec_505</th>\n      <th>roberta_vec_506</th>\n      <th>roberta_vec_507</th>\n      <th>roberta_vec_508</th>\n      <th>roberta_vec_509</th>\n      <th>roberta_vec_510</th>\n      <th>roberta_vec_511</th>\n      <th>roberta_vec_512</th>\n      <th>roberta_vec_513</th>\n      <th>roberta_vec_514</th>\n      <th>roberta_vec_515</th>\n      <th>roberta_vec_516</th>\n      <th>roberta_vec_517</th>\n      <th>roberta_vec_518</th>\n      <th>roberta_vec_519</th>\n      <th>roberta_vec_520</th>\n      <th>roberta_vec_521</th>\n      <th>roberta_vec_522</th>\n      <th>roberta_vec_523</th>\n      <th>roberta_vec_524</th>\n      <th>roberta_vec_525</th>\n      <th>roberta_vec_526</th>\n      <th>roberta_vec_527</th>\n      <th>roberta_vec_528</th>\n      <th>roberta_vec_529</th>\n      <th>roberta_vec_530</th>\n      <th>roberta_vec_531</th>\n      <th>roberta_vec_532</th>\n      <th>roberta_vec_533</th>\n      <th>roberta_vec_534</th>\n      <th>roberta_vec_535</th>\n      <th>roberta_vec_536</th>\n      <th>roberta_vec_537</th>\n      <th>roberta_vec_538</th>\n      <th>roberta_vec_539</th>\n      <th>roberta_vec_540</th>\n      <th>roberta_vec_541</th>\n      <th>roberta_vec_542</th>\n      <th>roberta_vec_543</th>\n      <th>roberta_vec_544</th>\n      <th>roberta_vec_545</th>\n      <th>roberta_vec_546</th>\n      <th>roberta_vec_547</th>\n      <th>roberta_vec_548</th>\n      <th>roberta_vec_549</th>\n      <th>roberta_vec_550</th>\n      <th>roberta_vec_551</th>\n      <th>roberta_vec_552</th>\n      <th>roberta_vec_553</th>\n      <th>roberta_vec_554</th>\n      <th>roberta_vec_555</th>\n      <th>roberta_vec_556</th>\n      <th>roberta_vec_557</th>\n      <th>roberta_vec_558</th>\n      <th>roberta_vec_559</th>\n      <th>roberta_vec_560</th>\n      <th>roberta_vec_561</th>\n      <th>roberta_vec_562</th>\n      <th>roberta_vec_563</th>\n      <th>roberta_vec_564</th>\n      <th>roberta_vec_565</th>\n      <th>roberta_vec_566</th>\n      <th>roberta_vec_567</th>\n      <th>roberta_vec_568</th>\n      <th>roberta_vec_569</th>\n      <th>roberta_vec_570</th>\n      <th>roberta_vec_571</th>\n      <th>roberta_vec_572</th>\n      <th>roberta_vec_573</th>\n      <th>roberta_vec_574</th>\n      <th>roberta_vec_575</th>\n      <th>roberta_vec_576</th>\n      <th>roberta_vec_577</th>\n      <th>roberta_vec_578</th>\n      <th>roberta_vec_579</th>\n      <th>roberta_vec_580</th>\n      <th>roberta_vec_581</th>\n      <th>roberta_vec_582</th>\n      <th>roberta_vec_583</th>\n      <th>roberta_vec_584</th>\n      <th>roberta_vec_585</th>\n      <th>roberta_vec_586</th>\n      <th>roberta_vec_587</th>\n      <th>roberta_vec_588</th>\n      <th>roberta_vec_589</th>\n      <th>roberta_vec_590</th>\n      <th>roberta_vec_591</th>\n      <th>roberta_vec_592</th>\n      <th>roberta_vec_593</th>\n      <th>roberta_vec_594</th>\n      <th>roberta_vec_595</th>\n      <th>roberta_vec_596</th>\n      <th>roberta_vec_597</th>\n      <th>roberta_vec_598</th>\n      <th>roberta_vec_599</th>\n      <th>roberta_vec_600</th>\n      <th>roberta_vec_601</th>\n      <th>roberta_vec_602</th>\n      <th>roberta_vec_603</th>\n      <th>roberta_vec_604</th>\n      <th>roberta_vec_605</th>\n      <th>roberta_vec_606</th>\n      <th>roberta_vec_607</th>\n      <th>roberta_vec_608</th>\n      <th>roberta_vec_609</th>\n      <th>roberta_vec_610</th>\n      <th>roberta_vec_611</th>\n      <th>roberta_vec_612</th>\n      <th>roberta_vec_613</th>\n      <th>roberta_vec_614</th>\n      <th>roberta_vec_615</th>\n      <th>roberta_vec_616</th>\n      <th>roberta_vec_617</th>\n      <th>roberta_vec_618</th>\n      <th>roberta_vec_619</th>\n      <th>roberta_vec_620</th>\n      <th>roberta_vec_621</th>\n      <th>roberta_vec_622</th>\n      <th>roberta_vec_623</th>\n      <th>roberta_vec_624</th>\n      <th>roberta_vec_625</th>\n      <th>roberta_vec_626</th>\n      <th>roberta_vec_627</th>\n      <th>roberta_vec_628</th>\n      <th>roberta_vec_629</th>\n      <th>roberta_vec_630</th>\n      <th>roberta_vec_631</th>\n      <th>roberta_vec_632</th>\n      <th>roberta_vec_633</th>\n      <th>roberta_vec_634</th>\n      <th>roberta_vec_635</th>\n      <th>roberta_vec_636</th>\n      <th>roberta_vec_637</th>\n      <th>roberta_vec_638</th>\n      <th>roberta_vec_639</th>\n      <th>roberta_vec_640</th>\n      <th>roberta_vec_641</th>\n      <th>roberta_vec_642</th>\n      <th>roberta_vec_643</th>\n      <th>roberta_vec_644</th>\n      <th>roberta_vec_645</th>\n      <th>roberta_vec_646</th>\n      <th>roberta_vec_647</th>\n      <th>roberta_vec_648</th>\n      <th>roberta_vec_649</th>\n      <th>roberta_vec_650</th>\n      <th>roberta_vec_651</th>\n      <th>roberta_vec_652</th>\n      <th>roberta_vec_653</th>\n      <th>roberta_vec_654</th>\n      <th>roberta_vec_655</th>\n      <th>roberta_vec_656</th>\n      <th>roberta_vec_657</th>\n      <th>roberta_vec_658</th>\n      <th>roberta_vec_659</th>\n      <th>roberta_vec_660</th>\n      <th>roberta_vec_661</th>\n      <th>roberta_vec_662</th>\n      <th>roberta_vec_663</th>\n      <th>roberta_vec_664</th>\n      <th>roberta_vec_665</th>\n      <th>roberta_vec_666</th>\n      <th>roberta_vec_667</th>\n      <th>roberta_vec_668</th>\n      <th>roberta_vec_669</th>\n      <th>roberta_vec_670</th>\n      <th>roberta_vec_671</th>\n      <th>roberta_vec_672</th>\n      <th>roberta_vec_673</th>\n      <th>roberta_vec_674</th>\n      <th>roberta_vec_675</th>\n      <th>roberta_vec_676</th>\n      <th>roberta_vec_677</th>\n      <th>roberta_vec_678</th>\n      <th>roberta_vec_679</th>\n      <th>roberta_vec_680</th>\n      <th>roberta_vec_681</th>\n      <th>roberta_vec_682</th>\n      <th>roberta_vec_683</th>\n      <th>roberta_vec_684</th>\n      <th>roberta_vec_685</th>\n      <th>roberta_vec_686</th>\n      <th>roberta_vec_687</th>\n      <th>roberta_vec_688</th>\n      <th>roberta_vec_689</th>\n      <th>roberta_vec_690</th>\n      <th>roberta_vec_691</th>\n      <th>roberta_vec_692</th>\n      <th>roberta_vec_693</th>\n      <th>roberta_vec_694</th>\n      <th>roberta_vec_695</th>\n      <th>roberta_vec_696</th>\n      <th>roberta_vec_697</th>\n      <th>roberta_vec_698</th>\n      <th>roberta_vec_699</th>\n      <th>roberta_vec_700</th>\n      <th>roberta_vec_701</th>\n      <th>roberta_vec_702</th>\n      <th>roberta_vec_703</th>\n      <th>roberta_vec_704</th>\n      <th>roberta_vec_705</th>\n      <th>roberta_vec_706</th>\n      <th>roberta_vec_707</th>\n      <th>roberta_vec_708</th>\n      <th>roberta_vec_709</th>\n      <th>roberta_vec_710</th>\n      <th>roberta_vec_711</th>\n      <th>roberta_vec_712</th>\n      <th>roberta_vec_713</th>\n      <th>roberta_vec_714</th>\n      <th>roberta_vec_715</th>\n      <th>roberta_vec_716</th>\n      <th>roberta_vec_717</th>\n      <th>roberta_vec_718</th>\n      <th>roberta_vec_719</th>\n      <th>roberta_vec_720</th>\n      <th>roberta_vec_721</th>\n      <th>roberta_vec_722</th>\n      <th>roberta_vec_723</th>\n      <th>roberta_vec_724</th>\n      <th>roberta_vec_725</th>\n      <th>roberta_vec_726</th>\n      <th>roberta_vec_727</th>\n      <th>roberta_vec_728</th>\n      <th>roberta_vec_729</th>\n      <th>roberta_vec_730</th>\n      <th>roberta_vec_731</th>\n      <th>roberta_vec_732</th>\n      <th>roberta_vec_733</th>\n      <th>roberta_vec_734</th>\n      <th>roberta_vec_735</th>\n      <th>roberta_vec_736</th>\n      <th>roberta_vec_737</th>\n      <th>roberta_vec_738</th>\n      <th>roberta_vec_739</th>\n      <th>roberta_vec_740</th>\n      <th>roberta_vec_741</th>\n      <th>roberta_vec_742</th>\n      <th>roberta_vec_743</th>\n      <th>roberta_vec_744</th>\n      <th>roberta_vec_745</th>\n      <th>roberta_vec_746</th>\n      <th>roberta_vec_747</th>\n      <th>roberta_vec_748</th>\n      <th>roberta_vec_749</th>\n      <th>roberta_vec_750</th>\n      <th>roberta_vec_751</th>\n      <th>roberta_vec_752</th>\n      <th>roberta_vec_753</th>\n      <th>roberta_vec_754</th>\n      <th>roberta_vec_755</th>\n      <th>roberta_vec_756</th>\n      <th>roberta_vec_757</th>\n      <th>roberta_vec_758</th>\n      <th>roberta_vec_759</th>\n      <th>roberta_vec_760</th>\n      <th>roberta_vec_761</th>\n      <th>roberta_vec_762</th>\n      <th>roberta_vec_763</th>\n      <th>roberta_vec_764</th>\n      <th>roberta_vec_765</th>\n      <th>roberta_vec_766</th>\n      <th>roberta_vec_767</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>hep-ph/9902295</td>\n      <td>Michael Kraemer</td>\n      <td>Mark E. Hayes (University College London) and ...</td>\n      <td>Heavy-Flavour Production at HERA</td>\n      <td>LaTeX, 21 pages, 13 Postscript figures. Summar...</td>\n      <td>J.Phys.G25:1477-1493,1999</td>\n      <td>10.1088/0954-3899/25/7/332</td>\n      <td>CERN-TH/99-30, UCL/HEP 99-03</td>\n      <td>hep-ph hep-ex</td>\n      <td>None</td>\n      <td>We review the theoretical and experimental s...</td>\n      <td>[{'version': 'v1', 'created': 'Wed, 10 Feb 199...</td>\n      <td>[[Hayes, Mark E., , University College London]...</td>\n      <td>0.693147</td>\n      <td>NaN</td>\n      <td>10.1088</td>\n      <td>IOP Publishing</td>\n      <td>4.60517</td>\n      <td>510044.0</td>\n      <td>2008-11-26</td>\n      <td>1999-02-10 08:15:58+00:00</td>\n      <td>1999-02-10 08:15:58+00:00</td>\n      <td>2008</td>\n      <td>1999</td>\n      <td>1999</td>\n      <td>11</td>\n      <td>2</td>\n      <td>2</td>\n      <td>200811</td>\n      <td>199902</td>\n      <td>199902</td>\n      <td>26</td>\n      <td>10</td>\n      <td>10</td>\n      <td>1.227658e+09</td>\n      <td>918634558.0</td>\n      <td>918634558.0</td>\n      <td>309023042</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>14209</td>\n      <td>10632</td>\n      <td>10632</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>Hayes</td>\n      <td>2</td>\n      <td>hep-ph</td>\n      <td>hep-ph</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>hep hep</td>\n      <td>hep-ex hep-ph</td>\n      <td>hep-ex hep-ph</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>122642</td>\n      <td>49</td>\n      <td>43317</td>\n      <td>209</td>\n      <td>8</td>\n      <td>20</td>\n      <td>20</td>\n      <td>2821</td>\n      <td>2331</td>\n      <td>17141</td>\n      <td>2.370957</td>\n      <td>84</td>\n      <td>199.160427</td>\n      <td>0.0</td>\n      <td>5.365976</td>\n      <td>2.441401</td>\n      <td>1.4487</td>\n      <td>0.0</td>\n      <td>1.098612</td>\n      <td>3.575908</td>\n      <td>2.313685</td>\n      <td>105982</td>\n      <td>245208.992755</td>\n      <td>0.0</td>\n      <td>8.74608</td>\n      <td>2.302585</td>\n      <td>1.347253</td>\n      <td>0.693147</td>\n      <td>1.386294</td>\n      <td>3.295837</td>\n      <td>2.313685</td>\n      <td>105982</td>\n      <td>245208.992755</td>\n      <td>0.0</td>\n      <td>8.74608</td>\n      <td>2.302585</td>\n      <td>1.347253</td>\n      <td>0.693147</td>\n      <td>1.386294</td>\n      <td>3.295837</td>\n      <td>2.921569</td>\n      <td>19</td>\n      <td>55.509815</td>\n      <td>0.693147</td>\n      <td>5.267858</td>\n      <td>2.564949</td>\n      <td>1.677225</td>\n      <td>0.693147</td>\n      <td>1.445186</td>\n      <td>4.537947</td>\n      <td>2.717936</td>\n      <td>32230</td>\n      <td>87599.069527</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>2.772589</td>\n      <td>1.410256</td>\n      <td>0.693147</td>\n      <td>1.791759</td>\n      <td>3.688879</td>\n      <td>2.751518</td>\n      <td>1211</td>\n      <td>3332.088784</td>\n      <td>0.0</td>\n      <td>7.883823</td>\n      <td>2.890372</td>\n      <td>1.412057</td>\n      <td>0.693147</td>\n      <td>1.791759</td>\n      <td>3.73767</td>\n      <td>2.647246</td>\n      <td>290654</td>\n      <td>769432.740981</td>\n      <td>0.0</td>\n      <td>10.133567</td>\n      <td>2.70805</td>\n      <td>1.405163</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.610918</td>\n      <td>2.162204</td>\n      <td>82437</td>\n      <td>178245.632013</td>\n      <td>0.0</td>\n      <td>8.479076</td>\n      <td>2.197225</td>\n      <td>1.360693</td>\n      <td>0.0</td>\n      <td>1.098612</td>\n      <td>3.135494</td>\n      <td>2.162204</td>\n      <td>82437</td>\n      <td>178245.632013</td>\n      <td>0.0</td>\n      <td>8.479076</td>\n      <td>2.197225</td>\n      <td>1.360693</td>\n      <td>0.0</td>\n      <td>1.098612</td>\n      <td>3.135494</td>\n      <td>2.256413</td>\n      <td>8557</td>\n      <td>19308.127915</td>\n      <td>0.0</td>\n      <td>7.909122</td>\n      <td>2.302585</td>\n      <td>1.3254</td>\n      <td>0.0</td>\n      <td>1.386294</td>\n      <td>3.218876</td>\n      <td>2.238519</td>\n      <td>19617</td>\n      <td>43913.017812</td>\n      <td>0.0</td>\n      <td>8.479076</td>\n      <td>2.302585</td>\n      <td>1.32983</td>\n      <td>0.0</td>\n      <td>1.386294</td>\n      <td>3.178054</td>\n      <td>2.256413</td>\n      <td>8557</td>\n      <td>19308.127915</td>\n      <td>0.0</td>\n      <td>7.909122</td>\n      <td>2.302585</td>\n      <td>1.3254</td>\n      <td>0.0</td>\n      <td>1.386294</td>\n      <td>3.218876</td>\n      <td>-2.228422</td>\n      <td>0.431752</td>\n      <td>-1.620538</td>\n      <td>0.510956</td>\n      <td>-1.67781</td>\n      <td>0.502275</td>\n      <td>-1.620538</td>\n      <td>0.510956</td>\n      <td>-2.024789</td>\n      <td>0.4554</td>\n      <td>-2.058371</td>\n      <td>0.451323</td>\n      <td>-1.954099</td>\n      <td>0.464226</td>\n      <td>-1.469057</td>\n      <td>0.535433</td>\n      <td>-1.469057</td>\n      <td>0.535433</td>\n      <td>-1.563266</td>\n      <td>0.519942</td>\n      <td>-1.545371</td>\n      <td>0.522815</td>\n      <td>-1.563266</td>\n      <td>0.519942</td>\n      <td>0.607884</td>\n      <td>1.183446</td>\n      <td>0.550612</td>\n      <td>1.16334</td>\n      <td>0.607884</td>\n      <td>1.183446</td>\n      <td>0.203633</td>\n      <td>1.054771</td>\n      <td>0.170051</td>\n      <td>1.045329</td>\n      <td>0.274323</td>\n      <td>1.075214</td>\n      <td>0.759365</td>\n      <td>1.240138</td>\n      <td>0.759365</td>\n      <td>1.240138</td>\n      <td>0.665156</td>\n      <td>1.20426</td>\n      <td>0.683051</td>\n      <td>1.210915</td>\n      <td>0.665156</td>\n      <td>1.20426</td>\n      <td>-0.057272</td>\n      <td>0.98301</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.40425</td>\n      <td>0.89127</td>\n      <td>-0.437833</td>\n      <td>0.883292</td>\n      <td>-0.333561</td>\n      <td>0.908544</td>\n      <td>0.151481</td>\n      <td>1.047904</td>\n      <td>0.151481</td>\n      <td>1.047904</td>\n      <td>0.057272</td>\n      <td>1.017587</td>\n      <td>0.075167</td>\n      <td>1.02321</td>\n      <td>0.057272</td>\n      <td>1.017587</td>\n      <td>0.057272</td>\n      <td>1.017284</td>\n      <td>-0.346978</td>\n      <td>0.906674</td>\n      <td>-0.380561</td>\n      <td>0.898558</td>\n      <td>-0.276289</td>\n      <td>0.924247</td>\n      <td>0.208753</td>\n      <td>1.066015</td>\n      <td>0.208753</td>\n      <td>1.066015</td>\n      <td>0.114544</td>\n      <td>1.035175</td>\n      <td>0.132439</td>\n      <td>1.040895</td>\n      <td>0.114544</td>\n      <td>1.035175</td>\n      <td>-0.40425</td>\n      <td>0.89127</td>\n      <td>-0.437833</td>\n      <td>0.883292</td>\n      <td>-0.333561</td>\n      <td>0.908544</td>\n      <td>0.151481</td>\n      <td>1.047904</td>\n      <td>0.151481</td>\n      <td>1.047904</td>\n      <td>0.057272</td>\n      <td>1.017587</td>\n      <td>0.075167</td>\n      <td>1.02321</td>\n      <td>0.057272</td>\n      <td>1.017587</td>\n      <td>-0.033583</td>\n      <td>0.991048</td>\n      <td>0.070689</td>\n      <td>1.019382</td>\n      <td>0.555732</td>\n      <td>1.175742</td>\n      <td>0.555732</td>\n      <td>1.175742</td>\n      <td>0.461523</td>\n      <td>1.141727</td>\n      <td>0.479417</td>\n      <td>1.148036</td>\n      <td>0.461523</td>\n      <td>1.141727</td>\n      <td>0.104272</td>\n      <td>1.028589</td>\n      <td>0.589314</td>\n      <td>1.186362</td>\n      <td>0.589314</td>\n      <td>1.186362</td>\n      <td>0.495105</td>\n      <td>1.15204</td>\n      <td>0.513</td>\n      <td>1.158406</td>\n      <td>0.495105</td>\n      <td>1.15204</td>\n      <td>0.485042</td>\n      <td>1.153387</td>\n      <td>0.485042</td>\n      <td>1.153387</td>\n      <td>0.390833</td>\n      <td>1.12002</td>\n      <td>0.408728</td>\n      <td>1.126208</td>\n      <td>0.390833</td>\n      <td>1.12002</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.094209</td>\n      <td>0.97107</td>\n      <td>-0.076314</td>\n      <td>0.976435</td>\n      <td>-0.094209</td>\n      <td>0.97107</td>\n      <td>-0.094209</td>\n      <td>0.97107</td>\n      <td>-0.076314</td>\n      <td>0.976435</td>\n      <td>-0.094209</td>\n      <td>0.97107</td>\n      <td>0.017895</td>\n      <td>1.005526</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.017895</td>\n      <td>0.994505</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.021159</td>\n      <td>0.091914</td>\n      <td>0.006219</td>\n      <td>0.160527</td>\n      <td>0.025339</td>\n      <td>-0.03908</td>\n      <td>0.190592</td>\n      <td>0.290296</td>\n      <td>-0.058764</td>\n      <td>-0.118665</td>\n      <td>-0.084186</td>\n      <td>-0.271262</td>\n      <td>-0.019865</td>\n      <td>-0.165075</td>\n      <td>0.082387</td>\n      <td>0.072393</td>\n      <td>0.055521</td>\n      <td>0.120005</td>\n      <td>0.05252</td>\n      <td>0.124498</td>\n      <td>-0.193668</td>\n      <td>0.285989</td>\n      <td>-0.161816</td>\n      <td>-0.048666</td>\n      <td>0.183767</td>\n      <td>0.044449</td>\n      <td>-0.061549</td>\n      <td>0.084727</td>\n      <td>-0.147473</td>\n      <td>0.036578</td>\n      <td>-0.025349</td>\n      <td>0.008562</td>\n      <td>0.050719</td>\n      <td>-0.05748</td>\n      <td>-0.068776</td>\n      <td>0.077502</td>\n      <td>-0.003565</td>\n      <td>0.00022</td>\n      <td>-0.125422</td>\n      <td>0.164068</td>\n      <td>-0.128167</td>\n      <td>0.594769</td>\n      <td>-0.019467</td>\n      <td>-0.176452</td>\n      <td>0.038716</td>\n      <td>-0.170457</td>\n      <td>0.060191</td>\n      <td>-0.106063</td>\n      <td>-0.011019</td>\n      <td>0.139933</td>\n      <td>-0.048145</td>\n      <td>-0.040929</td>\n      <td>-0.07358</td>\n      <td>-0.027</td>\n      <td>-0.005641</td>\n      <td>0.108138</td>\n      <td>0.027079</td>\n      <td>-0.02478</td>\n      <td>0.030061</td>\n      <td>0.051647</td>\n      <td>-0.008175</td>\n      <td>-0.049933</td>\n      <td>0.084675</td>\n      <td>0.094695</td>\n      <td>-0.102252</td>\n      <td>-0.017201</td>\n      <td>0.062901</td>\n      <td>0.290093</td>\n      <td>0.022563</td>\n      <td>-0.083934</td>\n      <td>-0.007238</td>\n      <td>0.000715</td>\n      <td>0.022674</td>\n      <td>0.161299</td>\n      <td>0.190071</td>\n      <td>0.126392</td>\n      <td>0.028833</td>\n      <td>-1.051996</td>\n      <td>-0.032828</td>\n      <td>-0.046011</td>\n      <td>0.018818</td>\n      <td>0.004218</td>\n      <td>1.101154</td>\n      <td>0.13049</td>\n      <td>0.130714</td>\n      <td>0.045734</td>\n      <td>0.054126</td>\n      <td>0.206092</td>\n      <td>-0.076571</td>\n      <td>-0.042371</td>\n      <td>-0.001927</td>\n      <td>0.017183</td>\n      <td>-0.013531</td>\n      <td>0.047222</td>\n      <td>0.036655</td>\n      <td>0.091576</td>\n      <td>0.012302</td>\n      <td>-0.661598</td>\n      <td>0.026064</td>\n      <td>-0.014221</td>\n      <td>0.055518</td>\n      <td>-0.114769</td>\n      <td>0.210896</td>\n      <td>0.008232</td>\n      <td>-0.122332</td>\n      <td>0.247001</td>\n      <td>0.050222</td>\n      <td>-0.054071</td>\n      <td>0.049657</td>\n      <td>0.006932</td>\n      <td>0.030806</td>\n      <td>0.011649</td>\n      <td>-0.055341</td>\n      <td>0.096672</td>\n      <td>0.004022</td>\n      <td>0.008461</td>\n      <td>0.112621</td>\n      <td>0.207396</td>\n      <td>-0.055466</td>\n      <td>0.298221</td>\n      <td>0.140239</td>\n      <td>0.101617</td>\n      <td>0.058855</td>\n      <td>-0.068055</td>\n      <td>0.149775</td>\n      <td>0.090257</td>\n      <td>-0.130411</td>\n      <td>0.142004</td>\n      <td>0.040393</td>\n      <td>0.136368</td>\n      <td>0.096402</td>\n      <td>-0.556203</td>\n      <td>-0.129729</td>\n      <td>-0.063907</td>\n      <td>0.03258</td>\n      <td>0.034312</td>\n      <td>0.152195</td>\n      <td>0.060326</td>\n      <td>-0.118853</td>\n      <td>-0.010308</td>\n      <td>0.062203</td>\n      <td>0.278545</td>\n      <td>-0.103212</td>\n      <td>-0.238326</td>\n      <td>0.116649</td>\n      <td>0.122331</td>\n      <td>-0.083412</td>\n      <td>-0.228263</td>\n      <td>0.040731</td>\n      <td>0.201998</td>\n      <td>0.187708</td>\n      <td>0.01139</td>\n      <td>-0.128236</td>\n      <td>-0.205447</td>\n      <td>0.015731</td>\n      <td>0.345745</td>\n      <td>0.121425</td>\n      <td>-0.165457</td>\n      <td>-0.182841</td>\n      <td>0.157746</td>\n      <td>-0.000315</td>\n      <td>-0.056863</td>\n      <td>0.232624</td>\n      <td>-0.027894</td>\n      <td>0.065723</td>\n      <td>-0.050886</td>\n      <td>-0.121151</td>\n      <td>0.078016</td>\n      <td>0.097516</td>\n      <td>-0.105188</td>\n      <td>-0.026527</td>\n      <td>0.046994</td>\n      <td>0.164441</td>\n      <td>-0.003296</td>\n      <td>0.001794</td>\n      <td>-0.008245</td>\n      <td>-0.090429</td>\n      <td>-0.019271</td>\n      <td>-0.039529</td>\n      <td>0.012262</td>\n      <td>-0.012059</td>\n      <td>-0.01711</td>\n      <td>-0.055293</td>\n      <td>-0.13158</td>\n      <td>0.038266</td>\n      <td>0.080573</td>\n      <td>-0.008577</td>\n      <td>0.135607</td>\n      <td>0.181005</td>\n      <td>-0.04383</td>\n      <td>-0.04078</td>\n      <td>-0.04855</td>\n      <td>-0.100859</td>\n      <td>-0.206905</td>\n      <td>0.061782</td>\n      <td>-0.170447</td>\n      <td>-0.083928</td>\n      <td>0.014301</td>\n      <td>0.403832</td>\n      <td>-0.06208</td>\n      <td>0.099142</td>\n      <td>-0.104929</td>\n      <td>0.040582</td>\n      <td>0.050998</td>\n      <td>0.164948</td>\n      <td>0.015544</td>\n      <td>-0.270143</td>\n      <td>0.071122</td>\n      <td>0.050514</td>\n      <td>-0.060482</td>\n      <td>-0.026421</td>\n      <td>0.041568</td>\n      <td>-0.147964</td>\n      <td>0.044363</td>\n      <td>-0.178689</td>\n      <td>0.098877</td>\n      <td>0.020214</td>\n      <td>-0.511027</td>\n      <td>-0.170577</td>\n      <td>-0.346905</td>\n      <td>0.0537</td>\n      <td>0.076849</td>\n      <td>-0.030922</td>\n      <td>0.008933</td>\n      <td>-0.082399</td>\n      <td>0.111505</td>\n      <td>-0.004161</td>\n      <td>-0.036977</td>\n      <td>0.049074</td>\n      <td>-0.154588</td>\n      <td>-0.005269</td>\n      <td>0.33097</td>\n      <td>0.027778</td>\n      <td>0.042298</td>\n      <td>0.154414</td>\n      <td>-0.188538</td>\n      <td>0.03051</td>\n      <td>-0.01406</td>\n      <td>-0.051922</td>\n      <td>-0.096339</td>\n      <td>-0.274143</td>\n      <td>0.021812</td>\n      <td>0.053583</td>\n      <td>0.0859</td>\n      <td>-0.060149</td>\n      <td>0.047214</td>\n      <td>-0.007236</td>\n      <td>-0.075574</td>\n      <td>-0.051089</td>\n      <td>-0.089726</td>\n      <td>0.009813</td>\n      <td>-0.01094</td>\n      <td>-0.001022</td>\n      <td>-0.015114</td>\n      <td>0.256334</td>\n      <td>-0.118728</td>\n      <td>-0.047792</td>\n      <td>0.032125</td>\n      <td>-0.162749</td>\n      <td>0.318361</td>\n      <td>-0.035677</td>\n      <td>-0.04819</td>\n      <td>0.147017</td>\n      <td>-0.047834</td>\n      <td>-0.039544</td>\n      <td>-0.104349</td>\n      <td>-0.109669</td>\n      <td>-0.027195</td>\n      <td>0.132058</td>\n      <td>0.014702</td>\n      <td>-0.055527</td>\n      <td>0.026971</td>\n      <td>-0.054908</td>\n      <td>-0.061237</td>\n      <td>-0.063213</td>\n      <td>0.29448</td>\n      <td>0.358346</td>\n      <td>-0.009177</td>\n      <td>-0.02539</td>\n      <td>-0.037266</td>\n      <td>0.214616</td>\n      <td>-0.145413</td>\n      <td>0.11474</td>\n      <td>-0.017633</td>\n      <td>0.018637</td>\n      <td>0.047162</td>\n      <td>0.059369</td>\n      <td>0.021426</td>\n      <td>-0.033502</td>\n      <td>0.075583</td>\n      <td>-0.066334</td>\n      <td>0.062335</td>\n      <td>-0.09216</td>\n      <td>-0.021661</td>\n      <td>0.096643</td>\n      <td>0.090622</td>\n      <td>0.158539</td>\n      <td>-0.025748</td>\n      <td>-0.072213</td>\n      <td>-0.108713</td>\n      <td>0.16286</td>\n      <td>-0.022591</td>\n      <td>-0.037101</td>\n      <td>-0.014994</td>\n      <td>-0.008607</td>\n      <td>0.071167</td>\n      <td>-0.183716</td>\n      <td>-0.007629</td>\n      <td>0.134016</td>\n      <td>0.082213</td>\n      <td>-0.111802</td>\n      <td>-0.008389</td>\n      <td>-0.06662</td>\n      <td>-0.08155</td>\n      <td>0.029572</td>\n      <td>0.189175</td>\n      <td>-0.084198</td>\n      <td>-0.055908</td>\n      <td>-0.009681</td>\n      <td>-0.078716</td>\n      <td>-0.174606</td>\n      <td>-0.072681</td>\n      <td>-0.138039</td>\n      <td>-0.089501</td>\n      <td>-0.231091</td>\n      <td>0.0599</td>\n      <td>-0.156146</td>\n      <td>0.057016</td>\n      <td>0.162935</td>\n      <td>0.182202</td>\n      <td>0.62401</td>\n      <td>0.425136</td>\n      <td>0.100822</td>\n      <td>0.002127</td>\n      <td>0.141032</td>\n      <td>-0.136996</td>\n      <td>-0.059905</td>\n      <td>0.155125</td>\n      <td>0.170771</td>\n      <td>0.056534</td>\n      <td>-0.136751</td>\n      <td>0.006972</td>\n      <td>-0.012448</td>\n      <td>0.01365</td>\n      <td>0.193015</td>\n      <td>0.014073</td>\n      <td>0.092219</td>\n      <td>-0.004052</td>\n      <td>0.277142</td>\n      <td>-0.150453</td>\n      <td>-0.17078</td>\n      <td>-0.110611</td>\n      <td>0.01494</td>\n      <td>-0.320114</td>\n      <td>-0.028465</td>\n      <td>0.23229</td>\n      <td>-0.095811</td>\n      <td>0.04421</td>\n      <td>-0.213199</td>\n      <td>0.068205</td>\n      <td>0.076826</td>\n      <td>0.200793</td>\n      <td>0.021035</td>\n      <td>0.129505</td>\n      <td>0.104688</td>\n      <td>-0.140625</td>\n      <td>0.072766</td>\n      <td>0.004655</td>\n      <td>0.176511</td>\n      <td>-0.004922</td>\n      <td>-0.034623</td>\n      <td>0.010512</td>\n      <td>-0.051318</td>\n      <td>-0.0018</td>\n      <td>0.047339</td>\n      <td>-0.068834</td>\n      <td>0.19413</td>\n      <td>-0.076413</td>\n      <td>0.028653</td>\n      <td>0.007937</td>\n      <td>0.094334</td>\n      <td>0.051268</td>\n      <td>0.020364</td>\n      <td>-0.026033</td>\n      <td>0.30898</td>\n      <td>-0.094115</td>\n      <td>0.308518</td>\n      <td>0.193439</td>\n      <td>-0.095692</td>\n      <td>-0.05352</td>\n      <td>0.002016</td>\n      <td>-0.064693</td>\n      <td>0.122262</td>\n      <td>-0.120535</td>\n      <td>0.041815</td>\n      <td>-0.054836</td>\n      <td>0.007822</td>\n      <td>-0.426448</td>\n      <td>0.047687</td>\n      <td>0.015008</td>\n      <td>-0.052534</td>\n      <td>0.140038</td>\n      <td>0.02585</td>\n      <td>-0.11479</td>\n      <td>0.053117</td>\n      <td>0.153543</td>\n      <td>-0.095239</td>\n      <td>-0.163483</td>\n      <td>-0.026048</td>\n      <td>0.162395</td>\n      <td>0.047188</td>\n      <td>0.073149</td>\n      <td>-0.008878</td>\n      <td>-0.032284</td>\n      <td>-0.084181</td>\n      <td>0.008437</td>\n      <td>0.006305</td>\n      <td>-0.028463</td>\n      <td>-0.281973</td>\n      <td>-0.162848</td>\n      <td>-0.154905</td>\n      <td>-0.123429</td>\n      <td>0.074748</td>\n      <td>-0.053285</td>\n      <td>-0.440077</td>\n      <td>-0.0793</td>\n      <td>0.107435</td>\n      <td>-0.146091</td>\n      <td>-0.00265</td>\n      <td>0.058089</td>\n      <td>0.160322</td>\n      <td>0.056062</td>\n      <td>-0.024009</td>\n      <td>-0.04205</td>\n      <td>-0.139201</td>\n      <td>-0.181151</td>\n      <td>0.117982</td>\n      <td>-0.014689</td>\n      <td>-0.114674</td>\n      <td>-0.019894</td>\n      <td>-0.40249</td>\n      <td>0.001017</td>\n      <td>-0.149398</td>\n      <td>-0.033341</td>\n      <td>-0.108509</td>\n      <td>-0.11537</td>\n      <td>0.057975</td>\n      <td>-0.065308</td>\n      <td>-0.108287</td>\n      <td>0.03235</td>\n      <td>-0.104107</td>\n      <td>0.015036</td>\n      <td>-0.196914</td>\n      <td>-1.347596</td>\n      <td>0.212447</td>\n      <td>0.166778</td>\n      <td>-0.031664</td>\n      <td>0.022859</td>\n      <td>-0.037949</td>\n      <td>-0.144512</td>\n      <td>0.182012</td>\n      <td>-0.054833</td>\n      <td>0.081778</td>\n      <td>-0.187795</td>\n      <td>0.076714</td>\n      <td>-0.174939</td>\n      <td>0.2275</td>\n      <td>-0.015552</td>\n      <td>-0.100666</td>\n      <td>0.072337</td>\n      <td>0.144437</td>\n      <td>-0.052423</td>\n      <td>-0.183578</td>\n      <td>-0.171214</td>\n      <td>-0.049964</td>\n      <td>-0.044868</td>\n      <td>-0.018483</td>\n      <td>0.296564</td>\n      <td>0.072446</td>\n      <td>-0.022746</td>\n      <td>-0.033217</td>\n      <td>-0.068958</td>\n      <td>0.101734</td>\n      <td>0.099974</td>\n      <td>0.015855</td>\n      <td>-0.128012</td>\n      <td>-0.034076</td>\n      <td>0.05335</td>\n      <td>0.158108</td>\n      <td>-0.105041</td>\n      <td>-0.104255</td>\n      <td>-0.083309</td>\n      <td>0.177937</td>\n      <td>0.150279</td>\n      <td>0.201614</td>\n      <td>0.027929</td>\n      <td>0.37666</td>\n      <td>-0.152545</td>\n      <td>-0.155271</td>\n      <td>0.043854</td>\n      <td>-0.028725</td>\n      <td>0.061899</td>\n      <td>0.019833</td>\n      <td>0.082157</td>\n      <td>-0.035213</td>\n      <td>-0.019426</td>\n      <td>-0.14763</td>\n      <td>0.056121</td>\n      <td>0.073331</td>\n      <td>-0.023424</td>\n      <td>-0.227543</td>\n      <td>-0.000602</td>\n      <td>0.121111</td>\n      <td>0.038206</td>\n      <td>-0.008902</td>\n      <td>0.024064</td>\n      <td>-0.037369</td>\n      <td>-0.074584</td>\n      <td>0.066258</td>\n      <td>0.241208</td>\n      <td>-0.019344</td>\n      <td>0.078863</td>\n      <td>0.062034</td>\n      <td>0.013586</td>\n      <td>-0.020986</td>\n      <td>0.024147</td>\n      <td>-0.012329</td>\n      <td>0.041916</td>\n      <td>0.01863</td>\n      <td>0.103666</td>\n      <td>0.267286</td>\n      <td>-0.004326</td>\n      <td>0.175795</td>\n      <td>-0.048771</td>\n      <td>-0.111534</td>\n      <td>0.042488</td>\n      <td>0.043012</td>\n      <td>-0.139547</td>\n      <td>-0.041013</td>\n      <td>-0.129658</td>\n      <td>-0.101868</td>\n      <td>0.148641</td>\n      <td>-0.034011</td>\n      <td>0.026582</td>\n      <td>0.050715</td>\n      <td>-0.080829</td>\n      <td>0.164848</td>\n      <td>0.020371</td>\n      <td>0.0302</td>\n      <td>0.065473</td>\n      <td>0.011906</td>\n      <td>-0.23679</td>\n      <td>0.132766</td>\n      <td>0.097148</td>\n      <td>0.023719</td>\n      <td>0.192919</td>\n      <td>0.194037</td>\n      <td>-0.051655</td>\n      <td>-0.142407</td>\n      <td>0.245611</td>\n      <td>-0.047618</td>\n      <td>-0.016797</td>\n      <td>-0.046147</td>\n      <td>0.004564</td>\n      <td>0.025534</td>\n      <td>-0.070372</td>\n      <td>0.030484</td>\n      <td>-0.007294</td>\n      <td>-0.008978</td>\n      <td>-0.083971</td>\n      <td>1.304693</td>\n      <td>0.116037</td>\n      <td>0.014827</td>\n      <td>-0.145521</td>\n      <td>0.185063</td>\n      <td>0.063887</td>\n      <td>0.015271</td>\n      <td>-0.126791</td>\n      <td>0.113292</td>\n      <td>0.000877</td>\n      <td>0.260103</td>\n      <td>-0.021803</td>\n      <td>-0.005241</td>\n      <td>0.012979</td>\n      <td>-0.040793</td>\n      <td>-0.197163</td>\n      <td>-0.093093</td>\n      <td>0.019604</td>\n      <td>11.406745</td>\n      <td>-0.001576</td>\n      <td>0.009103</td>\n      <td>0.069717</td>\n      <td>-0.11638</td>\n      <td>0.02271</td>\n      <td>-0.009541</td>\n      <td>0.030983</td>\n      <td>0.143954</td>\n      <td>0.074972</td>\n      <td>0.060721</td>\n      <td>-0.046683</td>\n      <td>0.148974</td>\n      <td>-0.080296</td>\n      <td>0.060301</td>\n      <td>0.032032</td>\n      <td>0.081368</td>\n      <td>0.086436</td>\n      <td>0.099291</td>\n      <td>-0.153126</td>\n      <td>0.092218</td>\n      <td>0.076372</td>\n      <td>0.109919</td>\n      <td>0.777324</td>\n      <td>-0.112435</td>\n      <td>0.144448</td>\n      <td>-0.006802</td>\n      <td>-0.016929</td>\n      <td>-0.025625</td>\n      <td>0.100593</td>\n      <td>0.054845</td>\n      <td>0.227658</td>\n      <td>0.099402</td>\n      <td>-0.053498</td>\n      <td>0.096838</td>\n      <td>-0.122846</td>\n      <td>-0.213329</td>\n      <td>0.13427</td>\n      <td>0.05979</td>\n      <td>0.052141</td>\n      <td>-0.00279</td>\n      <td>-0.05567</td>\n      <td>0.182667</td>\n      <td>-0.222613</td>\n      <td>0.039218</td>\n      <td>-0.059391</td>\n      <td>0.290423</td>\n      <td>0.122479</td>\n      <td>0.007779</td>\n      <td>0.019957</td>\n      <td>-0.07321</td>\n      <td>0.019736</td>\n      <td>0.088344</td>\n      <td>-0.171627</td>\n      <td>-0.050579</td>\n      <td>-0.108241</td>\n      <td>0.032379</td>\n      <td>-0.132383</td>\n      <td>-0.021107</td>\n      <td>-0.026656</td>\n      <td>-0.066542</td>\n      <td>-0.14062</td>\n      <td>0.068314</td>\n      <td>0.004812</td>\n      <td>-0.016412</td>\n      <td>0.185605</td>\n      <td>0.10553</td>\n      <td>0.001893</td>\n      <td>-0.111575</td>\n      <td>-0.084433</td>\n      <td>-0.002476</td>\n      <td>-0.057709</td>\n      <td>0.01483</td>\n      <td>-0.138764</td>\n      <td>-0.222441</td>\n      <td>-0.170947</td>\n      <td>-0.499743</td>\n      <td>-0.191339</td>\n      <td>-0.078408</td>\n      <td>-0.378901</td>\n      <td>0.087936</td>\n      <td>-0.070182</td>\n      <td>-0.114958</td>\n      <td>0.073736</td>\n      <td>-0.395247</td>\n      <td>0.114391</td>\n      <td>-0.142501</td>\n      <td>0.029733</td>\n      <td>-0.127189</td>\n      <td>0.196349</td>\n      <td>0.111586</td>\n      <td>0.101117</td>\n      <td>-0.010446</td>\n      <td>-0.116452</td>\n      <td>0.077068</td>\n      <td>-0.051625</td>\n      <td>0.050996</td>\n      <td>-0.055594</td>\n      <td>0.124248</td>\n      <td>-0.249727</td>\n      <td>-0.315202</td>\n      <td>-0.031932</td>\n      <td>-0.047831</td>\n      <td>0.133403</td>\n      <td>-0.121233</td>\n      <td>0.188389</td>\n      <td>0.022408</td>\n      <td>-0.072894</td>\n      <td>0.01557</td>\n      <td>0.019908</td>\n      <td>0.000764</td>\n      <td>0.043054</td>\n      <td>-0.211885</td>\n      <td>0.02381</td>\n      <td>-0.010709</td>\n      <td>-0.141905</td>\n      <td>0.087001</td>\n      <td>0.026661</td>\n      <td>0.035089</td>\n      <td>0.132267</td>\n      <td>-0.009111</td>\n      <td>0.065618</td>\n      <td>-0.040556</td>\n      <td>-0.068752</td>\n      <td>-0.0509</td>\n      <td>0.023299</td>\n      <td>-0.22655</td>\n      <td>-0.060595</td>\n      <td>-0.111451</td>\n      <td>-0.038315</td>\n      <td>-0.22186</td>\n      <td>0.064387</td>\n      <td>0.237368</td>\n      <td>0.070414</td>\n      <td>-0.043413</td>\n      <td>0.049146</td>\n      <td>-0.108834</td>\n      <td>0.060299</td>\n      <td>0.236724</td>\n      <td>0.153273</td>\n      <td>-0.048493</td>\n      <td>0.071393</td>\n      <td>-0.13742</td>\n      <td>-0.253319</td>\n      <td>0.117817</td>\n      <td>-0.173621</td>\n      <td>0.163909</td>\n      <td>0.327391</td>\n      <td>0.077823</td>\n      <td>0.129044</td>\n      <td>-0.030003</td>\n      <td>-0.139961</td>\n      <td>-0.198641</td>\n      <td>-0.338671</td>\n      <td>0.100887</td>\n      <td>0.011717</td>\n      <td>-0.131644</td>\n      <td>0.089834</td>\n      <td>0.063055</td>\n      <td>-0.182048</td>\n      <td>0.451386</td>\n      <td>-0.434589</td>\n      <td>-0.08692</td>\n      <td>-0.031759</td>\n      <td>-0.051711</td>\n      <td>0.093028</td>\n      <td>0.203682</td>\n      <td>0.021781</td>\n      <td>0.025653</td>\n      <td>0.132081</td>\n      <td>-0.090815</td>\n      <td>0.027593</td>\n      <td>-0.006494</td>\n      <td>0.045004</td>\n      <td>0.046405</td>\n      <td>0.1948</td>\n      <td>0.237505</td>\n      <td>0.184351</td>\n      <td>0.028982</td>\n      <td>-0.053141</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "196215"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df_train.submitter_label.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " Start training from score 2.134370\n",
      "feature_fraction_stage2, val_score: 0.953546:  83%|########3 | 5/6 [05:30<01:04, 64.75s/it]\u001b[32m[I 2021-03-27 08:18:46,938]\u001b[0m Trial 41 finished with value: 0.9583759073233898 and parameters: {'feature_fraction': 0.516}. Best is trial 37 with value: 0.9551573144554407.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.953546:  83%|########3 | 5/6 [05:30<01:04, 64.75s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.670089 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "feature_fraction_stage2, val_score: 0.953546: 100%|##########| 6/6 [06:43<00:00, 67.11s/it]\u001b[32m[I 2021-03-27 08:19:59,566]\u001b[0m Trial 42 finished with value: 0.9580717858544919 and parameters: {'feature_fraction': 0.58}. Best is trial 37 with value: 0.9551573144554407.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.953546: 100%|##########| 6/6 [06:43<00:00, 67.20s/it]\n",
      "regularization_factors, val_score: 0.953546:   0%|          | 0/20 [00:00<?, ?it/s][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.651522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:   5%|5         | 1/20 [01:06<21:09, 66.80s/it]\u001b[32m[I 2021-03-27 08:21:06,382]\u001b[0m Trial 43 finished with value: 0.9526942125987471 and parameters: {'lambda_l1': 3.2707572696344105e-08, 'lambda_l2': 0.0002033018152732567}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:   5%|5         | 1/20 [01:06<21:09, 66.80s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.638821 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  10%|#         | 2/20 [02:14<20:07, 67.06s/it]\u001b[32m[I 2021-03-27 08:22:14,060]\u001b[0m Trial 44 finished with value: 0.9545849909759574 and parameters: {'lambda_l1': 0.00012684545797783257, 'lambda_l2': 0.002996427568804849}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  10%|#         | 2/20 [02:14<20:07, 67.06s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.641619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  15%|#5        | 3/20 [03:25<19:20, 68.28s/it]\u001b[32m[I 2021-03-27 08:23:25,191]\u001b[0m Trial 45 finished with value: 0.9550945595877663 and parameters: {'lambda_l1': 2.612031995529709e-05, 'lambda_l2': 0.002377557500326306}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  15%|#5        | 3/20 [03:25<19:20, 68.28s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.643878 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  20%|##        | 4/20 [04:31<18:00, 67.55s/it]\u001b[32m[I 2021-03-27 08:24:31,028]\u001b[0m Trial 46 finished with value: 0.9578262434976175 and parameters: {'lambda_l1': 0.2972218094521605, 'lambda_l2': 2.077686057845348e-08}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  20%|##        | 4/20 [04:31<18:00, 67.55s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.633603 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  25%|##5       | 5/20 [05:50<17:43, 70.92s/it]\u001b[32m[I 2021-03-27 08:25:49,797]\u001b[0m Trial 47 finished with value: 0.9542664448842316 and parameters: {'lambda_l1': 3.137429270180744e-05, 'lambda_l2': 0.09956673331631256}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  25%|##5       | 5/20 [05:50<17:43, 70.92s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.638948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  30%|###       | 6/20 [06:51<15:51, 67.98s/it]\u001b[32m[I 2021-03-27 08:26:50,929]\u001b[0m Trial 48 finished with value: 0.9551745154976266 and parameters: {'lambda_l1': 0.004318868620268891, 'lambda_l2': 0.00023065180900197396}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  30%|###       | 6/20 [06:51<15:51, 67.98s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.638507 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  35%|###5      | 7/20 [07:55<14:28, 66.77s/it]\u001b[32m[I 2021-03-27 08:27:54,878]\u001b[0m Trial 49 finished with value: 0.9537379967004228 and parameters: {'lambda_l1': 1.1717862199666758, 'lambda_l2': 3.6153346521270797}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  35%|###5      | 7/20 [07:55<14:28, 66.77s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.655383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  40%|####      | 8/20 [09:02<13:23, 66.94s/it]\u001b[32m[I 2021-03-27 08:29:02,214]\u001b[0m Trial 50 finished with value: 0.9547762779821791 and parameters: {'lambda_l1': 0.0007507571072391607, 'lambda_l2': 0.00033177445341534196}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  40%|####      | 8/20 [09:02<13:23, 66.94s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.664495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  45%|####5     | 9/20 [10:11<12:23, 67.62s/it]\u001b[32m[I 2021-03-27 08:30:11,420]\u001b[0m Trial 51 finished with value: 0.9535463964508087 and parameters: {'lambda_l1': 2.074093559438842e-06, 'lambda_l2': 1.7779789622998548e-06}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  45%|####5     | 9/20 [10:11<12:23, 67.62s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.633139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  50%|#####     | 10/20 [11:39<12:16, 73.62s/it]\u001b[32m[I 2021-03-27 08:31:39,035]\u001b[0m Trial 52 finished with value: 0.9545277333766917 and parameters: {'lambda_l1': 6.652675308413599, 'lambda_l2': 8.310180152675574}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  50%|#####     | 10/20 [11:39<12:16, 73.62s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.643993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  55%|#####5    | 11/20 [12:48<10:50, 72.28s/it]\u001b[32m[I 2021-03-27 08:32:48,181]\u001b[0m Trial 53 finished with value: 0.9535463968334446 and parameters: {'lambda_l1': 1.3984140425081362e-08, 'lambda_l2': 8.157275926971665e-08}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  55%|#####5    | 11/20 [12:48<10:50, 72.28s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.622218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  60%|######    | 12/20 [13:57<09:30, 71.35s/it]\u001b[32m[I 2021-03-27 08:33:57,362]\u001b[0m Trial 54 finished with value: 0.9535463967193278 and parameters: {'lambda_l1': 3.8511232681761e-08, 'lambda_l2': 2.477026879877713e-06}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  60%|######    | 12/20 [13:57<09:30, 71.35s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.596999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  65%|######5   | 13/20 [15:07<08:16, 70.92s/it]\u001b[32m[I 2021-03-27 08:35:07,293]\u001b[0m Trial 55 finished with value: 0.9535463965988935 and parameters: {'lambda_l1': 4.7298764971070236e-07, 'lambda_l2': 3.710936146369574e-06}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  65%|######5   | 13/20 [15:07<08:16, 70.92s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.608165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  70%|#######   | 14/20 [16:16<07:01, 70.24s/it]\u001b[32m[I 2021-03-27 08:36:15,931]\u001b[0m Trial 56 finished with value: 0.9535463966578521 and parameters: {'lambda_l1': 5.224766675683508e-07, 'lambda_l2': 2.2072262697207006e-06}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  70%|#######   | 14/20 [16:16<07:01, 70.24s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.603407 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  75%|#######5  | 15/20 [17:25<05:49, 69.85s/it]\u001b[32m[I 2021-03-27 08:37:24,873]\u001b[0m Trial 57 finished with value: 0.9535463946128963 and parameters: {'lambda_l1': 8.04328873672495e-07, 'lambda_l2': 4.573196206315519e-05}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  75%|#######5  | 15/20 [17:25<05:49, 69.85s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.579259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  80%|########  | 16/20 [18:34<04:38, 69.53s/it]\u001b[32m[I 2021-03-27 08:38:33,670]\u001b[0m Trial 58 finished with value: 0.9535463926539499 and parameters: {'lambda_l1': 2.4484685639270707e-08, 'lambda_l2': 9.057869020595321e-05}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  80%|########  | 16/20 [18:34<04:38, 69.53s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.577897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  85%|########5 | 17/20 [19:45<03:30, 70.04s/it]\u001b[32m[I 2021-03-27 08:39:44,889]\u001b[0m Trial 59 finished with value: 0.9541649789346845 and parameters: {'lambda_l1': 2.133080429863582e-08, 'lambda_l2': 0.03413539352105277}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  85%|########5 | 17/20 [19:45<03:30, 70.04s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.586976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  90%|######### | 18/20 [20:53<02:19, 69.55s/it]\u001b[32m[I 2021-03-27 08:40:53,292]\u001b[0m Trial 60 finished with value: 0.9535463954238574 and parameters: {'lambda_l1': 1.0408846195148266e-08, 'lambda_l2': 3.053999891969109e-05}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  90%|######### | 18/20 [20:53<02:19, 69.55s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.538065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694:  95%|#########5| 19/20 [22:03<01:09, 69.69s/it]\u001b[32m[I 2021-03-27 08:42:03,315]\u001b[0m Trial 61 finished with value: 0.9555417359558704 and parameters: {'lambda_l1': 7.950496848551223e-08, 'lambda_l2': 0.03284863461573743}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694:  95%|#########5| 19/20 [22:03<01:09, 69.69s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.580500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "regularization_factors, val_score: 0.952694: 100%|##########| 20/20 [23:07<00:00, 67.84s/it]\u001b[32m[I 2021-03-27 08:43:06,847]\u001b[0m Trial 62 finished with value: 0.9548570990260709 and parameters: {'lambda_l1': 0.02287487030680652, 'lambda_l2': 1.719566492982721e-07}. Best is trial 43 with value: 0.9526942125987471.\u001b[0m\n",
      "regularization_factors, val_score: 0.952694: 100%|##########| 20/20 [23:07<00:00, 69.36s/it]\n",
      "min_data_in_leaf, val_score: 0.952694:   0%|          | 0/5 [00:00<?, ?it/s][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.587819 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "min_data_in_leaf, val_score: 0.952694:  20%|##        | 1/5 [01:15<05:03, 75.88s/it]\u001b[32m[I 2021-03-27 08:44:22,750]\u001b[0m Trial 63 finished with value: 0.9562598206070992 and parameters: {'min_child_samples': 25}. Best is trial 63 with value: 0.9562598206070992.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.952694:  20%|##        | 1/5 [01:15<05:03, 75.88s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.583738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "min_data_in_leaf, val_score: 0.952694:  40%|####      | 2/5 [02:16<03:34, 71.34s/it]\u001b[32m[I 2021-03-27 08:45:23,493]\u001b[0m Trial 64 finished with value: 0.9555746493146429 and parameters: {'min_child_samples': 50}. Best is trial 64 with value: 0.9555746493146429.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.952694:  40%|####      | 2/5 [02:16<03:34, 71.34s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.578428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "min_data_in_leaf, val_score: 0.952694:  60%|######    | 3/5 [03:23<02:19, 69.89s/it]\u001b[32m[I 2021-03-27 08:46:30,005]\u001b[0m Trial 65 finished with value: 0.9550814145859486 and parameters: {'min_child_samples': 5}. Best is trial 65 with value: 0.9550814145859486.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.952694:  60%|######    | 3/5 [03:23<02:19, 69.89s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.578905 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "min_data_in_leaf, val_score: 0.952694:  80%|########  | 4/5 [04:34<01:10, 70.28s/it]\u001b[32m[I 2021-03-27 08:47:41,188]\u001b[0m Trial 66 finished with value: 0.9576895897967577 and parameters: {'min_child_samples': 100}. Best is trial 65 with value: 0.9550814145859486.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.952694:  80%|########  | 4/5 [04:34<01:10, 70.28s/it][LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.585804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 242236\n",
      "[LightGBM] [Info] Number of data points in the train set: 334563, number of used features: 1016\n",
      "[LightGBM] [Info] Start training from score 2.134370\n",
      "min_data_in_leaf, val_score: 0.952694: 100%|##########| 5/5 [05:42<00:00, 69.62s/it]\u001b[32m[I 2021-03-27 08:48:49,283]\u001b[0m Trial 67 finished with value: 0.9566700418396694 and parameters: {'min_child_samples': 10}. Best is trial 65 with value: 0.9550814145859486.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.952694: 100%|##########| 5/5 [05:42<00:00, 68.49s/it]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7fd3446e3f70>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# 目的変数\n",
    "target = 'doi_cites'\n",
    "\n",
    "y_train = df_pre_train[target].values\n",
    "y_valid = df_pre_valid[target].values\n",
    "\n",
    "x_train = df_pre_train.copy()\n",
    "x_valid = df_pre_valid.copy()\n",
    "\n",
    "x_train = x_train.drop(\n",
    "    ['id', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license',\n",
    "    'abstract', 'versions', 'authors_parsed', 'pub_publisher',\n",
    "    'update_date_y', 'first_created_date', 'last_created_date', 'doi_id', 'submitter', 'cites', 'author_first', 'category_main', 'category_main_detail', 'category_name_parent_main_unique', 'category_name_parent_unique', 'category_name_unique'], axis=1\n",
    ")\n",
    "\n",
    "x_valid = x_valid.drop(\n",
    "    ['id', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license',\n",
    "    'abstract', 'versions', 'authors_parsed', 'pub_publisher',\n",
    "    'update_date_y', 'first_created_date', 'last_created_date', 'doi_id', 'submitter', 'cites', 'author_first', 'category_main', 'category_main_detail', 'category_name_parent_main_unique', 'category_name_parent_unique', 'category_name_unique'], axis=1\n",
    ")\n",
    "\n",
    "# drop\n",
    "x_train = x_train.drop(del_col, axis=1)\n",
    "x_valid = x_valid.drop(del_col, axis=1)\n",
    "\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    #'learning_rate': 0.02,\n",
    "    'num_iterations': 10000,\n",
    "    'early_stopping_rounds': 50\n",
    "}\n",
    "trains = lgbo.Dataset(x_train, y_train)\n",
    "valids = lgbo.Dataset(x_valid, y_valid)\n",
    "\n",
    "best = lgbo.train(params, trains, valid_sets=valids,\n",
    "                    verbose_eval=False,\n",
    "                    num_boost_round=100,\n",
    "                    early_stopping_rounds=50)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'objective': 'regression',\n",
       " 'metric': 'rmse',\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 3.2707572696344105e-08,\n",
       " 'lambda_l2': 0.0002033018152732567,\n",
       " 'num_leaves': 212,\n",
       " 'feature_fraction': 0.5,\n",
       " 'bagging_fraction': 1.0,\n",
       " 'bagging_freq': 0,\n",
       " 'min_child_samples': 20,\n",
       " 'num_iterations': 10000,\n",
       " 'early_stopping_round': 50,\n",
       " 'categorical_column': [241, 242, 243, 244, 245, 246, 248, 249, 250]}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "best.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((836407, 1324), (15117, 1324))"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df_pre_train = df_train[df_train['cites'].isnull()].reset_index(drop=True)\n",
    "df_pre_valid = df_train[df_train['cites'].isnull()==False].reset_index(drop=True)\n",
    "df_pre_train.shape, df_pre_valid.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\tvalid_0's rmse: 1.29575\n",
      "[20]\tvalid_0's rmse: 1.25119\n",
      "[30]\tvalid_0's rmse: 1.21271\n",
      "[40]\tvalid_0's rmse: 1.17887\n",
      "[50]\tvalid_0's rmse: 1.1499\n",
      "[60]\tvalid_0's rmse: 1.12571\n",
      "[70]\tvalid_0's rmse: 1.10455\n",
      "[80]\tvalid_0's rmse: 1.0867\n",
      "[90]\tvalid_0's rmse: 1.07119\n",
      "[100]\tvalid_0's rmse: 1.05783\n",
      "[110]\tvalid_0's rmse: 1.04666\n",
      "[120]\tvalid_0's rmse: 1.03684\n",
      "[130]\tvalid_0's rmse: 1.02802\n",
      "[140]\tvalid_0's rmse: 1.02091\n",
      "[150]\tvalid_0's rmse: 1.0146\n",
      "[160]\tvalid_0's rmse: 1.00898\n",
      "[170]\tvalid_0's rmse: 1.00394\n",
      "[180]\tvalid_0's rmse: 0.999782\n",
      "[190]\tvalid_0's rmse: 0.99592\n",
      "[200]\tvalid_0's rmse: 0.992443\n",
      "[210]\tvalid_0's rmse: 0.989357\n",
      "[220]\tvalid_0's rmse: 0.98647\n",
      "[230]\tvalid_0's rmse: 0.983841\n",
      "[240]\tvalid_0's rmse: 0.981316\n",
      "[250]\tvalid_0's rmse: 0.97899\n",
      "[260]\tvalid_0's rmse: 0.976763\n",
      "[270]\tvalid_0's rmse: 0.974798\n",
      "[280]\tvalid_0's rmse: 0.973143\n",
      "[290]\tvalid_0's rmse: 0.971353\n",
      "[300]\tvalid_0's rmse: 0.969649\n",
      "[310]\tvalid_0's rmse: 0.968079\n",
      "[320]\tvalid_0's rmse: 0.966177\n",
      "[330]\tvalid_0's rmse: 0.964635\n",
      "[340]\tvalid_0's rmse: 0.96301\n",
      "[350]\tvalid_0's rmse: 0.961398\n",
      "[360]\tvalid_0's rmse: 0.959942\n",
      "[370]\tvalid_0's rmse: 0.958652\n",
      "[380]\tvalid_0's rmse: 0.957448\n",
      "[390]\tvalid_0's rmse: 0.956103\n",
      "[400]\tvalid_0's rmse: 0.95506\n",
      "[410]\tvalid_0's rmse: 0.953989\n",
      "[420]\tvalid_0's rmse: 0.952789\n",
      "[430]\tvalid_0's rmse: 0.951794\n",
      "[440]\tvalid_0's rmse: 0.950899\n",
      "[450]\tvalid_0's rmse: 0.950086\n",
      "[460]\tvalid_0's rmse: 0.949324\n",
      "[470]\tvalid_0's rmse: 0.948411\n",
      "[480]\tvalid_0's rmse: 0.947632\n",
      "[490]\tvalid_0's rmse: 0.946808\n",
      "[500]\tvalid_0's rmse: 0.945897\n",
      "[510]\tvalid_0's rmse: 0.945145\n",
      "[520]\tvalid_0's rmse: 0.944509\n",
      "[530]\tvalid_0's rmse: 0.9437\n",
      "[540]\tvalid_0's rmse: 0.943147\n",
      "[550]\tvalid_0's rmse: 0.942526\n",
      "[560]\tvalid_0's rmse: 0.941859\n",
      "[570]\tvalid_0's rmse: 0.941324\n",
      "[580]\tvalid_0's rmse: 0.940729\n",
      "[590]\tvalid_0's rmse: 0.940163\n",
      "[600]\tvalid_0's rmse: 0.939793\n",
      "[610]\tvalid_0's rmse: 0.939326\n",
      "[620]\tvalid_0's rmse: 0.938886\n",
      "[630]\tvalid_0's rmse: 0.93842\n",
      "[640]\tvalid_0's rmse: 0.938009\n",
      "[650]\tvalid_0's rmse: 0.937614\n",
      "[660]\tvalid_0's rmse: 0.937204\n",
      "[670]\tvalid_0's rmse: 0.936745\n",
      "[680]\tvalid_0's rmse: 0.936362\n",
      "[690]\tvalid_0's rmse: 0.936051\n",
      "[700]\tvalid_0's rmse: 0.935717\n",
      "[710]\tvalid_0's rmse: 0.935433\n",
      "[720]\tvalid_0's rmse: 0.935114\n",
      "[730]\tvalid_0's rmse: 0.934825\n",
      "[740]\tvalid_0's rmse: 0.934472\n",
      "[750]\tvalid_0's rmse: 0.934131\n",
      "[760]\tvalid_0's rmse: 0.933869\n",
      "[770]\tvalid_0's rmse: 0.933601\n",
      "[780]\tvalid_0's rmse: 0.933315\n",
      "[790]\tvalid_0's rmse: 0.933034\n",
      "[800]\tvalid_0's rmse: 0.93277\n",
      "[810]\tvalid_0's rmse: 0.932548\n",
      "[820]\tvalid_0's rmse: 0.932352\n",
      "[830]\tvalid_0's rmse: 0.932119\n",
      "[840]\tvalid_0's rmse: 0.931877\n",
      "[850]\tvalid_0's rmse: 0.931695\n",
      "[860]\tvalid_0's rmse: 0.931513\n",
      "[870]\tvalid_0's rmse: 0.931308\n",
      "[880]\tvalid_0's rmse: 0.931101\n",
      "[890]\tvalid_0's rmse: 0.930934\n",
      "[900]\tvalid_0's rmse: 0.930719\n",
      "[910]\tvalid_0's rmse: 0.930537\n",
      "[920]\tvalid_0's rmse: 0.930311\n",
      "[930]\tvalid_0's rmse: 0.930151\n",
      "[940]\tvalid_0's rmse: 0.929981\n",
      "[950]\tvalid_0's rmse: 0.92982\n",
      "[960]\tvalid_0's rmse: 0.929632\n",
      "[970]\tvalid_0's rmse: 0.92949\n",
      "[980]\tvalid_0's rmse: 0.929337\n",
      "[990]\tvalid_0's rmse: 0.929224\n",
      "[1000]\tvalid_0's rmse: 0.92903\n",
      "[1010]\tvalid_0's rmse: 0.928891\n",
      "[1020]\tvalid_0's rmse: 0.928725\n",
      "[1030]\tvalid_0's rmse: 0.928562\n",
      "[1040]\tvalid_0's rmse: 0.928453\n",
      "[1050]\tvalid_0's rmse: 0.928294\n",
      "[1060]\tvalid_0's rmse: 0.928143\n",
      "[1070]\tvalid_0's rmse: 0.927997\n",
      "[1080]\tvalid_0's rmse: 0.927856\n",
      "[1090]\tvalid_0's rmse: 0.927687\n",
      "[1100]\tvalid_0's rmse: 0.927518\n",
      "[1110]\tvalid_0's rmse: 0.927365\n",
      "[1120]\tvalid_0's rmse: 0.927234\n",
      "[1130]\tvalid_0's rmse: 0.927129\n",
      "[1140]\tvalid_0's rmse: 0.927006\n",
      "[1150]\tvalid_0's rmse: 0.926903\n",
      "[1160]\tvalid_0's rmse: 0.926782\n",
      "[1170]\tvalid_0's rmse: 0.926631\n",
      "[1180]\tvalid_0's rmse: 0.92655\n",
      "[1190]\tvalid_0's rmse: 0.926433\n",
      "[1200]\tvalid_0's rmse: 0.926347\n",
      "[1210]\tvalid_0's rmse: 0.926224\n",
      "[1220]\tvalid_0's rmse: 0.926093\n",
      "[1230]\tvalid_0's rmse: 0.925996\n",
      "[1240]\tvalid_0's rmse: 0.925882\n",
      "[1250]\tvalid_0's rmse: 0.925778\n",
      "[1260]\tvalid_0's rmse: 0.925712\n",
      "[1270]\tvalid_0's rmse: 0.925617\n",
      "[1280]\tvalid_0's rmse: 0.925519\n",
      "[1290]\tvalid_0's rmse: 0.925403\n",
      "[1300]\tvalid_0's rmse: 0.925298\n",
      "[1310]\tvalid_0's rmse: 0.925211\n",
      "[1320]\tvalid_0's rmse: 0.925077\n",
      "[1330]\tvalid_0's rmse: 0.92498\n",
      "[1340]\tvalid_0's rmse: 0.924894\n",
      "[1350]\tvalid_0's rmse: 0.924829\n",
      "[1360]\tvalid_0's rmse: 0.924765\n",
      "[1370]\tvalid_0's rmse: 0.924689\n",
      "[1380]\tvalid_0's rmse: 0.924617\n",
      "[1390]\tvalid_0's rmse: 0.924571\n",
      "[1400]\tvalid_0's rmse: 0.924516\n",
      "[1410]\tvalid_0's rmse: 0.924449\n",
      "[1420]\tvalid_0's rmse: 0.924387\n",
      "[1430]\tvalid_0's rmse: 0.924303\n",
      "[1440]\tvalid_0's rmse: 0.924218\n",
      "[1450]\tvalid_0's rmse: 0.924155\n",
      "[1460]\tvalid_0's rmse: 0.924141\n",
      "[1470]\tvalid_0's rmse: 0.924011\n",
      "[1480]\tvalid_0's rmse: 0.923937\n",
      "[1490]\tvalid_0's rmse: 0.923906\n",
      "[1500]\tvalid_0's rmse: 0.923828\n",
      "[1510]\tvalid_0's rmse: 0.923767\n",
      "[1520]\tvalid_0's rmse: 0.923741\n",
      "[1530]\tvalid_0's rmse: 0.923698\n",
      "[1540]\tvalid_0's rmse: 0.923609\n",
      "[1550]\tvalid_0's rmse: 0.9235\n",
      "[1560]\tvalid_0's rmse: 0.923394\n",
      "[1570]\tvalid_0's rmse: 0.923326\n",
      "[1580]\tvalid_0's rmse: 0.923241\n",
      "[1590]\tvalid_0's rmse: 0.923127\n",
      "[1600]\tvalid_0's rmse: 0.92309\n",
      "[1610]\tvalid_0's rmse: 0.922997\n",
      "[1620]\tvalid_0's rmse: 0.922955\n",
      "[1630]\tvalid_0's rmse: 0.922926\n",
      "[1640]\tvalid_0's rmse: 0.922863\n",
      "[1650]\tvalid_0's rmse: 0.922796\n",
      "[1660]\tvalid_0's rmse: 0.922744\n",
      "[1670]\tvalid_0's rmse: 0.922664\n",
      "[1680]\tvalid_0's rmse: 0.922595\n",
      "[1690]\tvalid_0's rmse: 0.922576\n",
      "[1700]\tvalid_0's rmse: 0.92251\n",
      "[1710]\tvalid_0's rmse: 0.922431\n",
      "[1720]\tvalid_0's rmse: 0.922377\n",
      "[1730]\tvalid_0's rmse: 0.922347\n",
      "[1740]\tvalid_0's rmse: 0.922289\n",
      "[1750]\tvalid_0's rmse: 0.92224\n",
      "[1760]\tvalid_0's rmse: 0.922192\n",
      "[1770]\tvalid_0's rmse: 0.922132\n",
      "[1780]\tvalid_0's rmse: 0.922101\n",
      "[1790]\tvalid_0's rmse: 0.9221\n",
      "[1800]\tvalid_0's rmse: 0.922063\n",
      "[1810]\tvalid_0's rmse: 0.922007\n",
      "[1820]\tvalid_0's rmse: 0.92192\n",
      "[1830]\tvalid_0's rmse: 0.921872\n",
      "[1840]\tvalid_0's rmse: 0.921817\n",
      "[1850]\tvalid_0's rmse: 0.921773\n",
      "[1860]\tvalid_0's rmse: 0.921703\n",
      "[1870]\tvalid_0's rmse: 0.921638\n",
      "[1880]\tvalid_0's rmse: 0.921617\n",
      "[1890]\tvalid_0's rmse: 0.92156\n",
      "[1900]\tvalid_0's rmse: 0.921491\n",
      "[1910]\tvalid_0's rmse: 0.921454\n",
      "[1920]\tvalid_0's rmse: 0.921458\n",
      "[1930]\tvalid_0's rmse: 0.921397\n",
      "[1940]\tvalid_0's rmse: 0.921336\n",
      "[1950]\tvalid_0's rmse: 0.92131\n",
      "[1960]\tvalid_0's rmse: 0.921289\n",
      "[1970]\tvalid_0's rmse: 0.92119\n",
      "[1980]\tvalid_0's rmse: 0.921117\n",
      "[1990]\tvalid_0's rmse: 0.921078\n",
      "[2000]\tvalid_0's rmse: 0.920995\n",
      "[2010]\tvalid_0's rmse: 0.920992\n",
      "[2020]\tvalid_0's rmse: 0.920932\n",
      "[2030]\tvalid_0's rmse: 0.920909\n",
      "[2040]\tvalid_0's rmse: 0.920907\n",
      "[2050]\tvalid_0's rmse: 0.920905\n",
      "[2060]\tvalid_0's rmse: 0.920878\n",
      "[2070]\tvalid_0's rmse: 0.920848\n",
      "[2080]\tvalid_0's rmse: 0.920835\n",
      "[2090]\tvalid_0's rmse: 0.920761\n",
      "[2100]\tvalid_0's rmse: 0.92074\n",
      "[2110]\tvalid_0's rmse: 0.920731\n",
      "[2120]\tvalid_0's rmse: 0.920705\n",
      "[2130]\tvalid_0's rmse: 0.920683\n",
      "[2140]\tvalid_0's rmse: 0.920664\n",
      "[2150]\tvalid_0's rmse: 0.920617\n",
      "[2160]\tvalid_0's rmse: 0.920599\n",
      "[2170]\tvalid_0's rmse: 0.920532\n",
      "[2180]\tvalid_0's rmse: 0.920525\n",
      "[2190]\tvalid_0's rmse: 0.920485\n",
      "[2200]\tvalid_0's rmse: 0.920458\n",
      "[2210]\tvalid_0's rmse: 0.920446\n",
      "[2220]\tvalid_0's rmse: 0.920421\n",
      "[2230]\tvalid_0's rmse: 0.920382\n",
      "[2240]\tvalid_0's rmse: 0.92035\n",
      "[2250]\tvalid_0's rmse: 0.920354\n",
      "[2260]\tvalid_0's rmse: 0.920338\n",
      "[2270]\tvalid_0's rmse: 0.92033\n",
      "[2280]\tvalid_0's rmse: 0.920303\n",
      "[2290]\tvalid_0's rmse: 0.920264\n",
      "[2300]\tvalid_0's rmse: 0.920255\n",
      "[2310]\tvalid_0's rmse: 0.920245\n",
      "[2320]\tvalid_0's rmse: 0.920205\n",
      "[2330]\tvalid_0's rmse: 0.920166\n",
      "[2340]\tvalid_0's rmse: 0.920128\n",
      "[2350]\tvalid_0's rmse: 0.920158\n",
      "[2360]\tvalid_0's rmse: 0.920138\n",
      "[2370]\tvalid_0's rmse: 0.920119\n",
      "[2380]\tvalid_0's rmse: 0.920091\n",
      "[2390]\tvalid_0's rmse: 0.920061\n",
      "[2400]\tvalid_0's rmse: 0.92002\n",
      "[2410]\tvalid_0's rmse: 0.92001\n",
      "[2420]\tvalid_0's rmse: 0.919961\n",
      "[2430]\tvalid_0's rmse: 0.919933\n",
      "[2440]\tvalid_0's rmse: 0.919914\n",
      "[2450]\tvalid_0's rmse: 0.919909\n",
      "[2460]\tvalid_0's rmse: 0.919916\n",
      "[2470]\tvalid_0's rmse: 0.919915\n",
      "[2480]\tvalid_0's rmse: 0.919906\n",
      "[2490]\tvalid_0's rmse: 0.919881\n",
      "[2500]\tvalid_0's rmse: 0.919843\n",
      "[2510]\tvalid_0's rmse: 0.919827\n",
      "[2520]\tvalid_0's rmse: 0.919759\n",
      "[2530]\tvalid_0's rmse: 0.919769\n",
      "[2540]\tvalid_0's rmse: 0.919745\n",
      "[2550]\tvalid_0's rmse: 0.919734\n",
      "[2560]\tvalid_0's rmse: 0.919726\n",
      "[2570]\tvalid_0's rmse: 0.9197\n",
      "[2580]\tvalid_0's rmse: 0.919664\n",
      "[2590]\tvalid_0's rmse: 0.919643\n",
      "[2600]\tvalid_0's rmse: 0.919617\n",
      "[2610]\tvalid_0's rmse: 0.91959\n",
      "[2620]\tvalid_0's rmse: 0.919596\n",
      "[2630]\tvalid_0's rmse: 0.919563\n",
      "[2640]\tvalid_0's rmse: 0.919578\n",
      "[2650]\tvalid_0's rmse: 0.919544\n",
      "[2660]\tvalid_0's rmse: 0.919505\n",
      "[2670]\tvalid_0's rmse: 0.91949\n",
      "[2680]\tvalid_0's rmse: 0.919448\n",
      "[2690]\tvalid_0's rmse: 0.919454\n",
      "[2700]\tvalid_0's rmse: 0.919441\n",
      "[2710]\tvalid_0's rmse: 0.91943\n",
      "[2720]\tvalid_0's rmse: 0.919404\n",
      "[2730]\tvalid_0's rmse: 0.919377\n",
      "[2740]\tvalid_0's rmse: 0.919379\n",
      "[2750]\tvalid_0's rmse: 0.919384\n",
      "[2760]\tvalid_0's rmse: 0.919347\n",
      "[2770]\tvalid_0's rmse: 0.919339\n",
      "[2780]\tvalid_0's rmse: 0.919288\n",
      "[2790]\tvalid_0's rmse: 0.91926\n",
      "[2800]\tvalid_0's rmse: 0.919282\n",
      "[2810]\tvalid_0's rmse: 0.919276\n",
      "[2820]\tvalid_0's rmse: 0.919255\n",
      "[2830]\tvalid_0's rmse: 0.919203\n",
      "[2840]\tvalid_0's rmse: 0.9192\n",
      "[2850]\tvalid_0's rmse: 0.919182\n",
      "[2860]\tvalid_0's rmse: 0.919168\n",
      "[2870]\tvalid_0's rmse: 0.919167\n",
      "[2880]\tvalid_0's rmse: 0.91916\n",
      "[2890]\tvalid_0's rmse: 0.919138\n",
      "[2900]\tvalid_0's rmse: 0.919131\n",
      "[2910]\tvalid_0's rmse: 0.919126\n",
      "[2920]\tvalid_0's rmse: 0.919106\n",
      "[2930]\tvalid_0's rmse: 0.919097\n",
      "[2940]\tvalid_0's rmse: 0.919105\n",
      "[2950]\tvalid_0's rmse: 0.91908\n",
      "[2960]\tvalid_0's rmse: 0.919076\n",
      "[2970]\tvalid_0's rmse: 0.919036\n",
      "[2980]\tvalid_0's rmse: 0.919015\n",
      "[2990]\tvalid_0's rmse: 0.919019\n",
      "[3000]\tvalid_0's rmse: 0.918999\n",
      "[3010]\tvalid_0's rmse: 0.918965\n",
      "[3020]\tvalid_0's rmse: 0.918962\n",
      "[3030]\tvalid_0's rmse: 0.918931\n",
      "[3040]\tvalid_0's rmse: 0.918948\n",
      "[3050]\tvalid_0's rmse: 0.918925\n",
      "[3060]\tvalid_0's rmse: 0.918908\n",
      "[3070]\tvalid_0's rmse: 0.918876\n",
      "[3080]\tvalid_0's rmse: 0.918856\n",
      "[3090]\tvalid_0's rmse: 0.91883\n",
      "[3100]\tvalid_0's rmse: 0.918811\n",
      "[3110]\tvalid_0's rmse: 0.91883\n",
      "[3120]\tvalid_0's rmse: 0.918791\n",
      "[3130]\tvalid_0's rmse: 0.918768\n",
      "[3140]\tvalid_0's rmse: 0.918758\n",
      "[3150]\tvalid_0's rmse: 0.918722\n",
      "[3160]\tvalid_0's rmse: 0.918727\n",
      "[3170]\tvalid_0's rmse: 0.918721\n",
      "[3180]\tvalid_0's rmse: 0.91872\n",
      "[3190]\tvalid_0's rmse: 0.918682\n",
      "[3200]\tvalid_0's rmse: 0.918669\n",
      "[3210]\tvalid_0's rmse: 0.91863\n",
      "[3220]\tvalid_0's rmse: 0.918615\n",
      "[3230]\tvalid_0's rmse: 0.918587\n",
      "[3240]\tvalid_0's rmse: 0.918552\n",
      "[3250]\tvalid_0's rmse: 0.918522\n",
      "[3260]\tvalid_0's rmse: 0.918489\n",
      "[3270]\tvalid_0's rmse: 0.918465\n",
      "[3280]\tvalid_0's rmse: 0.918466\n",
      "[3290]\tvalid_0's rmse: 0.918423\n",
      "[3300]\tvalid_0's rmse: 0.918401\n",
      "[3310]\tvalid_0's rmse: 0.918395\n",
      "[3320]\tvalid_0's rmse: 0.918378\n",
      "[3330]\tvalid_0's rmse: 0.918348\n",
      "[3340]\tvalid_0's rmse: 0.918322\n",
      "[3350]\tvalid_0's rmse: 0.918315\n",
      "[3360]\tvalid_0's rmse: 0.918314\n",
      "[3370]\tvalid_0's rmse: 0.918305\n",
      "[3380]\tvalid_0's rmse: 0.918322\n",
      "[3390]\tvalid_0's rmse: 0.918325\n",
      "[3400]\tvalid_0's rmse: 0.918343\n",
      "[3410]\tvalid_0's rmse: 0.918355\n",
      "Early stopping, best iteration is:\n",
      "[3368]\tvalid_0's rmse: 0.918299\n",
      "rmsle: 0.9182986879892852\n"
     ]
    }
   ],
   "source": [
    "target = 'doi_cites'\n",
    "\n",
    "y_train = df_pre_train[target].values\n",
    "y_valid = df_pre_valid[target].values\n",
    "\n",
    "x_train = df_pre_train.copy()\n",
    "x_valid = df_pre_valid.copy()\n",
    "\n",
    "x_train = x_train.drop(\n",
    "    ['id', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license',\n",
    "    'abstract', 'versions', 'authors_parsed', 'pub_publisher',\n",
    "    'update_date_y', 'first_created_date', 'last_created_date', 'doi_id', 'submitter', 'cites', 'author_first', 'category_main', 'category_main_detail', 'category_name_parent_main_unique', 'category_name_parent_unique', 'category_name_unique'], axis=1\n",
    ")\n",
    "\n",
    "x_valid = x_valid.drop(\n",
    "    ['id', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license',\n",
    "    'abstract', 'versions', 'authors_parsed', 'pub_publisher',\n",
    "    'update_date_y', 'first_created_date', 'last_created_date', 'doi_id', 'submitter', 'cites', 'author_first', 'category_main', 'category_main_detail', 'category_name_parent_main_unique', 'category_name_parent_unique', 'category_name_unique'], axis=1\n",
    ")\n",
    "\n",
    "# drop\n",
    "x_train = x_train.drop(del_col, axis=1)\n",
    "x_valid = x_valid.drop(del_col, axis=1)\n",
    "\n",
    "lgb_params = {'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_pre_filter': False,\n",
    "    'lambda_l1': 3.2707572696344105e-08,\n",
    "    'lambda_l2': 0.0002033018152732567,\n",
    "    'num_leaves': 212,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 1.0,\n",
    "    'bagging_freq': 0,\n",
    "    'min_child_samples': 20,\n",
    "    'num_iterations': 10000,\n",
    "    'early_stopping_rounds': 50,\n",
    "}\n",
    "\n",
    "# LightGBM\n",
    "model = lgb.LGBMRegressor(**lgb_params)\n",
    "model.fit(x_train, y_train,\n",
    "            eval_set=(x_valid, y_valid),\n",
    "            verbose=10,\n",
    "            early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "pred_doi_cites = model.predict(x_valid)\n",
    "rmsle = mean_squared_error(y_valid, pred_doi_cites, squared=False)\n",
    "print(f\"rmsle: {rmsle}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(f'../models/pred_doi_cites_lgb.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                   importance\n",
       "author_first_label               8.681567e+06\n",
       "update_date_unixtime             6.078218e+06\n",
       "doi_id_label                     5.961599e+06\n",
       "submitter_label                  5.736721e+06\n",
       "update_date_days                 5.349270e+06\n",
       "update_ym                        4.723300e+06\n",
       "pub_publisher_label              4.413643e+06\n",
       "category_name_label              4.211784e+06\n",
       "author_num                       2.114714e+06\n",
       "category_name_parent_label       1.908630e+06\n",
       "update_year                      1.608894e+06\n",
       "category_name_parent_main_label  1.282862e+06\n",
       "category_main_label              1.073643e+06\n",
       "first_created_days               9.331153e+05\n",
       "last_created_days                8.562520e+05\n",
       "first_created_ym                 6.876030e+05\n",
       "pub_journals                     5.375467e+05\n",
       "first_created_unixtime           4.667287e+05\n",
       "roberta_vec_539                  4.143509e+05\n",
       "pub_dois                         4.053277e+05\n",
       "last_created_ym                  3.165391e+05\n",
       "roberta_vec_102                  3.145730e+05\n",
       "last_created_unixtime            3.120696e+05\n",
       "diff_created_unixtime            2.089895e+05\n",
       "rate_created_days                1.890169e+05\n",
       "diff_created_days                1.751837e+05\n",
       "diff_update_date_unixtime        1.373052e+05\n",
       "astro-ph_y                       1.268878e+05\n",
       "is_null_journal-ref              1.258629e+05\n",
       "roberta_vec_39                   1.221578e+05\n",
       "roberta_vec_417                  1.180001e+05\n",
       "num_created                      1.024677e+05\n",
       "roberta_vec_416                  1.015128e+05\n",
       "roberta_vec_115                  9.552389e+04\n",
       "roberta_vec_610                  7.641134e+04\n",
       "roberta_vec_77                   6.761704e+04\n",
       "roberta_vec_300                  6.754094e+04\n",
       "roberta_vec_561                  6.718397e+04\n",
       "roberta_vec_394                  6.467537e+04\n",
       "is_null_report-no                6.250817e+04\n",
       "first_created_year               5.937631e+04\n",
       "roberta_vec_551                  5.931176e+04\n",
       "roberta_vec_637                  5.607687e+04\n",
       "roberta_vec_715                  5.081483e+04\n",
       "roberta_vec_472                  4.938412e+04\n",
       "roberta_vec_355                  4.735340e+04\n",
       "roberta_vec_752                  4.507068e+04\n",
       "roberta_vec_279                  3.949565e+04\n",
       "astro_y                          3.946260e+04\n",
       "roberta_vec_194                  3.839534e+04"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>author_first_label</th>\n      <td>8.681567e+06</td>\n    </tr>\n    <tr>\n      <th>update_date_unixtime</th>\n      <td>6.078218e+06</td>\n    </tr>\n    <tr>\n      <th>doi_id_label</th>\n      <td>5.961599e+06</td>\n    </tr>\n    <tr>\n      <th>submitter_label</th>\n      <td>5.736721e+06</td>\n    </tr>\n    <tr>\n      <th>update_date_days</th>\n      <td>5.349270e+06</td>\n    </tr>\n    <tr>\n      <th>update_ym</th>\n      <td>4.723300e+06</td>\n    </tr>\n    <tr>\n      <th>pub_publisher_label</th>\n      <td>4.413643e+06</td>\n    </tr>\n    <tr>\n      <th>category_name_label</th>\n      <td>4.211784e+06</td>\n    </tr>\n    <tr>\n      <th>author_num</th>\n      <td>2.114714e+06</td>\n    </tr>\n    <tr>\n      <th>category_name_parent_label</th>\n      <td>1.908630e+06</td>\n    </tr>\n    <tr>\n      <th>update_year</th>\n      <td>1.608894e+06</td>\n    </tr>\n    <tr>\n      <th>category_name_parent_main_label</th>\n      <td>1.282862e+06</td>\n    </tr>\n    <tr>\n      <th>category_main_label</th>\n      <td>1.073643e+06</td>\n    </tr>\n    <tr>\n      <th>first_created_days</th>\n      <td>9.331153e+05</td>\n    </tr>\n    <tr>\n      <th>last_created_days</th>\n      <td>8.562520e+05</td>\n    </tr>\n    <tr>\n      <th>first_created_ym</th>\n      <td>6.876030e+05</td>\n    </tr>\n    <tr>\n      <th>pub_journals</th>\n      <td>5.375467e+05</td>\n    </tr>\n    <tr>\n      <th>first_created_unixtime</th>\n      <td>4.667287e+05</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_539</th>\n      <td>4.143509e+05</td>\n    </tr>\n    <tr>\n      <th>pub_dois</th>\n      <td>4.053277e+05</td>\n    </tr>\n    <tr>\n      <th>last_created_ym</th>\n      <td>3.165391e+05</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_102</th>\n      <td>3.145730e+05</td>\n    </tr>\n    <tr>\n      <th>last_created_unixtime</th>\n      <td>3.120696e+05</td>\n    </tr>\n    <tr>\n      <th>diff_created_unixtime</th>\n      <td>2.089895e+05</td>\n    </tr>\n    <tr>\n      <th>rate_created_days</th>\n      <td>1.890169e+05</td>\n    </tr>\n    <tr>\n      <th>diff_created_days</th>\n      <td>1.751837e+05</td>\n    </tr>\n    <tr>\n      <th>diff_update_date_unixtime</th>\n      <td>1.373052e+05</td>\n    </tr>\n    <tr>\n      <th>astro-ph_y</th>\n      <td>1.268878e+05</td>\n    </tr>\n    <tr>\n      <th>is_null_journal-ref</th>\n      <td>1.258629e+05</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_39</th>\n      <td>1.221578e+05</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_417</th>\n      <td>1.180001e+05</td>\n    </tr>\n    <tr>\n      <th>num_created</th>\n      <td>1.024677e+05</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_416</th>\n      <td>1.015128e+05</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_115</th>\n      <td>9.552389e+04</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_610</th>\n      <td>7.641134e+04</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_77</th>\n      <td>6.761704e+04</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_300</th>\n      <td>6.754094e+04</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_561</th>\n      <td>6.718397e+04</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_394</th>\n      <td>6.467537e+04</td>\n    </tr>\n    <tr>\n      <th>is_null_report-no</th>\n      <td>6.250817e+04</td>\n    </tr>\n    <tr>\n      <th>first_created_year</th>\n      <td>5.937631e+04</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_551</th>\n      <td>5.931176e+04</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_637</th>\n      <td>5.607687e+04</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_715</th>\n      <td>5.081483e+04</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_472</th>\n      <td>4.938412e+04</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_355</th>\n      <td>4.735340e+04</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_752</th>\n      <td>4.507068e+04</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_279</th>\n      <td>3.949565e+04</td>\n    </tr>\n    <tr>\n      <th>astro_y</th>\n      <td>3.946260e+04</td>\n    </tr>\n    <tr>\n      <th>roberta_vec_194</th>\n      <td>3.839534e+04</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "feature = x_train.columns\n",
    "df_feature = pd.DataFrame(model.booster_.feature_importance(importance_type='gain'), index=feature, columns=['importance']).sort_values('importance', ascending=False)\n",
    "df_feature.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_train = df_train[df_train['cites'].isnull()].reset_index(drop=True)\n",
    "df_pre_valid = df_train[df_train['cites'].isnull()==False].reset_index(drop=True)\n",
    "df_pre_train.shape, df_pre_valid.shape\n",
    "\n",
    "target = 'doi_cites'\n",
    "\n",
    "y_train = df_pre_train[target].values\n",
    "y_valid = df_pre_valid[target].values\n",
    "\n",
    "x_train = df_pre_train.copy()\n",
    "x_valid = df_pre_valid.copy()\n",
    "\n",
    "x_train = x_train.drop(\n",
    "    ['id', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license',\n",
    "    'abstract', 'versions', 'authors_parsed', 'pub_publisher',\n",
    "    'update_date_y', 'first_created_date', 'last_created_date', 'doi_id', 'submitter', 'cites', 'author_first', 'category_main', 'category_main_detail', 'category_name_parent_main_unique', 'category_name_parent_unique', 'category_name_unique'], axis=1\n",
    ")\n",
    "\n",
    "x_valid = x_valid.drop(\n",
    "    ['id', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license',\n",
    "    'abstract', 'versions', 'authors_parsed', 'pub_publisher',\n",
    "    'update_date_y', 'first_created_date', 'last_created_date', 'doi_id', 'submitter', 'cites', 'author_first', 'category_main', 'category_main_detail', 'category_name_parent_main_unique', 'category_name_parent_unique', 'category_name_unique'], axis=1\n",
    ")\n",
    "\n",
    "# drop\n",
    "x_train = x_train.drop(del_col, axis=1)\n",
    "x_valid = x_valid.drop(del_col, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=1.0, bagging_freq=0, early_stopping_rounds=50,\n",
       "              feature_fraction=0.5, feature_pre_filter=False,\n",
       "              lambda_l1=3.2707572696344105e-08, lambda_l2=0.0002033018152732567,\n",
       "              learning_rate=0.01, metric='rmse', num_iterations=10000,\n",
       "              num_leaves=212, objective='regression')"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "with open('../models/pred_doi_cites_lgb.pickle', mode='rb') as f:\n",
    "    model = pickle.load(f)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(15117, 1022)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rmsle: 0.9182986879892852\n"
     ]
    }
   ],
   "source": [
    "pred_doi_cites = model.predict(x_valid)\n",
    "rmsle = mean_squared_error(y_valid, pred_doi_cites, squared=False)\n",
    "print(f\"rmsle: {rmsle}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(15117, 1325)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "df_train = df_train[df_train['cites'].isnull() == False].reset_index(drop=True)\n",
    "df_train['cites'] = np.log1p(df_train['cites'])\n",
    "df_train['pred_doi_cites'] = pred_doi_cites\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "c_588  roberta_vec_589  roberta_vec_590  \\\n",
       "0         0.041590        12.282816         0.079741        -0.060993   \n",
       "1         0.033566        12.149046         0.052788        -0.019711   \n",
       "2        -0.036725        11.943683         0.009125        -0.021048   \n",
       "3         0.084762        11.718389        -0.038374        -0.011906   \n",
       "4         0.108661        11.651844        -0.063221        -0.013029   \n",
       "\n",
       "   roberta_vec_591  roberta_vec_592  roberta_vec_593  roberta_vec_594  \\\n",
       "0         0.063440        -0.268049        -0.094870         0.024665   \n",
       "1         0.077738        -0.249763        -0.025224         0.030602   \n",
       "2         0.045698        -0.227533        -0.049069        -0.027898   \n",
       "3        -0.001632        -0.224642        -0.079878        -0.099328   \n",
       "4         0.032770        -0.256653         0.004424        -0.158540   \n",
       "\n",
       "   roberta_vec_595  roberta_vec_596  roberta_vec_597  roberta_vec_598  \\\n",
       "0         0.001880         0.187180         0.040667         0.006068   \n",
       "1        -0.058953         0.241092         0.107833        -0.050415   \n",
       "2         0.021803         0.141909         0.143262         0.028343   \n",
       "3         0.124719         0.179040         0.142606        -0.036361   \n",
       "4         0.057062         0.279952         0.185350        -0.026612   \n",
       "\n",
       "   roberta_vec_599  roberta_vec_600  roberta_vec_601  roberta_vec_602  \\\n",
       "0         0.023697         0.187934        -0.221596         0.068942   \n",
       "1        -0.096370         0.116405        -0.205613         0.103472   \n",
       "2         0.013603         0.211610        -0.150498        -0.003177   \n",
       "3        -0.080203         0.120659        -0.049413         0.000616   \n",
       "4        -0.073181         0.144206        -0.092292         0.073268   \n",
       "\n",
       "   roberta_vec_603  roberta_vec_604  roberta_vec_605  roberta_vec_606  \\\n",
       "0         0.005736         0.098046         0.084538         0.091436   \n",
       "1         0.020359         0.087780         0.165174         0.151956   \n",
       "2         0.041957        -0.032630         0.211481         0.146104   \n",
       "3         0.114762         0.136759         0.128589         0.083282   \n",
       "4         0.087679         0.119163         0.097052         0.128711   \n",
       "\n",
       "   roberta_vec_607  roberta_vec_608  roberta_vec_609  roberta_vec_610  \\\n",
       "0        -0.074916         0.082979         0.174427        -0.010615   \n",
       "1        -0.155135         0.048719         0.136277         0.014289   \n",
       "2        -0.185986         0.156548         0.081175         0.050995   \n",
       "3        -0.119280         0.175465         0.175211         0.079239   \n",
       "4        -0.163158         0.196666         0.096923         0.021693   \n",
       "\n",
       "   roberta_vec_611  roberta_vec_612  roberta_vec_613  roberta_vec_614  \\\n",
       "0         0.956542        -0.157765         0.202592        -0.065107   \n",
       "1         0.941260        -0.143702         0.072925        -0.135912   \n",
       "2         0.899738        -0.161586         0.089926        -0.130796   \n",
       "3         0.902776        -0.046504         0.255126        -0.002165   \n",
       "4         0.768567        -0.057968         0.151456        -0.043386   \n",
       "\n",
       "   roberta_vec_615  roberta_vec_616  roberta_vec_617  roberta_vec_618  \\\n",
       "0         0.086034         0.082509         0.116933        -0.130661   \n",
       "1         0.126020         0.117271         0.201530        -0.107007   \n",
       "2        -0.123190         0.086665         0.235653        -0.042723   \n",
       "3        -0.024777         0.089408         0.198304         0.016981   \n",
       "4        -0.004558         0.144182         0.187076         0.072016   \n",
       "\n",
       "   roberta_vec_619  roberta_vec_620  roberta_vec_621  roberta_vec_622  \\\n",
       "0         0.056648        -0.001675        -0.010225         0.085153   \n",
       "1         0.218389         0.065998        -0.083668         0.053277   \n",
       "2         0.308070         0.091798        -0.021499        -0.014432   \n",
       "3         0.230631         0.072909        -0.130362        -0.013177   \n",
       "4         0.245870         0.112004        -0.171178         0.033259   \n",
       "\n",
       "   roberta_vec_623  roberta_vec_624  roberta_vec_625  roberta_vec_626  \\\n",
       "0        -0.174518        -0.064504         0.161261         0.148312   \n",
       "1        -0.147597         0.029762         0.036007        -0.030452   \n",
       "2        -0.194382        -0.075963        -0.058627         0.029009   \n",
       "3        -0.092629        -0.196001         0.119407         0.157684   \n",
       "4        -0.128406        -0.077126         0.080339         0.073449   \n",
       "\n",
       "   roberta_vec_627  roberta_vec_628  roberta_vec_629  roberta_vec_630  \\\n",
       "0        -0.075701        -0.122330        -0.085408         0.294268   \n",
       "1        -0.116128        -0.074170        -0.051684         0.312923   \n",
       "2        -0.040424        -0.000913        -0.051034         0.290411   \n",
       "3        -0.082814         0.007755        -0.103149         0.284048   \n",
       "4        -0.066963         0.034276        -0.062217         0.260491   \n",
       "\n",
       "   roberta_vec_631  roberta_vec_632  roberta_vec_633  roberta_vec_634  \\\n",
       "0        -0.416573         0.074603        -0.209788         0.263318   \n",
       "1        -0.386000         0.135494        -0.153733         0.340041   \n",
       "2        -0.335732         0.170565        -0.109928         0.268078   \n",
       "3        -0.280611         0.074010        -0.182096         0.179465   \n",
       "4        -0.303431         0.150829        -0.128606         0.227643   \n",
       "\n",
       "   roberta_vec_635  roberta_vec_636  roberta_vec_637  roberta_vec_638  \\\n",
       "0         0.310230         0.106331        -0.020113        -0.048917   \n",
       "1         0.280454        -0.007710        -0.049217        -0.141264   \n",
       "2         0.145914        -0.000980        -0.030464        -0.043086   \n",
       "3         0.221553         0.046697        -0.015358         0.039650   \n",
       "4         0.210428         0.000352         0.009954         0.003607   \n",
       "\n",
       "   roberta_vec_639  roberta_vec_640  roberta_vec_641  roberta_vec_642  \\\n",
       "0        -0.070626         0.096161        -0.188461        -0.075923   \n",
       "1         0.125780         0.236436        -0.288352        -0.001873   \n",
       "2         0.071312         0.179723        -0.211874        -0.034422   \n",
       "3         0.081511         0.152500        -0.169331         0.018030   \n",
       "4         0.166239         0.208197        -0.103583        -0.023853   \n",
       "\n",
       "   roberta_vec_643  roberta_vec_644  roberta_vec_645  roberta_vec_646  \\\n",
       "0        -0.234436        -0.020224        -0.090146         0.016027   \n",
       "1        -0.319654         0.173400        -0.170278        -0.004907   \n",
       "2        -0.307077         0.115623        -0.215856         0.036417   \n",
       "3        -0.237969         0.178311        -0.111561         0.018086   \n",
       "4        -0.391357         0.242652        -0.155245         0.061709   \n",
       "\n",
       "   roberta_vec_647  roberta_vec_648  roberta_vec_649  roberta_vec_650  \\\n",
       "0        -0.018889        -0.114500        -0.154983         0.169091   \n",
       "1        -0.229392        -0.096131        -0.110653         0.114807   \n",
       "2         0.080084        -0.064783        -0.117606         0.127752   \n",
       "3         0.100239        -0.067606        -0.186209         0.084413   \n",
       "4         0.067378        -0.110414        -0.202818         0.078849   \n",
       "\n",
       "   roberta_vec_651  roberta_vec_652  roberta_vec_653  roberta_vec_654  \\\n",
       "0         0.098314        -0.164059         0.183723         0.187806   \n",
       "1        -0.077159        -0.306470         0.196648         0.239305   \n",
       "2        -0.012882        -0.205301         0.157589         0.201962   \n",
       "3         0.027594        -0.156649         0.218658         0.102454   \n",
       "4         0.033232        -0.106000         0.203802         0.149935   \n",
       "\n",
       "   roberta_vec_655  roberta_vec_656  roberta_vec_657  roberta_vec_658  \\\n",
       "0        -0.131551        -0.050215        -0.003589         0.114386   \n",
       "1        -0.083120        -0.006266        -0.016332         0.137176   \n",
       "2        -0.038278        -0.064192        -0.049258         0.068961   \n",
       "3        -0.092495        -0.029048         0.020467         0.097208   \n",
       "4        -0.084985         0.015760        -0.048469         0.037743   \n",
       "\n",
       "   roberta_vec_659  roberta_vec_660  roberta_vec_661  roberta_vec_662  \\\n",
       "0        -0.010999        -0.022542         0.019642         0.036398   \n",
       "1        -0.049121         0.032097         0.005268        -0.033742   \n",
       "2        -0.065382         0.155502        -0.050035        -0.160276   \n",
       "3         0.068601         0.086373        -0.188948        -0.147831   \n",
       "4         0.096856         0.143798        -0.136856        -0.153232   \n",
       "\n",
       "   roberta_vec_663  roberta_vec_664  roberta_vec_665  roberta_vec_666  \\\n",
       "0        -0.196691        -0.317724        -0.190680        -0.218587   \n",
       "1        -0.187795        -0.041616        -0.180207        -0.083809   \n",
       "2        -0.177191        -0.369967        -0.243220        -0.121856   \n",
       "3        -0.122629        -0.531026        -0.195737        -0.144605   \n",
       "4        -0.142397        -0.389400        -0.224074        -0.056422   \n",
       "\n",
       "   roberta_vec_667  roberta_vec_668  roberta_vec_669  roberta_vec_670  \\\n",
       "0        -0.481349         0.132161        -0.064054        -0.135440   \n",
       "1        -0.611474         0.132549        -0.097444        -0.145588   \n",
       "2        -0.589593         0.068379        -0.086653        -0.168638   \n",
       "3        -0.535961         0.136488        -0.121504        -0.160985   \n",
       "4        -0.552498         0.065720        -0.176695        -0.079383   \n",
       "\n",
       "   roberta_vec_671  roberta_vec_672  roberta_vec_673  roberta_vec_674  \\\n",
       "0        -0.064407        -0.314367         0.120350        -0.229907   \n",
       "1        -0.041082        -0.310184         0.206319        -0.203444   \n",
       "2        -0.069073        -0.301360         0.129226        -0.260017   \n",
       "3        -0.042444        -0.374259         0.128434        -0.260724   \n",
       "4        -0.159391        -0.431184         0.239871        -0.236745   \n",
       "\n",
       "   roberta_vec_675  roberta_vec_676  roberta_vec_677  roberta_vec_678  \\\n",
       "0        -0.007436        -0.172259         0.153599         0.202984   \n",
       "1        -0.015584        -0.259579         0.176335         0.069659   \n",
       "2         0.015221        -0.192370         0.160849         0.069858   \n",
       "3         0.105819        -0.173876         0.085959        -0.007114   \n",
       "4        -0.009576        -0.231364         0.272019        -0.037922   \n",
       "\n",
       "   roberta_vec_679  roberta_vec_680  roberta_vec_681  roberta_vec_682  \\\n",
       "0         0.076411         0.012433        -0.207095         0.033421   \n",
       "1         0.012299         0.034244        -0.047717         0.055669   \n",
       "2         0.022968         0.029516        -0.123990        -0.063065   \n",
       "3         0.041356        -0.002045        -0.151803        -0.026553   \n",
       "4         0.021827        -0.032716        -0.068437        -0.025033   \n",
       "\n",
       "   roberta_vec_683  roberta_vec_684  roberta_vec_685  roberta_vec_686  \\\n",
       "0        -0.124092         0.215223        -0.086132         0.223904   \n",
       "1         0.003719         0.157838         0.011114         0.159168   \n",
       "2         0.016799         0.150945        -0.042326         0.166288   \n",
       "3        -0.087048         0.191028        -0.033562         0.107551   \n",
       "4        -0.066474         0.184263         0.019257         0.092456   \n",
       "\n",
       "   roberta_vec_687  roberta_vec_688  roberta_vec_689  roberta_vec_690  \\\n",
       "0        -0.158670        -0.441802        -0.005265        -0.076523   \n",
       "1        -0.178828        -0.446934        -0.108646        -0.151625   \n",
       "2        -0.224466        -0.486029        -0.094058        -0.081841   \n",
       "3        -0.343149        -0.405529        -0.163232        -0.044187   \n",
       "4        -0.381083        -0.404728        -0.135549        -0.092909   \n",
       "\n",
       "   roberta_vec_691  roberta_vec_692  roberta_vec_693  roberta_vec_694  \\\n",
       "0         0.007779        -0.222167         0.287094         0.008407   \n",
       "1        -0.023107        -0.169719         0.207508         0.013606   \n",
       "2         0.025652        -0.132757         0.218547         0.033704   \n",
       "3         0.012854        -0.204581         0.237232         0.145715   \n",
       "4        -0.032019        -0.154564         0.245694         0.074664   \n",
       "\n",
       "   roberta_vec_695  roberta_vec_696  roberta_vec_697  roberta_vec_698  \\\n",
       "0        -0.118400         0.111839         0.096921        -0.109888   \n",
       "1        -0.166397         0.069439         0.024586         0.034726   \n",
       "2        -0.149657         0.231645        -0.030209         0.024474   \n",
       "3        -0.125335         0.063213         0.022032        -0.027339   \n",
       "4        -0.105019         0.138730        -0.051127        -0.003614   \n",
       "\n",
       "   roberta_vec_699  roberta_vec_700  roberta_vec_701  roberta_vec_702  \\\n",
       "0        -0.032591        -0.262088        -0.143762         0.001745   \n",
       "1         0.034124        -0.228854        -0.116306         0.059327   \n",
       "2         0.034954        -0.164471         0.055304         0.095869   \n",
       "3         0.020471        -0.217706        -0.001401         0.026243   \n",
       "4        -0.033994        -0.271795        -0.037047         0.037355   \n",
       "\n",
       "   roberta_vec_703  roberta_vec_704  roberta_vec_705  roberta_vec_706  \\\n",
       "0        -0.218345         0.092003         0.029601         0.020191   \n",
       "1        -0.123236         0.111901        -0.049153         0.095247   \n",
       "2        -0.088905         0.045986         0.045848         0.143413   \n",
       "3        -0.193529         0.096699         0.059963         0.140285   \n",
       "4        -0.192260         0.073082         0.057636         0.151957   \n",
       "\n",
       "   roberta_vec_707  roberta_vec_708  roberta_vec_709  roberta_vec_710  \\\n",
       "0         0.111559         0.005574         0.073102         0.047232   \n",
       "1         0.112716        -0.023128         0.058711         0.217587   \n",
       "2         0.154731         0.029386         0.088313         0.053296   \n",
       "3         0.135448         0.055009         0.066489        -0.012882   \n",
       "4         0.184729         0.029239         0.083366        -0.029940   \n",
       "\n",
       "   roberta_vec_711  roberta_vec_712  roberta_vec_713  roberta_vec_714  \\\n",
       "0         0.086060        -0.080468        -0.066836        -0.307097   \n",
       "1        -0.042789        -0.056089         0.032627        -0.305321   \n",
       "2         0.047181        -0.065630         0.069703        -0.299505   \n",
       "3        -0.105614         0.000890         0.034355        -0.243062   \n",
       "4        -0.066711         0.070731         0.069432        -0.227426   \n",
       "\n",
       "   roberta_vec_715  roberta_vec_716  roberta_vec_717  roberta_vec_718  \\\n",
       "0        -0.099523        -0.264944        -0.165200        -0.270445   \n",
       "1         0.011219        -0.307083        -0.103627        -0.063050   \n",
       "2         0.075170        -0.240123        -0.079219        -0.327267   \n",
       "3        -0.077763        -0.166024        -0.111253        -0.237102   \n",
       "4         0.069980        -0.165079        -0.067444        -0.143729   \n",
       "\n",
       "   roberta_vec_719  roberta_vec_720  roberta_vec_721  roberta_vec_722  \\\n",
       "0         0.170967         0.298262         0.029693         0.077656   \n",
       "1         0.115984         0.333934         0.055513         0.008051   \n",
       "2        -0.042809         0.261281         0.048127         0.019458   \n",
       "3        -0.003095         0.293362         0.116610         0.061845   \n",
       "4         0.032275         0.372293         0.041818         0.032854   \n",
       "\n",
       "   roberta_vec_723  roberta_vec_724  roberta_vec_725  roberta_vec_726  \\\n",
       "0         0.060142        -0.168183         0.077958         0.168891   \n",
       "1         0.104721        -0.171578         0.106401         0.010308   \n",
       "2         0.078568        -0.075148         0.000128         0.282459   \n",
       "3        -0.008980        -0.138449         0.131612         0.244518   \n",
       "4         0.146240        -0.137018         0.190802         0.108555   \n",
       "\n",
       "   roberta_vec_727  roberta_vec_728  roberta_vec_729  roberta_vec_730  \\\n",
       "0         0.268703        -0.149408         0.017111        -0.243692   \n",
       "1         0.284912        -0.166739         0.050573        -0.231710   \n",
       "2         0.175455        -0.112744         0.021762        -0.165469   \n",
       "3         0.182840        -0.164329         0.032314        -0.118185   \n",
       "4         0.170401        -0.170474         0.065800        -0.162053   \n",
       "\n",
       "   roberta_vec_731  roberta_vec_732  roberta_vec_733  roberta_vec_734  \\\n",
       "0        -0.190973         0.018555        -0.244975         0.347429   \n",
       "1        -0.157238        -0.059044        -0.136545         0.288966   \n",
       "2        -0.224393         0.010819        -0.105791         0.194084   \n",
       "3        -0.233659        -0.006937        -0.106601         0.157456   \n",
       "4        -0.241368         0.072358        -0.170512         0.181729   \n",
       "\n",
       "   roberta_vec_735  roberta_vec_736  roberta_vec_737  roberta_vec_738  \\\n",
       "0         0.465995         0.145426         0.111209        -0.079083   \n",
       "1         0.348855         0.023084         0.088987        -0.136358   \n",
       "2         0.509446         0.113878         0.128766        -0.175802   \n",
       "3         0.394210         0.110254         0.207392        -0.067668   \n",
       "4         0.394393         0.065402         0.205735        -0.103268   \n",
       "\n",
       "   roberta_vec_739  roberta_vec_740  roberta_vec_741  roberta_vec_742  \\\n",
       "0        -0.170695        -0.197767        -0.445161         0.029228   \n",
       "1        -0.047009        -0.123210        -0.567249        -0.003853   \n",
       "2        -0.106955        -0.074288        -0.475873         0.123529   \n",
       "3        -0.081145        -0.097108        -0.460185        -0.036846   \n",
       "4        -0.039133        -0.052853        -0.421413         0.066120   \n",
       "\n",
       "   roberta_vec_743  roberta_vec_744  roberta_vec_745  roberta_vec_746  \\\n",
       "0         0.070849        -0.132407         0.278908         0.070082   \n",
       "1         0.035672        -0.251738         0.176084         0.085949   \n",
       "2         0.007325        -0.199647         0.186245         0.054156   \n",
       "3        -0.012913        -0.114179         0.144275         0.079694   \n",
       "4        -0.036619        -0.129683         0.152110         0.058852   \n",
       "\n",
       "   roberta_vec_747  roberta_vec_748  roberta_vec_749  roberta_vec_750  \\\n",
       "0        -0.276633         0.595741        -0.456886        -0.110131   \n",
       "1        -0.187019         0.486607        -0.545987        -0.139555   \n",
       "2        -0.242084         0.522150        -0.427954        -0.208415   \n",
       "3        -0.229887         0.509667        -0.542613        -0.205718   \n",
       "4        -0.200716         0.425438        -0.633506        -0.184883   \n",
       "\n",
       "   roberta_vec_751  roberta_vec_752  roberta_vec_753  roberta_vec_754  \\\n",
       "0         0.033729        -0.337985         0.117239         0.234729   \n",
       "1        -0.130976        -0.378054         0.055729         0.309306   \n",
       "2        -0.076315        -0.206017         0.068706         0.202041   \n",
       "3        -0.047161        -0.217698         0.119367         0.281846   \n",
       "4        -0.046711        -0.277424         0.150955         0.174308   \n",
       "\n",
       "   roberta_vec_755  roberta_vec_756  roberta_vec_757  roberta_vec_758  \\\n",
       "0        -0.072439         0.113650         0.048002        -0.158772   \n",
       "1        -0.156030         0.109429        -0.000374        -0.064217   \n",
       "2        -0.066067         0.023081         0.112127        -0.219112   \n",
       "3        -0.062160         0.066264         0.083418        -0.142987   \n",
       "4        -0.191348         0.121687        -0.002648        -0.149996   \n",
       "\n",
       "   roberta_vec_759  roberta_vec_760  roberta_vec_761  roberta_vec_762  \\\n",
       "0        -0.019317         0.086273         0.033125         0.099122   \n",
       "1        -0.051321         0.107150         0.062490         0.142579   \n",
       "2         0.008264        -0.003492         0.049195         0.105896   \n",
       "3        -0.000292        -0.025965         0.085525         0.040288   \n",
       "4         0.036184        -0.095719         0.059427         0.130815   \n",
       "\n",
       "   roberta_vec_763  roberta_vec_764  roberta_vec_765  roberta_vec_766  \\\n",
       "0         0.154706         0.245593         0.108821        -0.001333   \n",
       "1         0.072491         0.352781         0.152339         0.023221   \n",
       "2         0.144375         0.173970         0.156530         0.093464   \n",
       "3         0.167405         0.262107         0.174512        -0.008561   \n",
       "4         0.155930         0.243257         0.070662         0.079929   \n",
       "\n",
       "   roberta_vec_767  pred_doi_cites  fold_no  \n",
       "0         0.027004        2.973234        4  \n",
       "1        -0.117772        3.154369        6  \n",
       "2        -0.037191        1.208062        2  \n",
       "3        -0.022450        4.429161        3  \n",
       "4        -0.050331        2.182262        3  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>submitter</th>\n      <th>authors</th>\n      <th>title</th>\n      <th>comments</th>\n      <th>journal-ref</th>\n      <th>doi</th>\n      <th>report-no</th>\n      <th>categories</th>\n      <th>license</th>\n      <th>abstract</th>\n      <th>versions</th>\n      <th>authors_parsed</th>\n      <th>doi_cites</th>\n      <th>cites</th>\n      <th>doi_id</th>\n      <th>pub_publisher</th>\n      <th>pub_journals</th>\n      <th>pub_dois</th>\n      <th>update_date_y</th>\n      <th>first_created_date</th>\n      <th>last_created_date</th>\n      <th>update_year</th>\n      <th>first_created_year</th>\n      <th>last_created_year</th>\n      <th>update_month</th>\n      <th>first_created_month</th>\n      <th>last_created_month</th>\n      <th>update_ym</th>\n      <th>first_created_ym</th>\n      <th>last_created_ym</th>\n      <th>update_day</th>\n      <th>first_created_day</th>\n      <th>last_created_day</th>\n      <th>update_date_unixtime</th>\n      <th>first_created_unixtime</th>\n      <th>last_created_unixtime</th>\n      <th>diff_update_date_unixtime</th>\n      <th>diff_created_unixtime</th>\n      <th>num_created</th>\n      <th>update_date_days</th>\n      <th>first_created_days</th>\n      <th>last_created_days</th>\n      <th>diff_created_days</th>\n      <th>rate_created_days</th>\n      <th>author_first</th>\n      <th>author_num</th>\n      <th>category_main_detail</th>\n      <th>category_main</th>\n      <th>cs_y</th>\n      <th>econ_y</th>\n      <th>eess_y</th>\n      <th>math_y</th>\n      <th>nlin_y</th>\n      <th>physics_y</th>\n      <th>stat_y</th>\n      <th>category_name_parent_main_unique</th>\n      <th>category_name_parent_unique</th>\n      <th>category_name_unique</th>\n      <th>acc-phys_y</th>\n      <th>adap-org_y</th>\n      <th>alg-geom_y</th>\n      <th>ao-sci_y</th>\n      <th>astro-ph_y</th>\n      <th>atom-ph_y</th>\n      <th>bayes-an_y</th>\n      <th>chao-dyn_y</th>\n      <th>chem-ph_y</th>\n      <th>cmp-lg_y</th>\n      <th>comp-gas_y</th>\n      <th>cond-mat_y</th>\n      <th>dg-ga_y</th>\n      <th>funct-an_y</th>\n      <th>gr-qc_y</th>\n      <th>hep-ex_y</th>\n      <th>hep-lat_y</th>\n      <th>hep-ph_y</th>\n      <th>hep-th_y</th>\n      <th>math-ph_y</th>\n      <th>mtrl-th_y</th>\n      <th>nucl-ex_y</th>\n      <th>nucl-th_y</th>\n      <th>patt-sol_y</th>\n      <th>plasm-ph_y</th>\n      <th>q-alg_y</th>\n      <th>q-bio_y</th>\n      <th>q-fin_y</th>\n      <th>quant-ph_y</th>\n      <th>solv-int_y</th>\n      <th>supr-con_y</th>\n      <th>acc_y</th>\n      <th>adap_y</th>\n      <th>alg_y</th>\n      <th>ao_y</th>\n      <th>astro_y</th>\n      <th>atom_y</th>\n      <th>bayes_y</th>\n      <th>chao_y</th>\n      <th>chem_y</th>\n      <th>cmp_y</th>\n      <th>comp_y</th>\n      <th>cond_y</th>\n      <th>cs</th>\n      <th>dg_y</th>\n      <th>econ</th>\n      <th>eess</th>\n      <th>funct_y</th>\n      <th>gr_y</th>\n      <th>hep_y</th>\n      <th>math</th>\n      <th>mtrl_y</th>\n      <th>nlin</th>\n      <th>nucl_y</th>\n      <th>patt_y</th>\n      <th>physics</th>\n      <th>plasm_y</th>\n      <th>q_y</th>\n      <th>quant_y</th>\n      <th>solv_y</th>\n      <th>stat</th>\n      <th>supr_y</th>\n      <th>astro-ph.co</th>\n      <th>astro-ph.ep</th>\n      <th>astro-ph.ga</th>\n      <th>astro-ph.he</th>\n      <th>astro-ph.im</th>\n      <th>astro-ph.sr</th>\n      <th>cond-mat.dis-nn</th>\n      <th>cond-mat.mes-hall</th>\n      <th>cond-mat.mtrl-sci</th>\n      <th>cond-mat.other</th>\n      <th>cond-mat.quant-gas</th>\n      <th>cond-mat.soft</th>\n      <th>cond-mat.stat-mech</th>\n      <th>cond-mat.str-el</th>\n      <th>cond-mat.supr-con</th>\n      <th>cs.ai</th>\n      <th>cs.ar</th>\n      <th>cs.cc</th>\n      <th>cs.ce</th>\n      <th>cs.cg</th>\n      <th>cs.cl</th>\n      <th>cs.cr</th>\n      <th>cs.cv</th>\n      <th>cs.cy</th>\n      <th>cs.db</th>\n      <th>cs.dc</th>\n      <th>cs.dl</th>\n      <th>cs.dm</th>\n      <th>cs.ds</th>\n      <th>cs.et</th>\n      <th>cs.fl</th>\n      <th>cs.gl</th>\n      <th>cs.gr</th>\n      <th>cs.gt</th>\n      <th>cs.hc</th>\n      <th>cs.ir</th>\n      <th>cs.it</th>\n      <th>cs.lg</th>\n      <th>cs.lo</th>\n      <th>cs.ma</th>\n      <th>cs.mm</th>\n      <th>cs.ms</th>\n      <th>cs.na</th>\n      <th>cs.ne</th>\n      <th>cs.ni</th>\n      <th>cs.oh</th>\n      <th>cs.os</th>\n      <th>cs.pf</th>\n      <th>cs.pl</th>\n      <th>cs.ro</th>\n      <th>cs.sc</th>\n      <th>cs.sd</th>\n      <th>cs.se</th>\n      <th>cs.si</th>\n      <th>cs.sy</th>\n      <th>econ.em</th>\n      <th>econ.gn</th>\n      <th>econ.th</th>\n      <th>eess.as</th>\n      <th>eess.iv</th>\n      <th>eess.sp</th>\n      <th>eess.sy</th>\n      <th>math.ac</th>\n      <th>math.ag</th>\n      <th>math.ap</th>\n      <th>math.at</th>\n      <th>math.ca</th>\n      <th>math.co</th>\n      <th>math.ct</th>\n      <th>math.cv</th>\n      <th>math.dg</th>\n      <th>math.ds</th>\n      <th>math.fa</th>\n      <th>math.gm</th>\n      <th>math.gn</th>\n      <th>math.gr</th>\n      <th>math.gt</th>\n      <th>math.ho</th>\n      <th>math.it</th>\n      <th>math.kt</th>\n      <th>math.lo</th>\n      <th>math.mg</th>\n      <th>math.mp</th>\n      <th>math.na</th>\n      <th>math.nt</th>\n      <th>math.oa</th>\n      <th>math.oc</th>\n      <th>math.pr</th>\n      <th>math.qa</th>\n      <th>math.ra</th>\n      <th>math.rt</th>\n      <th>math.sg</th>\n      <th>math.sp</th>\n      <th>math.st</th>\n      <th>nlin.ao</th>\n      <th>nlin.cd</th>\n      <th>nlin.cg</th>\n      <th>nlin.ps</th>\n      <th>nlin.si</th>\n      <th>physics.acc-ph</th>\n      <th>physics.ao-ph</th>\n      <th>physics.app-ph</th>\n      <th>physics.atm-clus</th>\n      <th>physics.atom-ph</th>\n      <th>physics.bio-ph</th>\n      <th>physics.chem-ph</th>\n      <th>physics.class-ph</th>\n      <th>physics.comp-ph</th>\n      <th>physics.data-an</th>\n      <th>physics.ed-ph</th>\n      <th>physics.flu-dyn</th>\n      <th>physics.gen-ph</th>\n      <th>physics.geo-ph</th>\n      <th>physics.hist-ph</th>\n      <th>physics.ins-det</th>\n      <th>physics.med-ph</th>\n      <th>physics.optics</th>\n      <th>physics.plasm-ph</th>\n      <th>physics.pop-ph</th>\n      <th>physics.soc-ph</th>\n      <th>physics.space-ph</th>\n      <th>q-bio.bm</th>\n      <th>q-bio.cb</th>\n      <th>q-bio.gn</th>\n      <th>q-bio.mn</th>\n      <th>q-bio.nc</th>\n      <th>q-bio.ot</th>\n      <th>q-bio.pe</th>\n      <th>q-bio.qm</th>\n      <th>q-bio.sc</th>\n      <th>q-bio.to</th>\n      <th>q-fin.cp</th>\n      <th>q-fin.ec</th>\n      <th>q-fin.gn</th>\n      <th>q-fin.mf</th>\n      <th>q-fin.pm</th>\n      <th>q-fin.pr</th>\n      <th>q-fin.rm</th>\n      <th>q-fin.st</th>\n      <th>q-fin.tr</th>\n      <th>stat.ap</th>\n      <th>stat.co</th>\n      <th>stat.me</th>\n      <th>stat.ml</th>\n      <th>stat.ot</th>\n      <th>stat.th</th>\n      <th>submitter_label</th>\n      <th>doi_id_label</th>\n      <th>author_first_label</th>\n      <th>pub_publisher_label</th>\n      <th>license_label</th>\n      <th>category_main_label</th>\n      <th>category_main_detail_label</th>\n      <th>category_name_parent_label</th>\n      <th>category_name_parent_main_label</th>\n      <th>category_name_label</th>\n      <th>doi_cites_mean_author_first_label</th>\n      <th>doi_cites_count_author_first_label</th>\n      <th>doi_cites_sum_author_first_label</th>\n      <th>doi_cites_min_author_first_label</th>\n      <th>doi_cites_max_author_first_label</th>\n      <th>doi_cites_median_author_first_label</th>\n      <th>doi_cites_std_author_first_label</th>\n      <th>doi_cites_q10_author_first_label</th>\n      <th>doi_cites_q25_author_first_label</th>\n      <th>doi_cites_q75_author_first_label</th>\n      <th>doi_cites_mean_doi_id_label</th>\n      <th>doi_cites_count_doi_id_label</th>\n      <th>doi_cites_sum_doi_id_label</th>\n      <th>doi_cites_min_doi_id_label</th>\n      <th>doi_cites_max_doi_id_label</th>\n      <th>doi_cites_median_doi_id_label</th>\n      <th>doi_cites_std_doi_id_label</th>\n      <th>doi_cites_q10_doi_id_label</th>\n      <th>doi_cites_q25_doi_id_label</th>\n      <th>doi_cites_q75_doi_id_label</th>\n      <th>doi_cites_mean_pub_publisher_label</th>\n      <th>doi_cites_count_pub_publisher_label</th>\n      <th>doi_cites_sum_pub_publisher_label</th>\n      <th>doi_cites_min_pub_publisher_label</th>\n      <th>doi_cites_max_pub_publisher_label</th>\n      <th>doi_cites_median_pub_publisher_label</th>\n      <th>doi_cites_std_pub_publisher_label</th>\n      <th>doi_cites_q10_pub_publisher_label</th>\n      <th>doi_cites_q25_pub_publisher_label</th>\n      <th>doi_cites_q75_pub_publisher_label</th>\n      <th>doi_cites_mean_submitter_label</th>\n      <th>doi_cites_count_submitter_label</th>\n      <th>doi_cites_sum_submitter_label</th>\n      <th>doi_cites_min_submitter_label</th>\n      <th>doi_cites_max_submitter_label</th>\n      <th>doi_cites_median_submitter_label</th>\n      <th>doi_cites_std_submitter_label</th>\n      <th>doi_cites_q10_submitter_label</th>\n      <th>doi_cites_q25_submitter_label</th>\n      <th>doi_cites_q75_submitter_label</th>\n      <th>doi_cites_mean_update_ym</th>\n      <th>doi_cites_count_update_ym</th>\n      <th>doi_cites_sum_update_ym</th>\n      <th>doi_cites_min_update_ym</th>\n      <th>doi_cites_max_update_ym</th>\n      <th>doi_cites_median_update_ym</th>\n      <th>doi_cites_std_update_ym</th>\n      <th>doi_cites_q10_update_ym</th>\n      <th>doi_cites_q25_update_ym</th>\n      <th>doi_cites_q75_update_ym</th>\n      <th>doi_cites_mean_first_created_ym</th>\n      <th>doi_cites_count_first_created_ym</th>\n      <th>doi_cites_sum_first_created_ym</th>\n      <th>doi_cites_min_first_created_ym</th>\n      <th>doi_cites_max_first_created_ym</th>\n      <th>doi_cites_median_first_created_ym</th>\n      <th>doi_cites_std_first_created_ym</th>\n      <th>doi_cites_q10_first_created_ym</th>\n      <th>doi_cites_q25_first_created_ym</th>\n      <th>doi_cites_q75_first_created_ym</th>\n      <th>doi_cites_mean_license_label</th>\n      <th>doi_cites_count_license_label</th>\n      <th>doi_cites_sum_license_label</th>\n      <th>doi_cites_min_license_label</th>\n      <th>doi_cites_max_license_label</th>\n      <th>doi_cites_median_license_label</th>\n      <th>doi_cites_std_license_label</th>\n      <th>doi_cites_q10_license_label</th>\n      <th>doi_cites_q25_license_label</th>\n      <th>doi_cites_q75_license_label</th>\n      <th>doi_cites_mean_category_main_label</th>\n      <th>doi_cites_count_category_main_label</th>\n      <th>doi_cites_sum_category_main_label</th>\n      <th>doi_cites_min_category_main_label</th>\n      <th>doi_cites_max_category_main_label</th>\n      <th>doi_cites_median_category_main_label</th>\n      <th>doi_cites_std_category_main_label</th>\n      <th>doi_cites_q10_category_main_label</th>\n      <th>doi_cites_q25_category_main_label</th>\n      <th>doi_cites_q75_category_main_label</th>\n      <th>doi_cites_mean_category_main_detail_label</th>\n      <th>doi_cites_count_category_main_detail_label</th>\n      <th>doi_cites_sum_category_main_detail_label</th>\n      <th>doi_cites_min_category_main_detail_label</th>\n      <th>doi_cites_max_category_main_detail_label</th>\n      <th>doi_cites_median_category_main_detail_label</th>\n      <th>doi_cites_std_category_main_detail_label</th>\n      <th>doi_cites_q10_category_main_detail_label</th>\n      <th>doi_cites_q25_category_main_detail_label</th>\n      <th>doi_cites_q75_category_main_detail_label</th>\n      <th>doi_cites_mean_category_name_parent_label</th>\n      <th>doi_cites_count_category_name_parent_label</th>\n      <th>doi_cites_sum_category_name_parent_label</th>\n      <th>doi_cites_min_category_name_parent_label</th>\n      <th>doi_cites_max_category_name_parent_label</th>\n      <th>doi_cites_median_category_name_parent_label</th>\n      <th>doi_cites_std_category_name_parent_label</th>\n      <th>doi_cites_q10_category_name_parent_label</th>\n      <th>doi_cites_q25_category_name_parent_label</th>\n      <th>doi_cites_q75_category_name_parent_label</th>\n      <th>doi_cites_mean_category_name_parent_main_label</th>\n      <th>doi_cites_count_category_name_parent_main_label</th>\n      <th>doi_cites_sum_category_name_parent_main_label</th>\n      <th>doi_cites_min_category_name_parent_main_label</th>\n      <th>doi_cites_max_category_name_parent_main_label</th>\n      <th>doi_cites_median_category_name_parent_main_label</th>\n      <th>doi_cites_std_category_name_parent_main_label</th>\n      <th>doi_cites_q10_category_name_parent_main_label</th>\n      <th>doi_cites_q25_category_name_parent_main_label</th>\n      <th>doi_cites_q75_category_name_parent_main_label</th>\n      <th>doi_cites_mean_category_name_label</th>\n      <th>doi_cites_count_category_name_label</th>\n      <th>doi_cites_sum_category_name_label</th>\n      <th>doi_cites_min_category_name_label</th>\n      <th>doi_cites_max_category_name_label</th>\n      <th>doi_cites_median_category_name_label</th>\n      <th>doi_cites_std_category_name_label</th>\n      <th>doi_cites_q10_category_name_label</th>\n      <th>doi_cites_q25_category_name_label</th>\n      <th>doi_cites_q75_category_name_label</th>\n      <th>diff_doi_cites_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_submitter_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_submitter_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_doi_id_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_author_first_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_update_ym_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_update_ym_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_license_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_license_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_first_created_ym_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_main_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_license_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_license_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_main_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_main_detail_label</th>\n      <th>diff_doi_cites_mean_category_main_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_main_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_main_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_doi_cites_mean_category_name_parent_main_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_label_doi_cites_mean_category_name_label</th>\n      <th>diff_doi_cites_mean_category_name_parent_main_label_doi_cites_mean_category_name_label</th>\n      <th>diff_rate_doi_cites_mean_category_name_parent_main_label_doi_cites_mean_category_name_label</th>\n      <th>is_null_comments</th>\n      <th>is_null_report-no</th>\n      <th>is_null_journal-ref</th>\n      <th>roberta_vec_0</th>\n      <th>roberta_vec_1</th>\n      <th>roberta_vec_2</th>\n      <th>roberta_vec_3</th>\n      <th>roberta_vec_4</th>\n      <th>roberta_vec_5</th>\n      <th>roberta_vec_6</th>\n      <th>roberta_vec_7</th>\n      <th>roberta_vec_8</th>\n      <th>roberta_vec_9</th>\n      <th>roberta_vec_10</th>\n      <th>roberta_vec_11</th>\n      <th>roberta_vec_12</th>\n      <th>roberta_vec_13</th>\n      <th>roberta_vec_14</th>\n      <th>roberta_vec_15</th>\n      <th>roberta_vec_16</th>\n      <th>roberta_vec_17</th>\n      <th>roberta_vec_18</th>\n      <th>roberta_vec_19</th>\n      <th>roberta_vec_20</th>\n      <th>roberta_vec_21</th>\n      <th>roberta_vec_22</th>\n      <th>roberta_vec_23</th>\n      <th>roberta_vec_24</th>\n      <th>roberta_vec_25</th>\n      <th>roberta_vec_26</th>\n      <th>roberta_vec_27</th>\n      <th>roberta_vec_28</th>\n      <th>roberta_vec_29</th>\n      <th>roberta_vec_30</th>\n      <th>roberta_vec_31</th>\n      <th>roberta_vec_32</th>\n      <th>roberta_vec_33</th>\n      <th>roberta_vec_34</th>\n      <th>roberta_vec_35</th>\n      <th>roberta_vec_36</th>\n      <th>roberta_vec_37</th>\n      <th>roberta_vec_38</th>\n      <th>roberta_vec_39</th>\n      <th>roberta_vec_40</th>\n      <th>roberta_vec_41</th>\n      <th>roberta_vec_42</th>\n      <th>roberta_vec_43</th>\n      <th>roberta_vec_44</th>\n      <th>roberta_vec_45</th>\n      <th>roberta_vec_46</th>\n      <th>roberta_vec_47</th>\n      <th>roberta_vec_48</th>\n      <th>roberta_vec_49</th>\n      <th>roberta_vec_50</th>\n      <th>roberta_vec_51</th>\n      <th>roberta_vec_52</th>\n      <th>roberta_vec_53</th>\n      <th>roberta_vec_54</th>\n      <th>roberta_vec_55</th>\n      <th>roberta_vec_56</th>\n      <th>roberta_vec_57</th>\n      <th>roberta_vec_58</th>\n      <th>roberta_vec_59</th>\n      <th>roberta_vec_60</th>\n      <th>roberta_vec_61</th>\n      <th>roberta_vec_62</th>\n      <th>roberta_vec_63</th>\n      <th>roberta_vec_64</th>\n      <th>roberta_vec_65</th>\n      <th>roberta_vec_66</th>\n      <th>roberta_vec_67</th>\n      <th>roberta_vec_68</th>\n      <th>roberta_vec_69</th>\n      <th>roberta_vec_70</th>\n      <th>roberta_vec_71</th>\n      <th>roberta_vec_72</th>\n      <th>roberta_vec_73</th>\n      <th>roberta_vec_74</th>\n      <th>roberta_vec_75</th>\n      <th>roberta_vec_76</th>\n      <th>roberta_vec_77</th>\n      <th>roberta_vec_78</th>\n      <th>roberta_vec_79</th>\n      <th>roberta_vec_80</th>\n      <th>roberta_vec_81</th>\n      <th>roberta_vec_82</th>\n      <th>roberta_vec_83</th>\n      <th>roberta_vec_84</th>\n      <th>roberta_vec_85</th>\n      <th>roberta_vec_86</th>\n      <th>roberta_vec_87</th>\n      <th>roberta_vec_88</th>\n      <th>roberta_vec_89</th>\n      <th>roberta_vec_90</th>\n      <th>roberta_vec_91</th>\n      <th>roberta_vec_92</th>\n      <th>roberta_vec_93</th>\n      <th>roberta_vec_94</th>\n      <th>roberta_vec_95</th>\n      <th>roberta_vec_96</th>\n      <th>roberta_vec_97</th>\n      <th>roberta_vec_98</th>\n      <th>roberta_vec_99</th>\n      <th>roberta_vec_100</th>\n      <th>roberta_vec_101</th>\n      <th>roberta_vec_102</th>\n      <th>roberta_vec_103</th>\n      <th>roberta_vec_104</th>\n      <th>roberta_vec_105</th>\n      <th>roberta_vec_106</th>\n      <th>roberta_vec_107</th>\n      <th>roberta_vec_108</th>\n      <th>roberta_vec_109</th>\n      <th>roberta_vec_110</th>\n      <th>roberta_vec_111</th>\n      <th>roberta_vec_112</th>\n      <th>roberta_vec_113</th>\n      <th>roberta_vec_114</th>\n      <th>roberta_vec_115</th>\n      <th>roberta_vec_116</th>\n      <th>roberta_vec_117</th>\n      <th>roberta_vec_118</th>\n      <th>roberta_vec_119</th>\n      <th>roberta_vec_120</th>\n      <th>roberta_vec_121</th>\n      <th>roberta_vec_122</th>\n      <th>roberta_vec_123</th>\n      <th>roberta_vec_124</th>\n      <th>roberta_vec_125</th>\n      <th>roberta_vec_126</th>\n      <th>roberta_vec_127</th>\n      <th>roberta_vec_128</th>\n      <th>roberta_vec_129</th>\n      <th>roberta_vec_130</th>\n      <th>roberta_vec_131</th>\n      <th>roberta_vec_132</th>\n      <th>roberta_vec_133</th>\n      <th>roberta_vec_134</th>\n      <th>roberta_vec_135</th>\n      <th>roberta_vec_136</th>\n      <th>roberta_vec_137</th>\n      <th>roberta_vec_138</th>\n      <th>roberta_vec_139</th>\n      <th>roberta_vec_140</th>\n      <th>roberta_vec_141</th>\n      <th>roberta_vec_142</th>\n      <th>roberta_vec_143</th>\n      <th>roberta_vec_144</th>\n      <th>roberta_vec_145</th>\n      <th>roberta_vec_146</th>\n      <th>roberta_vec_147</th>\n      <th>roberta_vec_148</th>\n      <th>roberta_vec_149</th>\n      <th>roberta_vec_150</th>\n      <th>roberta_vec_151</th>\n      <th>roberta_vec_152</th>\n      <th>roberta_vec_153</th>\n      <th>roberta_vec_154</th>\n      <th>roberta_vec_155</th>\n      <th>roberta_vec_156</th>\n      <th>roberta_vec_157</th>\n      <th>roberta_vec_158</th>\n      <th>roberta_vec_159</th>\n      <th>roberta_vec_160</th>\n      <th>roberta_vec_161</th>\n      <th>roberta_vec_162</th>\n      <th>roberta_vec_163</th>\n      <th>roberta_vec_164</th>\n      <th>roberta_vec_165</th>\n      <th>roberta_vec_166</th>\n      <th>roberta_vec_167</th>\n      <th>roberta_vec_168</th>\n      <th>roberta_vec_169</th>\n      <th>roberta_vec_170</th>\n      <th>roberta_vec_171</th>\n      <th>roberta_vec_172</th>\n      <th>roberta_vec_173</th>\n      <th>roberta_vec_174</th>\n      <th>roberta_vec_175</th>\n      <th>roberta_vec_176</th>\n      <th>roberta_vec_177</th>\n      <th>roberta_vec_178</th>\n      <th>roberta_vec_179</th>\n      <th>roberta_vec_180</th>\n      <th>roberta_vec_181</th>\n      <th>roberta_vec_182</th>\n      <th>roberta_vec_183</th>\n      <th>roberta_vec_184</th>\n      <th>roberta_vec_185</th>\n      <th>roberta_vec_186</th>\n      <th>roberta_vec_187</th>\n      <th>roberta_vec_188</th>\n      <th>roberta_vec_189</th>\n      <th>roberta_vec_190</th>\n      <th>roberta_vec_191</th>\n      <th>roberta_vec_192</th>\n      <th>roberta_vec_193</th>\n      <th>roberta_vec_194</th>\n      <th>roberta_vec_195</th>\n      <th>roberta_vec_196</th>\n      <th>roberta_vec_197</th>\n      <th>roberta_vec_198</th>\n      <th>roberta_vec_199</th>\n      <th>roberta_vec_200</th>\n      <th>roberta_vec_201</th>\n      <th>roberta_vec_202</th>\n      <th>roberta_vec_203</th>\n      <th>roberta_vec_204</th>\n      <th>roberta_vec_205</th>\n      <th>roberta_vec_206</th>\n      <th>roberta_vec_207</th>\n      <th>roberta_vec_208</th>\n      <th>roberta_vec_209</th>\n      <th>roberta_vec_210</th>\n      <th>roberta_vec_211</th>\n      <th>roberta_vec_212</th>\n      <th>roberta_vec_213</th>\n      <th>roberta_vec_214</th>\n      <th>roberta_vec_215</th>\n      <th>roberta_vec_216</th>\n      <th>roberta_vec_217</th>\n      <th>roberta_vec_218</th>\n      <th>roberta_vec_219</th>\n      <th>roberta_vec_220</th>\n      <th>roberta_vec_221</th>\n      <th>roberta_vec_222</th>\n      <th>roberta_vec_223</th>\n      <th>roberta_vec_224</th>\n      <th>roberta_vec_225</th>\n      <th>roberta_vec_226</th>\n      <th>roberta_vec_227</th>\n      <th>roberta_vec_228</th>\n      <th>roberta_vec_229</th>\n      <th>roberta_vec_230</th>\n      <th>roberta_vec_231</th>\n      <th>roberta_vec_232</th>\n      <th>roberta_vec_233</th>\n      <th>roberta_vec_234</th>\n      <th>roberta_vec_235</th>\n      <th>roberta_vec_236</th>\n      <th>roberta_vec_237</th>\n      <th>roberta_vec_238</th>\n      <th>roberta_vec_239</th>\n      <th>roberta_vec_240</th>\n      <th>roberta_vec_241</th>\n      <th>roberta_vec_242</th>\n      <th>roberta_vec_243</th>\n      <th>roberta_vec_244</th>\n      <th>roberta_vec_245</th>\n      <th>roberta_vec_246</th>\n      <th>roberta_vec_247</th>\n      <th>roberta_vec_248</th>\n      <th>roberta_vec_249</th>\n      <th>roberta_vec_250</th>\n      <th>roberta_vec_251</th>\n      <th>roberta_vec_252</th>\n      <th>roberta_vec_253</th>\n      <th>roberta_vec_254</th>\n      <th>roberta_vec_255</th>\n      <th>roberta_vec_256</th>\n      <th>roberta_vec_257</th>\n      <th>roberta_vec_258</th>\n      <th>roberta_vec_259</th>\n      <th>roberta_vec_260</th>\n      <th>roberta_vec_261</th>\n      <th>roberta_vec_262</th>\n      <th>roberta_vec_263</th>\n      <th>roberta_vec_264</th>\n      <th>roberta_vec_265</th>\n      <th>roberta_vec_266</th>\n      <th>roberta_vec_267</th>\n      <th>roberta_vec_268</th>\n      <th>roberta_vec_269</th>\n      <th>roberta_vec_270</th>\n      <th>roberta_vec_271</th>\n      <th>roberta_vec_272</th>\n      <th>roberta_vec_273</th>\n      <th>roberta_vec_274</th>\n      <th>roberta_vec_275</th>\n      <th>roberta_vec_276</th>\n      <th>roberta_vec_277</th>\n      <th>roberta_vec_278</th>\n      <th>roberta_vec_279</th>\n      <th>roberta_vec_280</th>\n      <th>roberta_vec_281</th>\n      <th>roberta_vec_282</th>\n      <th>roberta_vec_283</th>\n      <th>roberta_vec_284</th>\n      <th>roberta_vec_285</th>\n      <th>roberta_vec_286</th>\n      <th>roberta_vec_287</th>\n      <th>roberta_vec_288</th>\n      <th>roberta_vec_289</th>\n      <th>roberta_vec_290</th>\n      <th>roberta_vec_291</th>\n      <th>roberta_vec_292</th>\n      <th>roberta_vec_293</th>\n      <th>roberta_vec_294</th>\n      <th>roberta_vec_295</th>\n      <th>roberta_vec_296</th>\n      <th>roberta_vec_297</th>\n      <th>roberta_vec_298</th>\n      <th>roberta_vec_299</th>\n      <th>roberta_vec_300</th>\n      <th>roberta_vec_301</th>\n      <th>roberta_vec_302</th>\n      <th>roberta_vec_303</th>\n      <th>roberta_vec_304</th>\n      <th>roberta_vec_305</th>\n      <th>roberta_vec_306</th>\n      <th>roberta_vec_307</th>\n      <th>roberta_vec_308</th>\n      <th>roberta_vec_309</th>\n      <th>roberta_vec_310</th>\n      <th>roberta_vec_311</th>\n      <th>roberta_vec_312</th>\n      <th>roberta_vec_313</th>\n      <th>roberta_vec_314</th>\n      <th>roberta_vec_315</th>\n      <th>roberta_vec_316</th>\n      <th>roberta_vec_317</th>\n      <th>roberta_vec_318</th>\n      <th>roberta_vec_319</th>\n      <th>roberta_vec_320</th>\n      <th>roberta_vec_321</th>\n      <th>roberta_vec_322</th>\n      <th>roberta_vec_323</th>\n      <th>roberta_vec_324</th>\n      <th>roberta_vec_325</th>\n      <th>roberta_vec_326</th>\n      <th>roberta_vec_327</th>\n      <th>roberta_vec_328</th>\n      <th>roberta_vec_329</th>\n      <th>roberta_vec_330</th>\n      <th>roberta_vec_331</th>\n      <th>roberta_vec_332</th>\n      <th>roberta_vec_333</th>\n      <th>roberta_vec_334</th>\n      <th>roberta_vec_335</th>\n      <th>roberta_vec_336</th>\n      <th>roberta_vec_337</th>\n      <th>roberta_vec_338</th>\n      <th>roberta_vec_339</th>\n      <th>roberta_vec_340</th>\n      <th>roberta_vec_341</th>\n      <th>roberta_vec_342</th>\n      <th>roberta_vec_343</th>\n      <th>roberta_vec_344</th>\n      <th>roberta_vec_345</th>\n      <th>roberta_vec_346</th>\n      <th>roberta_vec_347</th>\n      <th>roberta_vec_348</th>\n      <th>roberta_vec_349</th>\n      <th>roberta_vec_350</th>\n      <th>roberta_vec_351</th>\n      <th>roberta_vec_352</th>\n      <th>roberta_vec_353</th>\n      <th>roberta_vec_354</th>\n      <th>roberta_vec_355</th>\n      <th>roberta_vec_356</th>\n      <th>roberta_vec_357</th>\n      <th>roberta_vec_358</th>\n      <th>roberta_vec_359</th>\n      <th>roberta_vec_360</th>\n      <th>roberta_vec_361</th>\n      <th>roberta_vec_362</th>\n      <th>roberta_vec_363</th>\n      <th>roberta_vec_364</th>\n      <th>roberta_vec_365</th>\n      <th>roberta_vec_366</th>\n      <th>roberta_vec_367</th>\n      <th>roberta_vec_368</th>\n      <th>roberta_vec_369</th>\n      <th>roberta_vec_370</th>\n      <th>roberta_vec_371</th>\n      <th>roberta_vec_372</th>\n      <th>roberta_vec_373</th>\n      <th>roberta_vec_374</th>\n      <th>roberta_vec_375</th>\n      <th>roberta_vec_376</th>\n      <th>roberta_vec_377</th>\n      <th>roberta_vec_378</th>\n      <th>roberta_vec_379</th>\n      <th>roberta_vec_380</th>\n      <th>roberta_vec_381</th>\n      <th>roberta_vec_382</th>\n      <th>roberta_vec_383</th>\n      <th>roberta_vec_384</th>\n      <th>roberta_vec_385</th>\n      <th>roberta_vec_386</th>\n      <th>roberta_vec_387</th>\n      <th>roberta_vec_388</th>\n      <th>roberta_vec_389</th>\n      <th>roberta_vec_390</th>\n      <th>roberta_vec_391</th>\n      <th>roberta_vec_392</th>\n      <th>roberta_vec_393</th>\n      <th>roberta_vec_394</th>\n      <th>roberta_vec_395</th>\n      <th>roberta_vec_396</th>\n      <th>roberta_vec_397</th>\n      <th>roberta_vec_398</th>\n      <th>roberta_vec_399</th>\n      <th>roberta_vec_400</th>\n      <th>roberta_vec_401</th>\n      <th>roberta_vec_402</th>\n      <th>roberta_vec_403</th>\n      <th>roberta_vec_404</th>\n      <th>roberta_vec_405</th>\n      <th>roberta_vec_406</th>\n      <th>roberta_vec_407</th>\n      <th>roberta_vec_408</th>\n      <th>roberta_vec_409</th>\n      <th>roberta_vec_410</th>\n      <th>roberta_vec_411</th>\n      <th>roberta_vec_412</th>\n      <th>roberta_vec_413</th>\n      <th>roberta_vec_414</th>\n      <th>roberta_vec_415</th>\n      <th>roberta_vec_416</th>\n      <th>roberta_vec_417</th>\n      <th>roberta_vec_418</th>\n      <th>roberta_vec_419</th>\n      <th>roberta_vec_420</th>\n      <th>roberta_vec_421</th>\n      <th>roberta_vec_422</th>\n      <th>roberta_vec_423</th>\n      <th>roberta_vec_424</th>\n      <th>roberta_vec_425</th>\n      <th>roberta_vec_426</th>\n      <th>roberta_vec_427</th>\n      <th>roberta_vec_428</th>\n      <th>roberta_vec_429</th>\n      <th>roberta_vec_430</th>\n      <th>roberta_vec_431</th>\n      <th>roberta_vec_432</th>\n      <th>roberta_vec_433</th>\n      <th>roberta_vec_434</th>\n      <th>roberta_vec_435</th>\n      <th>roberta_vec_436</th>\n      <th>roberta_vec_437</th>\n      <th>roberta_vec_438</th>\n      <th>roberta_vec_439</th>\n      <th>roberta_vec_440</th>\n      <th>roberta_vec_441</th>\n      <th>roberta_vec_442</th>\n      <th>roberta_vec_443</th>\n      <th>roberta_vec_444</th>\n      <th>roberta_vec_445</th>\n      <th>roberta_vec_446</th>\n      <th>roberta_vec_447</th>\n      <th>roberta_vec_448</th>\n      <th>roberta_vec_449</th>\n      <th>roberta_vec_450</th>\n      <th>roberta_vec_451</th>\n      <th>roberta_vec_452</th>\n      <th>roberta_vec_453</th>\n      <th>roberta_vec_454</th>\n      <th>roberta_vec_455</th>\n      <th>roberta_vec_456</th>\n      <th>roberta_vec_457</th>\n      <th>roberta_vec_458</th>\n      <th>roberta_vec_459</th>\n      <th>roberta_vec_460</th>\n      <th>roberta_vec_461</th>\n      <th>roberta_vec_462</th>\n      <th>roberta_vec_463</th>\n      <th>roberta_vec_464</th>\n      <th>roberta_vec_465</th>\n      <th>roberta_vec_466</th>\n      <th>roberta_vec_467</th>\n      <th>roberta_vec_468</th>\n      <th>roberta_vec_469</th>\n      <th>roberta_vec_470</th>\n      <th>roberta_vec_471</th>\n      <th>roberta_vec_472</th>\n      <th>roberta_vec_473</th>\n      <th>roberta_vec_474</th>\n      <th>roberta_vec_475</th>\n      <th>roberta_vec_476</th>\n      <th>roberta_vec_477</th>\n      <th>roberta_vec_478</th>\n      <th>roberta_vec_479</th>\n      <th>roberta_vec_480</th>\n      <th>roberta_vec_481</th>\n      <th>roberta_vec_482</th>\n      <th>roberta_vec_483</th>\n      <th>roberta_vec_484</th>\n      <th>roberta_vec_485</th>\n      <th>roberta_vec_486</th>\n      <th>roberta_vec_487</th>\n      <th>roberta_vec_488</th>\n      <th>roberta_vec_489</th>\n      <th>roberta_vec_490</th>\n      <th>roberta_vec_491</th>\n      <th>roberta_vec_492</th>\n      <th>roberta_vec_493</th>\n      <th>roberta_vec_494</th>\n      <th>roberta_vec_495</th>\n      <th>roberta_vec_496</th>\n      <th>roberta_vec_497</th>\n      <th>roberta_vec_498</th>\n      <th>roberta_vec_499</th>\n      <th>roberta_vec_500</th>\n      <th>roberta_vec_501</th>\n      <th>roberta_vec_502</th>\n      <th>roberta_vec_503</th>\n      <th>roberta_vec_504</th>\n      <th>roberta_vec_505</th>\n      <th>roberta_vec_506</th>\n      <th>roberta_vec_507</th>\n      <th>roberta_vec_508</th>\n      <th>roberta_vec_509</th>\n      <th>roberta_vec_510</th>\n      <th>roberta_vec_511</th>\n      <th>roberta_vec_512</th>\n      <th>roberta_vec_513</th>\n      <th>roberta_vec_514</th>\n      <th>roberta_vec_515</th>\n      <th>roberta_vec_516</th>\n      <th>roberta_vec_517</th>\n      <th>roberta_vec_518</th>\n      <th>roberta_vec_519</th>\n      <th>roberta_vec_520</th>\n      <th>roberta_vec_521</th>\n      <th>roberta_vec_522</th>\n      <th>roberta_vec_523</th>\n      <th>roberta_vec_524</th>\n      <th>roberta_vec_525</th>\n      <th>roberta_vec_526</th>\n      <th>roberta_vec_527</th>\n      <th>roberta_vec_528</th>\n      <th>roberta_vec_529</th>\n      <th>roberta_vec_530</th>\n      <th>roberta_vec_531</th>\n      <th>roberta_vec_532</th>\n      <th>roberta_vec_533</th>\n      <th>roberta_vec_534</th>\n      <th>roberta_vec_535</th>\n      <th>roberta_vec_536</th>\n      <th>roberta_vec_537</th>\n      <th>roberta_vec_538</th>\n      <th>roberta_vec_539</th>\n      <th>roberta_vec_540</th>\n      <th>roberta_vec_541</th>\n      <th>roberta_vec_542</th>\n      <th>roberta_vec_543</th>\n      <th>roberta_vec_544</th>\n      <th>roberta_vec_545</th>\n      <th>roberta_vec_546</th>\n      <th>roberta_vec_547</th>\n      <th>roberta_vec_548</th>\n      <th>roberta_vec_549</th>\n      <th>roberta_vec_550</th>\n      <th>roberta_vec_551</th>\n      <th>roberta_vec_552</th>\n      <th>roberta_vec_553</th>\n      <th>roberta_vec_554</th>\n      <th>roberta_vec_555</th>\n      <th>roberta_vec_556</th>\n      <th>roberta_vec_557</th>\n      <th>roberta_vec_558</th>\n      <th>roberta_vec_559</th>\n      <th>roberta_vec_560</th>\n      <th>roberta_vec_561</th>\n      <th>roberta_vec_562</th>\n      <th>roberta_vec_563</th>\n      <th>roberta_vec_564</th>\n      <th>roberta_vec_565</th>\n      <th>roberta_vec_566</th>\n      <th>roberta_vec_567</th>\n      <th>roberta_vec_568</th>\n      <th>roberta_vec_569</th>\n      <th>roberta_vec_570</th>\n      <th>roberta_vec_571</th>\n      <th>roberta_vec_572</th>\n      <th>roberta_vec_573</th>\n      <th>roberta_vec_574</th>\n      <th>roberta_vec_575</th>\n      <th>roberta_vec_576</th>\n      <th>roberta_vec_577</th>\n      <th>roberta_vec_578</th>\n      <th>roberta_vec_579</th>\n      <th>roberta_vec_580</th>\n      <th>roberta_vec_581</th>\n      <th>roberta_vec_582</th>\n      <th>roberta_vec_583</th>\n      <th>roberta_vec_584</th>\n      <th>roberta_vec_585</th>\n      <th>roberta_vec_586</th>\n      <th>roberta_vec_587</th>\n      <th>roberta_vec_588</th>\n      <th>roberta_vec_589</th>\n      <th>roberta_vec_590</th>\n      <th>roberta_vec_591</th>\n      <th>roberta_vec_592</th>\n      <th>roberta_vec_593</th>\n      <th>roberta_vec_594</th>\n      <th>roberta_vec_595</th>\n      <th>roberta_vec_596</th>\n      <th>roberta_vec_597</th>\n      <th>roberta_vec_598</th>\n      <th>roberta_vec_599</th>\n      <th>roberta_vec_600</th>\n      <th>roberta_vec_601</th>\n      <th>roberta_vec_602</th>\n      <th>roberta_vec_603</th>\n      <th>roberta_vec_604</th>\n      <th>roberta_vec_605</th>\n      <th>roberta_vec_606</th>\n      <th>roberta_vec_607</th>\n      <th>roberta_vec_608</th>\n      <th>roberta_vec_609</th>\n      <th>roberta_vec_610</th>\n      <th>roberta_vec_611</th>\n      <th>roberta_vec_612</th>\n      <th>roberta_vec_613</th>\n      <th>roberta_vec_614</th>\n      <th>roberta_vec_615</th>\n      <th>roberta_vec_616</th>\n      <th>roberta_vec_617</th>\n      <th>roberta_vec_618</th>\n      <th>roberta_vec_619</th>\n      <th>roberta_vec_620</th>\n      <th>roberta_vec_621</th>\n      <th>roberta_vec_622</th>\n      <th>roberta_vec_623</th>\n      <th>roberta_vec_624</th>\n      <th>roberta_vec_625</th>\n      <th>roberta_vec_626</th>\n      <th>roberta_vec_627</th>\n      <th>roberta_vec_628</th>\n      <th>roberta_vec_629</th>\n      <th>roberta_vec_630</th>\n      <th>roberta_vec_631</th>\n      <th>roberta_vec_632</th>\n      <th>roberta_vec_633</th>\n      <th>roberta_vec_634</th>\n      <th>roberta_vec_635</th>\n      <th>roberta_vec_636</th>\n      <th>roberta_vec_637</th>\n      <th>roberta_vec_638</th>\n      <th>roberta_vec_639</th>\n      <th>roberta_vec_640</th>\n      <th>roberta_vec_641</th>\n      <th>roberta_vec_642</th>\n      <th>roberta_vec_643</th>\n      <th>roberta_vec_644</th>\n      <th>roberta_vec_645</th>\n      <th>roberta_vec_646</th>\n      <th>roberta_vec_647</th>\n      <th>roberta_vec_648</th>\n      <th>roberta_vec_649</th>\n      <th>roberta_vec_650</th>\n      <th>roberta_vec_651</th>\n      <th>roberta_vec_652</th>\n      <th>roberta_vec_653</th>\n      <th>roberta_vec_654</th>\n      <th>roberta_vec_655</th>\n      <th>roberta_vec_656</th>\n      <th>roberta_vec_657</th>\n      <th>roberta_vec_658</th>\n      <th>roberta_vec_659</th>\n      <th>roberta_vec_660</th>\n      <th>roberta_vec_661</th>\n      <th>roberta_vec_662</th>\n      <th>roberta_vec_663</th>\n      <th>roberta_vec_664</th>\n      <th>roberta_vec_665</th>\n      <th>roberta_vec_666</th>\n      <th>roberta_vec_667</th>\n      <th>roberta_vec_668</th>\n      <th>roberta_vec_669</th>\n      <th>roberta_vec_670</th>\n      <th>roberta_vec_671</th>\n      <th>roberta_vec_672</th>\n      <th>roberta_vec_673</th>\n      <th>roberta_vec_674</th>\n      <th>roberta_vec_675</th>\n      <th>roberta_vec_676</th>\n      <th>roberta_vec_677</th>\n      <th>roberta_vec_678</th>\n      <th>roberta_vec_679</th>\n      <th>roberta_vec_680</th>\n      <th>roberta_vec_681</th>\n      <th>roberta_vec_682</th>\n      <th>roberta_vec_683</th>\n      <th>roberta_vec_684</th>\n      <th>roberta_vec_685</th>\n      <th>roberta_vec_686</th>\n      <th>roberta_vec_687</th>\n      <th>roberta_vec_688</th>\n      <th>roberta_vec_689</th>\n      <th>roberta_vec_690</th>\n      <th>roberta_vec_691</th>\n      <th>roberta_vec_692</th>\n      <th>roberta_vec_693</th>\n      <th>roberta_vec_694</th>\n      <th>roberta_vec_695</th>\n      <th>roberta_vec_696</th>\n      <th>roberta_vec_697</th>\n      <th>roberta_vec_698</th>\n      <th>roberta_vec_699</th>\n      <th>roberta_vec_700</th>\n      <th>roberta_vec_701</th>\n      <th>roberta_vec_702</th>\n      <th>roberta_vec_703</th>\n      <th>roberta_vec_704</th>\n      <th>roberta_vec_705</th>\n      <th>roberta_vec_706</th>\n      <th>roberta_vec_707</th>\n      <th>roberta_vec_708</th>\n      <th>roberta_vec_709</th>\n      <th>roberta_vec_710</th>\n      <th>roberta_vec_711</th>\n      <th>roberta_vec_712</th>\n      <th>roberta_vec_713</th>\n      <th>roberta_vec_714</th>\n      <th>roberta_vec_715</th>\n      <th>roberta_vec_716</th>\n      <th>roberta_vec_717</th>\n      <th>roberta_vec_718</th>\n      <th>roberta_vec_719</th>\n      <th>roberta_vec_720</th>\n      <th>roberta_vec_721</th>\n      <th>roberta_vec_722</th>\n      <th>roberta_vec_723</th>\n      <th>roberta_vec_724</th>\n      <th>roberta_vec_725</th>\n      <th>roberta_vec_726</th>\n      <th>roberta_vec_727</th>\n      <th>roberta_vec_728</th>\n      <th>roberta_vec_729</th>\n      <th>roberta_vec_730</th>\n      <th>roberta_vec_731</th>\n      <th>roberta_vec_732</th>\n      <th>roberta_vec_733</th>\n      <th>roberta_vec_734</th>\n      <th>roberta_vec_735</th>\n      <th>roberta_vec_736</th>\n      <th>roberta_vec_737</th>\n      <th>roberta_vec_738</th>\n      <th>roberta_vec_739</th>\n      <th>roberta_vec_740</th>\n      <th>roberta_vec_741</th>\n      <th>roberta_vec_742</th>\n      <th>roberta_vec_743</th>\n      <th>roberta_vec_744</th>\n      <th>roberta_vec_745</th>\n      <th>roberta_vec_746</th>\n      <th>roberta_vec_747</th>\n      <th>roberta_vec_748</th>\n      <th>roberta_vec_749</th>\n      <th>roberta_vec_750</th>\n      <th>roberta_vec_751</th>\n      <th>roberta_vec_752</th>\n      <th>roberta_vec_753</th>\n      <th>roberta_vec_754</th>\n      <th>roberta_vec_755</th>\n      <th>roberta_vec_756</th>\n      <th>roberta_vec_757</th>\n      <th>roberta_vec_758</th>\n      <th>roberta_vec_759</th>\n      <th>roberta_vec_760</th>\n      <th>roberta_vec_761</th>\n      <th>roberta_vec_762</th>\n      <th>roberta_vec_763</th>\n      <th>roberta_vec_764</th>\n      <th>roberta_vec_765</th>\n      <th>roberta_vec_766</th>\n      <th>roberta_vec_767</th>\n      <th>pred_doi_cites</th>\n      <th>fold_no</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1403.7138</td>\n      <td>Aigen Li</td>\n      <td>Qi Li, S.L. Liang, Aigen Li (University of Mis...</td>\n      <td>Spectropolarimetric Constraints on the Nature ...</td>\n      <td>5 pages, 2 figures; accepted for publication i...</td>\n      <td>None</td>\n      <td>10.1093/mnrasl/slu021</td>\n      <td>None</td>\n      <td>astro-ph.GA</td>\n      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n      <td>While it is well recognized that interstella...</td>\n      <td>[{'version': 'v1', 'created': 'Thu, 27 Mar 201...</td>\n      <td>[[Li, Qi, , University of Missouri], [Liang, S...</td>\n      <td>2.197225</td>\n      <td>2.079442</td>\n      <td>10.1093</td>\n      <td>Oxford University Press</td>\n      <td>5.831882</td>\n      <td>1091568.0</td>\n      <td>2015-06-19</td>\n      <td>2014-03-27 17:25:40+00:00</td>\n      <td>2014-03-27 17:25:40+00:00</td>\n      <td>2015</td>\n      <td>2014</td>\n      <td>2014</td>\n      <td>6</td>\n      <td>3</td>\n      <td>3</td>\n      <td>201506</td>\n      <td>201403</td>\n      <td>201403</td>\n      <td>19</td>\n      <td>27</td>\n      <td>27</td>\n      <td>1.434672e+09</td>\n      <td>1.395941e+09</td>\n      <td>1.395941e+09</td>\n      <td>38730860</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>16605</td>\n      <td>16156</td>\n      <td>16156</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>Li</td>\n      <td>3</td>\n      <td>astro-ph</td>\n      <td>astro-ph</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>astro</td>\n      <td>astro-ph</td>\n      <td>astro-ph.ga</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3033</td>\n      <td>53</td>\n      <td>60233</td>\n      <td>344</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>92</td>\n      <td>88</td>\n      <td>1965</td>\n      <td>1.975857</td>\n      <td>6310</td>\n      <td>12467.660571</td>\n      <td>0.000000</td>\n      <td>8.491670</td>\n      <td>1.945910</td>\n      <td>1.362610</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>2.890372</td>\n      <td>2.072728</td>\n      <td>29728</td>\n      <td>61618.072584</td>\n      <td>0.0</td>\n      <td>8.079308</td>\n      <td>2.079442</td>\n      <td>1.299295</td>\n      <td>0.000000</td>\n      <td>1.098612</td>\n      <td>3.044522</td>\n      <td>2.072728</td>\n      <td>29728</td>\n      <td>61618.072584</td>\n      <td>0.0</td>\n      <td>8.079308</td>\n      <td>2.079442</td>\n      <td>1.299295</td>\n      <td>0.000000</td>\n      <td>1.098612</td>\n      <td>3.044522</td>\n      <td>2.280909</td>\n      <td>43</td>\n      <td>98.079103</td>\n      <td>0.000000</td>\n      <td>4.094345</td>\n      <td>2.302585</td>\n      <td>0.998816</td>\n      <td>0.774240</td>\n      <td>1.945910</td>\n      <td>2.917405</td>\n      <td>2.415190</td>\n      <td>75054</td>\n      <td>181269.657847</td>\n      <td>0.0</td>\n      <td>9.532134</td>\n      <td>2.484907</td>\n      <td>1.274504</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.295837</td>\n      <td>2.303734</td>\n      <td>4588</td>\n      <td>10569.530184</td>\n      <td>0.0</td>\n      <td>6.881411</td>\n      <td>2.302585</td>\n      <td>1.259877</td>\n      <td>0.693147</td>\n      <td>1.386294</td>\n      <td>3.178054</td>\n      <td>1.940007</td>\n      <td>599833</td>\n      <td>1.163680e+06</td>\n      <td>0.0</td>\n      <td>9.104313</td>\n      <td>1.94591</td>\n      <td>1.350045</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>2.890372</td>\n      <td>2.640078</td>\n      <td>195890</td>\n      <td>517164.893335</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>2.772589</td>\n      <td>1.436436</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.663562</td>\n      <td>2.640078</td>\n      <td>195890</td>\n      <td>517164.893335</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>2.772589</td>\n      <td>1.436436</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.663562</td>\n      <td>2.714563</td>\n      <td>145677</td>\n      <td>395449.460335</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>2.833213</td>\n      <td>1.429668</td>\n      <td>0.693147</td>\n      <td>1.791759</td>\n      <td>3.713572</td>\n      <td>2.714563</td>\n      <td>145677</td>\n      <td>395449.460335</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>2.833213</td>\n      <td>1.429668</td>\n      <td>0.693147</td>\n      <td>1.791759</td>\n      <td>3.713572</td>\n      <td>2.190419</td>\n      <td>18375</td>\n      <td>40248.956553</td>\n      <td>0.0</td>\n      <td>7.996317</td>\n      <td>2.302585</td>\n      <td>1.303520</td>\n      <td>0.000000</td>\n      <td>1.386294</td>\n      <td>3.135494</td>\n      <td>-0.083685</td>\n      <td>0.974493</td>\n      <td>0.124496</td>\n      <td>1.040516</td>\n      <td>0.221367</td>\n      <td>1.074388</td>\n      <td>0.124496</td>\n      <td>1.040516</td>\n      <td>-0.217965</td>\n      <td>0.936178</td>\n      <td>-0.106509</td>\n      <td>0.967761</td>\n      <td>0.257218</td>\n      <td>1.087489</td>\n      <td>-0.442853</td>\n      <td>0.878340</td>\n      <td>-0.442853</td>\n      <td>0.878340</td>\n      <td>-0.517339</td>\n      <td>0.860727</td>\n      <td>-0.517339</td>\n      <td>0.860727</td>\n      <td>0.006805</td>\n      <td>1.002133</td>\n      <td>0.208181</td>\n      <td>1.067751</td>\n      <td>0.305052</td>\n      <td>1.102509</td>\n      <td>0.208181</td>\n      <td>1.067751</td>\n      <td>-0.134280</td>\n      <td>0.960681</td>\n      <td>-0.022824</td>\n      <td>0.993091</td>\n      <td>0.340902</td>\n      <td>1.115953</td>\n      <td>-0.359169</td>\n      <td>0.901329</td>\n      <td>-0.359169</td>\n      <td>0.901329</td>\n      <td>-0.433654</td>\n      <td>0.883256</td>\n      <td>-0.433654</td>\n      <td>0.883256</td>\n      <td>0.090490</td>\n      <td>1.028363</td>\n      <td>0.096871</td>\n      <td>1.032552</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.342461</td>\n      <td>0.899724</td>\n      <td>-0.231005</td>\n      <td>0.930078</td>\n      <td>0.132722</td>\n      <td>1.045143</td>\n      <td>-0.567350</td>\n      <td>0.844138</td>\n      <td>-0.567350</td>\n      <td>0.844138</td>\n      <td>-0.641835</td>\n      <td>0.827211</td>\n      <td>-0.641835</td>\n      <td>0.827211</td>\n      <td>-0.117691</td>\n      <td>0.963111</td>\n      <td>-0.096871</td>\n      <td>0.968474</td>\n      <td>-0.439332</td>\n      <td>0.871359</td>\n      <td>-0.327876</td>\n      <td>0.900756</td>\n      <td>0.035850</td>\n      <td>1.012194</td>\n      <td>-0.664221</td>\n      <td>0.817526</td>\n      <td>-0.664221</td>\n      <td>0.817526</td>\n      <td>-0.738706</td>\n      <td>0.801132</td>\n      <td>-0.738706</td>\n      <td>0.801132</td>\n      <td>-0.214562</td>\n      <td>0.932748</td>\n      <td>-0.342461</td>\n      <td>0.899724</td>\n      <td>-0.231005</td>\n      <td>0.930078</td>\n      <td>0.132722</td>\n      <td>1.045143</td>\n      <td>-0.567350</td>\n      <td>0.844138</td>\n      <td>-0.567350</td>\n      <td>0.844138</td>\n      <td>-0.641835</td>\n      <td>0.827211</td>\n      <td>-0.641835</td>\n      <td>0.827211</td>\n      <td>-0.117691</td>\n      <td>0.963111</td>\n      <td>0.111456</td>\n      <td>1.033736</td>\n      <td>0.475183</td>\n      <td>1.161626</td>\n      <td>-0.224888</td>\n      <td>0.938219</td>\n      <td>-0.224888</td>\n      <td>0.938219</td>\n      <td>-0.299374</td>\n      <td>0.919405</td>\n      <td>-0.299374</td>\n      <td>0.919405</td>\n      <td>0.224770</td>\n      <td>1.070452</td>\n      <td>0.363727</td>\n      <td>1.123716</td>\n      <td>-0.336344</td>\n      <td>0.907600</td>\n      <td>-0.336344</td>\n      <td>0.907600</td>\n      <td>-0.410830</td>\n      <td>0.889400</td>\n      <td>-0.410830</td>\n      <td>0.889400</td>\n      <td>0.113314</td>\n      <td>1.035517</td>\n      <td>-0.700071</td>\n      <td>0.807677</td>\n      <td>-0.700071</td>\n      <td>0.807677</td>\n      <td>-0.774556</td>\n      <td>0.791481</td>\n      <td>-0.774556</td>\n      <td>0.791481</td>\n      <td>-0.250412</td>\n      <td>0.921511</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.074485</td>\n      <td>0.979948</td>\n      <td>-0.074485</td>\n      <td>0.979948</td>\n      <td>0.449659</td>\n      <td>1.140940</td>\n      <td>-0.074485</td>\n      <td>0.979948</td>\n      <td>-0.074485</td>\n      <td>0.979948</td>\n      <td>0.449659</td>\n      <td>1.140940</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.524144</td>\n      <td>1.164287</td>\n      <td>0.524144</td>\n      <td>1.164287</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.029891</td>\n      <td>0.073973</td>\n      <td>-0.094534</td>\n      <td>0.126522</td>\n      <td>0.181991</td>\n      <td>0.180853</td>\n      <td>0.266522</td>\n      <td>0.097094</td>\n      <td>-0.123095</td>\n      <td>-0.095310</td>\n      <td>-0.179767</td>\n      <td>-0.457392</td>\n      <td>-0.080620</td>\n      <td>0.089512</td>\n      <td>0.082543</td>\n      <td>0.058552</td>\n      <td>0.027953</td>\n      <td>0.203759</td>\n      <td>0.072223</td>\n      <td>0.050499</td>\n      <td>-0.263804</td>\n      <td>0.327592</td>\n      <td>-0.103400</td>\n      <td>-0.121484</td>\n      <td>0.250285</td>\n      <td>0.074155</td>\n      <td>0.144880</td>\n      <td>0.259375</td>\n      <td>-0.288159</td>\n      <td>0.185025</td>\n      <td>-0.030508</td>\n      <td>0.070924</td>\n      <td>0.074667</td>\n      <td>0.036750</td>\n      <td>-0.061805</td>\n      <td>0.060579</td>\n      <td>-0.036916</td>\n      <td>0.039057</td>\n      <td>-0.296855</td>\n      <td>0.009289</td>\n      <td>-0.116685</td>\n      <td>0.715590</td>\n      <td>0.101258</td>\n      <td>-0.220261</td>\n      <td>-0.076486</td>\n      <td>-0.188758</td>\n      <td>-0.017060</td>\n      <td>0.037125</td>\n      <td>0.057709</td>\n      <td>0.166822</td>\n      <td>-0.043452</td>\n      <td>-0.019516</td>\n      <td>-0.173006</td>\n      <td>0.089757</td>\n      <td>-0.190869</td>\n      <td>0.095406</td>\n      <td>0.060052</td>\n      <td>-0.132407</td>\n      <td>0.005342</td>\n      <td>0.033320</td>\n      <td>0.024427</td>\n      <td>-0.118014</td>\n      <td>0.251357</td>\n      <td>0.040674</td>\n      <td>-0.027957</td>\n      <td>-0.026394</td>\n      <td>0.024263</td>\n      <td>0.372732</td>\n      <td>-0.025801</td>\n      <td>-0.246002</td>\n      <td>0.109832</td>\n      <td>-0.017500</td>\n      <td>-0.001231</td>\n      <td>0.143727</td>\n      <td>0.264198</td>\n      <td>0.107633</td>\n      <td>0.060340</td>\n      <td>-0.318682</td>\n      <td>0.006398</td>\n      <td>0.099760</td>\n      <td>-0.036413</td>\n      <td>0.055242</td>\n      <td>1.750735</td>\n      <td>0.118613</td>\n      <td>0.177189</td>\n      <td>-0.080642</td>\n      <td>0.044355</td>\n      <td>0.100218</td>\n      <td>-0.051002</td>\n      <td>-0.008707</td>\n      <td>-0.134474</td>\n      <td>-0.004757</td>\n      <td>-0.011541</td>\n      <td>-0.217713</td>\n      <td>-0.042322</td>\n      <td>0.052909</td>\n      <td>0.071231</td>\n      <td>-1.152318</td>\n      <td>-0.056206</td>\n      <td>-0.060179</td>\n      <td>0.189230</td>\n      <td>-0.190384</td>\n      <td>-0.025456</td>\n      <td>-0.110303</td>\n      <td>-0.197674</td>\n      <td>0.367068</td>\n      <td>0.155162</td>\n      <td>0.074811</td>\n      <td>0.099640</td>\n      <td>-0.164789</td>\n      <td>-0.005851</td>\n      <td>0.046500</td>\n      <td>-0.082355</td>\n      <td>0.045490</td>\n      <td>0.089042</td>\n      <td>-0.197250</td>\n      <td>0.304634</td>\n      <td>0.225055</td>\n      <td>-0.028008</td>\n      <td>0.400766</td>\n      <td>0.061376</td>\n      <td>0.030082</td>\n      <td>0.118571</td>\n      <td>-0.081431</td>\n      <td>0.120853</td>\n      <td>0.292195</td>\n      <td>-0.141171</td>\n      <td>0.152237</td>\n      <td>-0.059104</td>\n      <td>0.141703</td>\n      <td>0.194321</td>\n      <td>-0.700524</td>\n      <td>-0.223517</td>\n      <td>-0.198950</td>\n      <td>-0.048475</td>\n      <td>0.045590</td>\n      <td>0.032415</td>\n      <td>0.005680</td>\n      <td>-0.050341</td>\n      <td>0.018153</td>\n      <td>0.211495</td>\n      <td>0.210816</td>\n      <td>-0.227017</td>\n      <td>-0.245958</td>\n      <td>0.129851</td>\n      <td>0.194608</td>\n      <td>-0.074843</td>\n      <td>-0.347806</td>\n      <td>0.008898</td>\n      <td>0.330879</td>\n      <td>0.291852</td>\n      <td>0.063475</td>\n      <td>-0.278245</td>\n      <td>-0.312510</td>\n      <td>-0.021102</td>\n      <td>0.703451</td>\n      <td>0.140124</td>\n      <td>-0.089052</td>\n      <td>-0.132394</td>\n      <td>0.020074</td>\n      <td>-0.022465</td>\n      <td>-0.017278</td>\n      <td>0.210464</td>\n      <td>-0.011987</td>\n      <td>0.079176</td>\n      <td>0.043043</td>\n      <td>-0.103480</td>\n      <td>0.110626</td>\n      <td>-0.036202</td>\n      <td>0.118242</td>\n      <td>-0.063492</td>\n      <td>-0.068646</td>\n      <td>0.113279</td>\n      <td>0.071068</td>\n      <td>-0.098169</td>\n      <td>-0.019378</td>\n      <td>-0.040457</td>\n      <td>-0.086607</td>\n      <td>0.030731</td>\n      <td>0.072026</td>\n      <td>-0.039192</td>\n      <td>0.114556</td>\n      <td>-0.050776</td>\n      <td>-0.167901</td>\n      <td>0.117924</td>\n      <td>0.215771</td>\n      <td>-0.063886</td>\n      <td>0.233640</td>\n      <td>0.017298</td>\n      <td>-0.082844</td>\n      <td>-0.067025</td>\n      <td>-0.149464</td>\n      <td>-0.188189</td>\n      <td>-0.230590</td>\n      <td>-0.033488</td>\n      <td>-0.188438</td>\n      <td>0.027001</td>\n      <td>0.040944</td>\n      <td>0.488956</td>\n      <td>-0.082526</td>\n      <td>0.172990</td>\n      <td>-0.299410</td>\n      <td>0.126077</td>\n      <td>-0.053900</td>\n      <td>0.195646</td>\n      <td>0.008763</td>\n      <td>-0.449042</td>\n      <td>0.146812</td>\n      <td>0.036107</td>\n      <td>-0.134739</td>\n      <td>0.037157</td>\n      <td>-0.022641</td>\n      <td>-0.343432</td>\n      <td>0.162106</td>\n      <td>-0.195311</td>\n      <td>0.222396</td>\n      <td>0.097842</td>\n      <td>-0.307882</td>\n      <td>-0.325424</td>\n      <td>-0.210876</td>\n      <td>0.034118</td>\n      <td>0.114727</td>\n      <td>-0.092626</td>\n      <td>0.038293</td>\n      <td>-0.076999</td>\n      <td>0.035965</td>\n      <td>-0.001640</td>\n      <td>-0.067477</td>\n      <td>0.206820</td>\n      <td>-0.090941</td>\n      <td>0.011562</td>\n      <td>0.523355</td>\n      <td>0.205579</td>\n      <td>0.027943</td>\n      <td>0.256336</td>\n      <td>-0.408816</td>\n      <td>-0.036333</td>\n      <td>-0.039951</td>\n      <td>-0.067919</td>\n      <td>-0.153039</td>\n      <td>-0.669205</td>\n      <td>0.106416</td>\n      <td>0.140609</td>\n      <td>0.073663</td>\n      <td>-0.095072</td>\n      <td>-0.029422</td>\n      <td>-0.021315</td>\n      <td>0.024537</td>\n      <td>0.019863</td>\n      <td>0.111300</td>\n      <td>-0.087268</td>\n      <td>0.033974</td>\n      <td>-0.041300</td>\n      <td>-0.002832</td>\n      <td>0.191807</td>\n      <td>-0.201867</td>\n      <td>0.066451</td>\n      <td>-0.023439</td>\n      <td>-0.336640</td>\n      <td>0.714376</td>\n      <td>0.090345</td>\n      <td>-0.191597</td>\n      <td>0.216644</td>\n      <td>-0.025642</td>\n      <td>-0.009782</td>\n      <td>-0.170917</td>\n      <td>-0.068642</td>\n      <td>-0.000975</td>\n      <td>0.141942</td>\n      <td>-0.020858</td>\n      <td>-0.093970</td>\n      <td>0.041412</td>\n      <td>0.061818</td>\n      <td>-0.048434</td>\n      <td>-0.040401</td>\n      <td>0.484307</td>\n      <td>0.199968</td>\n      <td>-0.079365</td>\n      <td>-0.282826</td>\n      <td>0.018812</td>\n      <td>0.142837</td>\n      <td>-0.237518</td>\n      <td>0.177559</td>\n      <td>0.108307</td>\n      <td>0.015854</td>\n      <td>-0.030182</td>\n      <td>-0.045865</td>\n      <td>-0.018895</td>\n      <td>-0.130665</td>\n      <td>0.158553</td>\n      <td>-0.120776</td>\n      <td>-0.060394</td>\n      <td>-0.245842</td>\n      <td>-0.171578</td>\n      <td>0.217652</td>\n      <td>-0.069561</td>\n      <td>0.218517</td>\n      <td>-0.073254</td>\n      <td>-0.029057</td>\n      <td>-0.045591</td>\n      <td>0.122068</td>\n      <td>0.059425</td>\n      <td>0.108973</td>\n      <td>0.037294</td>\n      <td>-0.031758</td>\n      <td>0.092954</td>\n      <td>-0.298806</td>\n      <td>0.093344</td>\n      <td>0.245105</td>\n      <td>-0.030890</td>\n      <td>-0.109563</td>\n      <td>0.203928</td>\n      <td>-0.128601</td>\n      <td>-0.157510</td>\n      <td>0.035108</td>\n      <td>0.130013</td>\n      <td>-0.028601</td>\n      <td>-0.050875</td>\n      <td>-0.020912</td>\n      <td>-0.041617</td>\n      <td>-0.225915</td>\n      <td>-0.147597</td>\n      <td>-0.346384</td>\n      <td>-0.230136</td>\n      <td>-0.450810</td>\n      <td>0.134377</td>\n      <td>-0.004222</td>\n      <td>0.075372</td>\n      <td>0.246987</td>\n      <td>0.189751</td>\n      <td>1.171893</td>\n      <td>0.423259</td>\n      <td>0.129495</td>\n      <td>0.047323</td>\n      <td>0.245403</td>\n      <td>0.009631</td>\n      <td>-0.154849</td>\n      <td>0.136841</td>\n      <td>0.310707</td>\n      <td>0.053092</td>\n      <td>-0.118639</td>\n      <td>0.132450</td>\n      <td>0.005078</td>\n      <td>0.145224</td>\n      <td>0.315543</td>\n      <td>-0.036221</td>\n      <td>0.078925</td>\n      <td>0.070699</td>\n      <td>0.323266</td>\n      <td>-0.204535</td>\n      <td>-0.196910</td>\n      <td>-0.252790</td>\n      <td>-0.010706</td>\n      <td>-0.160570</td>\n      <td>0.057044</td>\n      <td>0.159736</td>\n      <td>-0.112820</td>\n      <td>-0.079733</td>\n      <td>-0.325507</td>\n      <td>-0.188891</td>\n      <td>-0.037424</td>\n      <td>0.284523</td>\n      <td>0.069744</td>\n      <td>0.039888</td>\n      <td>0.027148</td>\n      <td>-0.180135</td>\n      <td>0.270566</td>\n      <td>0.013305</td>\n      <td>0.162727</td>\n      <td>-0.054204</td>\n      <td>0.033835</td>\n      <td>-0.054199</td>\n      <td>-0.152402</td>\n      <td>-0.004757</td>\n      <td>0.042156</td>\n      <td>-0.095999</td>\n      <td>0.147516</td>\n      <td>-0.211751</td>\n      <td>-0.189056</td>\n      <td>0.024325</td>\n      <td>0.074092</td>\n      <td>0.093858</td>\n      <td>-0.106786</td>\n      <td>-0.024229</td>\n      <td>0.361901</td>\n      <td>-0.034408</td>\n      <td>0.173214</td>\n      <td>0.111462</td>\n      <td>0.031852</td>\n      <td>-0.091530</td>\n      <td>0.117739</td>\n      <td>-0.177065</td>\n      <td>-0.025684</td>\n      <td>-0.056748</td>\n      <td>0.097014</td>\n      <td>-0.064555</td>\n      <td>0.157628</td>\n      <td>-0.842130</td>\n      <td>0.027633</td>\n      <td>-0.110726</td>\n      <td>-0.012435</td>\n      <td>0.236703</td>\n      <td>-0.001034</td>\n      <td>-0.164259</td>\n      <td>-0.005667</td>\n      <td>0.162612</td>\n      <td>-0.022558</td>\n      <td>-0.138819</td>\n      <td>-0.067203</td>\n      <td>0.209579</td>\n      <td>0.166321</td>\n      <td>0.142380</td>\n      <td>0.005088</td>\n      <td>-0.166263</td>\n      <td>0.056212</td>\n      <td>0.058532</td>\n      <td>-0.172003</td>\n      <td>-0.141869</td>\n      <td>-0.301240</td>\n      <td>-0.030682</td>\n      <td>-0.228836</td>\n      <td>-0.144127</td>\n      <td>-0.019697</td>\n      <td>-0.055351</td>\n      <td>-0.520402</td>\n      <td>-0.117021</td>\n      <td>0.305170</td>\n      <td>-0.069852</td>\n      <td>0.036632</td>\n      <td>0.167410</td>\n      <td>0.169146</td>\n      <td>0.017619</td>\n      <td>-0.193883</td>\n      <td>-0.067916</td>\n      <td>-0.273727</td>\n      <td>-0.106562</td>\n      <td>0.097620</td>\n      <td>-0.108914</td>\n      <td>-0.194292</td>\n      <td>-0.050219</td>\n      <td>-0.271828</td>\n      <td>0.080720</td>\n      <td>-0.100197</td>\n      <td>0.024852</td>\n      <td>-0.165310</td>\n      <td>-0.122422</td>\n      <td>0.084847</td>\n      <td>-0.124941</td>\n      <td>-0.003572</td>\n      <td>-0.107315</td>\n      <td>-0.084064</td>\n      <td>-0.034131</td>\n      <td>-0.471144</td>\n      <td>-1.553909</td>\n      <td>0.427042</td>\n      <td>0.133481</td>\n      <td>-0.149348</td>\n      <td>0.085869</td>\n      <td>-0.026381</td>\n      <td>-0.150482</td>\n      <td>0.119490</td>\n      <td>-0.034511</td>\n      <td>0.125529</td>\n      <td>-0.220760</td>\n      <td>0.051272</td>\n      <td>-0.168972</td>\n      <td>0.112018</td>\n      <td>-0.060398</td>\n      <td>0.114152</td>\n      <td>-0.003922</td>\n      <td>0.152598</td>\n      <td>-0.037112</td>\n      <td>-0.117118</td>\n      <td>-0.172868</td>\n      <td>-0.065486</td>\n      <td>-0.066613</td>\n      <td>-0.090606</td>\n      <td>0.447557</td>\n      <td>0.253628</td>\n      <td>0.099496</td>\n      <td>-0.023720</td>\n      <td>0.066293</td>\n      <td>0.149939</td>\n      <td>-0.021434</td>\n      <td>0.059437</td>\n      <td>-0.148900</td>\n      <td>0.022031</td>\n      <td>0.026973</td>\n      <td>0.262776</td>\n      <td>-0.259509</td>\n      <td>-0.190479</td>\n      <td>-0.141458</td>\n      <td>0.170991</td>\n      <td>0.259469</td>\n      <td>0.175812</td>\n      <td>0.248569</td>\n      <td>0.095650</td>\n      <td>-0.208740</td>\n      <td>-0.208450</td>\n      <td>0.113769</td>\n      <td>0.068898</td>\n      <td>0.112756</td>\n      <td>-0.066803</td>\n      <td>0.130623</td>\n      <td>-0.038461</td>\n      <td>-0.045274</td>\n      <td>-0.221130</td>\n      <td>0.057998</td>\n      <td>0.130247</td>\n      <td>0.042116</td>\n      <td>-0.316556</td>\n      <td>0.075787</td>\n      <td>0.082944</td>\n      <td>0.086548</td>\n      <td>0.098292</td>\n      <td>0.178536</td>\n      <td>0.093488</td>\n      <td>-0.212282</td>\n      <td>0.030044</td>\n      <td>0.373361</td>\n      <td>0.072374</td>\n      <td>0.007171</td>\n      <td>0.050314</td>\n      <td>0.078846</td>\n      <td>-0.125926</td>\n      <td>0.117607</td>\n      <td>-0.182267</td>\n      <td>-0.018583</td>\n      <td>0.010673</td>\n      <td>-0.101328</td>\n      <td>0.451349</td>\n      <td>-0.107005</td>\n      <td>0.303963</td>\n      <td>0.109850</td>\n      <td>-0.300183</td>\n      <td>-0.026005</td>\n      <td>0.120307</td>\n      <td>-0.143743</td>\n      <td>-0.053908</td>\n      <td>0.038879</td>\n      <td>-0.207219</td>\n      <td>0.136312</td>\n      <td>0.055207</td>\n      <td>0.080172</td>\n      <td>-0.067663</td>\n      <td>-0.183928</td>\n      <td>0.204984</td>\n      <td>-0.001481</td>\n      <td>0.160431</td>\n      <td>0.064906</td>\n      <td>-0.046666</td>\n      <td>0.114594</td>\n      <td>0.185505</td>\n      <td>0.041588</td>\n      <td>-0.106086</td>\n      <td>0.324065</td>\n      <td>-0.014526</td>\n      <td>-0.138719</td>\n      <td>-0.217771</td>\n      <td>0.388382</td>\n      <td>0.036732</td>\n      <td>0.290681</td>\n      <td>-0.118576</td>\n      <td>-0.056008</td>\n      <td>0.174637</td>\n      <td>-0.108908</td>\n      <td>0.021874</td>\n      <td>0.055263</td>\n      <td>0.050176</td>\n      <td>0.018494</td>\n      <td>1.638244</td>\n      <td>0.292534</td>\n      <td>-0.157802</td>\n      <td>-0.064801</td>\n      <td>0.262028</td>\n      <td>0.053501</td>\n      <td>-0.101853</td>\n      <td>0.026971</td>\n      <td>0.278975</td>\n      <td>0.123910</td>\n      <td>0.080051</td>\n      <td>-0.116013</td>\n      <td>-0.184120</td>\n      <td>0.175828</td>\n      <td>-0.047083</td>\n      <td>-0.045451</td>\n      <td>-0.185876</td>\n      <td>0.041590</td>\n      <td>12.282816</td>\n      <td>0.079741</td>\n      <td>-0.060993</td>\n      <td>0.063440</td>\n      <td>-0.268049</td>\n      <td>-0.094870</td>\n      <td>0.024665</td>\n      <td>0.001880</td>\n      <td>0.187180</td>\n      <td>0.040667</td>\n      <td>0.006068</td>\n      <td>0.023697</td>\n      <td>0.187934</td>\n      <td>-0.221596</td>\n      <td>0.068942</td>\n      <td>0.005736</td>\n      <td>0.098046</td>\n      <td>0.084538</td>\n      <td>0.091436</td>\n      <td>-0.074916</td>\n      <td>0.082979</td>\n      <td>0.174427</td>\n      <td>-0.010615</td>\n      <td>0.956542</td>\n      <td>-0.157765</td>\n      <td>0.202592</td>\n      <td>-0.065107</td>\n      <td>0.086034</td>\n      <td>0.082509</td>\n      <td>0.116933</td>\n      <td>-0.130661</td>\n      <td>0.056648</td>\n      <td>-0.001675</td>\n      <td>-0.010225</td>\n      <td>0.085153</td>\n      <td>-0.174518</td>\n      <td>-0.064504</td>\n      <td>0.161261</td>\n      <td>0.148312</td>\n      <td>-0.075701</td>\n      <td>-0.122330</td>\n      <td>-0.085408</td>\n      <td>0.294268</td>\n      <td>-0.416573</td>\n      <td>0.074603</td>\n      <td>-0.209788</td>\n      <td>0.263318</td>\n      <td>0.310230</td>\n      <td>0.106331</td>\n      <td>-0.020113</td>\n      <td>-0.048917</td>\n      <td>-0.070626</td>\n      <td>0.096161</td>\n      <td>-0.188461</td>\n      <td>-0.075923</td>\n      <td>-0.234436</td>\n      <td>-0.020224</td>\n      <td>-0.090146</td>\n      <td>0.016027</td>\n      <td>-0.018889</td>\n      <td>-0.114500</td>\n      <td>-0.154983</td>\n      <td>0.169091</td>\n      <td>0.098314</td>\n      <td>-0.164059</td>\n      <td>0.183723</td>\n      <td>0.187806</td>\n      <td>-0.131551</td>\n      <td>-0.050215</td>\n      <td>-0.003589</td>\n      <td>0.114386</td>\n      <td>-0.010999</td>\n      <td>-0.022542</td>\n      <td>0.019642</td>\n      <td>0.036398</td>\n      <td>-0.196691</td>\n      <td>-0.317724</td>\n      <td>-0.190680</td>\n      <td>-0.218587</td>\n      <td>-0.481349</td>\n      <td>0.132161</td>\n      <td>-0.064054</td>\n      <td>-0.135440</td>\n      <td>-0.064407</td>\n      <td>-0.314367</td>\n      <td>0.120350</td>\n      <td>-0.229907</td>\n      <td>-0.007436</td>\n      <td>-0.172259</td>\n      <td>0.153599</td>\n      <td>0.202984</td>\n      <td>0.076411</td>\n      <td>0.012433</td>\n      <td>-0.207095</td>\n      <td>0.033421</td>\n      <td>-0.124092</td>\n      <td>0.215223</td>\n      <td>-0.086132</td>\n      <td>0.223904</td>\n      <td>-0.158670</td>\n      <td>-0.441802</td>\n      <td>-0.005265</td>\n      <td>-0.076523</td>\n      <td>0.007779</td>\n      <td>-0.222167</td>\n      <td>0.287094</td>\n      <td>0.008407</td>\n      <td>-0.118400</td>\n      <td>0.111839</td>\n      <td>0.096921</td>\n      <td>-0.109888</td>\n      <td>-0.032591</td>\n      <td>-0.262088</td>\n      <td>-0.143762</td>\n      <td>0.001745</td>\n      <td>-0.218345</td>\n      <td>0.092003</td>\n      <td>0.029601</td>\n      <td>0.020191</td>\n      <td>0.111559</td>\n      <td>0.005574</td>\n      <td>0.073102</td>\n      <td>0.047232</td>\n      <td>0.086060</td>\n      <td>-0.080468</td>\n      <td>-0.066836</td>\n      <td>-0.307097</td>\n      <td>-0.099523</td>\n      <td>-0.264944</td>\n      <td>-0.165200</td>\n      <td>-0.270445</td>\n      <td>0.170967</td>\n      <td>0.298262</td>\n      <td>0.029693</td>\n      <td>0.077656</td>\n      <td>0.060142</td>\n      <td>-0.168183</td>\n      <td>0.077958</td>\n      <td>0.168891</td>\n      <td>0.268703</td>\n      <td>-0.149408</td>\n      <td>0.017111</td>\n      <td>-0.243692</td>\n      <td>-0.190973</td>\n      <td>0.018555</td>\n      <td>-0.244975</td>\n      <td>0.347429</td>\n      <td>0.465995</td>\n      <td>0.145426</td>\n      <td>0.111209</td>\n      <td>-0.079083</td>\n      <td>-0.170695</td>\n      <td>-0.197767</td>\n      <td>-0.445161</td>\n      <td>0.029228</td>\n      <td>0.070849</td>\n      <td>-0.132407</td>\n      <td>0.278908</td>\n      <td>0.070082</td>\n      <td>-0.276633</td>\n      <td>0.595741</td>\n      <td>-0.456886</td>\n      <td>-0.110131</td>\n      <td>0.033729</td>\n      <td>-0.337985</td>\n      <td>0.117239</td>\n      <td>0.234729</td>\n      <td>-0.072439</td>\n      <td>0.113650</td>\n      <td>0.048002</td>\n      <td>-0.158772</td>\n      <td>-0.019317</td>\n      <td>0.086273</td>\n      <td>0.033125</td>\n      <td>0.099122</td>\n      <td>0.154706</td>\n      <td>0.245593</td>\n      <td>0.108821</td>\n      <td>-0.001333</td>\n      <td>0.027004</td>\n      <td>2.973234</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1405.5857</td>\n      <td>Michael Mortonson</td>\n      <td>Michael J. Mortonson, Uro\\v{s} Seljak</td>\n      <td>A joint analysis of Planck and BICEP2 B modes ...</td>\n      <td>13 pages, 4 figures; submitted to JCAP; refere...</td>\n      <td>JCAP10(2014)035</td>\n      <td>10.1088/1475-7516/2014/10/035</td>\n      <td>None</td>\n      <td>astro-ph.CO gr-qc hep-ph hep-th</td>\n      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n      <td>We analyze BICEP2 and Planck data using a mo...</td>\n      <td>[{'version': 'v1', 'created': 'Thu, 22 May 201...</td>\n      <td>[[Mortonson, Michael J., ], [Seljak, Uroš, ]]</td>\n      <td>4.812184</td>\n      <td>5.241747</td>\n      <td>10.1088</td>\n      <td>IOP Publishing</td>\n      <td>4.605170</td>\n      <td>510044.0</td>\n      <td>2014-10-17</td>\n      <td>2014-05-22 18:53:31+00:00</td>\n      <td>2014-09-26 18:28:05+00:00</td>\n      <td>2014</td>\n      <td>2014</td>\n      <td>2014</td>\n      <td>10</td>\n      <td>5</td>\n      <td>9</td>\n      <td>201410</td>\n      <td>201405</td>\n      <td>201409</td>\n      <td>17</td>\n      <td>22</td>\n      <td>26</td>\n      <td>1.413504e+09</td>\n      <td>1.400785e+09</td>\n      <td>1.411756e+09</td>\n      <td>12719189</td>\n      <td>10971274.0</td>\n      <td>2</td>\n      <td>16360</td>\n      <td>16212</td>\n      <td>16339</td>\n      <td>127</td>\n      <td>63.500000</td>\n      <td>Mortonson</td>\n      <td>2</td>\n      <td>astro-ph</td>\n      <td>astro-ph</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>astro gr hep hep</td>\n      <td>astro-ph gr-qc hep-ph hep-th</td>\n      <td>astro-ph.co gr-qc hep-ph hep-th</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>122888</td>\n      <td>49</td>\n      <td>70004</td>\n      <td>209</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>493</td>\n      <td>416</td>\n      <td>1288</td>\n      <td>3.462599</td>\n      <td>17</td>\n      <td>58.864177</td>\n      <td>1.945910</td>\n      <td>4.812184</td>\n      <td>3.218876</td>\n      <td>0.885703</td>\n      <td>2.417859</td>\n      <td>2.890372</td>\n      <td>4.158883</td>\n      <td>2.313685</td>\n      <td>105982</td>\n      <td>245208.992755</td>\n      <td>0.0</td>\n      <td>8.746080</td>\n      <td>2.302585</td>\n      <td>1.347253</td>\n      <td>0.693147</td>\n      <td>1.386294</td>\n      <td>3.295837</td>\n      <td>2.313685</td>\n      <td>105982</td>\n      <td>245208.992755</td>\n      <td>0.0</td>\n      <td>8.746080</td>\n      <td>2.302585</td>\n      <td>1.347253</td>\n      <td>0.693147</td>\n      <td>1.386294</td>\n      <td>3.295837</td>\n      <td>3.557392</td>\n      <td>16</td>\n      <td>56.918266</td>\n      <td>2.197225</td>\n      <td>4.812184</td>\n      <td>3.275540</td>\n      <td>0.820869</td>\n      <td>2.699081</td>\n      <td>2.969392</td>\n      <td>4.241321</td>\n      <td>2.310727</td>\n      <td>4152</td>\n      <td>9594.140257</td>\n      <td>0.0</td>\n      <td>7.280008</td>\n      <td>2.302585</td>\n      <td>1.355448</td>\n      <td>0.693147</td>\n      <td>1.386294</td>\n      <td>3.218876</td>\n      <td>2.258292</td>\n      <td>4372</td>\n      <td>9873.250453</td>\n      <td>0.0</td>\n      <td>7.730175</td>\n      <td>2.302585</td>\n      <td>1.231808</td>\n      <td>0.693147</td>\n      <td>1.386294</td>\n      <td>3.091042</td>\n      <td>1.940007</td>\n      <td>599833</td>\n      <td>1.163680e+06</td>\n      <td>0.0</td>\n      <td>9.104313</td>\n      <td>1.94591</td>\n      <td>1.350045</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>2.890372</td>\n      <td>2.640078</td>\n      <td>195890</td>\n      <td>517164.893335</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>2.772589</td>\n      <td>1.436436</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.663562</td>\n      <td>2.640078</td>\n      <td>195890</td>\n      <td>517164.893335</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>2.772589</td>\n      <td>1.436436</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.663562</td>\n      <td>2.880736</td>\n      <td>3846</td>\n      <td>11079.309114</td>\n      <td>0.0</td>\n      <td>8.035926</td>\n      <td>2.944439</td>\n      <td>1.387706</td>\n      <td>1.098612</td>\n      <td>1.945910</td>\n      <td>3.828641</td>\n      <td>2.873969</td>\n      <td>3883</td>\n      <td>11159.621875</td>\n      <td>0.0</td>\n      <td>8.035926</td>\n      <td>2.944439</td>\n      <td>1.389296</td>\n      <td>1.098612</td>\n      <td>1.945910</td>\n      <td>3.806662</td>\n      <td>2.542085</td>\n      <td>2108</td>\n      <td>5358.716135</td>\n      <td>0.0</td>\n      <td>7.474205</td>\n      <td>2.639057</td>\n      <td>1.282945</td>\n      <td>0.693147</td>\n      <td>1.791759</td>\n      <td>3.401197</td>\n      <td>1.254793</td>\n      <td>1.275331</td>\n      <td>2.498499</td>\n      <td>1.753994</td>\n      <td>1.349586</td>\n      <td>1.302421</td>\n      <td>2.498499</td>\n      <td>1.753994</td>\n      <td>2.501457</td>\n      <td>1.755561</td>\n      <td>2.553893</td>\n      <td>1.783813</td>\n      <td>2.872177</td>\n      <td>1.976929</td>\n      <td>2.172106</td>\n      <td>1.596720</td>\n      <td>2.172106</td>\n      <td>1.596720</td>\n      <td>1.931449</td>\n      <td>1.497702</td>\n      <td>1.938215</td>\n      <td>1.500318</td>\n      <td>2.270099</td>\n      <td>1.640893</td>\n      <td>1.243706</td>\n      <td>1.375324</td>\n      <td>0.094793</td>\n      <td>1.021242</td>\n      <td>1.243706</td>\n      <td>1.375324</td>\n      <td>1.246664</td>\n      <td>1.376553</td>\n      <td>1.299100</td>\n      <td>1.398706</td>\n      <td>1.617385</td>\n      <td>1.550130</td>\n      <td>0.917314</td>\n      <td>1.252004</td>\n      <td>0.917314</td>\n      <td>1.252004</td>\n      <td>0.676656</td>\n      <td>1.174363</td>\n      <td>0.683423</td>\n      <td>1.176414</td>\n      <td>1.015306</td>\n      <td>1.286641</td>\n      <td>-1.148913</td>\n      <td>0.742546</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.002958</td>\n      <td>1.000893</td>\n      <td>0.055394</td>\n      <td>1.017001</td>\n      <td>0.373678</td>\n      <td>1.127101</td>\n      <td>-0.326393</td>\n      <td>0.910334</td>\n      <td>-0.326393</td>\n      <td>0.910334</td>\n      <td>-0.567050</td>\n      <td>0.853881</td>\n      <td>-0.560284</td>\n      <td>0.855372</td>\n      <td>-0.228400</td>\n      <td>0.935518</td>\n      <td>1.148913</td>\n      <td>1.346718</td>\n      <td>1.151871</td>\n      <td>1.347921</td>\n      <td>1.204307</td>\n      <td>1.369613</td>\n      <td>1.522592</td>\n      <td>1.517887</td>\n      <td>0.822521</td>\n      <td>1.225962</td>\n      <td>0.822521</td>\n      <td>1.225962</td>\n      <td>0.581863</td>\n      <td>1.149936</td>\n      <td>0.588630</td>\n      <td>1.151945</td>\n      <td>0.920513</td>\n      <td>1.259879</td>\n      <td>0.002958</td>\n      <td>1.000893</td>\n      <td>0.055394</td>\n      <td>1.017001</td>\n      <td>0.373678</td>\n      <td>1.127101</td>\n      <td>-0.326393</td>\n      <td>0.910334</td>\n      <td>-0.326393</td>\n      <td>0.910334</td>\n      <td>-0.567050</td>\n      <td>0.853881</td>\n      <td>-0.560284</td>\n      <td>0.855372</td>\n      <td>-0.228400</td>\n      <td>0.935518</td>\n      <td>0.052436</td>\n      <td>1.016093</td>\n      <td>0.370720</td>\n      <td>1.126095</td>\n      <td>-0.329351</td>\n      <td>0.909521</td>\n      <td>-0.329351</td>\n      <td>0.909521</td>\n      <td>-0.570008</td>\n      <td>0.853119</td>\n      <td>-0.563242</td>\n      <td>0.854609</td>\n      <td>-0.231358</td>\n      <td>0.934683</td>\n      <td>0.318285</td>\n      <td>1.108260</td>\n      <td>-0.381787</td>\n      <td>0.895116</td>\n      <td>-0.381787</td>\n      <td>0.895116</td>\n      <td>-0.622444</td>\n      <td>0.839607</td>\n      <td>-0.615678</td>\n      <td>0.841073</td>\n      <td>-0.283794</td>\n      <td>0.919879</td>\n      <td>-0.700071</td>\n      <td>0.807677</td>\n      <td>-0.700071</td>\n      <td>0.807677</td>\n      <td>-0.940729</td>\n      <td>0.757590</td>\n      <td>-0.933962</td>\n      <td>0.758913</td>\n      <td>-0.602078</td>\n      <td>0.830021</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.240658</td>\n      <td>0.937987</td>\n      <td>-0.233891</td>\n      <td>0.939625</td>\n      <td>0.097993</td>\n      <td>1.027665</td>\n      <td>-0.240658</td>\n      <td>0.937987</td>\n      <td>-0.233891</td>\n      <td>0.939625</td>\n      <td>0.097993</td>\n      <td>1.027665</td>\n      <td>0.006767</td>\n      <td>1.001747</td>\n      <td>0.338650</td>\n      <td>1.095608</td>\n      <td>0.331884</td>\n      <td>1.093697</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.106689</td>\n      <td>0.096661</td>\n      <td>-0.085257</td>\n      <td>0.170849</td>\n      <td>0.228557</td>\n      <td>-0.015929</td>\n      <td>0.218602</td>\n      <td>0.115935</td>\n      <td>-0.114532</td>\n      <td>-0.041913</td>\n      <td>-0.280049</td>\n      <td>-0.316037</td>\n      <td>0.081699</td>\n      <td>0.000767</td>\n      <td>0.008402</td>\n      <td>0.159766</td>\n      <td>0.165167</td>\n      <td>0.177519</td>\n      <td>0.025201</td>\n      <td>0.106031</td>\n      <td>-0.182975</td>\n      <td>0.322555</td>\n      <td>-0.148209</td>\n      <td>-0.066234</td>\n      <td>0.360090</td>\n      <td>0.112757</td>\n      <td>0.080112</td>\n      <td>0.180167</td>\n      <td>-0.232795</td>\n      <td>0.117121</td>\n      <td>-0.000931</td>\n      <td>0.187347</td>\n      <td>0.064031</td>\n      <td>-0.040666</td>\n      <td>-0.050130</td>\n      <td>-0.061977</td>\n      <td>-0.007930</td>\n      <td>0.057334</td>\n      <td>-0.383450</td>\n      <td>0.079792</td>\n      <td>-0.039399</td>\n      <td>0.720806</td>\n      <td>-0.007725</td>\n      <td>-0.171891</td>\n      <td>-0.094048</td>\n      <td>-0.186083</td>\n      <td>0.009172</td>\n      <td>0.122302</td>\n      <td>-0.048451</td>\n      <td>0.140538</td>\n      <td>0.067924</td>\n      <td>-0.064853</td>\n      <td>-0.116790</td>\n      <td>0.019235</td>\n      <td>-0.042266</td>\n      <td>0.030180</td>\n      <td>-0.022746</td>\n      <td>0.147852</td>\n      <td>-0.005656</td>\n      <td>-0.027981</td>\n      <td>-0.025677</td>\n      <td>-0.317784</td>\n      <td>0.172559</td>\n      <td>0.265649</td>\n      <td>-0.004519</td>\n      <td>-0.007689</td>\n      <td>0.058715</td>\n      <td>0.233586</td>\n      <td>0.010346</td>\n      <td>-0.230091</td>\n      <td>-0.027315</td>\n      <td>-0.108018</td>\n      <td>0.033987</td>\n      <td>0.177319</td>\n      <td>0.159197</td>\n      <td>0.201489</td>\n      <td>0.148096</td>\n      <td>-0.681110</td>\n      <td>0.082591</td>\n      <td>0.205589</td>\n      <td>-0.072989</td>\n      <td>0.098726</td>\n      <td>1.460827</td>\n      <td>0.140312</td>\n      <td>0.129674</td>\n      <td>-0.100670</td>\n      <td>0.116050</td>\n      <td>0.083768</td>\n      <td>-0.044950</td>\n      <td>0.046189</td>\n      <td>-0.126911</td>\n      <td>-0.083236</td>\n      <td>-0.018723</td>\n      <td>-0.108265</td>\n      <td>-0.096291</td>\n      <td>-0.032141</td>\n      <td>0.030658</td>\n      <td>-1.423888</td>\n      <td>-0.018308</td>\n      <td>0.142786</td>\n      <td>0.098601</td>\n      <td>-0.203690</td>\n      <td>0.037845</td>\n      <td>-0.115890</td>\n      <td>-0.169963</td>\n      <td>0.434114</td>\n      <td>0.137431</td>\n      <td>0.093100</td>\n      <td>-0.006186</td>\n      <td>-0.111833</td>\n      <td>0.065362</td>\n      <td>0.113184</td>\n      <td>-0.104002</td>\n      <td>0.101748</td>\n      <td>0.098081</td>\n      <td>-0.103681</td>\n      <td>0.325966</td>\n      <td>0.243259</td>\n      <td>-0.155149</td>\n      <td>0.515708</td>\n      <td>0.039121</td>\n      <td>0.077220</td>\n      <td>0.087235</td>\n      <td>-0.028862</td>\n      <td>0.145868</td>\n      <td>0.163139</td>\n      <td>-0.020414</td>\n      <td>0.204764</td>\n      <td>-0.078171</td>\n      <td>0.234116</td>\n      <td>0.157518</td>\n      <td>-0.588080</td>\n      <td>-0.148251</td>\n      <td>-0.188761</td>\n      <td>-0.151361</td>\n      <td>0.079774</td>\n      <td>-0.083731</td>\n      <td>0.071460</td>\n      <td>-0.004751</td>\n      <td>0.042767</td>\n      <td>0.143576</td>\n      <td>0.157891</td>\n      <td>-0.209118</td>\n      <td>-0.197583</td>\n      <td>0.192532</td>\n      <td>0.118830</td>\n      <td>-0.032486</td>\n      <td>-0.197717</td>\n      <td>0.048301</td>\n      <td>0.166421</td>\n      <td>0.214181</td>\n      <td>0.148172</td>\n      <td>-0.289947</td>\n      <td>-0.323667</td>\n      <td>-0.150069</td>\n      <td>0.683203</td>\n      <td>0.108692</td>\n      <td>-0.145632</td>\n      <td>0.014729</td>\n      <td>-0.017408</td>\n      <td>0.033193</td>\n      <td>-0.085116</td>\n      <td>0.081296</td>\n      <td>0.014131</td>\n      <td>0.086181</td>\n      <td>-0.064274</td>\n      <td>-0.116177</td>\n      <td>0.068913</td>\n      <td>-0.100193</td>\n      <td>0.020076</td>\n      <td>-0.113650</td>\n      <td>-0.090350</td>\n      <td>0.143324</td>\n      <td>0.119548</td>\n      <td>-0.080569</td>\n      <td>0.082065</td>\n      <td>-0.137666</td>\n      <td>-0.063878</td>\n      <td>0.058815</td>\n      <td>0.093614</td>\n      <td>-0.023716</td>\n      <td>0.074879</td>\n      <td>0.098216</td>\n      <td>-0.122689</td>\n      <td>0.107038</td>\n      <td>-0.002196</td>\n      <td>-0.111367</td>\n      <td>0.323567</td>\n      <td>-0.017652</td>\n      <td>-0.080741</td>\n      <td>-0.024784</td>\n      <td>-0.084675</td>\n      <td>-0.156599</td>\n      <td>-0.215438</td>\n      <td>-0.034031</td>\n      <td>-0.152236</td>\n      <td>-0.050263</td>\n      <td>0.055357</td>\n      <td>0.250663</td>\n      <td>-0.114731</td>\n      <td>0.136917</td>\n      <td>-0.320303</td>\n      <td>0.062832</td>\n      <td>-0.018570</td>\n      <td>0.204020</td>\n      <td>0.014725</td>\n      <td>-0.373413</td>\n      <td>-0.021981</td>\n      <td>0.012825</td>\n      <td>-0.023030</td>\n      <td>0.082520</td>\n      <td>0.091580</td>\n      <td>-0.273733</td>\n      <td>0.183646</td>\n      <td>-0.168271</td>\n      <td>0.139121</td>\n      <td>0.060510</td>\n      <td>-0.232226</td>\n      <td>-0.308105</td>\n      <td>-0.158718</td>\n      <td>-0.017783</td>\n      <td>0.086882</td>\n      <td>-0.141424</td>\n      <td>-0.012542</td>\n      <td>0.042945</td>\n      <td>-0.085448</td>\n      <td>0.067201</td>\n      <td>-0.119579</td>\n      <td>0.102467</td>\n      <td>-0.116019</td>\n      <td>-0.001257</td>\n      <td>0.404416</td>\n      <td>0.313694</td>\n      <td>0.050169</td>\n      <td>0.136860</td>\n      <td>-0.399030</td>\n      <td>-0.009352</td>\n      <td>0.021727</td>\n      <td>-0.164164</td>\n      <td>-0.157287</td>\n      <td>-0.768654</td>\n      <td>-0.000166</td>\n      <td>0.200911</td>\n      <td>0.110998</td>\n      <td>-0.090584</td>\n      <td>0.149695</td>\n      <td>-0.045115</td>\n      <td>-0.195699</td>\n      <td>0.052729</td>\n      <td>0.004195</td>\n      <td>-0.107189</td>\n      <td>0.144304</td>\n      <td>0.064344</td>\n      <td>0.065908</td>\n      <td>0.139589</td>\n      <td>-0.243548</td>\n      <td>0.096933</td>\n      <td>-0.134869</td>\n      <td>-0.331963</td>\n      <td>0.616384</td>\n      <td>0.033699</td>\n      <td>-0.121195</td>\n      <td>0.234892</td>\n      <td>-0.057047</td>\n      <td>0.016235</td>\n      <td>-0.134531</td>\n      <td>-0.027028</td>\n      <td>0.054334</td>\n      <td>0.122324</td>\n      <td>0.036071</td>\n      <td>-0.150864</td>\n      <td>0.093658</td>\n      <td>0.074198</td>\n      <td>-0.090122</td>\n      <td>-0.135492</td>\n      <td>0.493441</td>\n      <td>0.256015</td>\n      <td>-0.025004</td>\n      <td>-0.157481</td>\n      <td>0.027424</td>\n      <td>0.141670</td>\n      <td>-0.048801</td>\n      <td>0.066857</td>\n      <td>0.179784</td>\n      <td>-0.089254</td>\n      <td>-0.078412</td>\n      <td>0.242003</td>\n      <td>-0.044503</td>\n      <td>-0.168966</td>\n      <td>0.102760</td>\n      <td>-0.125682</td>\n      <td>-0.131750</td>\n      <td>-0.152472</td>\n      <td>0.020987</td>\n      <td>0.194332</td>\n      <td>0.036416</td>\n      <td>0.194594</td>\n      <td>-0.058079</td>\n      <td>0.028480</td>\n      <td>-0.148669</td>\n      <td>0.128975</td>\n      <td>-0.070672</td>\n      <td>0.124587</td>\n      <td>0.045444</td>\n      <td>-0.065784</td>\n      <td>0.118355</td>\n      <td>-0.162230</td>\n      <td>-0.000975</td>\n      <td>0.182277</td>\n      <td>-0.058043</td>\n      <td>-0.192583</td>\n      <td>0.084746</td>\n      <td>-0.051868</td>\n      <td>-0.021684</td>\n      <td>0.029572</td>\n      <td>0.190434</td>\n      <td>-0.153322</td>\n      <td>0.100547</td>\n      <td>0.016438</td>\n      <td>0.030090</td>\n      <td>-0.271785</td>\n      <td>-0.001377</td>\n      <td>-0.202443</td>\n      <td>-0.226051</td>\n      <td>-0.264157</td>\n      <td>0.139300</td>\n      <td>-0.073914</td>\n      <td>0.093044</td>\n      <td>0.219541</td>\n      <td>0.154412</td>\n      <td>1.327022</td>\n      <td>0.563168</td>\n      <td>0.175710</td>\n      <td>-0.080630</td>\n      <td>0.245853</td>\n      <td>0.024140</td>\n      <td>-0.058418</td>\n      <td>0.054246</td>\n      <td>0.274013</td>\n      <td>0.039049</td>\n      <td>-0.051425</td>\n      <td>-0.057492</td>\n      <td>0.106365</td>\n      <td>0.058885</td>\n      <td>0.303400</td>\n      <td>-0.059952</td>\n      <td>0.052255</td>\n      <td>0.078477</td>\n      <td>0.344559</td>\n      <td>-0.156540</td>\n      <td>-0.149459</td>\n      <td>-0.175235</td>\n      <td>-0.096360</td>\n      <td>-0.321122</td>\n      <td>-0.092529</td>\n      <td>0.067954</td>\n      <td>-0.236221</td>\n      <td>-0.039793</td>\n      <td>-0.225340</td>\n      <td>-0.383308</td>\n      <td>-0.152813</td>\n      <td>0.224674</td>\n      <td>-0.044086</td>\n      <td>0.104212</td>\n      <td>0.177586</td>\n      <td>-0.204635</td>\n      <td>0.228664</td>\n      <td>0.109329</td>\n      <td>0.121903</td>\n      <td>-0.115773</td>\n      <td>0.104601</td>\n      <td>0.057888</td>\n      <td>-0.117652</td>\n      <td>-0.104324</td>\n      <td>0.134173</td>\n      <td>-0.259472</td>\n      <td>0.159142</td>\n      <td>-0.186426</td>\n      <td>-0.224667</td>\n      <td>-0.031908</td>\n      <td>-0.023992</td>\n      <td>0.157893</td>\n      <td>-0.224642</td>\n      <td>-0.044007</td>\n      <td>0.370970</td>\n      <td>-0.098588</td>\n      <td>0.083123</td>\n      <td>0.246624</td>\n      <td>0.078750</td>\n      <td>-0.054559</td>\n      <td>0.067571</td>\n      <td>-0.114809</td>\n      <td>0.026363</td>\n      <td>-0.128940</td>\n      <td>0.083838</td>\n      <td>-0.018011</td>\n      <td>0.121785</td>\n      <td>-0.604013</td>\n      <td>-0.070415</td>\n      <td>-0.022270</td>\n      <td>0.078345</td>\n      <td>0.118066</td>\n      <td>0.044453</td>\n      <td>-0.071485</td>\n      <td>0.014087</td>\n      <td>0.180780</td>\n      <td>-0.089880</td>\n      <td>-0.139726</td>\n      <td>-0.107553</td>\n      <td>0.292497</td>\n      <td>0.134242</td>\n      <td>0.189744</td>\n      <td>0.030346</td>\n      <td>-0.071608</td>\n      <td>-0.018470</td>\n      <td>-0.009388</td>\n      <td>-0.091223</td>\n      <td>-0.236851</td>\n      <td>-0.263282</td>\n      <td>-0.085916</td>\n      <td>-0.031939</td>\n      <td>-0.157399</td>\n      <td>-0.042684</td>\n      <td>-0.029077</td>\n      <td>-0.435612</td>\n      <td>-0.142579</td>\n      <td>0.212879</td>\n      <td>0.009944</td>\n      <td>-0.037457</td>\n      <td>0.158548</td>\n      <td>0.110534</td>\n      <td>0.083527</td>\n      <td>-0.231483</td>\n      <td>-0.060833</td>\n      <td>-0.183733</td>\n      <td>-0.097880</td>\n      <td>0.107157</td>\n      <td>-0.217850</td>\n      <td>-0.115877</td>\n      <td>0.037825</td>\n      <td>-0.354344</td>\n      <td>0.019821</td>\n      <td>0.013490</td>\n      <td>-0.039675</td>\n      <td>-0.151258</td>\n      <td>-0.062437</td>\n      <td>0.077217</td>\n      <td>-0.082840</td>\n      <td>-0.161044</td>\n      <td>-0.140004</td>\n      <td>-0.228191</td>\n      <td>0.043486</td>\n      <td>-0.327433</td>\n      <td>-1.464426</td>\n      <td>0.309512</td>\n      <td>0.200035</td>\n      <td>-0.070795</td>\n      <td>0.041317</td>\n      <td>-0.029839</td>\n      <td>-0.136768</td>\n      <td>0.170875</td>\n      <td>0.063316</td>\n      <td>0.135547</td>\n      <td>-0.269064</td>\n      <td>0.036440</td>\n      <td>-0.200457</td>\n      <td>0.228488</td>\n      <td>-0.106061</td>\n      <td>-0.013615</td>\n      <td>0.074039</td>\n      <td>0.312686</td>\n      <td>-0.094369</td>\n      <td>-0.138735</td>\n      <td>-0.125981</td>\n      <td>-0.109813</td>\n      <td>-0.042848</td>\n      <td>-0.132928</td>\n      <td>0.246403</td>\n      <td>0.147859</td>\n      <td>-0.050107</td>\n      <td>-0.188052</td>\n      <td>0.072340</td>\n      <td>0.110755</td>\n      <td>0.007728</td>\n      <td>0.031509</td>\n      <td>-0.112875</td>\n      <td>0.024343</td>\n      <td>0.077814</td>\n      <td>0.176009</td>\n      <td>-0.202625</td>\n      <td>-0.309267</td>\n      <td>-0.066109</td>\n      <td>0.231504</td>\n      <td>0.191174</td>\n      <td>0.179446</td>\n      <td>0.323706</td>\n      <td>-0.049855</td>\n      <td>-0.350672</td>\n      <td>-0.306750</td>\n      <td>0.069974</td>\n      <td>0.060304</td>\n      <td>0.074133</td>\n      <td>-0.099113</td>\n      <td>0.230257</td>\n      <td>0.037989</td>\n      <td>-0.070723</td>\n      <td>-0.195232</td>\n      <td>0.034662</td>\n      <td>0.098447</td>\n      <td>-0.072708</td>\n      <td>-0.251961</td>\n      <td>-0.015479</td>\n      <td>0.030078</td>\n      <td>0.109286</td>\n      <td>0.063225</td>\n      <td>0.163723</td>\n      <td>0.100954</td>\n      <td>-0.040606</td>\n      <td>0.081477</td>\n      <td>0.457815</td>\n      <td>0.069638</td>\n      <td>-0.022119</td>\n      <td>0.068916</td>\n      <td>-0.029299</td>\n      <td>-0.047933</td>\n      <td>0.130911</td>\n      <td>-0.107221</td>\n      <td>0.010681</td>\n      <td>0.017530</td>\n      <td>-0.012998</td>\n      <td>0.315592</td>\n      <td>-0.132335</td>\n      <td>0.325063</td>\n      <td>0.033086</td>\n      <td>-0.284308</td>\n      <td>-0.074773</td>\n      <td>0.102727</td>\n      <td>-0.216311</td>\n      <td>0.059021</td>\n      <td>0.078432</td>\n      <td>-0.181384</td>\n      <td>0.158258</td>\n      <td>0.045622</td>\n      <td>0.032226</td>\n      <td>0.096019</td>\n      <td>-0.211077</td>\n      <td>0.075733</td>\n      <td>-0.044160</td>\n      <td>0.129608</td>\n      <td>0.076760</td>\n      <td>-0.075360</td>\n      <td>0.466614</td>\n      <td>0.090322</td>\n      <td>0.150747</td>\n      <td>-0.085852</td>\n      <td>0.307366</td>\n      <td>0.205635</td>\n      <td>-0.059250</td>\n      <td>-0.317916</td>\n      <td>0.371092</td>\n      <td>0.113249</td>\n      <td>0.095618</td>\n      <td>-0.035221</td>\n      <td>-0.096874</td>\n      <td>0.058885</td>\n      <td>-0.083357</td>\n      <td>-0.082363</td>\n      <td>0.053920</td>\n      <td>0.117225</td>\n      <td>-0.099438</td>\n      <td>1.554892</td>\n      <td>0.286132</td>\n      <td>-0.303300</td>\n      <td>0.046252</td>\n      <td>0.269663</td>\n      <td>0.030778</td>\n      <td>-0.066629</td>\n      <td>0.099054</td>\n      <td>0.214823</td>\n      <td>0.186653</td>\n      <td>0.099686</td>\n      <td>-0.048642</td>\n      <td>-0.147051</td>\n      <td>0.068232</td>\n      <td>-0.058560</td>\n      <td>0.071003</td>\n      <td>0.050570</td>\n      <td>0.033566</td>\n      <td>12.149046</td>\n      <td>0.052788</td>\n      <td>-0.019711</td>\n      <td>0.077738</td>\n      <td>-0.249763</td>\n      <td>-0.025224</td>\n      <td>0.030602</td>\n      <td>-0.058953</td>\n      <td>0.241092</td>\n      <td>0.107833</td>\n      <td>-0.050415</td>\n      <td>-0.096370</td>\n      <td>0.116405</td>\n      <td>-0.205613</td>\n      <td>0.103472</td>\n      <td>0.020359</td>\n      <td>0.087780</td>\n      <td>0.165174</td>\n      <td>0.151956</td>\n      <td>-0.155135</td>\n      <td>0.048719</td>\n      <td>0.136277</td>\n      <td>0.014289</td>\n      <td>0.941260</td>\n      <td>-0.143702</td>\n      <td>0.072925</td>\n      <td>-0.135912</td>\n      <td>0.126020</td>\n      <td>0.117271</td>\n      <td>0.201530</td>\n      <td>-0.107007</td>\n      <td>0.218389</td>\n      <td>0.065998</td>\n      <td>-0.083668</td>\n      <td>0.053277</td>\n      <td>-0.147597</td>\n      <td>0.029762</td>\n      <td>0.036007</td>\n      <td>-0.030452</td>\n      <td>-0.116128</td>\n      <td>-0.074170</td>\n      <td>-0.051684</td>\n      <td>0.312923</td>\n      <td>-0.386000</td>\n      <td>0.135494</td>\n      <td>-0.153733</td>\n      <td>0.340041</td>\n      <td>0.280454</td>\n      <td>-0.007710</td>\n      <td>-0.049217</td>\n      <td>-0.141264</td>\n      <td>0.125780</td>\n      <td>0.236436</td>\n      <td>-0.288352</td>\n      <td>-0.001873</td>\n      <td>-0.319654</td>\n      <td>0.173400</td>\n      <td>-0.170278</td>\n      <td>-0.004907</td>\n      <td>-0.229392</td>\n      <td>-0.096131</td>\n      <td>-0.110653</td>\n      <td>0.114807</td>\n      <td>-0.077159</td>\n      <td>-0.306470</td>\n      <td>0.196648</td>\n      <td>0.239305</td>\n      <td>-0.083120</td>\n      <td>-0.006266</td>\n      <td>-0.016332</td>\n      <td>0.137176</td>\n      <td>-0.049121</td>\n      <td>0.032097</td>\n      <td>0.005268</td>\n      <td>-0.033742</td>\n      <td>-0.187795</td>\n      <td>-0.041616</td>\n      <td>-0.180207</td>\n      <td>-0.083809</td>\n      <td>-0.611474</td>\n      <td>0.132549</td>\n      <td>-0.097444</td>\n      <td>-0.145588</td>\n      <td>-0.041082</td>\n      <td>-0.310184</td>\n      <td>0.206319</td>\n      <td>-0.203444</td>\n      <td>-0.015584</td>\n      <td>-0.259579</td>\n      <td>0.176335</td>\n      <td>0.069659</td>\n      <td>0.012299</td>\n      <td>0.034244</td>\n      <td>-0.047717</td>\n      <td>0.055669</td>\n      <td>0.003719</td>\n      <td>0.157838</td>\n      <td>0.011114</td>\n      <td>0.159168</td>\n      <td>-0.178828</td>\n      <td>-0.446934</td>\n      <td>-0.108646</td>\n      <td>-0.151625</td>\n      <td>-0.023107</td>\n      <td>-0.169719</td>\n      <td>0.207508</td>\n      <td>0.013606</td>\n      <td>-0.166397</td>\n      <td>0.069439</td>\n      <td>0.024586</td>\n      <td>0.034726</td>\n      <td>0.034124</td>\n      <td>-0.228854</td>\n      <td>-0.116306</td>\n      <td>0.059327</td>\n      <td>-0.123236</td>\n      <td>0.111901</td>\n      <td>-0.049153</td>\n      <td>0.095247</td>\n      <td>0.112716</td>\n      <td>-0.023128</td>\n      <td>0.058711</td>\n      <td>0.217587</td>\n      <td>-0.042789</td>\n      <td>-0.056089</td>\n      <td>0.032627</td>\n      <td>-0.305321</td>\n      <td>0.011219</td>\n      <td>-0.307083</td>\n      <td>-0.103627</td>\n      <td>-0.063050</td>\n      <td>0.115984</td>\n      <td>0.333934</td>\n      <td>0.055513</td>\n      <td>0.008051</td>\n      <td>0.104721</td>\n      <td>-0.171578</td>\n      <td>0.106401</td>\n      <td>0.010308</td>\n      <td>0.284912</td>\n      <td>-0.166739</td>\n      <td>0.050573</td>\n      <td>-0.231710</td>\n      <td>-0.157238</td>\n      <td>-0.059044</td>\n      <td>-0.136545</td>\n      <td>0.288966</td>\n      <td>0.348855</td>\n      <td>0.023084</td>\n      <td>0.088987</td>\n      <td>-0.136358</td>\n      <td>-0.047009</td>\n      <td>-0.123210</td>\n      <td>-0.567249</td>\n      <td>-0.003853</td>\n      <td>0.035672</td>\n      <td>-0.251738</td>\n      <td>0.176084</td>\n      <td>0.085949</td>\n      <td>-0.187019</td>\n      <td>0.486607</td>\n      <td>-0.545987</td>\n      <td>-0.139555</td>\n      <td>-0.130976</td>\n      <td>-0.378054</td>\n      <td>0.055729</td>\n      <td>0.309306</td>\n      <td>-0.156030</td>\n      <td>0.109429</td>\n      <td>-0.000374</td>\n      <td>-0.064217</td>\n      <td>-0.051321</td>\n      <td>0.107150</td>\n      <td>0.062490</td>\n      <td>0.142579</td>\n      <td>0.072491</td>\n      <td>0.352781</td>\n      <td>0.152339</td>\n      <td>0.023221</td>\n      <td>-0.117772</td>\n      <td>3.154369</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1807.01034</td>\n      <td>Evangelos Thomas Karamatskos</td>\n      <td>Evangelos T. Karamatskos, Sebastian Raabe, Ter...</td>\n      <td>Molecular movie of ultrafast coherent rotation...</td>\n      <td>9 Figures</td>\n      <td>Nat Commun 10, 3364 (2019)</td>\n      <td>10.1038/s41467-019-11122-y</td>\n      <td>None</td>\n      <td>physics.chem-ph physics.atom-ph quant-ph</td>\n      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n      <td>Recording molecular movies on ultrafast time...</td>\n      <td>[{'version': 'v1', 'created': 'Tue, 3 Jul 2018...</td>\n      <td>[[Karamatskos, Evangelos T., ], [Raabe, Sebast...</td>\n      <td>1.945910</td>\n      <td>2.197225</td>\n      <td>10.1038</td>\n      <td>Nature Publishing Group</td>\n      <td>5.105945</td>\n      <td>866445.0</td>\n      <td>2020-05-19</td>\n      <td>2018-07-03 09:11:19+00:00</td>\n      <td>2019-05-09 12:01:21+00:00</td>\n      <td>2020</td>\n      <td>2018</td>\n      <td>2019</td>\n      <td>5</td>\n      <td>7</td>\n      <td>5</td>\n      <td>202005</td>\n      <td>201807</td>\n      <td>201905</td>\n      <td>19</td>\n      <td>3</td>\n      <td>9</td>\n      <td>1.589846e+09</td>\n      <td>1.530609e+09</td>\n      <td>1.557403e+09</td>\n      <td>59237321</td>\n      <td>26794202.0</td>\n      <td>2</td>\n      <td>18401</td>\n      <td>17715</td>\n      <td>18025</td>\n      <td>310</td>\n      <td>155.000000</td>\n      <td>Karamatskos</td>\n      <td>13</td>\n      <td>physics</td>\n      <td>physics</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>physics physics quant</td>\n      <td>physics physics quant-ph</td>\n      <td>physics.atom-ph physics.chem-ph quant-ph</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>52993</td>\n      <td>22</td>\n      <td>50948</td>\n      <td>327</td>\n      <td>0</td>\n      <td>29</td>\n      <td>29</td>\n      <td>3527</td>\n      <td>2806</td>\n      <td>22852</td>\n      <td>1.572833</td>\n      <td>3</td>\n      <td>4.718499</td>\n      <td>0.693147</td>\n      <td>2.079442</td>\n      <td>1.945910</td>\n      <td>0.764750</td>\n      <td>0.943700</td>\n      <td>1.319529</td>\n      <td>2.012676</td>\n      <td>3.094469</td>\n      <td>11868</td>\n      <td>36725.162265</td>\n      <td>0.0</td>\n      <td>9.263786</td>\n      <td>3.135494</td>\n      <td>1.694765</td>\n      <td>0.693147</td>\n      <td>1.945910</td>\n      <td>4.317488</td>\n      <td>3.094469</td>\n      <td>11868</td>\n      <td>36725.162265</td>\n      <td>0.0</td>\n      <td>9.263786</td>\n      <td>3.135494</td>\n      <td>1.694765</td>\n      <td>0.693147</td>\n      <td>1.945910</td>\n      <td>4.317488</td>\n      <td>1.572833</td>\n      <td>3</td>\n      <td>4.718499</td>\n      <td>0.693147</td>\n      <td>2.079442</td>\n      <td>1.945910</td>\n      <td>0.764750</td>\n      <td>0.943700</td>\n      <td>1.319529</td>\n      <td>2.012676</td>\n      <td>0.235188</td>\n      <td>6130</td>\n      <td>1441.699761</td>\n      <td>0.0</td>\n      <td>6.516193</td>\n      <td>0.000000</td>\n      <td>0.611910</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.221578</td>\n      <td>5360</td>\n      <td>6547.656998</td>\n      <td>0.0</td>\n      <td>5.141664</td>\n      <td>1.098612</td>\n      <td>0.966951</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>1.945910</td>\n      <td>1.940007</td>\n      <td>599833</td>\n      <td>1.163680e+06</td>\n      <td>0.0</td>\n      <td>9.104313</td>\n      <td>1.94591</td>\n      <td>1.350045</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>2.890372</td>\n      <td>1.880741</td>\n      <td>70889</td>\n      <td>133323.877828</td>\n      <td>0.0</td>\n      <td>8.405591</td>\n      <td>1.791759</td>\n      <td>1.330488</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>2.833213</td>\n      <td>1.880741</td>\n      <td>70889</td>\n      <td>133323.877828</td>\n      <td>0.0</td>\n      <td>8.405591</td>\n      <td>1.791759</td>\n      <td>1.330488</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>2.833213</td>\n      <td>2.138703</td>\n      <td>1099</td>\n      <td>2350.435095</td>\n      <td>0.0</td>\n      <td>6.669498</td>\n      <td>2.197225</td>\n      <td>1.338952</td>\n      <td>0.000000</td>\n      <td>1.098612</td>\n      <td>2.995732</td>\n      <td>2.138703</td>\n      <td>1099</td>\n      <td>2350.435095</td>\n      <td>0.0</td>\n      <td>6.669498</td>\n      <td>2.197225</td>\n      <td>1.338952</td>\n      <td>0.000000</td>\n      <td>1.098612</td>\n      <td>2.995732</td>\n      <td>2.370734</td>\n      <td>107</td>\n      <td>253.668574</td>\n      <td>0.0</td>\n      <td>6.669498</td>\n      <td>2.302585</td>\n      <td>1.276478</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.218876</td>\n      <td>0.373077</td>\n      <td>1.145006</td>\n      <td>-1.148559</td>\n      <td>0.719485</td>\n      <td>0.373077</td>\n      <td>1.145006</td>\n      <td>-1.148559</td>\n      <td>0.719485</td>\n      <td>1.710723</td>\n      <td>2.384990</td>\n      <td>0.724332</td>\n      <td>1.326044</td>\n      <td>0.005903</td>\n      <td>1.002008</td>\n      <td>0.065169</td>\n      <td>1.022622</td>\n      <td>0.065169</td>\n      <td>1.022622</td>\n      <td>-0.192793</td>\n      <td>0.938575</td>\n      <td>-0.192793</td>\n      <td>0.938575</td>\n      <td>-0.424824</td>\n      <td>0.873967</td>\n      <td>-1.521636</td>\n      <td>0.628368</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-1.521636</td>\n      <td>0.628368</td>\n      <td>1.337645</td>\n      <td>2.082949</td>\n      <td>0.351255</td>\n      <td>1.158111</td>\n      <td>-0.367174</td>\n      <td>0.875111</td>\n      <td>-0.307908</td>\n      <td>0.893115</td>\n      <td>-0.307908</td>\n      <td>0.893115</td>\n      <td>-0.565870</td>\n      <td>0.819712</td>\n      <td>-0.565870</td>\n      <td>0.819712</td>\n      <td>-0.797901</td>\n      <td>0.763286</td>\n      <td>1.521636</td>\n      <td>1.591424</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.859282</td>\n      <td>3.314856</td>\n      <td>1.872892</td>\n      <td>1.843046</td>\n      <td>1.154462</td>\n      <td>1.392673</td>\n      <td>1.213728</td>\n      <td>1.421325</td>\n      <td>1.213728</td>\n      <td>1.421325</td>\n      <td>0.955766</td>\n      <td>1.304510</td>\n      <td>0.955766</td>\n      <td>1.304510</td>\n      <td>0.723735</td>\n      <td>1.214711</td>\n      <td>-1.521636</td>\n      <td>0.628368</td>\n      <td>1.337645</td>\n      <td>2.082949</td>\n      <td>0.351255</td>\n      <td>1.158111</td>\n      <td>-0.367174</td>\n      <td>0.875111</td>\n      <td>-0.307908</td>\n      <td>0.893115</td>\n      <td>-0.307908</td>\n      <td>0.893115</td>\n      <td>-0.565870</td>\n      <td>0.819712</td>\n      <td>-0.565870</td>\n      <td>0.819712</td>\n      <td>-0.797901</td>\n      <td>0.763286</td>\n      <td>2.859282</td>\n      <td>3.314856</td>\n      <td>1.872892</td>\n      <td>1.843046</td>\n      <td>1.154462</td>\n      <td>1.392673</td>\n      <td>1.213728</td>\n      <td>1.421325</td>\n      <td>1.213728</td>\n      <td>1.421325</td>\n      <td>0.955766</td>\n      <td>1.304510</td>\n      <td>0.955766</td>\n      <td>1.304510</td>\n      <td>0.723735</td>\n      <td>1.214711</td>\n      <td>-0.986390</td>\n      <td>0.555996</td>\n      <td>-1.704819</td>\n      <td>0.420131</td>\n      <td>-1.645554</td>\n      <td>0.428774</td>\n      <td>-1.645554</td>\n      <td>0.428774</td>\n      <td>-1.903516</td>\n      <td>0.393534</td>\n      <td>-1.903516</td>\n      <td>0.393534</td>\n      <td>-2.135547</td>\n      <td>0.366445</td>\n      <td>-0.718429</td>\n      <td>0.755637</td>\n      <td>-0.659164</td>\n      <td>0.771183</td>\n      <td>-0.659164</td>\n      <td>0.771183</td>\n      <td>-0.917126</td>\n      <td>0.707801</td>\n      <td>-0.917126</td>\n      <td>0.707801</td>\n      <td>-1.149157</td>\n      <td>0.659078</td>\n      <td>0.059266</td>\n      <td>1.020573</td>\n      <td>0.059266</td>\n      <td>1.020573</td>\n      <td>-0.198696</td>\n      <td>0.936695</td>\n      <td>-0.198696</td>\n      <td>0.936695</td>\n      <td>-0.430727</td>\n      <td>0.872216</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.257962</td>\n      <td>0.917813</td>\n      <td>-0.257962</td>\n      <td>0.917813</td>\n      <td>-0.489993</td>\n      <td>0.854633</td>\n      <td>-0.257962</td>\n      <td>0.917813</td>\n      <td>-0.257962</td>\n      <td>0.917813</td>\n      <td>-0.489993</td>\n      <td>0.854633</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.232031</td>\n      <td>0.931163</td>\n      <td>-0.232031</td>\n      <td>0.931163</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.058636</td>\n      <td>0.010714</td>\n      <td>-0.059075</td>\n      <td>0.174498</td>\n      <td>0.100554</td>\n      <td>-0.131889</td>\n      <td>0.225161</td>\n      <td>0.244286</td>\n      <td>0.006053</td>\n      <td>-0.035511</td>\n      <td>-0.138970</td>\n      <td>-0.300824</td>\n      <td>0.026777</td>\n      <td>-0.094231</td>\n      <td>0.070053</td>\n      <td>0.049529</td>\n      <td>0.032565</td>\n      <td>0.228549</td>\n      <td>-0.029385</td>\n      <td>0.202228</td>\n      <td>-0.196451</td>\n      <td>0.268336</td>\n      <td>-0.141619</td>\n      <td>-0.045674</td>\n      <td>0.251531</td>\n      <td>0.092396</td>\n      <td>-0.032728</td>\n      <td>0.143906</td>\n      <td>-0.253902</td>\n      <td>0.106192</td>\n      <td>0.025324</td>\n      <td>0.149742</td>\n      <td>-0.020356</td>\n      <td>-0.044923</td>\n      <td>-0.098189</td>\n      <td>-0.001313</td>\n      <td>-0.009215</td>\n      <td>0.025668</td>\n      <td>-0.242498</td>\n      <td>0.037107</td>\n      <td>-0.144369</td>\n      <td>0.629059</td>\n      <td>0.006858</td>\n      <td>-0.194995</td>\n      <td>-0.103956</td>\n      <td>-0.204220</td>\n      <td>0.076918</td>\n      <td>0.008045</td>\n      <td>-0.109790</td>\n      <td>0.213696</td>\n      <td>0.041697</td>\n      <td>0.010409</td>\n      <td>-0.092023</td>\n      <td>0.012192</td>\n      <td>0.049983</td>\n      <td>0.049678</td>\n      <td>0.025156</td>\n      <td>0.043410</td>\n      <td>0.065244</td>\n      <td>0.022134</td>\n      <td>0.073444</td>\n      <td>-0.187696</td>\n      <td>0.129353</td>\n      <td>0.149432</td>\n      <td>-0.172874</td>\n      <td>-0.044035</td>\n      <td>0.021388</td>\n      <td>0.312965</td>\n      <td>0.098221</td>\n      <td>-0.124847</td>\n      <td>0.055834</td>\n      <td>-0.125175</td>\n      <td>0.032435</td>\n      <td>0.128881</td>\n      <td>0.235134</td>\n      <td>0.140699</td>\n      <td>0.069918</td>\n      <td>-0.672858</td>\n      <td>0.009851</td>\n      <td>-0.029000</td>\n      <td>0.027043</td>\n      <td>-0.002515</td>\n      <td>1.163201</td>\n      <td>0.212105</td>\n      <td>0.192237</td>\n      <td>-0.029575</td>\n      <td>0.056986</td>\n      <td>0.041314</td>\n      <td>-0.052864</td>\n      <td>0.035421</td>\n      <td>0.022062</td>\n      <td>0.015643</td>\n      <td>-0.026877</td>\n      <td>0.081834</td>\n      <td>-0.032967</td>\n      <td>0.044244</td>\n      <td>0.107404</td>\n      <td>-0.799779</td>\n      <td>0.027633</td>\n      <td>0.050448</td>\n      <td>0.150503</td>\n      <td>-0.204648</td>\n      <td>0.100943</td>\n      <td>-0.063438</td>\n      <td>-0.115660</td>\n      <td>0.354749</td>\n      <td>0.053366</td>\n      <td>0.041253</td>\n      <td>-0.015168</td>\n      <td>-0.017427</td>\n      <td>0.013218</td>\n      <td>0.014239</td>\n      <td>-0.009076</td>\n      <td>0.149112</td>\n      <td>0.058685</td>\n      <td>-0.025347</td>\n      <td>0.161608</td>\n      <td>0.215013</td>\n      <td>-0.115882</td>\n      <td>0.378832</td>\n      <td>0.067918</td>\n      <td>0.053657</td>\n      <td>0.035087</td>\n      <td>0.019467</td>\n      <td>0.232082</td>\n      <td>0.133354</td>\n      <td>-0.099733</td>\n      <td>0.182296</td>\n      <td>-0.092707</td>\n      <td>0.146196</td>\n      <td>0.166810</td>\n      <td>-0.540334</td>\n      <td>-0.156816</td>\n      <td>-0.073869</td>\n      <td>-0.132254</td>\n      <td>0.040842</td>\n      <td>0.107210</td>\n      <td>0.153356</td>\n      <td>-0.050587</td>\n      <td>0.014879</td>\n      <td>0.059130</td>\n      <td>0.212193</td>\n      <td>-0.147306</td>\n      <td>-0.299204</td>\n      <td>0.169198</td>\n      <td>0.166360</td>\n      <td>-0.019866</td>\n      <td>-0.202794</td>\n      <td>0.077016</td>\n      <td>0.236030</td>\n      <td>0.133821</td>\n      <td>0.009633</td>\n      <td>-0.222554</td>\n      <td>-0.305388</td>\n      <td>0.086404</td>\n      <td>0.491844</td>\n      <td>0.043066</td>\n      <td>-0.104762</td>\n      <td>-0.050825</td>\n      <td>0.050087</td>\n      <td>0.077291</td>\n      <td>-0.013840</td>\n      <td>0.137109</td>\n      <td>-0.019664</td>\n      <td>0.106395</td>\n      <td>-0.061957</td>\n      <td>-0.056608</td>\n      <td>0.063086</td>\n      <td>-0.059875</td>\n      <td>0.010025</td>\n      <td>-0.054847</td>\n      <td>-0.039138</td>\n      <td>0.221609</td>\n      <td>0.029139</td>\n      <td>-0.082917</td>\n      <td>0.006236</td>\n      <td>-0.024076</td>\n      <td>-0.006428</td>\n      <td>-0.076604</td>\n      <td>0.005966</td>\n      <td>-0.058579</td>\n      <td>0.032553</td>\n      <td>-0.010159</td>\n      <td>-0.121083</td>\n      <td>0.096812</td>\n      <td>0.032963</td>\n      <td>0.008011</td>\n      <td>0.204217</td>\n      <td>-0.026281</td>\n      <td>-0.026648</td>\n      <td>-0.083031</td>\n      <td>-0.098149</td>\n      <td>-0.115228</td>\n      <td>-0.168603</td>\n      <td>0.034523</td>\n      <td>-0.229638</td>\n      <td>-0.015499</td>\n      <td>0.099428</td>\n      <td>0.316498</td>\n      <td>-0.059244</td>\n      <td>0.043453</td>\n      <td>-0.238379</td>\n      <td>-0.048957</td>\n      <td>0.029714</td>\n      <td>0.175877</td>\n      <td>-0.058545</td>\n      <td>-0.337008</td>\n      <td>0.139788</td>\n      <td>0.067161</td>\n      <td>-0.145479</td>\n      <td>0.020695</td>\n      <td>0.009076</td>\n      <td>-0.301170</td>\n      <td>0.149199</td>\n      <td>-0.067805</td>\n      <td>0.244772</td>\n      <td>0.102992</td>\n      <td>-0.486507</td>\n      <td>-0.255377</td>\n      <td>-0.370387</td>\n      <td>0.036015</td>\n      <td>0.131491</td>\n      <td>-0.050544</td>\n      <td>-0.000680</td>\n      <td>-0.045226</td>\n      <td>-0.034644</td>\n      <td>-0.040375</td>\n      <td>-0.047943</td>\n      <td>0.050667</td>\n      <td>-0.214451</td>\n      <td>0.039135</td>\n      <td>0.437912</td>\n      <td>0.147950</td>\n      <td>0.118242</td>\n      <td>0.141982</td>\n      <td>-0.171082</td>\n      <td>0.024772</td>\n      <td>-0.093310</td>\n      <td>-0.102587</td>\n      <td>-0.174004</td>\n      <td>-0.434453</td>\n      <td>-0.017174</td>\n      <td>0.117015</td>\n      <td>0.044358</td>\n      <td>-0.163389</td>\n      <td>0.056085</td>\n      <td>-0.041381</td>\n      <td>-0.064689</td>\n      <td>0.120794</td>\n      <td>-0.096786</td>\n      <td>0.035183</td>\n      <td>0.131595</td>\n      <td>-0.048496</td>\n      <td>0.094129</td>\n      <td>0.370886</td>\n      <td>-0.129678</td>\n      <td>0.024195</td>\n      <td>-0.052577</td>\n      <td>-0.168958</td>\n      <td>0.702454</td>\n      <td>-0.024561</td>\n      <td>-0.081442</td>\n      <td>0.230987</td>\n      <td>0.017456</td>\n      <td>-0.031369</td>\n      <td>-0.184837</td>\n      <td>-0.079899</td>\n      <td>-0.001711</td>\n      <td>0.116212</td>\n      <td>0.031062</td>\n      <td>-0.043929</td>\n      <td>0.058226</td>\n      <td>0.036402</td>\n      <td>-0.061046</td>\n      <td>-0.103579</td>\n      <td>0.487620</td>\n      <td>0.306732</td>\n      <td>-0.080666</td>\n      <td>-0.075670</td>\n      <td>-0.113858</td>\n      <td>0.228503</td>\n      <td>-0.137072</td>\n      <td>0.171435</td>\n      <td>0.000850</td>\n      <td>-0.092887</td>\n      <td>-0.006098</td>\n      <td>0.034071</td>\n      <td>-0.045583</td>\n      <td>-0.092175</td>\n      <td>0.170488</td>\n      <td>-0.092354</td>\n      <td>-0.064047</td>\n      <td>-0.077924</td>\n      <td>0.002353</td>\n      <td>0.212249</td>\n      <td>0.072543</td>\n      <td>0.185454</td>\n      <td>-0.137102</td>\n      <td>-0.034992</td>\n      <td>-0.068608</td>\n      <td>0.063293</td>\n      <td>-0.081857</td>\n      <td>-0.036655</td>\n      <td>-0.024877</td>\n      <td>-0.054160</td>\n      <td>-0.003148</td>\n      <td>-0.295286</td>\n      <td>-0.001143</td>\n      <td>0.103214</td>\n      <td>0.067657</td>\n      <td>-0.173375</td>\n      <td>0.118085</td>\n      <td>-0.092448</td>\n      <td>-0.070693</td>\n      <td>0.000386</td>\n      <td>0.146617</td>\n      <td>-0.203845</td>\n      <td>0.104012</td>\n      <td>0.035923</td>\n      <td>-0.065729</td>\n      <td>-0.255621</td>\n      <td>-0.018872</td>\n      <td>-0.235537</td>\n      <td>-0.139847</td>\n      <td>-0.229877</td>\n      <td>0.091465</td>\n      <td>-0.129228</td>\n      <td>0.063348</td>\n      <td>0.222444</td>\n      <td>0.167723</td>\n      <td>0.799401</td>\n      <td>0.287181</td>\n      <td>0.244295</td>\n      <td>-0.091952</td>\n      <td>0.189623</td>\n      <td>-0.004588</td>\n      <td>-0.096270</td>\n      <td>0.103717</td>\n      <td>0.122914</td>\n      <td>0.180498</td>\n      <td>-0.085829</td>\n      <td>0.037453</td>\n      <td>0.083641</td>\n      <td>-0.022197</td>\n      <td>0.278891</td>\n      <td>0.031745</td>\n      <td>0.040040</td>\n      <td>0.174038</td>\n      <td>0.237011</td>\n      <td>-0.129716</td>\n      <td>-0.090428</td>\n      <td>-0.092568</td>\n      <td>0.020385</td>\n      <td>-0.290315</td>\n      <td>-0.136768</td>\n      <td>0.094372</td>\n      <td>-0.239339</td>\n      <td>0.066254</td>\n      <td>-0.219372</td>\n      <td>-0.013867</td>\n      <td>-0.058819</td>\n      <td>0.167249</td>\n      <td>0.088246</td>\n      <td>0.139254</td>\n      <td>0.158456</td>\n      <td>-0.214302</td>\n      <td>0.059863</td>\n      <td>0.048369</td>\n      <td>0.140961</td>\n      <td>-0.052258</td>\n      <td>0.022215</td>\n      <td>0.117193</td>\n      <td>-0.198312</td>\n      <td>0.046149</td>\n      <td>0.065080</td>\n      <td>-0.131723</td>\n      <td>0.148405</td>\n      <td>-0.105136</td>\n      <td>-0.017348</td>\n      <td>0.017469</td>\n      <td>0.055048</td>\n      <td>0.062849</td>\n      <td>-0.145111</td>\n      <td>-0.090956</td>\n      <td>0.347868</td>\n      <td>-0.173408</td>\n      <td>0.239250</td>\n      <td>0.233804</td>\n      <td>-0.050822</td>\n      <td>-0.043957</td>\n      <td>-0.081328</td>\n      <td>-0.130969</td>\n      <td>0.090637</td>\n      <td>-0.005476</td>\n      <td>0.005979</td>\n      <td>-0.095841</td>\n      <td>-0.004443</td>\n      <td>-0.573272</td>\n      <td>0.052719</td>\n      <td>-0.039369</td>\n      <td>-0.043112</td>\n      <td>0.149305</td>\n      <td>0.013323</td>\n      <td>-0.167854</td>\n      <td>0.013867</td>\n      <td>0.147719</td>\n      <td>-0.077423</td>\n      <td>-0.168064</td>\n      <td>0.023604</td>\n      <td>0.206980</td>\n      <td>0.096800</td>\n      <td>0.054415</td>\n      <td>-0.025650</td>\n      <td>-0.099143</td>\n      <td>-0.107413</td>\n      <td>0.009039</td>\n      <td>-0.051372</td>\n      <td>-0.071684</td>\n      <td>-0.337983</td>\n      <td>-0.022913</td>\n      <td>-0.115648</td>\n      <td>-0.147720</td>\n      <td>0.076459</td>\n      <td>-0.032694</td>\n      <td>-0.405611</td>\n      <td>-0.093737</td>\n      <td>0.162254</td>\n      <td>-0.128603</td>\n      <td>-0.059068</td>\n      <td>0.152106</td>\n      <td>0.116981</td>\n      <td>0.034100</td>\n      <td>-0.091319</td>\n      <td>-0.047329</td>\n      <td>-0.220269</td>\n      <td>-0.181284</td>\n      <td>0.147484</td>\n      <td>-0.030241</td>\n      <td>-0.094176</td>\n      <td>-0.028390</td>\n      <td>-0.423256</td>\n      <td>-0.008630</td>\n      <td>-0.089388</td>\n      <td>-0.051802</td>\n      <td>-0.187190</td>\n      <td>-0.056604</td>\n      <td>0.039992</td>\n      <td>-0.045386</td>\n      <td>-0.159299</td>\n      <td>0.065161</td>\n      <td>-0.119729</td>\n      <td>0.084508</td>\n      <td>-0.238439</td>\n      <td>-1.245493</td>\n      <td>0.316492</td>\n      <td>0.124741</td>\n      <td>-0.094329</td>\n      <td>0.040135</td>\n      <td>-0.053970</td>\n      <td>-0.139376</td>\n      <td>0.224051</td>\n      <td>-0.015139</td>\n      <td>0.030831</td>\n      <td>-0.236975</td>\n      <td>0.074075</td>\n      <td>-0.125315</td>\n      <td>0.195212</td>\n      <td>-0.053042</td>\n      <td>-0.068390</td>\n      <td>-0.011739</td>\n      <td>0.259602</td>\n      <td>-0.029778</td>\n      <td>-0.204091</td>\n      <td>-0.161495</td>\n      <td>-0.101882</td>\n      <td>0.029371</td>\n      <td>-0.096685</td>\n      <td>0.242557</td>\n      <td>0.128139</td>\n      <td>-0.102149</td>\n      <td>-0.031175</td>\n      <td>0.023084</td>\n      <td>0.090913</td>\n      <td>-0.008003</td>\n      <td>-0.002542</td>\n      <td>-0.130041</td>\n      <td>0.017542</td>\n      <td>-0.032975</td>\n      <td>0.165599</td>\n      <td>-0.146561</td>\n      <td>-0.315001</td>\n      <td>-0.144236</td>\n      <td>0.203106</td>\n      <td>0.171700</td>\n      <td>0.188711</td>\n      <td>0.191581</td>\n      <td>0.262250</td>\n      <td>-0.171416</td>\n      <td>-0.238761</td>\n      <td>0.053921</td>\n      <td>0.084004</td>\n      <td>0.037835</td>\n      <td>-0.001942</td>\n      <td>0.284443</td>\n      <td>-0.001259</td>\n      <td>-0.081281</td>\n      <td>-0.196211</td>\n      <td>-0.050664</td>\n      <td>0.086285</td>\n      <td>-0.074844</td>\n      <td>-0.334811</td>\n      <td>-0.061478</td>\n      <td>0.093135</td>\n      <td>0.079076</td>\n      <td>0.130750</td>\n      <td>0.078746</td>\n      <td>0.000870</td>\n      <td>-0.100240</td>\n      <td>0.161888</td>\n      <td>0.394872</td>\n      <td>-0.018865</td>\n      <td>-0.040577</td>\n      <td>0.049906</td>\n      <td>0.058182</td>\n      <td>0.003539</td>\n      <td>0.048700</td>\n      <td>-0.061696</td>\n      <td>0.023898</td>\n      <td>0.019088</td>\n      <td>0.065765</td>\n      <td>0.357156</td>\n      <td>-0.058940</td>\n      <td>0.223929</td>\n      <td>-0.029957</td>\n      <td>-0.171474</td>\n      <td>-0.064602</td>\n      <td>0.085286</td>\n      <td>-0.104078</td>\n      <td>0.013963</td>\n      <td>0.001375</td>\n      <td>-0.199610</td>\n      <td>0.120454</td>\n      <td>0.029318</td>\n      <td>-0.056383</td>\n      <td>0.008633</td>\n      <td>-0.183540</td>\n      <td>0.099670</td>\n      <td>0.110537</td>\n      <td>0.078604</td>\n      <td>0.184638</td>\n      <td>0.028015</td>\n      <td>0.213840</td>\n      <td>0.150936</td>\n      <td>0.156741</td>\n      <td>-0.150252</td>\n      <td>0.356990</td>\n      <td>0.111836</td>\n      <td>-0.013739</td>\n      <td>-0.165121</td>\n      <td>0.342721</td>\n      <td>-0.046606</td>\n      <td>0.091505</td>\n      <td>-0.017373</td>\n      <td>0.065098</td>\n      <td>0.136549</td>\n      <td>-0.198081</td>\n      <td>-0.075013</td>\n      <td>0.051467</td>\n      <td>0.043023</td>\n      <td>0.005960</td>\n      <td>1.449147</td>\n      <td>0.158738</td>\n      <td>-0.154632</td>\n      <td>-0.046137</td>\n      <td>0.202906</td>\n      <td>0.046001</td>\n      <td>0.046088</td>\n      <td>-0.092596</td>\n      <td>0.124631</td>\n      <td>0.058095</td>\n      <td>0.245624</td>\n      <td>-0.043082</td>\n      <td>-0.064476</td>\n      <td>0.022578</td>\n      <td>-0.102995</td>\n      <td>-0.200476</td>\n      <td>0.017085</td>\n      <td>-0.036725</td>\n      <td>11.943683</td>\n      <td>0.009125</td>\n      <td>-0.021048</td>\n      <td>0.045698</td>\n      <td>-0.227533</td>\n      <td>-0.049069</td>\n      <td>-0.027898</td>\n      <td>0.021803</td>\n      <td>0.141909</td>\n      <td>0.143262</td>\n      <td>0.028343</td>\n      <td>0.013603</td>\n      <td>0.211610</td>\n      <td>-0.150498</td>\n      <td>-0.003177</td>\n      <td>0.041957</td>\n      <td>-0.032630</td>\n      <td>0.211481</td>\n      <td>0.146104</td>\n      <td>-0.185986</td>\n      <td>0.156548</td>\n      <td>0.081175</td>\n      <td>0.050995</td>\n      <td>0.899738</td>\n      <td>-0.161586</td>\n      <td>0.089926</td>\n      <td>-0.130796</td>\n      <td>-0.123190</td>\n      <td>0.086665</td>\n      <td>0.235653</td>\n      <td>-0.042723</td>\n      <td>0.308070</td>\n      <td>0.091798</td>\n      <td>-0.021499</td>\n      <td>-0.014432</td>\n      <td>-0.194382</td>\n      <td>-0.075963</td>\n      <td>-0.058627</td>\n      <td>0.029009</td>\n      <td>-0.040424</td>\n      <td>-0.000913</td>\n      <td>-0.051034</td>\n      <td>0.290411</td>\n      <td>-0.335732</td>\n      <td>0.170565</td>\n      <td>-0.109928</td>\n      <td>0.268078</td>\n      <td>0.145914</td>\n      <td>-0.000980</td>\n      <td>-0.030464</td>\n      <td>-0.043086</td>\n      <td>0.071312</td>\n      <td>0.179723</td>\n      <td>-0.211874</td>\n      <td>-0.034422</td>\n      <td>-0.307077</td>\n      <td>0.115623</td>\n      <td>-0.215856</td>\n      <td>0.036417</td>\n      <td>0.080084</td>\n      <td>-0.064783</td>\n      <td>-0.117606</td>\n      <td>0.127752</td>\n      <td>-0.012882</td>\n      <td>-0.205301</td>\n      <td>0.157589</td>\n      <td>0.201962</td>\n      <td>-0.038278</td>\n      <td>-0.064192</td>\n      <td>-0.049258</td>\n      <td>0.068961</td>\n      <td>-0.065382</td>\n      <td>0.155502</td>\n      <td>-0.050035</td>\n      <td>-0.160276</td>\n      <td>-0.177191</td>\n      <td>-0.369967</td>\n      <td>-0.243220</td>\n      <td>-0.121856</td>\n      <td>-0.589593</td>\n      <td>0.068379</td>\n      <td>-0.086653</td>\n      <td>-0.168638</td>\n      <td>-0.069073</td>\n      <td>-0.301360</td>\n      <td>0.129226</td>\n      <td>-0.260017</td>\n      <td>0.015221</td>\n      <td>-0.192370</td>\n      <td>0.160849</td>\n      <td>0.069858</td>\n      <td>0.022968</td>\n      <td>0.029516</td>\n      <td>-0.123990</td>\n      <td>-0.063065</td>\n      <td>0.016799</td>\n      <td>0.150945</td>\n      <td>-0.042326</td>\n      <td>0.166288</td>\n      <td>-0.224466</td>\n      <td>-0.486029</td>\n      <td>-0.094058</td>\n      <td>-0.081841</td>\n      <td>0.025652</td>\n      <td>-0.132757</td>\n      <td>0.218547</td>\n      <td>0.033704</td>\n      <td>-0.149657</td>\n      <td>0.231645</td>\n      <td>-0.030209</td>\n      <td>0.024474</td>\n      <td>0.034954</td>\n      <td>-0.164471</td>\n      <td>0.055304</td>\n      <td>0.095869</td>\n      <td>-0.088905</td>\n      <td>0.045986</td>\n      <td>0.045848</td>\n      <td>0.143413</td>\n      <td>0.154731</td>\n      <td>0.029386</td>\n      <td>0.088313</td>\n      <td>0.053296</td>\n      <td>0.047181</td>\n      <td>-0.065630</td>\n      <td>0.069703</td>\n      <td>-0.299505</td>\n      <td>0.075170</td>\n      <td>-0.240123</td>\n      <td>-0.079219</td>\n      <td>-0.327267</td>\n      <td>-0.042809</td>\n      <td>0.261281</td>\n      <td>0.048127</td>\n      <td>0.019458</td>\n      <td>0.078568</td>\n      <td>-0.075148</td>\n      <td>0.000128</td>\n      <td>0.282459</td>\n      <td>0.175455</td>\n      <td>-0.112744</td>\n      <td>0.021762</td>\n      <td>-0.165469</td>\n      <td>-0.224393</td>\n      <td>0.010819</td>\n      <td>-0.105791</td>\n      <td>0.194084</td>\n      <td>0.509446</td>\n      <td>0.113878</td>\n      <td>0.128766</td>\n      <td>-0.175802</td>\n      <td>-0.106955</td>\n      <td>-0.074288</td>\n      <td>-0.475873</td>\n      <td>0.123529</td>\n      <td>0.007325</td>\n      <td>-0.199647</td>\n      <td>0.186245</td>\n      <td>0.054156</td>\n      <td>-0.242084</td>\n      <td>0.522150</td>\n      <td>-0.427954</td>\n      <td>-0.208415</td>\n      <td>-0.076315</td>\n      <td>-0.206017</td>\n      <td>0.068706</td>\n      <td>0.202041</td>\n      <td>-0.066067</td>\n      <td>0.023081</td>\n      <td>0.112127</td>\n      <td>-0.219112</td>\n      <td>0.008264</td>\n      <td>-0.003492</td>\n      <td>0.049195</td>\n      <td>0.105896</td>\n      <td>0.144375</td>\n      <td>0.173970</td>\n      <td>0.156530</td>\n      <td>0.093464</td>\n      <td>-0.037191</td>\n      <td>1.208062</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>astro-ph/9908243</td>\n      <td>Peter Meszaros</td>\n      <td>C. Weth (1, 2), P. Meszaros (1,3,4), T. Kallma...</td>\n      <td>Early X-ray/UV Line Signatures of GRB Progenit...</td>\n      <td>revisions to ApJ ms first submitted 8/21/99; u...</td>\n      <td>Astrophys.J. 534 (2000) 581-586</td>\n      <td>10.1086/308792</td>\n      <td>None</td>\n      <td>astro-ph</td>\n      <td>None</td>\n      <td>We calculate the X-ray/UV spectral line sign...</td>\n      <td>[{'version': 'v1', 'created': 'Sat, 21 Aug 199...</td>\n      <td>[[Weth, C., ], [Meszaros, P., ], [Kallman, T.,...</td>\n      <td>3.555348</td>\n      <td>1.098612</td>\n      <td>10.1086</td>\n      <td>The University of Chicago Press</td>\n      <td>4.248495</td>\n      <td>325332.0</td>\n      <td>2009-10-31</td>\n      <td>1999-08-21 18:39:47+00:00</td>\n      <td>1999-12-02 20:50:59+00:00</td>\n      <td>2009</td>\n      <td>1999</td>\n      <td>1999</td>\n      <td>10</td>\n      <td>8</td>\n      <td>12</td>\n      <td>200910</td>\n      <td>199908</td>\n      <td>199912</td>\n      <td>31</td>\n      <td>21</td>\n      <td>2</td>\n      <td>1.256947e+09</td>\n      <td>9.352608e+08</td>\n      <td>9.441679e+08</td>\n      <td>321686413</td>\n      <td>8907072.0</td>\n      <td>3</td>\n      <td>14548</td>\n      <td>10824</td>\n      <td>10927</td>\n      <td>103</td>\n      <td>34.333333</td>\n      <td>Weth</td>\n      <td>4</td>\n      <td>astro-ph</td>\n      <td>astro-ph</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>astro</td>\n      <td>astro-ph</td>\n      <td>astro-ph</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>141917</td>\n      <td>47</td>\n      <td>106908</td>\n      <td>488</td>\n      <td>8</td>\n      <td>4</td>\n      <td>4</td>\n      <td>92</td>\n      <td>88</td>\n      <td>131</td>\n      <td>3.555348</td>\n      <td>1</td>\n      <td>3.555348</td>\n      <td>3.555348</td>\n      <td>3.555348</td>\n      <td>3.555348</td>\n      <td>NaN</td>\n      <td>3.555348</td>\n      <td>3.555348</td>\n      <td>3.555348</td>\n      <td>3.656215</td>\n      <td>28742</td>\n      <td>105086.939920</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>3.663562</td>\n      <td>1.098822</td>\n      <td>2.302585</td>\n      <td>2.995732</td>\n      <td>4.356709</td>\n      <td>3.656215</td>\n      <td>28742</td>\n      <td>105086.939920</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>3.663562</td>\n      <td>1.098822</td>\n      <td>2.302585</td>\n      <td>2.995732</td>\n      <td>4.356709</td>\n      <td>3.724077</td>\n      <td>46</td>\n      <td>171.307562</td>\n      <td>0.000000</td>\n      <td>6.651572</td>\n      <td>3.637240</td>\n      <td>1.689052</td>\n      <td>1.497866</td>\n      <td>2.753631</td>\n      <td>4.881364</td>\n      <td>2.621200</td>\n      <td>63528</td>\n      <td>166519.618737</td>\n      <td>0.0</td>\n      <td>8.306719</td>\n      <td>2.639057</td>\n      <td>1.384934</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.583519</td>\n      <td>2.614574</td>\n      <td>1343</td>\n      <td>3511.372690</td>\n      <td>0.0</td>\n      <td>7.584265</td>\n      <td>2.639057</td>\n      <td>1.470314</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.610918</td>\n      <td>2.647246</td>\n      <td>290654</td>\n      <td>7.694327e+05</td>\n      <td>0.0</td>\n      <td>10.133567</td>\n      <td>2.70805</td>\n      <td>1.405163</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.610918</td>\n      <td>2.640078</td>\n      <td>195890</td>\n      <td>517164.893335</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>2.772589</td>\n      <td>1.436436</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.663562</td>\n      <td>2.640078</td>\n      <td>195890</td>\n      <td>517164.893335</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>2.772589</td>\n      <td>1.436436</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.663562</td>\n      <td>2.714563</td>\n      <td>145677</td>\n      <td>395449.460335</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>2.833213</td>\n      <td>1.429668</td>\n      <td>0.693147</td>\n      <td>1.791759</td>\n      <td>3.713572</td>\n      <td>2.714563</td>\n      <td>145677</td>\n      <td>395449.460335</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>2.833213</td>\n      <td>1.429668</td>\n      <td>0.693147</td>\n      <td>1.791759</td>\n      <td>3.713572</td>\n      <td>3.176480</td>\n      <td>65431</td>\n      <td>207840.252246</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>3.332205</td>\n      <td>1.387947</td>\n      <td>1.098612</td>\n      <td>2.397895</td>\n      <td>4.110874</td>\n      <td>-0.168729</td>\n      <td>0.964283</td>\n      <td>-0.100867</td>\n      <td>0.978337</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.100867</td>\n      <td>0.978337</td>\n      <td>0.934148</td>\n      <td>1.257966</td>\n      <td>0.940774</td>\n      <td>1.260273</td>\n      <td>0.908102</td>\n      <td>1.248983</td>\n      <td>0.915270</td>\n      <td>1.251442</td>\n      <td>0.915270</td>\n      <td>1.251442</td>\n      <td>0.840785</td>\n      <td>1.226348</td>\n      <td>0.840785</td>\n      <td>1.226348</td>\n      <td>0.378868</td>\n      <td>1.090715</td>\n      <td>0.067862</td>\n      <td>1.014575</td>\n      <td>0.168729</td>\n      <td>1.037040</td>\n      <td>0.067862</td>\n      <td>1.014575</td>\n      <td>1.102877</td>\n      <td>1.304561</td>\n      <td>1.109504</td>\n      <td>1.306953</td>\n      <td>1.076831</td>\n      <td>1.295245</td>\n      <td>1.083999</td>\n      <td>1.297796</td>\n      <td>1.083999</td>\n      <td>1.297796</td>\n      <td>1.009514</td>\n      <td>1.271772</td>\n      <td>1.009514</td>\n      <td>1.271772</td>\n      <td>0.547598</td>\n      <td>1.131115</td>\n      <td>0.100867</td>\n      <td>1.022143</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.035015</td>\n      <td>1.285821</td>\n      <td>1.041641</td>\n      <td>1.288178</td>\n      <td>1.008969</td>\n      <td>1.276639</td>\n      <td>1.016137</td>\n      <td>1.279153</td>\n      <td>1.016137</td>\n      <td>1.279153</td>\n      <td>0.941652</td>\n      <td>1.253503</td>\n      <td>0.941652</td>\n      <td>1.253503</td>\n      <td>0.479735</td>\n      <td>1.114866</td>\n      <td>-0.100867</td>\n      <td>0.978337</td>\n      <td>0.934148</td>\n      <td>1.257966</td>\n      <td>0.940774</td>\n      <td>1.260273</td>\n      <td>0.908102</td>\n      <td>1.248983</td>\n      <td>0.915270</td>\n      <td>1.251442</td>\n      <td>0.915270</td>\n      <td>1.251442</td>\n      <td>0.840785</td>\n      <td>1.226348</td>\n      <td>0.840785</td>\n      <td>1.226348</td>\n      <td>0.378868</td>\n      <td>1.090715</td>\n      <td>1.035015</td>\n      <td>1.285821</td>\n      <td>1.041641</td>\n      <td>1.288178</td>\n      <td>1.008969</td>\n      <td>1.276639</td>\n      <td>1.016137</td>\n      <td>1.279153</td>\n      <td>1.016137</td>\n      <td>1.279153</td>\n      <td>0.941652</td>\n      <td>1.253503</td>\n      <td>0.941652</td>\n      <td>1.253503</td>\n      <td>0.479735</td>\n      <td>1.114866</td>\n      <td>0.006627</td>\n      <td>1.001833</td>\n      <td>-0.026046</td>\n      <td>0.992859</td>\n      <td>-0.018878</td>\n      <td>0.994814</td>\n      <td>-0.018878</td>\n      <td>0.994814</td>\n      <td>-0.093363</td>\n      <td>0.974866</td>\n      <td>-0.093363</td>\n      <td>0.974866</td>\n      <td>-0.555279</td>\n      <td>0.867046</td>\n      <td>-0.032672</td>\n      <td>0.991042</td>\n      <td>-0.025504</td>\n      <td>0.992993</td>\n      <td>-0.025504</td>\n      <td>0.992993</td>\n      <td>-0.099990</td>\n      <td>0.973082</td>\n      <td>-0.099990</td>\n      <td>0.973082</td>\n      <td>-0.561906</td>\n      <td>0.865459</td>\n      <td>0.007168</td>\n      <td>1.001969</td>\n      <td>0.007168</td>\n      <td>1.001969</td>\n      <td>-0.067317</td>\n      <td>0.981878</td>\n      <td>-0.067317</td>\n      <td>0.981878</td>\n      <td>-0.529233</td>\n      <td>0.873282</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.074485</td>\n      <td>0.979948</td>\n      <td>-0.074485</td>\n      <td>0.979948</td>\n      <td>-0.536402</td>\n      <td>0.871566</td>\n      <td>-0.074485</td>\n      <td>0.979948</td>\n      <td>-0.074485</td>\n      <td>0.979948</td>\n      <td>-0.536402</td>\n      <td>0.871566</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.461916</td>\n      <td>0.889401</td>\n      <td>-0.461916</td>\n      <td>0.889401</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.014395</td>\n      <td>0.040316</td>\n      <td>-0.006585</td>\n      <td>0.126848</td>\n      <td>0.212726</td>\n      <td>0.070803</td>\n      <td>0.241755</td>\n      <td>0.267412</td>\n      <td>-0.064004</td>\n      <td>-0.098026</td>\n      <td>-0.104129</td>\n      <td>-0.278143</td>\n      <td>-0.045478</td>\n      <td>-0.180248</td>\n      <td>0.162043</td>\n      <td>0.022745</td>\n      <td>0.039454</td>\n      <td>0.249000</td>\n      <td>0.066742</td>\n      <td>0.067162</td>\n      <td>-0.166020</td>\n      <td>0.239773</td>\n      <td>-0.052797</td>\n      <td>-0.072319</td>\n      <td>0.256792</td>\n      <td>0.137566</td>\n      <td>-0.085245</td>\n      <td>0.269796</td>\n      <td>-0.185596</td>\n      <td>0.050790</td>\n      <td>0.060654</td>\n      <td>0.019494</td>\n      <td>-0.013807</td>\n      <td>-0.025799</td>\n      <td>-0.139750</td>\n      <td>-0.012520</td>\n      <td>-0.024701</td>\n      <td>0.014373</td>\n      <td>-0.240063</td>\n      <td>0.095081</td>\n      <td>-0.253646</td>\n      <td>0.697968</td>\n      <td>0.111205</td>\n      <td>-0.203138</td>\n      <td>0.005281</td>\n      <td>-0.266690</td>\n      <td>0.091751</td>\n      <td>0.135966</td>\n      <td>-0.105726</td>\n      <td>0.140257</td>\n      <td>-0.127193</td>\n      <td>-0.068365</td>\n      <td>-0.097209</td>\n      <td>0.085209</td>\n      <td>-0.004637</td>\n      <td>0.020475</td>\n      <td>0.044677</td>\n      <td>0.073270</td>\n      <td>0.017781</td>\n      <td>0.129403</td>\n      <td>0.017203</td>\n      <td>-0.092696</td>\n      <td>0.130777</td>\n      <td>0.266385</td>\n      <td>-0.089212</td>\n      <td>-0.030700</td>\n      <td>0.016042</td>\n      <td>0.238878</td>\n      <td>-0.016649</td>\n      <td>-0.135385</td>\n      <td>0.039690</td>\n      <td>0.004393</td>\n      <td>-0.035969</td>\n      <td>0.105605</td>\n      <td>0.194074</td>\n      <td>0.008125</td>\n      <td>0.031153</td>\n      <td>-0.883288</td>\n      <td>-0.005592</td>\n      <td>0.035557</td>\n      <td>0.007486</td>\n      <td>0.086736</td>\n      <td>1.020782</td>\n      <td>0.146921</td>\n      <td>0.084700</td>\n      <td>-0.114722</td>\n      <td>0.002327</td>\n      <td>0.059267</td>\n      <td>0.023035</td>\n      <td>0.099756</td>\n      <td>-0.049315</td>\n      <td>0.000573</td>\n      <td>0.046366</td>\n      <td>-0.028310</td>\n      <td>-0.049696</td>\n      <td>0.060778</td>\n      <td>0.167912</td>\n      <td>-0.898141</td>\n      <td>-0.032359</td>\n      <td>0.047238</td>\n      <td>0.150196</td>\n      <td>-0.177945</td>\n      <td>0.095856</td>\n      <td>-0.055000</td>\n      <td>-0.187189</td>\n      <td>0.262604</td>\n      <td>0.134465</td>\n      <td>0.088091</td>\n      <td>-0.049437</td>\n      <td>-0.083170</td>\n      <td>-0.023037</td>\n      <td>-0.068187</td>\n      <td>-0.111820</td>\n      <td>0.220422</td>\n      <td>0.070814</td>\n      <td>-0.023280</td>\n      <td>0.192262</td>\n      <td>0.173185</td>\n      <td>-0.119966</td>\n      <td>0.318646</td>\n      <td>0.118725</td>\n      <td>0.066591</td>\n      <td>0.089681</td>\n      <td>-0.034637</td>\n      <td>0.179630</td>\n      <td>0.213383</td>\n      <td>-0.141436</td>\n      <td>0.154860</td>\n      <td>-0.064743</td>\n      <td>0.185055</td>\n      <td>0.077914</td>\n      <td>-0.570457</td>\n      <td>-0.169875</td>\n      <td>-0.086884</td>\n      <td>-0.011990</td>\n      <td>-0.002227</td>\n      <td>0.063539</td>\n      <td>0.116127</td>\n      <td>-0.022118</td>\n      <td>0.059615</td>\n      <td>0.151680</td>\n      <td>0.312870</td>\n      <td>-0.080331</td>\n      <td>-0.185251</td>\n      <td>0.175763</td>\n      <td>0.068033</td>\n      <td>-0.027991</td>\n      <td>-0.290627</td>\n      <td>0.001316</td>\n      <td>0.197334</td>\n      <td>0.173489</td>\n      <td>-0.009123</td>\n      <td>-0.255338</td>\n      <td>-0.276321</td>\n      <td>0.010751</td>\n      <td>0.323599</td>\n      <td>0.013591</td>\n      <td>-0.194354</td>\n      <td>-0.098156</td>\n      <td>0.085560</td>\n      <td>-0.013342</td>\n      <td>-0.056274</td>\n      <td>0.211737</td>\n      <td>-0.078829</td>\n      <td>0.143575</td>\n      <td>-0.016382</td>\n      <td>-0.113807</td>\n      <td>0.161036</td>\n      <td>-0.000522</td>\n      <td>-0.010081</td>\n      <td>-0.096027</td>\n      <td>-0.045169</td>\n      <td>0.205903</td>\n      <td>0.101635</td>\n      <td>-0.072784</td>\n      <td>-0.035826</td>\n      <td>-0.054418</td>\n      <td>-0.015063</td>\n      <td>-0.017550</td>\n      <td>0.041290</td>\n      <td>-0.098889</td>\n      <td>0.006737</td>\n      <td>-0.031829</td>\n      <td>-0.178458</td>\n      <td>0.072357</td>\n      <td>-0.011122</td>\n      <td>-0.114164</td>\n      <td>0.076534</td>\n      <td>-0.012500</td>\n      <td>-0.056580</td>\n      <td>0.054856</td>\n      <td>-0.035538</td>\n      <td>-0.118811</td>\n      <td>-0.221217</td>\n      <td>0.070619</td>\n      <td>-0.299445</td>\n      <td>0.028102</td>\n      <td>0.048711</td>\n      <td>0.318984</td>\n      <td>-0.028681</td>\n      <td>0.082179</td>\n      <td>-0.241116</td>\n      <td>0.132850</td>\n      <td>0.053384</td>\n      <td>0.305334</td>\n      <td>-0.076399</td>\n      <td>-0.382406</td>\n      <td>0.124662</td>\n      <td>-0.000141</td>\n      <td>-0.153085</td>\n      <td>0.013562</td>\n      <td>0.131116</td>\n      <td>-0.305380</td>\n      <td>0.137779</td>\n      <td>-0.123351</td>\n      <td>0.113804</td>\n      <td>0.023366</td>\n      <td>-0.691986</td>\n      <td>-0.226234</td>\n      <td>-0.439358</td>\n      <td>0.156078</td>\n      <td>0.122454</td>\n      <td>-0.069900</td>\n      <td>0.014667</td>\n      <td>-0.093751</td>\n      <td>0.159044</td>\n      <td>-0.017086</td>\n      <td>-0.039828</td>\n      <td>0.137597</td>\n      <td>-0.134000</td>\n      <td>0.075241</td>\n      <td>0.460601</td>\n      <td>0.194682</td>\n      <td>0.106394</td>\n      <td>0.162251</td>\n      <td>-0.102255</td>\n      <td>0.033476</td>\n      <td>-0.023335</td>\n      <td>-0.084755</td>\n      <td>-0.162164</td>\n      <td>-0.383545</td>\n      <td>0.146981</td>\n      <td>0.046603</td>\n      <td>0.103725</td>\n      <td>-0.108111</td>\n      <td>0.102718</td>\n      <td>-0.021282</td>\n      <td>-0.076587</td>\n      <td>0.044816</td>\n      <td>0.003569</td>\n      <td>-0.005044</td>\n      <td>0.158080</td>\n      <td>-0.036099</td>\n      <td>0.006859</td>\n      <td>0.275014</td>\n      <td>-0.085838</td>\n      <td>0.033226</td>\n      <td>-0.072988</td>\n      <td>-0.239376</td>\n      <td>0.588016</td>\n      <td>-0.049332</td>\n      <td>-0.138504</td>\n      <td>0.101199</td>\n      <td>0.034493</td>\n      <td>0.032310</td>\n      <td>-0.264636</td>\n      <td>-0.105204</td>\n      <td>-0.040831</td>\n      <td>0.109986</td>\n      <td>0.060860</td>\n      <td>-0.030608</td>\n      <td>0.004095</td>\n      <td>0.141148</td>\n      <td>-0.083403</td>\n      <td>-0.120084</td>\n      <td>0.314562</td>\n      <td>0.314164</td>\n      <td>0.013409</td>\n      <td>-0.059688</td>\n      <td>-0.075537</td>\n      <td>0.334556</td>\n      <td>-0.105893</td>\n      <td>0.028918</td>\n      <td>0.010418</td>\n      <td>-0.023963</td>\n      <td>-0.046750</td>\n      <td>0.004263</td>\n      <td>-0.138153</td>\n      <td>-0.079462</td>\n      <td>0.132184</td>\n      <td>-0.119096</td>\n      <td>-0.074472</td>\n      <td>-0.074425</td>\n      <td>0.039214</td>\n      <td>0.126273</td>\n      <td>0.043046</td>\n      <td>0.176546</td>\n      <td>0.026117</td>\n      <td>0.049155</td>\n      <td>-0.032219</td>\n      <td>0.038669</td>\n      <td>-0.068398</td>\n      <td>0.004747</td>\n      <td>0.048858</td>\n      <td>0.003641</td>\n      <td>0.043219</td>\n      <td>-0.279176</td>\n      <td>0.090642</td>\n      <td>0.193314</td>\n      <td>0.061296</td>\n      <td>-0.109620</td>\n      <td>0.101936</td>\n      <td>-0.032699</td>\n      <td>-0.070150</td>\n      <td>-0.077110</td>\n      <td>0.178501</td>\n      <td>-0.188987</td>\n      <td>-0.040373</td>\n      <td>-0.107662</td>\n      <td>-0.129262</td>\n      <td>-0.210910</td>\n      <td>-0.047515</td>\n      <td>-0.215605</td>\n      <td>-0.128098</td>\n      <td>-0.242442</td>\n      <td>0.137022</td>\n      <td>-0.169469</td>\n      <td>0.104505</td>\n      <td>0.274242</td>\n      <td>0.228840</td>\n      <td>0.659030</td>\n      <td>0.473674</td>\n      <td>0.191182</td>\n      <td>-0.052772</td>\n      <td>0.183293</td>\n      <td>-0.009096</td>\n      <td>-0.136143</td>\n      <td>0.134588</td>\n      <td>0.214055</td>\n      <td>0.031961</td>\n      <td>-0.084362</td>\n      <td>-0.019983</td>\n      <td>0.056233</td>\n      <td>0.041232</td>\n      <td>0.187793</td>\n      <td>0.034025</td>\n      <td>0.013881</td>\n      <td>0.126630</td>\n      <td>0.313428</td>\n      <td>-0.203654</td>\n      <td>-0.141390</td>\n      <td>-0.106300</td>\n      <td>-0.041599</td>\n      <td>-0.254144</td>\n      <td>-0.114789</td>\n      <td>0.216905</td>\n      <td>-0.174713</td>\n      <td>0.047400</td>\n      <td>-0.276530</td>\n      <td>0.135743</td>\n      <td>-0.015669</td>\n      <td>0.212420</td>\n      <td>0.064641</td>\n      <td>0.046356</td>\n      <td>0.086537</td>\n      <td>-0.300819</td>\n      <td>0.133173</td>\n      <td>-0.029847</td>\n      <td>0.144262</td>\n      <td>0.038215</td>\n      <td>-0.018609</td>\n      <td>0.144560</td>\n      <td>-0.080554</td>\n      <td>0.099522</td>\n      <td>0.082846</td>\n      <td>-0.104098</td>\n      <td>0.163453</td>\n      <td>-0.105722</td>\n      <td>-0.005788</td>\n      <td>-0.079458</td>\n      <td>0.056530</td>\n      <td>0.134542</td>\n      <td>-0.117285</td>\n      <td>-0.009504</td>\n      <td>0.333778</td>\n      <td>-0.093358</td>\n      <td>0.198542</td>\n      <td>0.162139</td>\n      <td>-0.148620</td>\n      <td>0.014203</td>\n      <td>-0.000961</td>\n      <td>-0.143421</td>\n      <td>0.018741</td>\n      <td>-0.023881</td>\n      <td>0.017504</td>\n      <td>-0.033296</td>\n      <td>0.043634</td>\n      <td>-0.530796</td>\n      <td>0.016288</td>\n      <td>-0.041590</td>\n      <td>-0.121004</td>\n      <td>0.256529</td>\n      <td>0.055697</td>\n      <td>-0.096192</td>\n      <td>-0.044014</td>\n      <td>0.176081</td>\n      <td>-0.094061</td>\n      <td>-0.120798</td>\n      <td>-0.024787</td>\n      <td>0.217521</td>\n      <td>0.074970</td>\n      <td>0.030706</td>\n      <td>-0.016892</td>\n      <td>-0.058280</td>\n      <td>-0.024367</td>\n      <td>-0.010494</td>\n      <td>-0.087810</td>\n      <td>-0.152776</td>\n      <td>-0.413415</td>\n      <td>-0.098316</td>\n      <td>-0.120644</td>\n      <td>-0.166903</td>\n      <td>0.086750</td>\n      <td>-0.031066</td>\n      <td>-0.417200</td>\n      <td>-0.031581</td>\n      <td>0.078387</td>\n      <td>-0.099511</td>\n      <td>0.016763</td>\n      <td>0.107805</td>\n      <td>0.152398</td>\n      <td>0.009406</td>\n      <td>-0.141831</td>\n      <td>-0.021426</td>\n      <td>-0.239185</td>\n      <td>-0.113952</td>\n      <td>0.039260</td>\n      <td>-0.066326</td>\n      <td>-0.128038</td>\n      <td>-0.101552</td>\n      <td>-0.372797</td>\n      <td>-0.023403</td>\n      <td>-0.097901</td>\n      <td>-0.039467</td>\n      <td>-0.150162</td>\n      <td>-0.075887</td>\n      <td>0.080679</td>\n      <td>-0.151617</td>\n      <td>-0.170695</td>\n      <td>0.000198</td>\n      <td>-0.042573</td>\n      <td>0.073403</td>\n      <td>-0.279140</td>\n      <td>-1.319828</td>\n      <td>0.277992</td>\n      <td>0.139795</td>\n      <td>-0.015011</td>\n      <td>-0.009274</td>\n      <td>-0.042717</td>\n      <td>-0.184760</td>\n      <td>0.291450</td>\n      <td>0.024838</td>\n      <td>0.028786</td>\n      <td>-0.244488</td>\n      <td>0.077660</td>\n      <td>-0.054341</td>\n      <td>0.235243</td>\n      <td>0.013562</td>\n      <td>-0.009636</td>\n      <td>-0.077241</td>\n      <td>0.207004</td>\n      <td>-0.115475</td>\n      <td>-0.193482</td>\n      <td>-0.142165</td>\n      <td>-0.115125</td>\n      <td>-0.034100</td>\n      <td>-0.115934</td>\n      <td>0.287656</td>\n      <td>0.145933</td>\n      <td>-0.093721</td>\n      <td>-0.035607</td>\n      <td>0.061535</td>\n      <td>0.120488</td>\n      <td>0.001517</td>\n      <td>-0.049003</td>\n      <td>-0.010541</td>\n      <td>0.026895</td>\n      <td>0.003773</td>\n      <td>0.213621</td>\n      <td>-0.145385</td>\n      <td>-0.092696</td>\n      <td>-0.139265</td>\n      <td>0.183296</td>\n      <td>0.240703</td>\n      <td>0.191990</td>\n      <td>0.190071</td>\n      <td>0.378330</td>\n      <td>-0.253577</td>\n      <td>-0.093517</td>\n      <td>0.072363</td>\n      <td>0.100284</td>\n      <td>0.115021</td>\n      <td>0.015604</td>\n      <td>0.025725</td>\n      <td>0.001527</td>\n      <td>-0.058733</td>\n      <td>-0.178453</td>\n      <td>0.039573</td>\n      <td>0.127935</td>\n      <td>0.004134</td>\n      <td>-0.256633</td>\n      <td>-0.137457</td>\n      <td>0.030484</td>\n      <td>0.081752</td>\n      <td>0.053804</td>\n      <td>0.018789</td>\n      <td>-0.000192</td>\n      <td>-0.123750</td>\n      <td>0.157126</td>\n      <td>0.309610</td>\n      <td>-0.003232</td>\n      <td>0.046127</td>\n      <td>0.078381</td>\n      <td>0.061917</td>\n      <td>0.015205</td>\n      <td>0.121157</td>\n      <td>-0.103559</td>\n      <td>-0.020847</td>\n      <td>-0.047766</td>\n      <td>-0.020523</td>\n      <td>0.301164</td>\n      <td>-0.085818</td>\n      <td>0.247234</td>\n      <td>-0.023593</td>\n      <td>-0.147539</td>\n      <td>-0.038534</td>\n      <td>0.039494</td>\n      <td>-0.194933</td>\n      <td>0.007838</td>\n      <td>-0.099993</td>\n      <td>-0.166663</td>\n      <td>0.063348</td>\n      <td>-0.041038</td>\n      <td>0.000037</td>\n      <td>-0.008255</td>\n      <td>-0.187349</td>\n      <td>0.238425</td>\n      <td>0.068897</td>\n      <td>0.083307</td>\n      <td>0.144361</td>\n      <td>-0.050611</td>\n      <td>0.421962</td>\n      <td>0.117088</td>\n      <td>0.146283</td>\n      <td>-0.136847</td>\n      <td>0.257327</td>\n      <td>0.109962</td>\n      <td>-0.036141</td>\n      <td>-0.100092</td>\n      <td>0.454077</td>\n      <td>-0.019662</td>\n      <td>0.073650</td>\n      <td>-0.050929</td>\n      <td>0.004876</td>\n      <td>0.077912</td>\n      <td>-0.092465</td>\n      <td>0.008673</td>\n      <td>0.043924</td>\n      <td>-0.102756</td>\n      <td>-0.011966</td>\n      <td>1.478923</td>\n      <td>0.141116</td>\n      <td>-0.104184</td>\n      <td>-0.212622</td>\n      <td>0.308052</td>\n      <td>0.027975</td>\n      <td>-0.096544</td>\n      <td>-0.030955</td>\n      <td>0.156021</td>\n      <td>0.030313</td>\n      <td>0.225954</td>\n      <td>-0.095054</td>\n      <td>-0.028980</td>\n      <td>0.018185</td>\n      <td>0.025814</td>\n      <td>-0.195067</td>\n      <td>-0.167623</td>\n      <td>0.084762</td>\n      <td>11.718389</td>\n      <td>-0.038374</td>\n      <td>-0.011906</td>\n      <td>-0.001632</td>\n      <td>-0.224642</td>\n      <td>-0.079878</td>\n      <td>-0.099328</td>\n      <td>0.124719</td>\n      <td>0.179040</td>\n      <td>0.142606</td>\n      <td>-0.036361</td>\n      <td>-0.080203</td>\n      <td>0.120659</td>\n      <td>-0.049413</td>\n      <td>0.000616</td>\n      <td>0.114762</td>\n      <td>0.136759</td>\n      <td>0.128589</td>\n      <td>0.083282</td>\n      <td>-0.119280</td>\n      <td>0.175465</td>\n      <td>0.175211</td>\n      <td>0.079239</td>\n      <td>0.902776</td>\n      <td>-0.046504</td>\n      <td>0.255126</td>\n      <td>-0.002165</td>\n      <td>-0.024777</td>\n      <td>0.089408</td>\n      <td>0.198304</td>\n      <td>0.016981</td>\n      <td>0.230631</td>\n      <td>0.072909</td>\n      <td>-0.130362</td>\n      <td>-0.013177</td>\n      <td>-0.092629</td>\n      <td>-0.196001</td>\n      <td>0.119407</td>\n      <td>0.157684</td>\n      <td>-0.082814</td>\n      <td>0.007755</td>\n      <td>-0.103149</td>\n      <td>0.284048</td>\n      <td>-0.280611</td>\n      <td>0.074010</td>\n      <td>-0.182096</td>\n      <td>0.179465</td>\n      <td>0.221553</td>\n      <td>0.046697</td>\n      <td>-0.015358</td>\n      <td>0.039650</td>\n      <td>0.081511</td>\n      <td>0.152500</td>\n      <td>-0.169331</td>\n      <td>0.018030</td>\n      <td>-0.237969</td>\n      <td>0.178311</td>\n      <td>-0.111561</td>\n      <td>0.018086</td>\n      <td>0.100239</td>\n      <td>-0.067606</td>\n      <td>-0.186209</td>\n      <td>0.084413</td>\n      <td>0.027594</td>\n      <td>-0.156649</td>\n      <td>0.218658</td>\n      <td>0.102454</td>\n      <td>-0.092495</td>\n      <td>-0.029048</td>\n      <td>0.020467</td>\n      <td>0.097208</td>\n      <td>0.068601</td>\n      <td>0.086373</td>\n      <td>-0.188948</td>\n      <td>-0.147831</td>\n      <td>-0.122629</td>\n      <td>-0.531026</td>\n      <td>-0.195737</td>\n      <td>-0.144605</td>\n      <td>-0.535961</td>\n      <td>0.136488</td>\n      <td>-0.121504</td>\n      <td>-0.160985</td>\n      <td>-0.042444</td>\n      <td>-0.374259</td>\n      <td>0.128434</td>\n      <td>-0.260724</td>\n      <td>0.105819</td>\n      <td>-0.173876</td>\n      <td>0.085959</td>\n      <td>-0.007114</td>\n      <td>0.041356</td>\n      <td>-0.002045</td>\n      <td>-0.151803</td>\n      <td>-0.026553</td>\n      <td>-0.087048</td>\n      <td>0.191028</td>\n      <td>-0.033562</td>\n      <td>0.107551</td>\n      <td>-0.343149</td>\n      <td>-0.405529</td>\n      <td>-0.163232</td>\n      <td>-0.044187</td>\n      <td>0.012854</td>\n      <td>-0.204581</td>\n      <td>0.237232</td>\n      <td>0.145715</td>\n      <td>-0.125335</td>\n      <td>0.063213</td>\n      <td>0.022032</td>\n      <td>-0.027339</td>\n      <td>0.020471</td>\n      <td>-0.217706</td>\n      <td>-0.001401</td>\n      <td>0.026243</td>\n      <td>-0.193529</td>\n      <td>0.096699</td>\n      <td>0.059963</td>\n      <td>0.140285</td>\n      <td>0.135448</td>\n      <td>0.055009</td>\n      <td>0.066489</td>\n      <td>-0.012882</td>\n      <td>-0.105614</td>\n      <td>0.000890</td>\n      <td>0.034355</td>\n      <td>-0.243062</td>\n      <td>-0.077763</td>\n      <td>-0.166024</td>\n      <td>-0.111253</td>\n      <td>-0.237102</td>\n      <td>-0.003095</td>\n      <td>0.293362</td>\n      <td>0.116610</td>\n      <td>0.061845</td>\n      <td>-0.008980</td>\n      <td>-0.138449</td>\n      <td>0.131612</td>\n      <td>0.244518</td>\n      <td>0.182840</td>\n      <td>-0.164329</td>\n      <td>0.032314</td>\n      <td>-0.118185</td>\n      <td>-0.233659</td>\n      <td>-0.006937</td>\n      <td>-0.106601</td>\n      <td>0.157456</td>\n      <td>0.394210</td>\n      <td>0.110254</td>\n      <td>0.207392</td>\n      <td>-0.067668</td>\n      <td>-0.081145</td>\n      <td>-0.097108</td>\n      <td>-0.460185</td>\n      <td>-0.036846</td>\n      <td>-0.012913</td>\n      <td>-0.114179</td>\n      <td>0.144275</td>\n      <td>0.079694</td>\n      <td>-0.229887</td>\n      <td>0.509667</td>\n      <td>-0.542613</td>\n      <td>-0.205718</td>\n      <td>-0.047161</td>\n      <td>-0.217698</td>\n      <td>0.119367</td>\n      <td>0.281846</td>\n      <td>-0.062160</td>\n      <td>0.066264</td>\n      <td>0.083418</td>\n      <td>-0.142987</td>\n      <td>-0.000292</td>\n      <td>-0.025965</td>\n      <td>0.085525</td>\n      <td>0.040288</td>\n      <td>0.167405</td>\n      <td>0.262107</td>\n      <td>0.174512</td>\n      <td>-0.008561</td>\n      <td>-0.022450</td>\n      <td>4.429161</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>hep-ph/0103252</td>\n      <td>Tommy Ohlsson</td>\n      <td>Tommy Ohlsson, Hakan Snellman</td>\n      <td>Neutrino oscillations with three flavors in ma...</td>\n      <td>13 pages, 8 figures, RevTeX. Final version to ...</td>\n      <td>Eur.Phys.J.C20:507-515,2001</td>\n      <td>10.1007/s100520100687</td>\n      <td>TUM-HEP-405/01</td>\n      <td>hep-ph</td>\n      <td>None</td>\n      <td>In this paper, we discuss the evolution oper...</td>\n      <td>[{'version': 'v1', 'created': 'Fri, 23 Mar 200...</td>\n      <td>[[Ohlsson, Tommy, ], [Snellman, Hakan, ]]</td>\n      <td>2.995732</td>\n      <td>2.708050</td>\n      <td>10.1007</td>\n      <td>Springer-Verlag</td>\n      <td>7.909857</td>\n      <td>4329354.0</td>\n      <td>2010-05-28</td>\n      <td>2001-03-23 13:55:22+00:00</td>\n      <td>2001-05-04 14:16:14+00:00</td>\n      <td>2010</td>\n      <td>2001</td>\n      <td>2001</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>201005</td>\n      <td>200103</td>\n      <td>200105</td>\n      <td>28</td>\n      <td>23</td>\n      <td>4</td>\n      <td>1.275005e+09</td>\n      <td>9.853557e+08</td>\n      <td>9.889858e+08</td>\n      <td>289649078</td>\n      <td>3630052.0</td>\n      <td>2</td>\n      <td>14757</td>\n      <td>11404</td>\n      <td>11446</td>\n      <td>42</td>\n      <td>21.000000</td>\n      <td>Ohlsson</td>\n      <td>2</td>\n      <td>hep-ph</td>\n      <td>hep-ph</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>hep</td>\n      <td>hep-ph</td>\n      <td>hep-ph</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>179924</td>\n      <td>10</td>\n      <td>74502</td>\n      <td>440</td>\n      <td>8</td>\n      <td>20</td>\n      <td>20</td>\n      <td>2948</td>\n      <td>2330</td>\n      <td>17403</td>\n      <td>2.145841</td>\n      <td>37</td>\n      <td>79.396107</td>\n      <td>0.000000</td>\n      <td>4.905275</td>\n      <td>2.397895</td>\n      <td>1.270819</td>\n      <td>0.000000</td>\n      <td>1.098612</td>\n      <td>3.178054</td>\n      <td>1.707669</td>\n      <td>69644</td>\n      <td>118928.891167</td>\n      <td>0.0</td>\n      <td>7.730175</td>\n      <td>1.609438</td>\n      <td>1.254878</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>2.564949</td>\n      <td>1.746283</td>\n      <td>85372</td>\n      <td>149083.676440</td>\n      <td>0.0</td>\n      <td>7.730175</td>\n      <td>1.791759</td>\n      <td>1.249781</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>2.639057</td>\n      <td>2.418981</td>\n      <td>43</td>\n      <td>104.016187</td>\n      <td>0.000000</td>\n      <td>4.905275</td>\n      <td>2.772589</td>\n      <td>1.196306</td>\n      <td>0.693147</td>\n      <td>1.868835</td>\n      <td>3.178054</td>\n      <td>2.728472</td>\n      <td>2548</td>\n      <td>6952.146967</td>\n      <td>0.0</td>\n      <td>7.510978</td>\n      <td>2.772589</td>\n      <td>1.313177</td>\n      <td>0.693147</td>\n      <td>1.945910</td>\n      <td>3.637586</td>\n      <td>2.707929</td>\n      <td>1941</td>\n      <td>5256.090936</td>\n      <td>0.0</td>\n      <td>7.250636</td>\n      <td>2.772589</td>\n      <td>1.438365</td>\n      <td>0.693147</td>\n      <td>1.791759</td>\n      <td>3.663562</td>\n      <td>2.647246</td>\n      <td>290654</td>\n      <td>7.694327e+05</td>\n      <td>0.0</td>\n      <td>10.133567</td>\n      <td>2.70805</td>\n      <td>1.405163</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.610918</td>\n      <td>2.162204</td>\n      <td>82437</td>\n      <td>178245.632013</td>\n      <td>0.0</td>\n      <td>8.479076</td>\n      <td>2.197225</td>\n      <td>1.360693</td>\n      <td>0.000000</td>\n      <td>1.098612</td>\n      <td>3.135494</td>\n      <td>2.162204</td>\n      <td>82437</td>\n      <td>178245.632013</td>\n      <td>0.0</td>\n      <td>8.479076</td>\n      <td>2.197225</td>\n      <td>1.360693</td>\n      <td>0.000000</td>\n      <td>1.098612</td>\n      <td>3.135494</td>\n      <td>2.063331</td>\n      <td>51453</td>\n      <td>106164.589126</td>\n      <td>0.0</td>\n      <td>8.421563</td>\n      <td>2.079442</td>\n      <td>1.354224</td>\n      <td>0.000000</td>\n      <td>1.098612</td>\n      <td>3.044522</td>\n      <td>2.040760</td>\n      <td>107865</td>\n      <td>220126.526402</td>\n      <td>0.0</td>\n      <td>8.520787</td>\n      <td>2.079442</td>\n      <td>1.330313</td>\n      <td>0.000000</td>\n      <td>1.098612</td>\n      <td>2.995732</td>\n      <td>2.063331</td>\n      <td>51453</td>\n      <td>106164.589126</td>\n      <td>0.0</td>\n      <td>8.421563</td>\n      <td>2.079442</td>\n      <td>1.354224</td>\n      <td>0.000000</td>\n      <td>1.098612</td>\n      <td>3.044522</td>\n      <td>0.576751</td>\n      <td>1.168691</td>\n      <td>1.288063</td>\n      <td>1.475709</td>\n      <td>0.849892</td>\n      <td>1.270164</td>\n      <td>1.249449</td>\n      <td>1.454960</td>\n      <td>0.267260</td>\n      <td>1.071681</td>\n      <td>0.287803</td>\n      <td>1.077618</td>\n      <td>0.348486</td>\n      <td>1.095548</td>\n      <td>0.833528</td>\n      <td>1.263591</td>\n      <td>0.833528</td>\n      <td>1.263591</td>\n      <td>0.932401</td>\n      <td>1.304375</td>\n      <td>0.954973</td>\n      <td>1.314057</td>\n      <td>0.932401</td>\n      <td>1.304375</td>\n      <td>0.711312</td>\n      <td>1.262703</td>\n      <td>0.273140</td>\n      <td>1.086826</td>\n      <td>0.672698</td>\n      <td>1.244949</td>\n      <td>-0.309491</td>\n      <td>0.916993</td>\n      <td>-0.288948</td>\n      <td>0.922073</td>\n      <td>-0.228265</td>\n      <td>0.937414</td>\n      <td>0.256777</td>\n      <td>1.081202</td>\n      <td>0.256777</td>\n      <td>1.081202</td>\n      <td>0.355650</td>\n      <td>1.116099</td>\n      <td>0.378222</td>\n      <td>1.124384</td>\n      <td>0.355650</td>\n      <td>1.116099</td>\n      <td>-0.438172</td>\n      <td>0.860714</td>\n      <td>-0.038614</td>\n      <td>0.985939</td>\n      <td>-1.020803</td>\n      <td>0.726214</td>\n      <td>-1.000261</td>\n      <td>0.730237</td>\n      <td>-0.939577</td>\n      <td>0.742387</td>\n      <td>-0.454535</td>\n      <td>0.856260</td>\n      <td>-0.454535</td>\n      <td>0.856260</td>\n      <td>-0.355662</td>\n      <td>0.883897</td>\n      <td>-0.333091</td>\n      <td>0.890458</td>\n      <td>-0.355662</td>\n      <td>0.883897</td>\n      <td>0.399558</td>\n      <td>1.145490</td>\n      <td>-0.582631</td>\n      <td>0.843735</td>\n      <td>-0.562089</td>\n      <td>0.848409</td>\n      <td>-0.501406</td>\n      <td>0.862525</td>\n      <td>-0.016364</td>\n      <td>0.994825</td>\n      <td>-0.016364</td>\n      <td>0.994825</td>\n      <td>0.082509</td>\n      <td>1.026935</td>\n      <td>0.105081</td>\n      <td>1.034558</td>\n      <td>0.082509</td>\n      <td>1.026935</td>\n      <td>-0.982189</td>\n      <td>0.736571</td>\n      <td>-0.961646</td>\n      <td>0.740651</td>\n      <td>-0.900963</td>\n      <td>0.752974</td>\n      <td>-0.415921</td>\n      <td>0.868471</td>\n      <td>-0.415921</td>\n      <td>0.868471</td>\n      <td>-0.317048</td>\n      <td>0.896502</td>\n      <td>-0.294476</td>\n      <td>0.903157</td>\n      <td>-0.317048</td>\n      <td>0.896502</td>\n      <td>0.020543</td>\n      <td>1.005540</td>\n      <td>0.081226</td>\n      <td>1.022270</td>\n      <td>0.566268</td>\n      <td>1.179074</td>\n      <td>0.566268</td>\n      <td>1.179074</td>\n      <td>0.665141</td>\n      <td>1.217130</td>\n      <td>0.687713</td>\n      <td>1.226165</td>\n      <td>0.665141</td>\n      <td>1.217130</td>\n      <td>0.060683</td>\n      <td>1.016638</td>\n      <td>0.545725</td>\n      <td>1.172577</td>\n      <td>0.545725</td>\n      <td>1.172577</td>\n      <td>0.644598</td>\n      <td>1.210424</td>\n      <td>0.667170</td>\n      <td>1.219409</td>\n      <td>0.644598</td>\n      <td>1.210424</td>\n      <td>0.485042</td>\n      <td>1.153387</td>\n      <td>0.485042</td>\n      <td>1.153387</td>\n      <td>0.583915</td>\n      <td>1.190614</td>\n      <td>0.606487</td>\n      <td>1.199452</td>\n      <td>0.583915</td>\n      <td>1.190614</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.098873</td>\n      <td>1.032276</td>\n      <td>0.121445</td>\n      <td>1.039939</td>\n      <td>0.098873</td>\n      <td>1.032276</td>\n      <td>0.098873</td>\n      <td>1.032276</td>\n      <td>0.121445</td>\n      <td>1.039939</td>\n      <td>0.098873</td>\n      <td>1.032276</td>\n      <td>0.022572</td>\n      <td>1.007423</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.022572</td>\n      <td>0.992632</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.045024</td>\n      <td>0.061441</td>\n      <td>0.026114</td>\n      <td>0.186450</td>\n      <td>0.307958</td>\n      <td>0.072466</td>\n      <td>0.257011</td>\n      <td>0.268695</td>\n      <td>-0.061639</td>\n      <td>-0.094779</td>\n      <td>-0.121741</td>\n      <td>-0.250290</td>\n      <td>0.009397</td>\n      <td>-0.171356</td>\n      <td>0.094235</td>\n      <td>0.088163</td>\n      <td>0.067743</td>\n      <td>0.185107</td>\n      <td>-0.061381</td>\n      <td>0.002100</td>\n      <td>-0.185709</td>\n      <td>0.153642</td>\n      <td>-0.202368</td>\n      <td>-0.085331</td>\n      <td>0.208823</td>\n      <td>0.157383</td>\n      <td>0.015314</td>\n      <td>0.177820</td>\n      <td>-0.241283</td>\n      <td>0.041038</td>\n      <td>0.094112</td>\n      <td>0.104222</td>\n      <td>0.013423</td>\n      <td>-0.014741</td>\n      <td>-0.061360</td>\n      <td>-0.059740</td>\n      <td>-0.093536</td>\n      <td>0.040848</td>\n      <td>-0.202473</td>\n      <td>0.025084</td>\n      <td>-0.203459</td>\n      <td>0.682886</td>\n      <td>0.079395</td>\n      <td>-0.135364</td>\n      <td>0.007933</td>\n      <td>-0.294504</td>\n      <td>0.030884</td>\n      <td>0.089232</td>\n      <td>-0.133020</td>\n      <td>0.212464</td>\n      <td>-0.078407</td>\n      <td>-0.028086</td>\n      <td>-0.133585</td>\n      <td>0.022868</td>\n      <td>0.065664</td>\n      <td>-0.041062</td>\n      <td>0.066463</td>\n      <td>0.231022</td>\n      <td>0.091057</td>\n      <td>0.108240</td>\n      <td>-0.002117</td>\n      <td>-0.076744</td>\n      <td>0.075257</td>\n      <td>0.284456</td>\n      <td>-0.055942</td>\n      <td>-0.030060</td>\n      <td>0.067542</td>\n      <td>0.255630</td>\n      <td>-0.089150</td>\n      <td>-0.129420</td>\n      <td>0.024403</td>\n      <td>-0.132183</td>\n      <td>0.009920</td>\n      <td>0.183269</td>\n      <td>0.181521</td>\n      <td>0.086513</td>\n      <td>0.028355</td>\n      <td>-1.615930</td>\n      <td>0.053578</td>\n      <td>0.018184</td>\n      <td>-0.019003</td>\n      <td>0.063449</td>\n      <td>0.743330</td>\n      <td>0.188787</td>\n      <td>0.200535</td>\n      <td>0.015952</td>\n      <td>0.006422</td>\n      <td>0.062562</td>\n      <td>0.046106</td>\n      <td>0.124889</td>\n      <td>-0.079480</td>\n      <td>0.002685</td>\n      <td>0.075083</td>\n      <td>0.010744</td>\n      <td>0.105034</td>\n      <td>0.040061</td>\n      <td>0.111879</td>\n      <td>-0.977696</td>\n      <td>-0.022940</td>\n      <td>0.056089</td>\n      <td>0.138631</td>\n      <td>-0.157961</td>\n      <td>0.105337</td>\n      <td>-0.055579</td>\n      <td>-0.115602</td>\n      <td>0.285886</td>\n      <td>0.045770</td>\n      <td>0.106218</td>\n      <td>-0.065913</td>\n      <td>-0.167793</td>\n      <td>0.023226</td>\n      <td>0.022350</td>\n      <td>-0.047034</td>\n      <td>0.229539</td>\n      <td>0.035684</td>\n      <td>-0.092818</td>\n      <td>0.221824</td>\n      <td>0.156967</td>\n      <td>-0.169832</td>\n      <td>0.327100</td>\n      <td>0.018974</td>\n      <td>0.088442</td>\n      <td>-0.020907</td>\n      <td>0.093161</td>\n      <td>0.274468</td>\n      <td>0.168415</td>\n      <td>-0.037457</td>\n      <td>0.175039</td>\n      <td>-0.142163</td>\n      <td>0.243652</td>\n      <td>0.014862</td>\n      <td>-0.555045</td>\n      <td>-0.209317</td>\n      <td>-0.178369</td>\n      <td>-0.164118</td>\n      <td>0.051424</td>\n      <td>-0.082794</td>\n      <td>0.191170</td>\n      <td>-0.029072</td>\n      <td>-0.070847</td>\n      <td>0.096028</td>\n      <td>0.228004</td>\n      <td>-0.033884</td>\n      <td>-0.169825</td>\n      <td>0.137567</td>\n      <td>0.060474</td>\n      <td>-0.040224</td>\n      <td>-0.217791</td>\n      <td>0.022308</td>\n      <td>0.112378</td>\n      <td>0.103478</td>\n      <td>0.103084</td>\n      <td>-0.190879</td>\n      <td>-0.231992</td>\n      <td>0.096861</td>\n      <td>0.480173</td>\n      <td>-0.007636</td>\n      <td>-0.238974</td>\n      <td>-0.039465</td>\n      <td>0.191938</td>\n      <td>-0.051125</td>\n      <td>-0.074155</td>\n      <td>0.140969</td>\n      <td>-0.006411</td>\n      <td>0.176998</td>\n      <td>-0.057825</td>\n      <td>-0.125296</td>\n      <td>0.150800</td>\n      <td>0.127588</td>\n      <td>0.067897</td>\n      <td>-0.174030</td>\n      <td>0.017394</td>\n      <td>0.159570</td>\n      <td>0.147627</td>\n      <td>-0.074560</td>\n      <td>-0.031528</td>\n      <td>-0.117566</td>\n      <td>-0.025238</td>\n      <td>-0.107860</td>\n      <td>-0.012475</td>\n      <td>-0.104021</td>\n      <td>-0.033846</td>\n      <td>0.017542</td>\n      <td>-0.159165</td>\n      <td>0.066054</td>\n      <td>0.018775</td>\n      <td>-0.129476</td>\n      <td>0.088261</td>\n      <td>-0.091472</td>\n      <td>-0.106108</td>\n      <td>0.033383</td>\n      <td>-0.059297</td>\n      <td>-0.149944</td>\n      <td>-0.225755</td>\n      <td>-0.040606</td>\n      <td>-0.255070</td>\n      <td>0.002444</td>\n      <td>0.015893</td>\n      <td>0.263539</td>\n      <td>-0.037503</td>\n      <td>0.085573</td>\n      <td>-0.341842</td>\n      <td>0.117793</td>\n      <td>0.108258</td>\n      <td>0.339525</td>\n      <td>-0.068815</td>\n      <td>-0.333756</td>\n      <td>0.159553</td>\n      <td>0.021009</td>\n      <td>-0.091770</td>\n      <td>0.019718</td>\n      <td>0.084636</td>\n      <td>-0.292808</td>\n      <td>0.024914</td>\n      <td>-0.091420</td>\n      <td>0.053642</td>\n      <td>0.082632</td>\n      <td>-0.439348</td>\n      <td>-0.191939</td>\n      <td>-0.403662</td>\n      <td>0.101641</td>\n      <td>0.094936</td>\n      <td>-0.089206</td>\n      <td>-0.114034</td>\n      <td>-0.169870</td>\n      <td>0.130361</td>\n      <td>0.037054</td>\n      <td>-0.010573</td>\n      <td>0.186188</td>\n      <td>-0.106118</td>\n      <td>0.006441</td>\n      <td>0.393563</td>\n      <td>0.241401</td>\n      <td>0.142665</td>\n      <td>0.118523</td>\n      <td>-0.175827</td>\n      <td>0.054609</td>\n      <td>0.018519</td>\n      <td>-0.108494</td>\n      <td>-0.214344</td>\n      <td>-0.456074</td>\n      <td>0.061460</td>\n      <td>0.057831</td>\n      <td>0.066821</td>\n      <td>-0.116205</td>\n      <td>0.153234</td>\n      <td>-0.086866</td>\n      <td>-0.012465</td>\n      <td>0.003945</td>\n      <td>-0.012793</td>\n      <td>0.024195</td>\n      <td>0.139661</td>\n      <td>-0.016817</td>\n      <td>0.034005</td>\n      <td>0.274794</td>\n      <td>-0.135067</td>\n      <td>-0.052155</td>\n      <td>-0.058958</td>\n      <td>-0.287165</td>\n      <td>0.617193</td>\n      <td>-0.129067</td>\n      <td>-0.179277</td>\n      <td>0.089431</td>\n      <td>0.050735</td>\n      <td>-0.007037</td>\n      <td>-0.317692</td>\n      <td>-0.169178</td>\n      <td>-0.068490</td>\n      <td>0.101219</td>\n      <td>0.009539</td>\n      <td>-0.107717</td>\n      <td>0.026075</td>\n      <td>0.086839</td>\n      <td>-0.047483</td>\n      <td>-0.136969</td>\n      <td>0.334363</td>\n      <td>0.318387</td>\n      <td>-0.046161</td>\n      <td>-0.106427</td>\n      <td>0.008149</td>\n      <td>0.325522</td>\n      <td>0.050240</td>\n      <td>0.056993</td>\n      <td>0.093178</td>\n      <td>-0.104203</td>\n      <td>0.007165</td>\n      <td>0.109145</td>\n      <td>-0.136339</td>\n      <td>-0.118205</td>\n      <td>0.202302</td>\n      <td>-0.060391</td>\n      <td>-0.194238</td>\n      <td>-0.057531</td>\n      <td>0.073516</td>\n      <td>0.095490</td>\n      <td>-0.006624</td>\n      <td>0.187697</td>\n      <td>-0.016216</td>\n      <td>-0.026481</td>\n      <td>-0.066832</td>\n      <td>0.036074</td>\n      <td>-0.071059</td>\n      <td>-0.020087</td>\n      <td>0.007296</td>\n      <td>-0.039265</td>\n      <td>0.145286</td>\n      <td>-0.287755</td>\n      <td>0.020310</td>\n      <td>0.132173</td>\n      <td>0.017299</td>\n      <td>-0.163920</td>\n      <td>0.018398</td>\n      <td>-0.051605</td>\n      <td>-0.025069</td>\n      <td>-0.092841</td>\n      <td>0.270640</td>\n      <td>-0.188679</td>\n      <td>0.062747</td>\n      <td>-0.007382</td>\n      <td>-0.068917</td>\n      <td>-0.196340</td>\n      <td>-0.014721</td>\n      <td>-0.193743</td>\n      <td>-0.104232</td>\n      <td>-0.219494</td>\n      <td>0.128811</td>\n      <td>-0.148385</td>\n      <td>0.093468</td>\n      <td>0.203894</td>\n      <td>0.202032</td>\n      <td>0.623950</td>\n      <td>0.543432</td>\n      <td>0.165606</td>\n      <td>-0.006492</td>\n      <td>0.246928</td>\n      <td>-0.007816</td>\n      <td>0.061278</td>\n      <td>0.083360</td>\n      <td>0.166092</td>\n      <td>0.119673</td>\n      <td>-0.104286</td>\n      <td>-0.087636</td>\n      <td>0.106660</td>\n      <td>0.010581</td>\n      <td>0.121943</td>\n      <td>0.002054</td>\n      <td>-0.032377</td>\n      <td>0.255948</td>\n      <td>0.312340</td>\n      <td>-0.340801</td>\n      <td>-0.229477</td>\n      <td>-0.089041</td>\n      <td>-0.092818</td>\n      <td>-0.233366</td>\n      <td>-0.083138</td>\n      <td>0.132514</td>\n      <td>-0.276736</td>\n      <td>0.018394</td>\n      <td>-0.222276</td>\n      <td>0.052933</td>\n      <td>-0.076984</td>\n      <td>0.201275</td>\n      <td>0.034494</td>\n      <td>0.026687</td>\n      <td>0.179343</td>\n      <td>-0.316101</td>\n      <td>-0.171431</td>\n      <td>-0.055398</td>\n      <td>0.100052</td>\n      <td>0.002029</td>\n      <td>0.021735</td>\n      <td>0.134354</td>\n      <td>-0.062151</td>\n      <td>0.116655</td>\n      <td>0.056437</td>\n      <td>-0.156781</td>\n      <td>0.131438</td>\n      <td>-0.122826</td>\n      <td>-0.012687</td>\n      <td>-0.153846</td>\n      <td>-0.012327</td>\n      <td>0.148026</td>\n      <td>-0.099284</td>\n      <td>-0.004246</td>\n      <td>0.327164</td>\n      <td>-0.133463</td>\n      <td>0.190452</td>\n      <td>0.223688</td>\n      <td>-0.069656</td>\n      <td>0.040422</td>\n      <td>-0.026260</td>\n      <td>-0.125241</td>\n      <td>0.027271</td>\n      <td>-0.011622</td>\n      <td>-0.010560</td>\n      <td>-0.049907</td>\n      <td>0.017461</td>\n      <td>-0.552175</td>\n      <td>0.026533</td>\n      <td>-0.036820</td>\n      <td>-0.000572</td>\n      <td>0.167837</td>\n      <td>0.068831</td>\n      <td>-0.044935</td>\n      <td>-0.014204</td>\n      <td>0.098796</td>\n      <td>-0.086490</td>\n      <td>-0.107535</td>\n      <td>-0.015403</td>\n      <td>0.194117</td>\n      <td>0.076873</td>\n      <td>0.127647</td>\n      <td>0.002150</td>\n      <td>-0.032479</td>\n      <td>0.042913</td>\n      <td>0.024105</td>\n      <td>-0.060284</td>\n      <td>-0.139178</td>\n      <td>-0.323357</td>\n      <td>-0.080555</td>\n      <td>-0.101109</td>\n      <td>-0.111122</td>\n      <td>0.082361</td>\n      <td>-0.090455</td>\n      <td>-0.517197</td>\n      <td>-0.067904</td>\n      <td>0.092758</td>\n      <td>-0.099924</td>\n      <td>-0.086709</td>\n      <td>0.199176</td>\n      <td>0.141999</td>\n      <td>0.092574</td>\n      <td>-0.120088</td>\n      <td>0.046302</td>\n      <td>-0.116925</td>\n      <td>-0.097921</td>\n      <td>0.037665</td>\n      <td>-0.058858</td>\n      <td>-0.050301</td>\n      <td>-0.093208</td>\n      <td>-0.452632</td>\n      <td>-0.104972</td>\n      <td>-0.097232</td>\n      <td>-0.044161</td>\n      <td>-0.160200</td>\n      <td>-0.096984</td>\n      <td>0.183310</td>\n      <td>-0.075674</td>\n      <td>-0.174416</td>\n      <td>-0.002953</td>\n      <td>-0.125316</td>\n      <td>0.022790</td>\n      <td>-0.325339</td>\n      <td>-1.249750</td>\n      <td>0.219210</td>\n      <td>0.112360</td>\n      <td>-0.027278</td>\n      <td>-0.026580</td>\n      <td>0.037978</td>\n      <td>-0.131886</td>\n      <td>0.285838</td>\n      <td>0.101624</td>\n      <td>0.100467</td>\n      <td>-0.274340</td>\n      <td>0.110626</td>\n      <td>-0.060746</td>\n      <td>0.268230</td>\n      <td>0.014230</td>\n      <td>-0.085340</td>\n      <td>-0.004362</td>\n      <td>0.237103</td>\n      <td>-0.127168</td>\n      <td>-0.253778</td>\n      <td>-0.125134</td>\n      <td>-0.144491</td>\n      <td>-0.060425</td>\n      <td>-0.220046</td>\n      <td>0.189528</td>\n      <td>0.214894</td>\n      <td>-0.068495</td>\n      <td>-0.009231</td>\n      <td>0.054502</td>\n      <td>0.090229</td>\n      <td>0.058186</td>\n      <td>0.016010</td>\n      <td>-0.028923</td>\n      <td>0.033826</td>\n      <td>0.007451</td>\n      <td>0.216144</td>\n      <td>-0.181054</td>\n      <td>-0.078201</td>\n      <td>-0.105774</td>\n      <td>0.225342</td>\n      <td>0.156310</td>\n      <td>0.191633</td>\n      <td>0.140339</td>\n      <td>0.310926</td>\n      <td>-0.164473</td>\n      <td>-0.166424</td>\n      <td>0.105870</td>\n      <td>0.122402</td>\n      <td>0.091046</td>\n      <td>-0.012213</td>\n      <td>0.057316</td>\n      <td>-0.016780</td>\n      <td>-0.070268</td>\n      <td>-0.214911</td>\n      <td>0.048223</td>\n      <td>0.114932</td>\n      <td>-0.035823</td>\n      <td>-0.263190</td>\n      <td>-0.165005</td>\n      <td>0.004527</td>\n      <td>0.098826</td>\n      <td>-0.015102</td>\n      <td>0.064898</td>\n      <td>0.020789</td>\n      <td>-0.102969</td>\n      <td>0.155976</td>\n      <td>0.334598</td>\n      <td>0.052263</td>\n      <td>0.014203</td>\n      <td>0.065645</td>\n      <td>0.021419</td>\n      <td>0.070152</td>\n      <td>0.060167</td>\n      <td>-0.075937</td>\n      <td>0.015064</td>\n      <td>0.033468</td>\n      <td>-0.011410</td>\n      <td>0.168035</td>\n      <td>-0.125233</td>\n      <td>0.246304</td>\n      <td>-0.048883</td>\n      <td>-0.210810</td>\n      <td>-0.090323</td>\n      <td>0.038766</td>\n      <td>-0.120851</td>\n      <td>0.086392</td>\n      <td>-0.097071</td>\n      <td>0.058819</td>\n      <td>0.102806</td>\n      <td>0.069638</td>\n      <td>0.018521</td>\n      <td>0.013644</td>\n      <td>-0.117160</td>\n      <td>0.269430</td>\n      <td>0.057706</td>\n      <td>0.085797</td>\n      <td>0.251591</td>\n      <td>-0.049474</td>\n      <td>0.498698</td>\n      <td>0.142232</td>\n      <td>0.112757</td>\n      <td>-0.088355</td>\n      <td>0.217441</td>\n      <td>0.224559</td>\n      <td>0.044062</td>\n      <td>-0.157754</td>\n      <td>0.462205</td>\n      <td>0.071578</td>\n      <td>0.045937</td>\n      <td>-0.017116</td>\n      <td>0.034172</td>\n      <td>0.005457</td>\n      <td>-0.087265</td>\n      <td>-0.045315</td>\n      <td>0.067920</td>\n      <td>-0.030377</td>\n      <td>-0.006359</td>\n      <td>1.455204</td>\n      <td>0.188507</td>\n      <td>-0.205016</td>\n      <td>-0.068080</td>\n      <td>0.292195</td>\n      <td>0.041927</td>\n      <td>-0.174890</td>\n      <td>-0.046047</td>\n      <td>0.206588</td>\n      <td>0.064226</td>\n      <td>0.367615</td>\n      <td>-0.070862</td>\n      <td>0.009342</td>\n      <td>0.090434</td>\n      <td>0.001255</td>\n      <td>-0.047360</td>\n      <td>-0.103646</td>\n      <td>0.108661</td>\n      <td>11.651844</td>\n      <td>-0.063221</td>\n      <td>-0.013029</td>\n      <td>0.032770</td>\n      <td>-0.256653</td>\n      <td>0.004424</td>\n      <td>-0.158540</td>\n      <td>0.057062</td>\n      <td>0.279952</td>\n      <td>0.185350</td>\n      <td>-0.026612</td>\n      <td>-0.073181</td>\n      <td>0.144206</td>\n      <td>-0.092292</td>\n      <td>0.073268</td>\n      <td>0.087679</td>\n      <td>0.119163</td>\n      <td>0.097052</td>\n      <td>0.128711</td>\n      <td>-0.163158</td>\n      <td>0.196666</td>\n      <td>0.096923</td>\n      <td>0.021693</td>\n      <td>0.768567</td>\n      <td>-0.057968</td>\n      <td>0.151456</td>\n      <td>-0.043386</td>\n      <td>-0.004558</td>\n      <td>0.144182</td>\n      <td>0.187076</td>\n      <td>0.072016</td>\n      <td>0.245870</td>\n      <td>0.112004</td>\n      <td>-0.171178</td>\n      <td>0.033259</td>\n      <td>-0.128406</td>\n      <td>-0.077126</td>\n      <td>0.080339</td>\n      <td>0.073449</td>\n      <td>-0.066963</td>\n      <td>0.034276</td>\n      <td>-0.062217</td>\n      <td>0.260491</td>\n      <td>-0.303431</td>\n      <td>0.150829</td>\n      <td>-0.128606</td>\n      <td>0.227643</td>\n      <td>0.210428</td>\n      <td>0.000352</td>\n      <td>0.009954</td>\n      <td>0.003607</td>\n      <td>0.166239</td>\n      <td>0.208197</td>\n      <td>-0.103583</td>\n      <td>-0.023853</td>\n      <td>-0.391357</td>\n      <td>0.242652</td>\n      <td>-0.155245</td>\n      <td>0.061709</td>\n      <td>0.067378</td>\n      <td>-0.110414</td>\n      <td>-0.202818</td>\n      <td>0.078849</td>\n      <td>0.033232</td>\n      <td>-0.106000</td>\n      <td>0.203802</td>\n      <td>0.149935</td>\n      <td>-0.084985</td>\n      <td>0.015760</td>\n      <td>-0.048469</td>\n      <td>0.037743</td>\n      <td>0.096856</td>\n      <td>0.143798</td>\n      <td>-0.136856</td>\n      <td>-0.153232</td>\n      <td>-0.142397</td>\n      <td>-0.389400</td>\n      <td>-0.224074</td>\n      <td>-0.056422</td>\n      <td>-0.552498</td>\n      <td>0.065720</td>\n      <td>-0.176695</td>\n      <td>-0.079383</td>\n      <td>-0.159391</td>\n      <td>-0.431184</td>\n      <td>0.239871</td>\n      <td>-0.236745</td>\n      <td>-0.009576</td>\n      <td>-0.231364</td>\n      <td>0.272019</td>\n      <td>-0.037922</td>\n      <td>0.021827</td>\n      <td>-0.032716</td>\n      <td>-0.068437</td>\n      <td>-0.025033</td>\n      <td>-0.066474</td>\n      <td>0.184263</td>\n      <td>0.019257</td>\n      <td>0.092456</td>\n      <td>-0.381083</td>\n      <td>-0.404728</td>\n      <td>-0.135549</td>\n      <td>-0.092909</td>\n      <td>-0.032019</td>\n      <td>-0.154564</td>\n      <td>0.245694</td>\n      <td>0.074664</td>\n      <td>-0.105019</td>\n      <td>0.138730</td>\n      <td>-0.051127</td>\n      <td>-0.003614</td>\n      <td>-0.033994</td>\n      <td>-0.271795</td>\n      <td>-0.037047</td>\n      <td>0.037355</td>\n      <td>-0.192260</td>\n      <td>0.073082</td>\n      <td>0.057636</td>\n      <td>0.151957</td>\n      <td>0.184729</td>\n      <td>0.029239</td>\n      <td>0.083366</td>\n      <td>-0.029940</td>\n      <td>-0.066711</td>\n      <td>0.070731</td>\n      <td>0.069432</td>\n      <td>-0.227426</td>\n      <td>0.069980</td>\n      <td>-0.165079</td>\n      <td>-0.067444</td>\n      <td>-0.143729</td>\n      <td>0.032275</td>\n      <td>0.372293</td>\n      <td>0.041818</td>\n      <td>0.032854</td>\n      <td>0.146240</td>\n      <td>-0.137018</td>\n      <td>0.190802</td>\n      <td>0.108555</td>\n      <td>0.170401</td>\n      <td>-0.170474</td>\n      <td>0.065800</td>\n      <td>-0.162053</td>\n      <td>-0.241368</td>\n      <td>0.072358</td>\n      <td>-0.170512</td>\n      <td>0.181729</td>\n      <td>0.394393</td>\n      <td>0.065402</td>\n      <td>0.205735</td>\n      <td>-0.103268</td>\n      <td>-0.039133</td>\n      <td>-0.052853</td>\n      <td>-0.421413</td>\n      <td>0.066120</td>\n      <td>-0.036619</td>\n      <td>-0.129683</td>\n      <td>0.152110</td>\n      <td>0.058852</td>\n      <td>-0.200716</td>\n      <td>0.425438</td>\n      <td>-0.633506</td>\n      <td>-0.184883</td>\n      <td>-0.046711</td>\n      <td>-0.277424</td>\n      <td>0.150955</td>\n      <td>0.174308</td>\n      <td>-0.191348</td>\n      <td>0.121687</td>\n      <td>-0.002648</td>\n      <td>-0.149996</td>\n      <td>0.036184</td>\n      <td>-0.095719</td>\n      <td>0.059427</td>\n      <td>0.130815</td>\n      <td>0.155930</td>\n      <td>0.243257</td>\n      <td>0.070662</td>\n      <td>0.079929</td>\n      <td>-0.050331</td>\n      <td>2.182262</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "NFOLDS = 10\n",
    "kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=777)\n",
    "\n",
    "df_train['fold_no'] = [0 for x in range(len(df_train))]\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df_train)):\n",
    "    df_train.loc[val_idx, 'fold_no'] = fold\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_0_cols = ['cs_y', 'econ_y', 'eess_y', 'math_y', 'nlin_y', 'physics_y',\n",
    "       'stat_y', 'acc-phys_y', 'adap-org_y', 'alg-geom_y', 'ao-sci_y',\n",
    "       'astro-ph_y', 'atom-ph_y', 'bayes-an_y', 'chao-dyn_y', 'chem-ph_y',\n",
    "       'cmp-lg_y', 'comp-gas_y', 'cond-mat_y', 'dg-ga_y', 'funct-an_y',\n",
    "       'gr-qc_y', 'math-ph_y', 'mtrl-th_y', 'nucl-th_y', 'patt-sol_y',\n",
    "       'plasm-ph_y', 'q-alg_y', 'q-bio_y', 'q-fin_y', 'quant-ph_y',\n",
    "       'solv-int_y', 'supr-con_y', 'acc_y', 'adap_y', 'alg_y', 'ao_y',\n",
    "       'astro_y', 'atom_y', 'bayes_y', 'chao_y', 'chem_y', 'cmp_y',\n",
    "       'comp_y', 'cond_y', 'cs', 'dg_y', 'econ', 'eess', 'funct_y',\n",
    "       'gr_y', 'math', 'mtrl_y', 'nlin', 'patt_y', 'physics', 'plasm_y',\n",
    "       'quant_y', 'solv_y', 'stat', 'supr_y', 'astro-ph.co',\n",
    "       'astro-ph.ga', 'astro-ph.he', 'astro-ph.im', 'astro-ph.sr',\n",
    "       'cond-mat.dis-nn', 'cond-mat.mes-hall', 'cond-mat.mtrl-sci',\n",
    "       'cond-mat.other', 'cond-mat.soft', 'cond-mat.stat-mech', 'cs.ai',\n",
    "       'cs.ar', 'cs.cc', 'cs.ce', 'cs.cg', 'cs.cl', 'cs.cr', 'cs.cv',\n",
    "       'cs.cy', 'cs.db', 'cs.dc', 'cs.dl', 'cs.dm', 'cs.et', 'cs.fl',\n",
    "       'cs.gl', 'cs.gr', 'cs.gt', 'cs.hc', 'cs.ir', 'cs.it', 'cs.lo',\n",
    "       'cs.ma', 'cs.mm', 'cs.ms', 'cs.na', 'cs.ne', 'cs.ni', 'cs.oh',\n",
    "       'cs.os', 'cs.pf', 'cs.pl', 'cs.ro', 'cs.sc', 'cs.sd', 'cs.se',\n",
    "       'cs.si', 'cs.sy', 'econ.em', 'econ.gn', 'econ.th', 'eess.as',\n",
    "       'eess.iv', 'eess.sp', 'eess.sy', 'math.ac', 'math.ap', 'math.at',\n",
    "       'math.ca', 'math.co', 'math.ct', 'math.cv', 'math.dg', 'math.ds',\n",
    "       'math.fa', 'math.gm', 'math.gn', 'math.gr', 'math.gt', 'math.ho',\n",
    "       'math.it', 'math.kt', 'math.lo', 'math.mg', 'math.mp', 'math.na',\n",
    "       'math.nt', 'math.oa', 'math.oc', 'math.pr', 'math.qa', 'math.ra',\n",
    "       'math.rt', 'math.sg', 'math.sp', 'math.st', 'nlin.ao', 'nlin.cd',\n",
    "       'nlin.cg', 'nlin.ps', 'nlin.si', 'physics.ao-ph', 'physics.app-ph',\n",
    "       'physics.atm-clus', 'physics.bio-ph', 'physics.chem-ph',\n",
    "       'physics.class-ph', 'physics.comp-ph', 'physics.data-an',\n",
    "       'physics.ed-ph', 'physics.flu-dyn', 'physics.gen-ph',\n",
    "       'physics.geo-ph', 'physics.hist-ph', 'physics.ins-det',\n",
    "       'physics.med-ph', 'physics.optics', 'physics.plasm-ph',\n",
    "       'physics.pop-ph', 'q-bio.bm', 'q-bio.cb', 'q-bio.gn', 'q-bio.mn',\n",
    "       'q-bio.nc', 'q-bio.ot', 'q-bio.pe', 'q-bio.qm', 'q-bio.sc',\n",
    "       'q-bio.to', 'q-fin.cp', 'q-fin.ec', 'q-fin.gn', 'q-fin.mf',\n",
    "       'q-fin.pm', 'q-fin.pr', 'q-fin.rm', 'q-fin.st', 'q-fin.tr',\n",
    "       'stat.ap', 'stat.co', 'stat.me', 'stat.ml', 'stat.ot', 'stat.th',\n",
    "       'submitter_label', 'license_label',\n",
    "       'doi_cites_min_author_first_label', 'doi_cites_min_doi_id_label',\n",
    "       'doi_cites_q10_doi_id_label', 'doi_cites_q25_doi_id_label',\n",
    "       'doi_cites_q75_doi_id_label', 'doi_cites_min_pub_publisher_label',\n",
    "       'doi_cites_median_pub_publisher_label',\n",
    "       'doi_cites_q10_pub_publisher_label',\n",
    "       'doi_cites_q25_pub_publisher_label',\n",
    "       'doi_cites_q75_pub_publisher_label', 'doi_cites_min_update_ym',\n",
    "       'doi_cites_min_first_created_ym', 'doi_cites_count_license_label',\n",
    "       'doi_cites_sum_license_label', 'doi_cites_min_license_label',\n",
    "       'doi_cites_median_license_label', 'doi_cites_q10_license_label',\n",
    "       'doi_cites_q25_license_label', 'doi_cites_q75_license_label',\n",
    "       'doi_cites_mean_category_main_label',\n",
    "       'doi_cites_min_category_main_label',\n",
    "       'doi_cites_q10_category_main_label',\n",
    "       'doi_cites_q25_category_main_label',\n",
    "       'doi_cites_mean_category_main_detail_label',\n",
    "       'doi_cites_count_category_main_detail_label',\n",
    "       'doi_cites_sum_category_main_detail_label',\n",
    "       'doi_cites_min_category_main_detail_label',\n",
    "       'doi_cites_median_category_main_detail_label',\n",
    "       'doi_cites_q10_category_main_detail_label',\n",
    "       'doi_cites_q25_category_main_detail_label',\n",
    "       'doi_cites_q75_category_main_detail_label',\n",
    "       'doi_cites_min_category_name_parent_label',\n",
    "       'doi_cites_q10_category_name_parent_label',\n",
    "       'doi_cites_min_category_name_parent_main_label',\n",
    "       'doi_cites_q10_category_name_parent_main_label',\n",
    "       'doi_cites_q25_category_name_parent_main_label',\n",
    "       'doi_cites_min_category_name_label',\n",
    "       'diff_doi_cites_mean_submitter_label_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_doi_cites_mean_doi_id_label_doi_cites_mean_pub_publisher_label',\n",
    "       'diff_rate_doi_cites_mean_author_first_label_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_pub_publisher_label_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_doi_cites_mean_license_label_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_doi_cites_mean_category_main_label_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_label_doi_cites_mean_category_main_detail_label',\n",
    "       'diff_rate_doi_cites_mean_category_main_detail_label_doi_cites_mean_category_name_parent_main_label',\n",
    "       'is_null_comments', 'is_null_journal-ref']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_statics_table(target, key, diff_key, x_train, x_valid, x_test):\n",
    "    target_encoding = pd.concat([\n",
    "        pd.DataFrame(x_train.groupby(key)[target].count()).rename(columns={target:f'te_{key}_{target}_count'}),\n",
    "        pd.DataFrame(x_train.groupby(key)[target].min()).rename(columns={target:f'te_{key}_{target}_min'}),\n",
    "        pd.DataFrame(x_train.groupby(key)[target].max()).rename(columns={target:f'te_{key}_{target}_max'}),\n",
    "        pd.DataFrame(x_train.groupby(key)[target].std()).rename(columns={target:f'te_{key}_{target}_std'}),\n",
    "        pd.DataFrame(x_train.groupby(key)[target].mean()).rename(columns={target:f'te_{key}_{target}_mean'}),\n",
    "        pd.DataFrame(x_train.groupby(key)[target].median()).rename(columns={target:f'te_{key}_{target}_median'}),\n",
    "    ], axis = 1)\n",
    "\n",
    "    x_train = pd.merge(x_train, target_encoding, on=key, how='left')\n",
    "    x_valid = pd.merge(x_valid, target_encoding, on=key, how='left')\n",
    "    x_test = pd.merge(x_test, target_encoding, on=key, how='left')\n",
    "\n",
    "    x_train[f'te_diff_{diff_key}_{key}_{target}_mean'] = x_train[diff_key] - x_train[f'te_{key}_{target}_mean']\n",
    "    x_train[f'te_rate_{diff_key}_{key}_{target}_mean'] = x_train[diff_key] / x_train[f'te_{key}_{target}_mean']\n",
    "    x_valid[f'te_diff_{diff_key}_{key}_{target}_mean'] = x_valid[diff_key] - x_valid[f'te_{key}_{target}_mean']\n",
    "    x_valid[f'te_rate_{diff_key}_{key}_{target}_mean'] = x_valid[diff_key] / x_valid[f'te_{key}_{target}_mean']\n",
    "    x_test[f'te_diff_{diff_key}_{key}_{target}_mean'] = x_test[diff_key] - x_test[f'te_{key}_{target}_mean']\n",
    "    x_test[f'te_rate_{diff_key}_{key}_{target}_mean'] = x_test[diff_key] / x_test[f'te_{key}_{target}_mean']\n",
    "\n",
    "    return x_train, x_valid, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['doi_id_label',\n",
       " 'author_first_label',\n",
       " 'pub_publisher_label',\n",
       " 'license_label',\n",
       " 'category_main_label',\n",
       " 'category_main_detail_label',\n",
       " 'category_name_parent_label',\n",
       " 'category_name_parent_main_label',\n",
       " 'category_name_label']"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "cat_columns = []\n",
    "for k, v in zip(df_train.columns, df_train.dtypes):\n",
    "    if 'label' in k and 'doi_cites' not in k  and 'submitter_label' not in k:\n",
    "        cat_columns.append(k)\n",
    "cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "l Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  10%|#         | 2/20 [00:37<03:40, 12.23s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  15%|#5        | 3/20 [00:37<03:35, 12.66s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:12:41,185]\u001b[0m Trial 45 finished with value: 0.505259168045804 and parameters: {'lambda_l1': 0.07433825739446001, 'lambda_l2': 1.961952345462962e-07}. Best is trial 45 with value: 0.505259168045804.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  15%|#5        | 3/20 [00:37<03:35, 12.66s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  15%|#5        | 3/20 [00:48<03:35, 12.66s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  20%|##        | 4/20 [00:48<03:16, 12.28s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:12:52,590]\u001b[0m Trial 46 finished with value: 0.5051216867595801 and parameters: {'lambda_l1': 3.747752071094339e-05, 'lambda_l2': 2.331625328613716e-08}. Best is trial 46 with value: 0.5051216867595801.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  20%|##        | 4/20 [00:48<03:16, 12.28s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  20%|##        | 4/20 [00:59<03:16, 12.28s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  25%|##5       | 5/20 [00:59<03:00, 12.02s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:13:03,985]\u001b[0m Trial 47 finished with value: 0.505121680534234 and parameters: {'lambda_l1': 5.1851124869386644e-06, 'lambda_l2': 2.424041337483453e-07}. Best is trial 47 with value: 0.505121680534234.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  25%|##5       | 5/20 [00:59<03:00, 12.02s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  25%|##5       | 5/20 [01:11<03:00, 12.02s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  30%|###       | 6/20 [01:11<02:44, 11.77s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:13:15,184]\u001b[0m Trial 48 finished with value: 0.5051216811117627 and parameters: {'lambda_l1': 6.266336426161385e-06, 'lambda_l2': 2.8114007168028726e-05}. Best is trial 47 with value: 0.505121680534234.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  30%|###       | 6/20 [01:11<02:44, 11.77s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  30%|###       | 6/20 [01:24<02:44, 11.77s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  35%|###5      | 7/20 [01:24<02:38, 12.16s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:13:28,249]\u001b[0m Trial 49 finished with value: 0.5051646382642082 and parameters: {'lambda_l1': 0.0004068838040194059, 'lambda_l2': 3.338244246794918}. Best is trial 47 with value: 0.505121680534234.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  35%|###5      | 7/20 [01:24<02:38, 12.16s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  35%|###5      | 7/20 [01:34<02:38, 12.16s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  40%|####      | 8/20 [01:34<02:17, 11.48s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:13:38,160]\u001b[0m Trial 50 finished with value: 0.5071957106697323 and parameters: {'lambda_l1': 0.7602627994753064, 'lambda_l2': 2.836741527743549e-05}. Best is trial 47 with value: 0.505121680534234.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  40%|####      | 8/20 [01:34<02:17, 11.48s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037935 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  40%|####      | 8/20 [01:46<02:17, 11.48s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  45%|####5     | 9/20 [01:46<02:10, 11.83s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:13:50,797]\u001b[0m Trial 51 finished with value: 0.5053477305286793 and parameters: {'lambda_l1': 5.492596494590127e-06, 'lambda_l2': 0.06471080874489847}. Best is trial 47 with value: 0.505121680534234.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  45%|####5     | 9/20 [01:46<02:10, 11.83s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  45%|####5     | 9/20 [01:58<02:10, 11.83s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  50%|#####     | 10/20 [01:58<01:57, 11.77s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:14:02,425]\u001b[0m Trial 52 finished with value: 0.5063765425668931 and parameters: {'lambda_l1': 0.009793942713326833, 'lambda_l2': 1.1104391140491599e-05}. Best is trial 47 with value: 0.505121680534234.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  50%|#####     | 10/20 [01:58<01:57, 11.77s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  50%|#####     | 10/20 [02:09<01:57, 11.77s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  55%|#####5    | 11/20 [02:09<01:45, 11.71s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:14:14,000]\u001b[0m Trial 53 finished with value: 0.5051216795522324 and parameters: {'lambda_l1': 1.6219036229495166e-08, 'lambda_l2': 1.0961971994637468e-06}. Best is trial 53 with value: 0.5051216795522324.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  55%|#####5    | 11/20 [02:09<01:45, 11.71s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044821 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  55%|#####5    | 11/20 [02:21<01:45, 11.71s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  60%|######    | 12/20 [02:21<01:33, 11.69s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:14:25,649]\u001b[0m Trial 54 finished with value: 0.505121679544902 and parameters: {'lambda_l1': 1.3178782278619613e-08, 'lambda_l2': 6.393603491825152e-07}. Best is trial 54 with value: 0.505121679544902.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  60%|######    | 12/20 [02:21<01:33, 11.69s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  60%|######    | 12/20 [02:32<01:33, 11.69s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  65%|######5   | 13/20 [02:32<01:20, 11.53s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:14:36,808]\u001b[0m Trial 55 finished with value: 0.5051216795565191 and parameters: {'lambda_l1': 1.0365005742851697e-08, 'lambda_l2': 1.1917382519128134e-06}. Best is trial 54 with value: 0.505121679544902.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  65%|######5   | 13/20 [02:32<01:20, 11.53s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037819 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  65%|######5   | 13/20 [02:44<01:20, 11.53s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  70%|#######   | 14/20 [02:44<01:09, 11.50s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:14:48,248]\u001b[0m Trial 56 finished with value: 0.5051216900236483 and parameters: {'lambda_l1': 1.0218508381783197e-08, 'lambda_l2': 0.0008249273297748703}. Best is trial 54 with value: 0.505121679544902.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  70%|#######   | 14/20 [02:44<01:09, 11.50s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036462 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  70%|#######   | 14/20 [03:00<01:09, 11.50s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  75%|#######5  | 15/20 [03:00<01:04, 12.95s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:15:04,556]\u001b[0m Trial 57 finished with value: 0.5044192119614074 and parameters: {'lambda_l1': 1.6070246107872794e-07, 'lambda_l2': 2.3034267853501663e-06}. Best is trial 57 with value: 0.5044192119614074.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  75%|#######5  | 15/20 [03:00<01:04, 12.95s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046330 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  75%|#######5  | 15/20 [03:11<01:04, 12.95s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  80%|########  | 16/20 [03:11<00:49, 12.45s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:15:15,843]\u001b[0m Trial 58 finished with value: 0.5051216796316345 and parameters: {'lambda_l1': 4.808156215137923e-07, 'lambda_l2': 1.1187493798244974e-08}. Best is trial 57 with value: 0.5044192119614074.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  80%|########  | 16/20 [03:11<00:49, 12.45s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  80%|########  | 16/20 [03:23<00:49, 12.45s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  85%|########5 | 17/20 [03:23<00:36, 12.15s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:15:27,310]\u001b[0m Trial 59 finished with value: 0.5051216796283162 and parameters: {'lambda_l1': 1.73379538707053e-07, 'lambda_l2': 4.466544830659009e-06}. Best is trial 57 with value: 0.5044192119614074.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  85%|########5 | 17/20 [03:23<00:36, 12.15s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  85%|########5 | 17/20 [03:34<00:36, 12.15s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  90%|######### | 18/20 [03:34<00:23, 11.89s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:15:38,576]\u001b[0m Trial 60 finished with value: 0.5051216827421197 and parameters: {'lambda_l1': 1.6289937532589285e-07, 'lambda_l2': 0.00024918492884940104}. Best is trial 57 with value: 0.5044192119614074.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  90%|######### | 18/20 [03:34<00:23, 11.89s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  90%|######### | 18/20 [03:45<00:23, 11.89s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  95%|#########5| 19/20 [03:45<00:11, 11.69s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:15:49,793]\u001b[0m Trial 61 finished with value: 0.5051216795470278 and parameters: {'lambda_l1': 6.992260239588313e-08, 'lambda_l2': 1.6499084235424737e-07}. Best is trial 57 with value: 0.5044192119614074.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  95%|#########5| 19/20 [03:45<00:11, 11.69s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419:  95%|#########5| 19/20 [04:02<00:11, 11.69s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.504419: 100%|##########| 20/20 [04:02<00:00, 13.13s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:16:06,302]\u001b[0m Trial 62 finished with value: 0.504419192671254 and parameters: {'lambda_l1': 8.856362720452578e-07, 'lambda_l2': 0.0002814044321572516}. Best is trial 62 with value: 0.504419192671254.\u001b[0m\n",
      "regularization_factors, val_score: 0.504419: 100%|##########| 20/20 [04:02<00:00, 12.11s/it]\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.504419:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036594 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.504419:   0%|          | 0/5 [00:10<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.504419:  20%|##        | 1/5 [00:10<00:42, 10.64s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:16:16,954]\u001b[0m Trial 63 finished with value: 0.5087776326451713 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 0.5087776326451713.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.504419:  20%|##        | 1/5 [00:10<00:42, 10.64s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037441 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.504419:  20%|##        | 1/5 [00:21<00:42, 10.64s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.504419:  40%|####      | 2/5 [00:21<00:32, 10.83s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:16:28,219]\u001b[0m Trial 64 finished with value: 0.5069838100506144 and parameters: {'min_child_samples': 50}. Best is trial 64 with value: 0.5069838100506144.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.504419:  40%|####      | 2/5 [00:21<00:32, 10.83s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.504419:  40%|####      | 2/5 [00:31<00:32, 10.83s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.504419:  60%|######    | 3/5 [00:31<00:20, 10.45s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:16:37,773]\u001b[0m Trial 65 finished with value: 0.5060242391748125 and parameters: {'min_child_samples': 25}. Best is trial 65 with value: 0.5060242391748125.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.504419:  60%|######    | 3/5 [00:31<00:20, 10.45s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034885 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.504419:  60%|######    | 3/5 [00:41<00:20, 10.45s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.504419:  80%|########  | 4/5 [00:41<00:10, 10.31s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:16:47,758]\u001b[0m Trial 66 finished with value: 0.5063988025224596 and parameters: {'min_child_samples': 10}. Best is trial 65 with value: 0.5060242391748125.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.504419:  80%|########  | 4/5 [00:41<00:10, 10.31s/it]\u001b[A\u001b[A\u001b[A[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250074\n",
      "[LightGBM] [Info] Number of data points in the train set: 12093, number of used features: 1057\n",
      "[LightGBM] [Info] Start training from score 2.576181\n",
      "\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.504419:  80%|########  | 4/5 [00:49<00:10, 10.31s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.504419: 100%|##########| 5/5 [00:49<00:00,  9.65s/it]\u001b[A\u001b[A\u001b[A\u001b[32m[I 2021-03-27 10:16:55,873]\u001b[0m Trial 67 finished with value: 0.5069206957935772 and parameters: {'min_child_samples': 100}. Best is trial 65 with value: 0.5060242391748125.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.504419: 100%|##########| 5/5 [00:49<00:00,  9.91s/it]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'objective': 'regression',\n",
       " 'metric': 'rmse',\n",
       " 'learning_rate': 0.01,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 8.856362720452578e-07,\n",
       " 'lambda_l2': 0.0002814044321572516,\n",
       " 'num_leaves': 31,\n",
       " 'feature_fraction': 0.42,\n",
       " 'bagging_fraction': 0.8762325382386713,\n",
       " 'bagging_freq': 4,\n",
       " 'min_child_samples': 20,\n",
       " 'num_iterations': 10000,\n",
       " 'early_stopping_round': 50,\n",
       " 'categorical_column': [46, 47, 48, 49, 51, 52, 53]}"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "# パラメーター調整\n",
    "result_y = []\n",
    "lgb_result_proba_test = []\n",
    "lgb_result_proba_valid = []\n",
    "\n",
    "fold_no = 2\n",
    "test_fold_no = fold_no\n",
    "valid_fold_no = fold_no + 1\n",
    "if valid_fold_no == NFOLDS:\n",
    "    valid_fold_no = 0\n",
    "\n",
    "# train\n",
    "train = df_train.copy()\n",
    "y_train = train[~train['fold_no'].isin([test_fold_no, valid_fold_no])]['cites'].values\n",
    "y_valid = train[train['fold_no'] == valid_fold_no]['cites'].values\n",
    "y_test = train[train['fold_no'] == test_fold_no]['cites'].values\n",
    "\n",
    "train = train.drop(\n",
    "    ['id', 'authors', 'title', 'comments',\n",
    "    'journal-ref', 'doi', 'report-no', 'categories', 'license',\n",
    "    'abstract', 'versions', 'authors_parsed', 'pub_publisher',\n",
    "    'update_date_y', 'first_created_date', 'last_created_date', 'doi_id', 'submitter', 'author_first', 'category_main', 'category_main_detail', 'category_name_parent_main_unique', 'category_name_parent_unique', 'category_name_unique']\n",
    "    , axis=1\n",
    ")\n",
    "train = train.drop(importance_0_cols, axis=1)\n",
    "train['diff_pred_doi_cites'] = train['pred_doi_cites'] - train['doi_cites']\n",
    "train['rate_pred_doi_cites'] = train['pred_doi_cites'] / (train['doi_cites'] + 1)\n",
    "\n",
    "x_train = train[~train['fold_no'].isin([test_fold_no, valid_fold_no])]\n",
    "x_valid = train[train['fold_no'] == valid_fold_no]\n",
    "x_test = train[train['fold_no'] == test_fold_no]\n",
    "\n",
    "'''\n",
    "# target encoding\n",
    "target = 'doi_cites'\n",
    "key = 'pred_doi_cites'\n",
    "x_train, x_valid, x_test = make_statics_table(target, key, 'doi_cites', x_train, x_valid, x_test)\n",
    "\n",
    "keys = ['update_ym']\n",
    "target = 'cites'\n",
    "for key in keys:\n",
    "    x_train, x_valid, x_test = make_statics_table(target, key, 'doi_cites', x_train, x_valid, x_test)\n",
    "'''\n",
    "\n",
    "# drop\n",
    "x_train = x_train.drop(['cites', 'fold_no'], axis=1)\n",
    "x_valid = x_valid.drop(['cites', 'fold_no'], axis=1)\n",
    "x_test = x_test.drop(['cites', 'fold_no'], axis=1)\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_iterations': 10000,\n",
    "    'early_stopping_rounds': 50\n",
    "}\n",
    "best_params, history = {}, []\n",
    "trains = lgb.Dataset(x_train, y_train)\n",
    "valids = lgb.Dataset(x_valid, y_valid)\n",
    "\n",
    "best = lgbo.train(params, trains, valid_sets=valids,\n",
    "                    verbose_eval=False,\n",
    "                    num_boost_round=100,\n",
    "                    early_stopping_rounds=50)\n",
    "best.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.657196\n",
      "[200]\tvalid_0's rmse: 0.526938\n",
      "[300]\tvalid_0's rmse: 0.497372\n",
      "[400]\tvalid_0's rmse: 0.488363\n",
      "[500]\tvalid_0's rmse: 0.484929\n",
      "[600]\tvalid_0's rmse: 0.483286\n",
      "[700]\tvalid_0's rmse: 0.482697\n",
      "Early stopping, best iteration is:\n",
      "[681]\tvalid_0's rmse: 0.482413\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.655603\n",
      "[200]\tvalid_0's rmse: 0.546492\n",
      "[300]\tvalid_0's rmse: 0.526142\n",
      "[400]\tvalid_0's rmse: 0.520056\n",
      "[500]\tvalid_0's rmse: 0.518055\n",
      "[600]\tvalid_0's rmse: 0.516832\n",
      "[700]\tvalid_0's rmse: 0.516372\n",
      "[800]\tvalid_0's rmse: 0.51645\n",
      "Early stopping, best iteration is:\n",
      "[754]\tvalid_0's rmse: 0.516253\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.671135\n",
      "[200]\tvalid_0's rmse: 0.548333\n",
      "[300]\tvalid_0's rmse: 0.521852\n",
      "[400]\tvalid_0's rmse: 0.51388\n",
      "[500]\tvalid_0's rmse: 0.510208\n",
      "[600]\tvalid_0's rmse: 0.50845\n",
      "[700]\tvalid_0's rmse: 0.507489\n",
      "[800]\tvalid_0's rmse: 0.506675\n",
      "[900]\tvalid_0's rmse: 0.50687\n",
      "Early stopping, best iteration is:\n",
      "[863]\tvalid_0's rmse: 0.506589\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.653183\n",
      "[200]\tvalid_0's rmse: 0.529516\n",
      "[300]\tvalid_0's rmse: 0.504174\n",
      "[400]\tvalid_0's rmse: 0.497026\n",
      "[500]\tvalid_0's rmse: 0.493704\n",
      "[600]\tvalid_0's rmse: 0.492477\n",
      "[700]\tvalid_0's rmse: 0.491641\n",
      "[800]\tvalid_0's rmse: 0.491241\n",
      "[900]\tvalid_0's rmse: 0.491078\n",
      "Early stopping, best iteration is:\n",
      "[872]\tvalid_0's rmse: 0.490947\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.658984\n",
      "[200]\tvalid_0's rmse: 0.533867\n",
      "[300]\tvalid_0's rmse: 0.507632\n",
      "[400]\tvalid_0's rmse: 0.500317\n",
      "[500]\tvalid_0's rmse: 0.497478\n",
      "[600]\tvalid_0's rmse: 0.495657\n",
      "[700]\tvalid_0's rmse: 0.494923\n",
      "[800]\tvalid_0's rmse: 0.494358\n",
      "[900]\tvalid_0's rmse: 0.494036\n",
      "[1000]\tvalid_0's rmse: 0.493881\n",
      "Early stopping, best iteration is:\n",
      "[976]\tvalid_0's rmse: 0.493769\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.664018\n",
      "[200]\tvalid_0's rmse: 0.546981\n",
      "[300]\tvalid_0's rmse: 0.52302\n",
      "[400]\tvalid_0's rmse: 0.515379\n",
      "[500]\tvalid_0's rmse: 0.511724\n",
      "[600]\tvalid_0's rmse: 0.509959\n",
      "[700]\tvalid_0's rmse: 0.509289\n",
      "[800]\tvalid_0's rmse: 0.50896\n",
      "Early stopping, best iteration is:\n",
      "[807]\tvalid_0's rmse: 0.508885\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.651114\n",
      "[200]\tvalid_0's rmse: 0.524338\n",
      "[300]\tvalid_0's rmse: 0.497807\n",
      "[400]\tvalid_0's rmse: 0.489682\n",
      "[500]\tvalid_0's rmse: 0.486882\n",
      "[600]\tvalid_0's rmse: 0.48573\n",
      "[700]\tvalid_0's rmse: 0.485087\n",
      "[800]\tvalid_0's rmse: 0.48469\n",
      "[900]\tvalid_0's rmse: 0.483918\n",
      "Early stopping, best iteration is:\n",
      "[903]\tvalid_0's rmse: 0.483873\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.656379\n",
      "[200]\tvalid_0's rmse: 0.525708\n",
      "[300]\tvalid_0's rmse: 0.496342\n",
      "[400]\tvalid_0's rmse: 0.486473\n",
      "[500]\tvalid_0's rmse: 0.482368\n",
      "[600]\tvalid_0's rmse: 0.480752\n",
      "[700]\tvalid_0's rmse: 0.479523\n",
      "[800]\tvalid_0's rmse: 0.478947\n",
      "[900]\tvalid_0's rmse: 0.478319\n",
      "[1000]\tvalid_0's rmse: 0.478034\n",
      "Early stopping, best iteration is:\n",
      "[961]\tvalid_0's rmse: 0.47798\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.66095\n",
      "[200]\tvalid_0's rmse: 0.544244\n",
      "[300]\tvalid_0's rmse: 0.521344\n",
      "[400]\tvalid_0's rmse: 0.515251\n",
      "[500]\tvalid_0's rmse: 0.512755\n",
      "[600]\tvalid_0's rmse: 0.511344\n",
      "[700]\tvalid_0's rmse: 0.510807\n",
      "[800]\tvalid_0's rmse: 0.510402\n",
      "[900]\tvalid_0's rmse: 0.510228\n",
      "Early stopping, best iteration is:\n",
      "[868]\tvalid_0's rmse: 0.510128\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.644267\n",
      "[200]\tvalid_0's rmse: 0.519948\n",
      "[300]\tvalid_0's rmse: 0.494803\n",
      "[400]\tvalid_0's rmse: 0.488183\n",
      "[500]\tvalid_0's rmse: 0.485173\n",
      "[600]\tvalid_0's rmse: 0.48369\n",
      "[700]\tvalid_0's rmse: 0.482881\n",
      "[800]\tvalid_0's rmse: 0.482473\n",
      "Early stopping, best iteration is:\n",
      "[807]\tvalid_0's rmse: 0.482368\n",
      "valid 0.4954986228633265\n"
     ]
    }
   ],
   "source": [
    "result_y_test = []\n",
    "result_y_valid = []\n",
    "lgb_result_proba_test = []\n",
    "lgb_result_proba_valid = []\n",
    "\n",
    "lgb_params = {'objective': 'regression',\n",
    " 'metric': 'rmse',\n",
    " 'learning_rate': 0.01,\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 8.856362720452578e-07,\n",
    " 'lambda_l2': 0.0002814044321572516,\n",
    " 'num_leaves': 31,\n",
    " 'feature_fraction': 0.42,\n",
    " 'bagging_fraction': 0.8762325382386713,\n",
    " 'bagging_freq': 4,\n",
    " 'min_child_samples': 20,\n",
    " 'num_iterations': 10000,\n",
    " 'early_stopping_round': 50,}\n",
    "\n",
    "for fold_no in range(NFOLDS):\n",
    "    test_fold_no = fold_no\n",
    "    valid_fold_no = fold_no + 1\n",
    "    if valid_fold_no == NFOLDS:\n",
    "        valid_fold_no = 0\n",
    "\n",
    "    # train\n",
    "    train = df_train.copy()\n",
    "    train['diff_pred_doi_cites'] = train['pred_doi_cites'] - train['doi_cites']\n",
    "    train['rate_pred_doi_cites'] = (train['pred_doi_cites'] + 1) - (train['doi_cites'] + 1)\n",
    "\n",
    "    y_train = train[~train['fold_no'].isin([valid_fold_no])]['cites'].values # test_fold_no, \n",
    "    y_valid = train[train['fold_no'] == valid_fold_no]['cites'].values\n",
    "    #y_test = train[train['fold_no'] == test_fold_no]['cites'].values\n",
    "\n",
    "    train = train.drop(\n",
    "        ['id', 'authors', 'title', 'comments',\n",
    "        'journal-ref', 'doi', 'report-no', 'categories', 'license',\n",
    "        'abstract', 'versions', 'authors_parsed', 'pub_publisher',\n",
    "        'update_date_y', 'first_created_date', 'last_created_date', 'doi_id', 'submitter', 'author_first', 'category_main', 'category_main_detail', 'category_name_parent_main_unique', 'category_name_parent_unique', 'category_name_unique']\n",
    "        , axis=1\n",
    "    )\n",
    "\n",
    "    train = train.drop(importance_0_cols, axis=1)\n",
    "\n",
    "    x_train = train[~train['fold_no'].isin([valid_fold_no])] # test_fold_no, \n",
    "    x_valid = train[train['fold_no'] == valid_fold_no]\n",
    "    #x_test = train[train['fold_no'] == test_fold_no]\n",
    "\n",
    "    '''\n",
    "    # target encoding\n",
    "    target = 'doi_cites'\n",
    "    key = 'pred_doi_cites'\n",
    "    x_train, x_valid, x_test = make_statics_table(target, key, 'doi_cites', x_train, x_valid, x_test)\n",
    "\n",
    "    keys = ['update_ym']\n",
    "    target = 'cites'\n",
    "    for key in keys:\n",
    "        x_train, x_valid, x_test = make_statics_table(target, key, 'doi_cites', x_train, x_valid, x_test)\n",
    "    '''\n",
    "\n",
    "    # drop\n",
    "    x_train = x_train.drop(['cites', 'fold_no'], axis=1)\n",
    "    x_valid = x_valid.drop(['cites', 'fold_no'], axis=1)\n",
    "    #x_test = x_test.drop(['cites', 'fold_no'], axis=1)\n",
    "\n",
    "    model = lgb.LGBMRegressor(**lgb_params)\n",
    "    model.fit(x_train, y_train,\n",
    "                eval_set=(x_valid, y_valid),\n",
    "                eval_metric='rmse',\n",
    "                verbose=100,\n",
    "                early_stopping_rounds=50,\n",
    "    )\n",
    "\n",
    "    pickle.dump(model, open(f'../models/lgb_{fold_no}.pickle', 'wb'))\n",
    "\n",
    "    #fold_result_test = model.predict(x_test)\n",
    "    fold_result_valid = model.predict(x_valid)\n",
    "    #result_y_test.extend(y_test)\n",
    "    result_y_valid.extend(y_valid)\n",
    "    #lgb_result_proba_test.extend(fold_result_test)\n",
    "    lgb_result_proba_valid.extend(fold_result_valid)\n",
    "    #rmsle = mean_squared_error(y_test, fold_result_test, squared=False)\n",
    "    #print(f\"fold {fold_no} lgb score: {rmsle}\")\n",
    "\n",
    "\n",
    "print(\"valid\", mean_squared_error(result_y_valid, lgb_result_proba_valid, squared=False))\n",
    "#print(\"test\", mean_squared_error(result_y_test, lgb_result_proba_test, squared=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.657161\n",
      "[200]\tvalid_0's rmse: 0.527814\n",
      "[300]\tvalid_0's rmse: 0.49796\n",
      "[400]\tvalid_0's rmse: 0.488952\n",
      "[500]\tvalid_0's rmse: 0.485522\n",
      "[600]\tvalid_0's rmse: 0.484172\n",
      "Early stopping, best iteration is:\n",
      "[636]\tvalid_0's rmse: 0.484062\n",
      "fold 0 lgb score: 0.48392110509477954\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.657631\n",
      "[200]\tvalid_0's rmse: 0.547982\n",
      "[300]\tvalid_0's rmse: 0.526844\n",
      "[400]\tvalid_0's rmse: 0.520888\n",
      "[500]\tvalid_0's rmse: 0.518247\n",
      "[600]\tvalid_0's rmse: 0.517048\n",
      "[700]\tvalid_0's rmse: 0.516515\n",
      "Early stopping, best iteration is:\n",
      "[688]\tvalid_0's rmse: 0.516359\n",
      "fold 1 lgb score: 0.48184847265284364\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.671414\n",
      "[200]\tvalid_0's rmse: 0.548522\n",
      "[300]\tvalid_0's rmse: 0.522596\n",
      "[400]\tvalid_0's rmse: 0.513917\n",
      "[500]\tvalid_0's rmse: 0.510041\n",
      "[600]\tvalid_0's rmse: 0.507917\n",
      "[700]\tvalid_0's rmse: 0.506666\n",
      "[800]\tvalid_0's rmse: 0.505913\n",
      "[900]\tvalid_0's rmse: 0.505496\n",
      "[1000]\tvalid_0's rmse: 0.505347\n",
      "Early stopping, best iteration is:\n",
      "[976]\tvalid_0's rmse: 0.505201\n",
      "fold 2 lgb score: 0.5136628963278062\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.65453\n",
      "[200]\tvalid_0's rmse: 0.530207\n",
      "[300]\tvalid_0's rmse: 0.504878\n",
      "[400]\tvalid_0's rmse: 0.497314\n",
      "[500]\tvalid_0's rmse: 0.494751\n",
      "[600]\tvalid_0's rmse: 0.493309\n",
      "[700]\tvalid_0's rmse: 0.49255\n",
      "Early stopping, best iteration is:\n",
      "[749]\tvalid_0's rmse: 0.492155\n",
      "fold 3 lgb score: 0.5083900109841151\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.659155\n",
      "[200]\tvalid_0's rmse: 0.533939\n",
      "[300]\tvalid_0's rmse: 0.508347\n",
      "[400]\tvalid_0's rmse: 0.501067\n",
      "[500]\tvalid_0's rmse: 0.497851\n",
      "[600]\tvalid_0's rmse: 0.496646\n",
      "[700]\tvalid_0's rmse: 0.495775\n",
      "Early stopping, best iteration is:\n",
      "[714]\tvalid_0's rmse: 0.495613\n",
      "fold 4 lgb score: 0.4940517806613665\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.662477\n",
      "[200]\tvalid_0's rmse: 0.546125\n",
      "[300]\tvalid_0's rmse: 0.522415\n",
      "[400]\tvalid_0's rmse: 0.515046\n",
      "[500]\tvalid_0's rmse: 0.512169\n",
      "[600]\tvalid_0's rmse: 0.51078\n",
      "[700]\tvalid_0's rmse: 0.510052\n",
      "Early stopping, best iteration is:\n",
      "[747]\tvalid_0's rmse: 0.509781\n",
      "fold 5 lgb score: 0.49437321899395836\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.650794\n",
      "[200]\tvalid_0's rmse: 0.523657\n",
      "[300]\tvalid_0's rmse: 0.497596\n",
      "[400]\tvalid_0's rmse: 0.489528\n",
      "[500]\tvalid_0's rmse: 0.486178\n",
      "[600]\tvalid_0's rmse: 0.484633\n",
      "[700]\tvalid_0's rmse: 0.483966\n",
      "[800]\tvalid_0's rmse: 0.483701\n",
      "[900]\tvalid_0's rmse: 0.483438\n",
      "Early stopping, best iteration is:\n",
      "[914]\tvalid_0's rmse: 0.483348\n",
      "fold 6 lgb score: 0.5089873282376596\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.655141\n",
      "[200]\tvalid_0's rmse: 0.524327\n",
      "[300]\tvalid_0's rmse: 0.494963\n",
      "[400]\tvalid_0's rmse: 0.485906\n",
      "[500]\tvalid_0's rmse: 0.481223\n",
      "[600]\tvalid_0's rmse: 0.479198\n",
      "[700]\tvalid_0's rmse: 0.477731\n",
      "[800]\tvalid_0's rmse: 0.476962\n",
      "[900]\tvalid_0's rmse: 0.476244\n",
      "Early stopping, best iteration is:\n",
      "[908]\tvalid_0's rmse: 0.476178\n",
      "fold 7 lgb score: 0.4847194704327787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.661982\n",
      "[200]\tvalid_0's rmse: 0.545723\n",
      "[300]\tvalid_0's rmse: 0.522977\n",
      "[400]\tvalid_0's rmse: 0.516315\n",
      "[500]\tvalid_0's rmse: 0.513624\n",
      "[600]\tvalid_0's rmse: 0.512102\n",
      "[700]\tvalid_0's rmse: 0.511546\n",
      "[800]\tvalid_0's rmse: 0.51116\n",
      "Early stopping, best iteration is:\n",
      "[790]\tvalid_0's rmse: 0.510977\n",
      "fold 8 lgb score: 0.47661375230095065\n",
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.856362720452578e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.856362720452578e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8762325382386713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8762325382386713\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002814044321572516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002814044321572516\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.643712\n",
      "[200]\tvalid_0's rmse: 0.520526\n",
      "[300]\tvalid_0's rmse: 0.494567\n",
      "[400]\tvalid_0's rmse: 0.487765\n",
      "[500]\tvalid_0's rmse: 0.4851\n",
      "[600]\tvalid_0's rmse: 0.483736\n",
      "[700]\tvalid_0's rmse: 0.483046\n",
      "[800]\tvalid_0's rmse: 0.48269\n",
      "Early stopping, best iteration is:\n",
      "[810]\tvalid_0's rmse: 0.482606\n",
      "fold 9 lgb score: 0.5130359858120248\n",
      "valid 0.49581078022677194\n",
      "test 0.4961407012855213\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'objective': 'regression',\n",
       " 'metric': 'rmse',\n",
       " 'learning_rate': 0.02,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 0.37475414935822066,\n",
       " 'lambda_l2': 1.4093025694101272e-07,\n",
       " 'num_leaves': 31,\n",
       " 'feature_fraction': 0.8,\n",
       " 'bagging_fraction': 1.0,\n",
       " 'bagging_freq': 0,\n",
       " 'min_child_samples': 20,\n",
       " 'num_iterations': 2000,\n",
       " 'early_stopping_round': 50,\n",
       " 'categorical_column': [242, 243, 244, 245, 246, 247, 249, 250, 251]}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "best.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               null_num\n",
       "submitter                                           132\n",
       "comments                                           1943\n",
       "journal-ref                                        5189\n",
       "report-no                                         13230\n",
       "license                                            4759\n",
       "pub_publisher                                       110\n",
       "pub_journals                                        110\n",
       "pub_dois                                            110\n",
       "doi_cites_std_author_first_label                    673\n",
       "doi_cites_std_doi_id_label                            5\n",
       "doi_cites_std_pub_publisher_label                     2\n",
       "doi_cites_std_submitter_label                      1318\n",
       "doi_cites_std_category_name_parent_label             20\n",
       "doi_cites_std_category_name_parent_main_label        16\n",
       "doi_cites_std_category_name_label                   229"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>null_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>submitter</th>\n      <td>132</td>\n    </tr>\n    <tr>\n      <th>comments</th>\n      <td>1943</td>\n    </tr>\n    <tr>\n      <th>journal-ref</th>\n      <td>5189</td>\n    </tr>\n    <tr>\n      <th>report-no</th>\n      <td>13230</td>\n    </tr>\n    <tr>\n      <th>license</th>\n      <td>4759</td>\n    </tr>\n    <tr>\n      <th>pub_publisher</th>\n      <td>110</td>\n    </tr>\n    <tr>\n      <th>pub_journals</th>\n      <td>110</td>\n    </tr>\n    <tr>\n      <th>pub_dois</th>\n      <td>110</td>\n    </tr>\n    <tr>\n      <th>doi_cites_std_author_first_label</th>\n      <td>673</td>\n    </tr>\n    <tr>\n      <th>doi_cites_std_doi_id_label</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>doi_cites_std_pub_publisher_label</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>doi_cites_std_submitter_label</th>\n      <td>1318</td>\n    </tr>\n    <tr>\n      <th>doi_cites_std_category_name_parent_label</th>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>doi_cites_std_category_name_parent_main_label</th>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>doi_cites_std_category_name_label</th>\n      <td>229</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "df_temp = pd.DataFrame(df_train.isna().sum()).rename(columns={0:'null_num'})\n",
    "df_temp[df_temp['null_num'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "result_y_test = []\n",
    "result_y_valid = []\n",
    "#rf_result_proba_test = []\n",
    "rf_result_proba_valid = []\n",
    "\n",
    "for fold_no in range(NFOLDS):\n",
    "    test_fold_no = fold_no\n",
    "    valid_fold_no = fold_no + 1\n",
    "    if valid_fold_no == NFOLDS:\n",
    "        valid_fold_no = 0\n",
    "\n",
    "    # train\n",
    "    train = df_train.copy()\n",
    "    train = train.fillna(0)\n",
    "\n",
    "    y_train = train[~train['fold_no'].isin([valid_fold_no])]['cites'].values # test_fold_no, \n",
    "    y_valid = train[train['fold_no'] == valid_fold_no]['cites'].values\n",
    "    #y_test = train[train['fold_no'] == test_fold_no]['cites'].values\n",
    "\n",
    "    train = train.drop(\n",
    "        ['id', 'authors', 'title', 'comments',\n",
    "        'journal-ref', 'doi', 'report-no', 'categories', 'license',\n",
    "        'abstract', 'versions', 'authors_parsed', 'pub_publisher',\n",
    "        'update_date_y', 'first_created_date', 'last_created_date', 'doi_id', 'submitter', 'author_first', 'category_main', 'category_main_detail', 'category_name_parent_main_unique', 'category_name_parent_unique', 'category_name_unique']\n",
    "        , axis=1\n",
    "    )\n",
    "\n",
    "    train = train.drop(importance_0_cols, axis=1)\n",
    "\n",
    "    x_train = train[~train['fold_no'].isin([test_fold_no, valid_fold_no])]\n",
    "    x_valid = train[train['fold_no'] == valid_fold_no]\n",
    "    #x_test = train[train['fold_no'] == test_fold_no]\n",
    "\n",
    "    '''\n",
    "    # target encoding\n",
    "    target = 'doi_cites'\n",
    "    key = 'pred_doi_cites'\n",
    "    x_train, x_valid, x_test = make_statics_table(target, key, 'doi_cites', x_train, x_valid, x_test)\n",
    "\n",
    "    keys = ['update_ym']\n",
    "    target = 'cites'\n",
    "    for key in keys:\n",
    "        x_train, x_valid, x_test = make_statics_table(target, key, 'doi_cites', x_train, x_valid, x_test)\n",
    "    '''\n",
    "\n",
    "    # drop\n",
    "    x_train = x_train.drop(['cites', 'fold_no'], axis=1)\n",
    "    x_valid = x_valid.drop(['cites', 'fold_no'], axis=1)\n",
    "    x_test = x_test.drop(['cites', 'fold_no'], axis=1)\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=24, criterion='mse', n_jobs=-1)\n",
    "    model.fit(x_train, y_train)\n",
    "    pickle.dump(model, open(f'../models/rf_{fold_no}.pickle', 'wb'))\n",
    "\n",
    "    #fold_result_test = model.predict(x_test)\n",
    "    fold_result_valid = model.predict(x_valid)\n",
    "    #result_y_test.extend(y_test)\n",
    "    result_y_valid.extend(y_valid)\n",
    "    #rf_result_proba_test.extend(fold_result_test)\n",
    "    rf_result_proba_valid.extend(fold_result_valid)\n",
    "    #rmsle = mean_squared_error(y_test, fold_result_test, squared=False)\n",
    "    print(f\"fold {fold_no} rf score: {rmsle}\")\n",
    "\n",
    "\n",
    "print(\"valid\", mean_squared_error(result_y_valid, rf_result_proba_valid, squared=False))\n",
    "#print(\"test\", mean_squared_error(result_y_test, rf_result_proba_test, squared=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fold 0 lgb score: 0.5067466282059222\n",
      "fold 1 lgb score: 0.5070536799832732\n",
      "fold 2 lgb score: 0.5487566432941057\n",
      "fold 3 lgb score: 0.5383677383941933\n",
      "fold 4 lgb score: 0.5168385937490483\n",
      "fold 5 lgb score: 0.5184729250645402\n",
      "fold 6 lgb score: 0.5322843342453957\n",
      "fold 7 lgb score: 0.5063263130545729\n",
      "fold 8 lgb score: 0.5133449767316445\n",
      "fold 9 lgb score: 0.537059322815508\n",
      "valid 0.5240871872446028\n",
      "test 0.5227295515450585\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    categorical_features_indices = np.where(x_train.dtypes == 'category')[0]\n",
    "\n",
    "    train_pool = Pool(x_train, y_train, cat_features=categorical_features_indices)\n",
    "    test_pool = Pool(x_test, y_test, cat_features=categorical_features_indices)\n",
    "    \n",
    "    # Parameters\n",
    "    param = {\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\n",
    "            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
    "        ),\n",
    "        \"iterations\": 1000,\n",
    "        \"learning_rate\": 0.03,\n",
    "        \"loss_function\": \"RMSE\",\n",
    "        \"eval_metric\": \"RMSE\",\n",
    "        \"random_seed\": SEED,\n",
    "    }\n",
    "\n",
    "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
    "\n",
    "    model = CatBoostRegressor(**param)\n",
    "    model.fit(train_pool, eval_set=test_pool, verbose=200, early_stopping_rounds=20)\n",
    "    \n",
    "    preds = model.predict(test_pool)\n",
    "    return mean_squared_error(y_test, preds, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m[I 2021-03-27 13:14:24,743]\u001b[0m A new study created in memory with name: no-name-3c4a5d94-a509-479b-bb32-0534db732653\u001b[0m\n",
      "0:\tlearn: 1.1743888\ttest: 1.1671023\tbest: 1.1671023 (0)\ttotal: 8.96ms\tremaining: 8.95s\n",
      "200:\tlearn: 0.5220326\ttest: 0.5447488\tbest: 0.5447488 (200)\ttotal: 1.28s\tremaining: 5.08s\n",
      "400:\tlearn: 0.5033216\ttest: 0.5310702\tbest: 0.5310702 (400)\ttotal: 2.41s\tremaining: 3.6s\n",
      "600:\tlearn: 0.4937408\ttest: 0.5264670\tbest: 0.5264670 (600)\ttotal: 3.5s\tremaining: 2.33s\n",
      "800:\tlearn: 0.4871515\ttest: 0.5240054\tbest: 0.5240054 (800)\ttotal: 4.64s\tremaining: 1.15s\n",
      "\u001b[32m[I 2021-03-27 13:14:31,065]\u001b[0m Trial 0 finished with value: 0.5227816675067647 and parameters: {'colsample_bylevel': 0.09178193753637694, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'subsample': 0.22889708929840308}. Best is trial 0 with value: 0.5227816675067647.\u001b[0m\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5227816692\n",
      "bestIteration = 950\n",
      "\n",
      "Shrink model to first 951 iterations.\n",
      "0:\tlearn: 1.1701072\ttest: 1.1628915\tbest: 1.1628915 (0)\ttotal: 15ms\tremaining: 15s\n",
      "200:\tlearn: 0.5060998\ttest: 0.5355241\tbest: 0.5355241 (200)\ttotal: 2.98s\tremaining: 11.9s\n",
      "400:\tlearn: 0.4866027\ttest: 0.5258138\tbest: 0.5258138 (400)\ttotal: 5.57s\tremaining: 8.32s\n",
      "600:\tlearn: 0.4742182\ttest: 0.5221911\tbest: 0.5221911 (600)\ttotal: 7.9s\tremaining: 5.24s\n",
      "800:\tlearn: 0.4645218\ttest: 0.5204861\tbest: 0.5204524 (793)\ttotal: 10.4s\tremaining: 2.58s\n",
      "\u001b[32m[I 2021-03-27 13:14:44,373]\u001b[0m Trial 1 finished with value: 0.5192035142839727 and parameters: {'colsample_bylevel': 0.03530634402380269, 'depth': 5, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'bagging_temperature': 1.4237092235466464}. Best is trial 1 with value: 0.5192035142839727.\u001b[0m\n",
      "999:\tlearn: 0.4555187\ttest: 0.5192138\tbest: 0.5192035 (997)\ttotal: 12.6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.5192035158\n",
      "bestIteration = 997\n",
      "\n",
      "Shrink model to first 998 iterations.\n",
      "0:\tlearn: 1.1698347\ttest: 1.1625281\tbest: 1.1625281 (0)\ttotal: 15.3ms\tremaining: 15.3s\n",
      "200:\tlearn: 0.4986444\ttest: 0.5320849\tbest: 0.5320849 (200)\ttotal: 2.39s\tremaining: 9.5s\n",
      "400:\tlearn: 0.4761593\ttest: 0.5234304\tbest: 0.5234304 (400)\ttotal: 4.33s\tremaining: 6.47s\n",
      "600:\tlearn: 0.4596755\ttest: 0.5197349\tbest: 0.5197349 (600)\ttotal: 6.37s\tremaining: 4.23s\n",
      "800:\tlearn: 0.4455293\ttest: 0.5172312\tbest: 0.5172312 (800)\ttotal: 8.13s\tremaining: 2.02s\n",
      "\u001b[32m[I 2021-03-27 13:14:54,945]\u001b[0m Trial 2 finished with value: 0.5160497383405246 and parameters: {'colsample_bylevel': 0.06436981813677815, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'bagging_temperature': 1.773816352733344}. Best is trial 2 with value: 0.5160497383405246.\u001b[0m\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.51604974\n",
      "bestIteration = 976\n",
      "\n",
      "Shrink model to first 977 iterations.\n",
      "0:\tlearn: 1.1803660\ttest: 1.1726114\tbest: 1.1726114 (0)\ttotal: 10.4ms\tremaining: 10.4s\n",
      "200:\tlearn: 0.5995501\ttest: 0.6155142\tbest: 0.6155126 (199)\ttotal: 1.42s\tremaining: 5.63s\n",
      "400:\tlearn: 0.5594432\ttest: 0.5802986\tbest: 0.5802986 (400)\ttotal: 2.39s\tremaining: 3.58s\n",
      "600:\tlearn: 0.5405498\ttest: 0.5645676\tbest: 0.5645676 (600)\ttotal: 3.43s\tremaining: 2.27s\n",
      "800:\tlearn: 0.5315189\ttest: 0.5577950\tbest: 0.5577950 (800)\ttotal: 4.39s\tremaining: 1.09s\n",
      "\u001b[32m[I 2021-03-27 13:15:00,685]\u001b[0m Trial 3 finished with value: 0.5533013861815453 and parameters: {'colsample_bylevel': 0.017194952111980265, 'depth': 3, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'bagging_temperature': 8.489018279705345}. Best is trial 2 with value: 0.5160497383405246.\u001b[0m\n",
      "999:\tlearn: 0.5254568\ttest: 0.5533025\tbest: 0.5533014 (998)\ttotal: 5.21s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.5533013872\n",
      "bestIteration = 998\n",
      "\n",
      "Shrink model to first 999 iterations.\n",
      "0:\tlearn: 1.1804561\ttest: 1.1727788\tbest: 1.1727788 (0)\ttotal: 1.89ms\tremaining: 1.89s\n",
      "200:\tlearn: 0.6098123\ttest: 0.6199893\tbest: 0.6199893 (200)\ttotal: 811ms\tremaining: 3.22s\n",
      "400:\tlearn: 0.5628249\ttest: 0.5795178\tbest: 0.5795178 (400)\ttotal: 1.57s\tremaining: 2.35s\n",
      "600:\tlearn: 0.5485430\ttest: 0.5668997\tbest: 0.5668997 (600)\ttotal: 2.3s\tremaining: 1.53s\n",
      "800:\tlearn: 0.5406215\ttest: 0.5596083\tbest: 0.5596083 (800)\ttotal: 2.83s\tremaining: 704ms\n",
      "\u001b[32m[I 2021-03-27 13:15:04,571]\u001b[0m Trial 4 finished with value: 0.5550109967352977 and parameters: {'colsample_bylevel': 0.04906487561232437, 'depth': 1, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'bagging_temperature': 7.0517061840530095}. Best is trial 2 with value: 0.5160497383405246.\u001b[0m\n",
      "999:\tlearn: 0.5352099\ttest: 0.5550110\tbest: 0.5550110 (999)\ttotal: 3.29s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.5550109979\n",
      "bestIteration = 999\n",
      "\n",
      "0:\tlearn: 1.1691391\ttest: 1.1621991\tbest: 1.1621991 (0)\ttotal: 29.8ms\tremaining: 29.7s\n",
      "200:\tlearn: 0.4746109\ttest: 0.5299144\tbest: 0.5299144 (200)\ttotal: 5.39s\tremaining: 21.4s\n",
      "400:\tlearn: 0.4315044\ttest: 0.5202047\tbest: 0.5202047 (400)\ttotal: 10.5s\tremaining: 15.6s\n",
      "600:\tlearn: 0.3958929\ttest: 0.5174034\tbest: 0.5174034 (600)\ttotal: 15.2s\tremaining: 10.1s\n",
      "\u001b[32m[I 2021-03-27 13:15:21,875]\u001b[0m Trial 5 finished with value: 0.5170299998354149 and parameters: {'colsample_bylevel': 0.0780327198793032, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'subsample': 0.47765509235266557}. Best is trial 2 with value: 0.5160497383405246.\u001b[0m\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5170300011\n",
      "bestIteration = 646\n",
      "\n",
      "Shrink model to first 647 iterations.\n",
      "0:\tlearn: 1.1723096\ttest: 1.1650342\tbest: 1.1650342 (0)\ttotal: 15.4ms\tremaining: 15.4s\n",
      "200:\tlearn: 0.5056445\ttest: 0.5344420\tbest: 0.5344420 (200)\ttotal: 2s\tremaining: 7.94s\n",
      "400:\tlearn: 0.4814008\ttest: 0.5216839\tbest: 0.5216631 (399)\ttotal: 3.84s\tremaining: 5.74s\n",
      "600:\tlearn: 0.4642226\ttest: 0.5174658\tbest: 0.5174580 (599)\ttotal: 5.75s\tremaining: 3.81s\n",
      "800:\tlearn: 0.4494347\ttest: 0.5150231\tbest: 0.5150231 (800)\ttotal: 7.38s\tremaining: 1.83s\n",
      "\u001b[32m[I 2021-03-27 13:15:31,628]\u001b[0m Trial 6 finished with value: 0.5133738901947987 and parameters: {'colsample_bylevel': 0.02596296252322562, 'depth': 5, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS'}. Best is trial 6 with value: 0.5133738901947987.\u001b[0m\n",
      "999:\tlearn: 0.4359535\ttest: 0.5134023\tbest: 0.5133739 (997)\ttotal: 9.1s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.5133738917\n",
      "bestIteration = 997\n",
      "\n",
      "Shrink model to first 998 iterations.\n",
      "0:\tlearn: 1.1859909\ttest: 1.1783570\tbest: 1.1783570 (0)\ttotal: 6.34ms\tremaining: 6.34s\n",
      "200:\tlearn: 0.5235839\ttest: 0.5465569\tbest: 0.5465569 (200)\ttotal: 1.14s\tremaining: 4.52s\n",
      "400:\tlearn: 0.5021806\ttest: 0.5296872\tbest: 0.5296872 (400)\ttotal: 1.9s\tremaining: 2.84s\n",
      "600:\tlearn: 0.4918916\ttest: 0.5247606\tbest: 0.5247606 (600)\ttotal: 2.84s\tremaining: 1.89s\n",
      "800:\tlearn: 0.4846214\ttest: 0.5225554\tbest: 0.5225554 (800)\ttotal: 3.76s\tremaining: 933ms\n",
      "\u001b[32m[I 2021-03-27 13:15:36,970]\u001b[0m Trial 7 finished with value: 0.5211465693367602 and parameters: {'colsample_bylevel': 0.06882769576062191, 'depth': 2, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'subsample': 0.9649739202529878}. Best is trial 6 with value: 0.5133738901947987.\u001b[0m\n",
      "999:\tlearn: 0.4785125\ttest: 0.5211560\tbest: 0.5211466 (997)\ttotal: 4.59s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.521146571\n",
      "bestIteration = 997\n",
      "\n",
      "Shrink model to first 998 iterations.\n",
      "0:\tlearn: 1.1711719\ttest: 1.1636533\tbest: 1.1636533 (0)\ttotal: 21.4ms\tremaining: 21.4s\n",
      "200:\tlearn: 0.4735309\ttest: 0.5298970\tbest: 0.5298970 (200)\ttotal: 3.28s\tremaining: 13s\n",
      "400:\tlearn: 0.4256570\ttest: 0.5189129\tbest: 0.5189129 (400)\ttotal: 5.9s\tremaining: 8.81s\n",
      "\u001b[32m[I 2021-03-27 13:15:45,485]\u001b[0m Trial 8 finished with value: 0.5160839152748805 and parameters: {'colsample_bylevel': 0.02507185734367059, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS'}. Best is trial 6 with value: 0.5133738901947987.\u001b[0m\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5160839167\n",
      "bestIteration = 538\n",
      "\n",
      "Shrink model to first 539 iterations.\n",
      "0:\tlearn: 1.1729746\ttest: 1.1653265\tbest: 1.1653265 (0)\ttotal: 4.46ms\tremaining: 4.45s\n",
      "200:\tlearn: 0.5221320\ttest: 0.5418632\tbest: 0.5418632 (200)\ttotal: 1.09s\tremaining: 4.33s\n",
      "400:\tlearn: 0.5015319\ttest: 0.5260923\tbest: 0.5260923 (400)\ttotal: 1.92s\tremaining: 2.87s\n",
      "600:\tlearn: 0.4912131\ttest: 0.5210230\tbest: 0.5210230 (600)\ttotal: 3s\tremaining: 1.99s\n",
      "800:\tlearn: 0.4836954\ttest: 0.5187777\tbest: 0.5187758 (795)\ttotal: 4.31s\tremaining: 1.07s\n",
      "\u001b[32m[I 2021-03-27 13:15:51,230]\u001b[0m Trial 9 finished with value: 0.5171200735109395 and parameters: {'colsample_bylevel': 0.07630173834869354, 'depth': 2, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS'}. Best is trial 6 with value: 0.5133738901947987.\u001b[0m\n",
      "999:\tlearn: 0.4776386\ttest: 0.5171313\tbest: 0.5171201 (996)\ttotal: 5.1s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.5171200751\n",
      "bestIteration = 996\n",
      "\n",
      "Shrink model to first 997 iterations.\n",
      "0:\tlearn: 1.1692356\ttest: 1.1625135\tbest: 1.1625135 (0)\ttotal: 383ms\tremaining: 6m 23s\n",
      "200:\tlearn: 0.4709531\ttest: 0.5348974\tbest: 0.5348974 (200)\ttotal: 1m 7s\tremaining: 4m 28s\n",
      "400:\tlearn: 0.4329989\ttest: 0.5234405\tbest: 0.5234316 (399)\ttotal: 2m 13s\tremaining: 3m 19s\n",
      "600:\tlearn: 0.4018707\ttest: 0.5186515\tbest: 0.5186072 (599)\ttotal: 3m 15s\tremaining: 2m 9s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5172388472\n",
      "bestIteration = 701\n",
      "\n",
      "Shrink model to first 702 iterations.\n",
      "\u001b[32m[I 2021-03-27 13:19:45,963]\u001b[0m Trial 10 finished with value: 0.5172388457105925 and parameters: {'colsample_bylevel': 0.012034276168009485, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS'}. Best is trial 6 with value: 0.5133738901947987.\u001b[0m\n",
      "0:\tlearn: 1.1714604\ttest: 1.1638548\tbest: 1.1638548 (0)\ttotal: 22.9ms\tremaining: 22.9s\n",
      "200:\tlearn: 0.4983571\ttest: 0.5298354\tbest: 0.5298354 (200)\ttotal: 6.64s\tremaining: 26.4s\n",
      "400:\tlearn: 0.4737780\ttest: 0.5188214\tbest: 0.5188214 (400)\ttotal: 13.1s\tremaining: 19.5s\n",
      "600:\tlearn: 0.4523830\ttest: 0.5150113\tbest: 0.5150113 (600)\ttotal: 19.5s\tremaining: 13s\n",
      "800:\tlearn: 0.4328805\ttest: 0.5132298\tbest: 0.5131927 (795)\ttotal: 26s\tremaining: 6.46s\n",
      "\u001b[32m[I 2021-03-27 13:20:14,984]\u001b[0m Trial 11 finished with value: 0.5125028838315051 and parameters: {'colsample_bylevel': 0.052922169991835896, 'depth': 6, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS'}. Best is trial 11 with value: 0.5125028838315051.\u001b[0m\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5125028852\n",
      "bestIteration = 854\n",
      "\n",
      "Shrink model to first 855 iterations.\n",
      "0:\tlearn: 1.1696358\ttest: 1.1624778\tbest: 1.1624778 (0)\ttotal: 71.2ms\tremaining: 1m 11s\n",
      "200:\tlearn: 0.4900229\ttest: 0.5304728\tbest: 0.5304728 (200)\ttotal: 10.5s\tremaining: 41.9s\n",
      "400:\tlearn: 0.4622376\ttest: 0.5199645\tbest: 0.5199645 (400)\ttotal: 20.6s\tremaining: 30.8s\n",
      "600:\tlearn: 0.4355969\ttest: 0.5162238\tbest: 0.5162238 (600)\ttotal: 31s\tremaining: 20.6s\n",
      "\u001b[32m[I 2021-03-27 13:20:48,791]\u001b[0m Trial 12 finished with value: 0.5157985708284907 and parameters: {'colsample_bylevel': 0.03967199268584552, 'depth': 8, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS'}. Best is trial 11 with value: 0.5125028838315051.\u001b[0m\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5157985722\n",
      "bestIteration = 622\n",
      "\n",
      "Shrink model to first 623 iterations.\n",
      "0:\tlearn: 1.1702937\ttest: 1.1635999\tbest: 1.1635999 (0)\ttotal: 10.5ms\tremaining: 10.5s\n",
      "200:\tlearn: 0.5023370\ttest: 0.5321857\tbest: 0.5321857 (200)\ttotal: 2.67s\tremaining: 10.6s\n",
      "400:\tlearn: 0.4796644\ttest: 0.5207440\tbest: 0.5207440 (400)\ttotal: 5.13s\tremaining: 7.66s\n",
      "600:\tlearn: 0.4622057\ttest: 0.5162725\tbest: 0.5162725 (600)\ttotal: 7.58s\tremaining: 5.04s\n",
      "800:\tlearn: 0.4456892\ttest: 0.5141238\tbest: 0.5140614 (794)\ttotal: 10.1s\tremaining: 2.51s\n",
      "\u001b[32m[I 2021-03-27 13:21:00,923]\u001b[0m Trial 13 finished with value: 0.5133350317146614 and parameters: {'colsample_bylevel': 0.04972781796748116, 'depth': 5, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS'}. Best is trial 11 with value: 0.5125028838315051.\u001b[0m\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5133350333\n",
      "bestIteration = 906\n",
      "\n",
      "Shrink model to first 907 iterations.\n",
      "0:\tlearn: 1.1679935\ttest: 1.1608362\tbest: 1.1608362 (0)\ttotal: 728ms\tremaining: 12m 7s\n",
      "200:\tlearn: 0.4780723\ttest: 0.5292360\tbest: 0.5292360 (200)\ttotal: 1m 48s\tremaining: 7m 10s\n",
      "400:\tlearn: 0.4472430\ttest: 0.5199779\tbest: 0.5199779 (400)\ttotal: 3m 32s\tremaining: 5m 17s\n",
      "600:\tlearn: 0.4133899\ttest: 0.5158366\tbest: 0.5158226 (591)\ttotal: 5m 20s\tremaining: 3m 33s\n",
      "\u001b[32m[I 2021-03-27 13:26:59,646]\u001b[0m Trial 14 finished with value: 0.5151047308021028 and parameters: {'colsample_bylevel': 0.05242934212629483, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS'}. Best is trial 11 with value: 0.5125028838315051.\u001b[0m\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5151047323\n",
      "bestIteration = 648\n",
      "\n",
      "Shrink model to first 649 iterations.\n"
     ]
    }
   ],
   "source": [
    "fold_no = 2\n",
    "test_fold_no = fold_no\n",
    "valid_fold_no = fold_no + 1\n",
    "if valid_fold_no == NFOLDS:\n",
    "    valid_fold_no = 0\n",
    "\n",
    "# train\n",
    "train = df_train.copy()\n",
    "y_train = train[~train['fold_no'].isin([test_fold_no, valid_fold_no])]['cites'].values\n",
    "y_test = train[train['fold_no'].isin([test_fold_no, valid_fold_no])]['cites'].values\n",
    "\n",
    "train = train.drop(\n",
    "    ['id', 'authors', 'title', 'comments',\n",
    "    'journal-ref', 'doi', 'report-no', 'categories', 'license',\n",
    "    'abstract', 'versions', 'authors_parsed', 'pub_publisher',\n",
    "    'update_date_y', 'first_created_date', 'last_created_date', 'doi_id', 'submitter', 'author_first', 'category_main', 'category_main_detail', 'category_name_parent_main_unique', 'category_name_parent_unique', 'category_name_unique']\n",
    "    , axis=1\n",
    ")\n",
    "train = train.drop(importance_0_cols, axis=1)\n",
    "train['diff_pred_doi_cites'] = train['pred_doi_cites'] - train['doi_cites']\n",
    "train['rate_pred_doi_cites'] = train['pred_doi_cites'] / (train['doi_cites'] + 1)\n",
    "\n",
    "x_train = train[~train['fold_no'].isin([test_fold_no, valid_fold_no])]\n",
    "x_test = train[train['fold_no'].isin([test_fold_no, valid_fold_no])]\n",
    "\n",
    "x_train = x_train.drop(['cites', 'fold_no'], axis=1)\n",
    "x_test = x_test.drop(['cites', 'fold_no'], axis=1)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30, timeout=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of finished trials: 15\nBest trial:\n  Value: 0.5125028838315051\n  Params: \n    colsample_bylevel: 0.052922169991835896\n    depth: 6\n    boosting_type: Ordered\n    bootstrap_type: MVS\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning rate set to 0.017583\n",
      "0:\tlearn: 1.1796004\ttest: 1.1766413\tbest: 1.1766413 (0)\ttotal: 26.1ms\tremaining: 4m 21s\n",
      "100:\tlearn: 0.5916054\ttest: 0.5863165\tbest: 0.5863165 (100)\ttotal: 3.5s\tremaining: 5m 43s\n",
      "200:\tlearn: 0.5259664\ttest: 0.5166985\tbest: 0.5166985 (200)\ttotal: 6.71s\tremaining: 5m 27s\n",
      "300:\tlearn: 0.5097635\ttest: 0.5012702\tbest: 0.5012702 (300)\ttotal: 9.92s\tremaining: 5m 19s\n",
      "400:\tlearn: 0.5005640\ttest: 0.4946617\tbest: 0.4946617 (400)\ttotal: 13.2s\tremaining: 5m 15s\n",
      "500:\tlearn: 0.4936871\ttest: 0.4908874\tbest: 0.4908874 (500)\ttotal: 16.4s\tremaining: 5m 10s\n",
      "600:\tlearn: 0.4877124\ttest: 0.4877624\tbest: 0.4877620 (598)\ttotal: 19.6s\tremaining: 5m 6s\n",
      "700:\tlearn: 0.4815889\ttest: 0.4857584\tbest: 0.4857584 (700)\ttotal: 22.8s\tremaining: 5m 2s\n",
      "800:\tlearn: 0.4758116\ttest: 0.4844378\tbest: 0.4844255 (798)\ttotal: 26s\tremaining: 4m 58s\n",
      "900:\tlearn: 0.4700288\ttest: 0.4833306\tbest: 0.4833306 (900)\ttotal: 29.4s\tremaining: 4m 56s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.4830493477\n",
      "bestIteration = 926\n",
      "\n",
      "Shrink model to first 927 iterations.\n",
      "fold 0 lgb score: 0.4884814030249669\n",
      "Learning rate set to 0.017583\n",
      "0:\tlearn: 1.1817956\ttest: 1.1562588\tbest: 1.1562588 (0)\ttotal: 25.5ms\tremaining: 4m 14s\n",
      "100:\tlearn: 0.5897155\ttest: 0.5997159\tbest: 0.5997159 (100)\ttotal: 3.65s\tremaining: 5m 58s\n",
      "200:\tlearn: 0.5230176\ttest: 0.5469304\tbest: 0.5469304 (200)\ttotal: 7.08s\tremaining: 5m 44s\n",
      "300:\tlearn: 0.5064220\ttest: 0.5369945\tbest: 0.5369945 (300)\ttotal: 10.2s\tremaining: 5m 29s\n",
      "400:\tlearn: 0.4971011\ttest: 0.5326567\tbest: 0.5326567 (400)\ttotal: 13.5s\tremaining: 5m 22s\n",
      "500:\tlearn: 0.4897723\ttest: 0.5294603\tbest: 0.5294603 (500)\ttotal: 16.6s\tremaining: 5m 15s\n",
      "600:\tlearn: 0.4835971\ttest: 0.5274205\tbest: 0.5274205 (600)\ttotal: 19.8s\tremaining: 5m 10s\n",
      "700:\tlearn: 0.4762283\ttest: 0.5256767\tbest: 0.5256756 (699)\ttotal: 23s\tremaining: 5m 5s\n",
      "800:\tlearn: 0.4691349\ttest: 0.5242245\tbest: 0.5242245 (800)\ttotal: 26.2s\tremaining: 5m 1s\n",
      "900:\tlearn: 0.4624887\ttest: 0.5227891\tbest: 0.5227891 (900)\ttotal: 29.4s\tremaining: 4m 57s\n",
      "1000:\tlearn: 0.4560677\ttest: 0.5221008\tbest: 0.5221008 (1000)\ttotal: 32.6s\tremaining: 4m 53s\n",
      "1100:\tlearn: 0.4493650\ttest: 0.5213907\tbest: 0.5213574 (1091)\ttotal: 35.9s\tremaining: 4m 49s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5210658776\n",
      "bestIteration = 1176\n",
      "\n",
      "Shrink model to first 1177 iterations.\n",
      "fold 1 lgb score: 0.4827692585441189\n",
      "Learning rate set to 0.017583\n",
      "0:\tlearn: 1.1801278\ttest: 1.1886764\tbest: 1.1886764 (0)\ttotal: 29.2ms\tremaining: 4m 51s\n",
      "100:\tlearn: 0.5860323\ttest: 0.6086205\tbest: 0.6086205 (100)\ttotal: 3.33s\tremaining: 5m 26s\n",
      "200:\tlearn: 0.5185678\ttest: 0.5426632\tbest: 0.5426632 (200)\ttotal: 6.52s\tremaining: 5m 17s\n",
      "300:\tlearn: 0.5016818\ttest: 0.5279123\tbest: 0.5279123 (300)\ttotal: 9.67s\tremaining: 5m 11s\n",
      "400:\tlearn: 0.4922722\ttest: 0.5212511\tbest: 0.5212511 (400)\ttotal: 12.9s\tremaining: 5m 9s\n",
      "500:\tlearn: 0.4853445\ttest: 0.5173145\tbest: 0.5173073 (499)\ttotal: 16.1s\tremaining: 5m 4s\n",
      "600:\tlearn: 0.4786373\ttest: 0.5138530\tbest: 0.5138530 (600)\ttotal: 19.3s\tremaining: 5m 1s\n",
      "700:\tlearn: 0.4719093\ttest: 0.5111058\tbest: 0.5111058 (700)\ttotal: 22.5s\tremaining: 4m 58s\n",
      "800:\tlearn: 0.4652758\ttest: 0.5095208\tbest: 0.5095208 (800)\ttotal: 25.7s\tremaining: 4m 55s\n",
      "900:\tlearn: 0.4594286\ttest: 0.5083472\tbest: 0.5083472 (900)\ttotal: 28.9s\tremaining: 4m 52s\n",
      "1000:\tlearn: 0.4534816\ttest: 0.5073901\tbest: 0.5073901 (1000)\ttotal: 32.1s\tremaining: 4m 48s\n",
      "1100:\tlearn: 0.4475929\ttest: 0.5068545\tbest: 0.5068440 (1099)\ttotal: 35.3s\tremaining: 4m 45s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5064506133\n",
      "bestIteration = 1157\n",
      "\n",
      "Shrink model to first 1158 iterations.\n",
      "fold 2 lgb score: 0.5204641164692774\n",
      "Learning rate set to 0.017583\n",
      "0:\tlearn: 1.1784282\ttest: 1.1719431\tbest: 1.1719431 (0)\ttotal: 27.8ms\tremaining: 4m 38s\n",
      "100:\tlearn: 0.5883020\ttest: 0.5908995\tbest: 0.5908995 (100)\ttotal: 3.34s\tremaining: 5m 27s\n",
      "200:\tlearn: 0.5213186\ttest: 0.5274209\tbest: 0.5274209 (200)\ttotal: 6.53s\tremaining: 5m 18s\n",
      "300:\tlearn: 0.5052327\ttest: 0.5144351\tbest: 0.5144351 (300)\ttotal: 9.72s\tremaining: 5m 13s\n",
      "400:\tlearn: 0.4956132\ttest: 0.5087708\tbest: 0.5087708 (400)\ttotal: 12.9s\tremaining: 5m 9s\n",
      "500:\tlearn: 0.4887405\ttest: 0.5049037\tbest: 0.5049037 (500)\ttotal: 16.1s\tremaining: 5m 6s\n",
      "600:\tlearn: 0.4828424\ttest: 0.5015001\tbest: 0.5015001 (600)\ttotal: 19.3s\tremaining: 5m 1s\n",
      "700:\tlearn: 0.4766987\ttest: 0.4990784\tbest: 0.4990784 (700)\ttotal: 22.5s\tremaining: 4m 58s\n",
      "800:\tlearn: 0.4701944\ttest: 0.4976475\tbest: 0.4976228 (797)\ttotal: 25.7s\tremaining: 4m 55s\n",
      "900:\tlearn: 0.4650367\ttest: 0.4964396\tbest: 0.4964396 (900)\ttotal: 28.9s\tremaining: 4m 52s\n",
      "1000:\tlearn: 0.4593646\ttest: 0.4955190\tbest: 0.4954586 (987)\ttotal: 32.2s\tremaining: 4m 49s\n",
      "1100:\tlearn: 0.4536228\ttest: 0.4945569\tbest: 0.4945569 (1100)\ttotal: 35.3s\tremaining: 4m 45s\n",
      "1200:\tlearn: 0.4480866\ttest: 0.4937428\tbest: 0.4937408 (1199)\ttotal: 38.6s\tremaining: 4m 42s\n",
      "1300:\tlearn: 0.4429813\ttest: 0.4930585\tbest: 0.4930448 (1294)\ttotal: 41.8s\tremaining: 4m 39s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.4929097074\n",
      "bestIteration = 1318\n",
      "\n",
      "Shrink model to first 1319 iterations.\n",
      "fold 3 lgb score: 0.506226815878713\n",
      "Learning rate set to 0.017583\n",
      "0:\tlearn: 1.1774838\ttest: 1.1960676\tbest: 1.1960676 (0)\ttotal: 28.8ms\tremaining: 4m 47s\n",
      "100:\tlearn: 0.5887595\ttest: 0.6039126\tbest: 0.6039126 (100)\ttotal: 3.39s\tremaining: 5m 32s\n",
      "200:\tlearn: 0.5230133\ttest: 0.5378307\tbest: 0.5378307 (200)\ttotal: 6.56s\tremaining: 5m 19s\n",
      "300:\tlearn: 0.5064815\ttest: 0.5220010\tbest: 0.5220010 (300)\ttotal: 9.77s\tremaining: 5m 14s\n",
      "400:\tlearn: 0.4968518\ttest: 0.5141763\tbest: 0.5141763 (400)\ttotal: 13s\tremaining: 5m 11s\n",
      "500:\tlearn: 0.4896110\ttest: 0.5093757\tbest: 0.5093757 (500)\ttotal: 16.2s\tremaining: 5m 7s\n",
      "600:\tlearn: 0.4831679\ttest: 0.5060412\tbest: 0.5060412 (600)\ttotal: 19.3s\tremaining: 5m 2s\n",
      "700:\tlearn: 0.4768872\ttest: 0.5035359\tbest: 0.5035359 (700)\ttotal: 22.5s\tremaining: 4m 58s\n",
      "800:\tlearn: 0.4705725\ttest: 0.5021541\tbest: 0.5021291 (799)\ttotal: 25.7s\tremaining: 4m 55s\n",
      "900:\tlearn: 0.4646289\ttest: 0.5009857\tbest: 0.5009857 (900)\ttotal: 28.9s\tremaining: 4m 51s\n",
      "1000:\tlearn: 0.4586646\ttest: 0.5000781\tbest: 0.5000781 (1000)\ttotal: 32s\tremaining: 4m 48s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.4999626769\n",
      "bestIteration = 1008\n",
      "\n",
      "Shrink model to first 1009 iterations.\n",
      "fold 4 lgb score: 0.4952808246608715\n",
      "Learning rate set to 0.017583\n",
      "0:\tlearn: 1.1773937\ttest: 1.1732069\tbest: 1.1732069 (0)\ttotal: 26.8ms\tremaining: 4m 28s\n",
      "100:\tlearn: 0.5866411\ttest: 0.6005054\tbest: 0.6005054 (100)\ttotal: 3.36s\tremaining: 5m 28s\n",
      "200:\tlearn: 0.5206615\ttest: 0.5401325\tbest: 0.5401325 (200)\ttotal: 6.56s\tremaining: 5m 19s\n",
      "300:\tlearn: 0.5049854\ttest: 0.5273272\tbest: 0.5273272 (300)\ttotal: 9.69s\tremaining: 5m 12s\n",
      "400:\tlearn: 0.4952971\ttest: 0.5210724\tbest: 0.5210724 (400)\ttotal: 12.9s\tremaining: 5m 9s\n",
      "500:\tlearn: 0.4881639\ttest: 0.5168876\tbest: 0.5168795 (499)\ttotal: 16.1s\tremaining: 5m 5s\n",
      "600:\tlearn: 0.4812204\ttest: 0.5138079\tbest: 0.5138079 (600)\ttotal: 19.3s\tremaining: 5m 1s\n",
      "700:\tlearn: 0.4742753\ttest: 0.5112495\tbest: 0.5112495 (700)\ttotal: 22.5s\tremaining: 4m 57s\n",
      "800:\tlearn: 0.4682020\ttest: 0.5097896\tbest: 0.5097896 (800)\ttotal: 25.7s\tremaining: 4m 55s\n",
      "900:\tlearn: 0.4620248\ttest: 0.5088206\tbest: 0.5088206 (900)\ttotal: 29s\tremaining: 4m 52s\n",
      "1000:\tlearn: 0.4561951\ttest: 0.5078889\tbest: 0.5078854 (999)\ttotal: 32.2s\tremaining: 4m 49s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5078853609\n",
      "bestIteration = 999\n",
      "\n",
      "Shrink model to first 1000 iterations.\n",
      "fold 5 lgb score: 0.49927279506797334\n",
      "Learning rate set to 0.017583\n",
      "0:\tlearn: 1.1779397\ttest: 1.1803461\tbest: 1.1803461 (0)\ttotal: 29.4ms\tremaining: 4m 54s\n",
      "100:\tlearn: 0.5900069\ttest: 0.5813528\tbest: 0.5813528 (100)\ttotal: 3.36s\tremaining: 5m 29s\n",
      "200:\tlearn: 0.5227077\ttest: 0.5191686\tbest: 0.5191686 (200)\ttotal: 6.56s\tremaining: 5m 20s\n",
      "300:\tlearn: 0.5069204\ttest: 0.5085628\tbest: 0.5085628 (300)\ttotal: 9.73s\tremaining: 5m 13s\n",
      "400:\tlearn: 0.4970869\ttest: 0.5031746\tbest: 0.5031746 (400)\ttotal: 12.9s\tremaining: 5m 8s\n",
      "500:\tlearn: 0.4898235\ttest: 0.4996261\tbest: 0.4996261 (500)\ttotal: 16.1s\tremaining: 5m 4s\n",
      "600:\tlearn: 0.4828486\ttest: 0.4974083\tbest: 0.4973923 (598)\ttotal: 19.3s\tremaining: 5m 1s\n",
      "700:\tlearn: 0.4765126\ttest: 0.4956937\tbest: 0.4956937 (700)\ttotal: 22.5s\tremaining: 4m 57s\n",
      "800:\tlearn: 0.4693469\ttest: 0.4938171\tbest: 0.4937891 (799)\ttotal: 25.7s\tremaining: 4m 54s\n",
      "900:\tlearn: 0.4626420\ttest: 0.4929713\tbest: 0.4929339 (895)\ttotal: 28.8s\tremaining: 4m 51s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.4929338794\n",
      "bestIteration = 895\n",
      "\n",
      "Shrink model to first 896 iterations.\n",
      "fold 6 lgb score: 0.5096244880766019\n",
      "Learning rate set to 0.017583\n",
      "0:\tlearn: 1.1772753\ttest: 1.1871799\tbest: 1.1871799 (0)\ttotal: 27.5ms\tremaining: 4m 34s\n",
      "100:\tlearn: 0.5922088\ttest: 0.5828969\tbest: 0.5828969 (100)\ttotal: 3.34s\tremaining: 5m 27s\n",
      "200:\tlearn: 0.5254001\ttest: 0.5143585\tbest: 0.5143585 (200)\ttotal: 6.5s\tremaining: 5m 16s\n",
      "300:\tlearn: 0.5093297\ttest: 0.5021909\tbest: 0.5021909 (300)\ttotal: 9.77s\tremaining: 5m 14s\n",
      "400:\tlearn: 0.5001572\ttest: 0.4973916\tbest: 0.4973916 (400)\ttotal: 12.9s\tremaining: 5m 9s\n",
      "500:\tlearn: 0.4923188\ttest: 0.4938612\tbest: 0.4938612 (500)\ttotal: 16.1s\tremaining: 5m 4s\n",
      "600:\tlearn: 0.4863468\ttest: 0.4915093\tbest: 0.4915093 (600)\ttotal: 19.3s\tremaining: 5m 1s\n",
      "700:\tlearn: 0.4791223\ttest: 0.4891051\tbest: 0.4891051 (700)\ttotal: 22.5s\tremaining: 4m 57s\n",
      "800:\tlearn: 0.4723451\ttest: 0.4874513\tbest: 0.4874445 (799)\ttotal: 25.7s\tremaining: 4m 55s\n",
      "900:\tlearn: 0.4657548\ttest: 0.4860341\tbest: 0.4860163 (899)\ttotal: 28.9s\tremaining: 4m 51s\n",
      "1000:\tlearn: 0.4591104\ttest: 0.4846771\tbest: 0.4846771 (1000)\ttotal: 32.1s\tremaining: 4m 48s\n",
      "1100:\tlearn: 0.4527687\ttest: 0.4834110\tbest: 0.4834110 (1100)\ttotal: 35.4s\tremaining: 4m 45s\n",
      "1200:\tlearn: 0.4467769\ttest: 0.4827610\tbest: 0.4827307 (1197)\ttotal: 38.6s\tremaining: 4m 42s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.482599774\n",
      "bestIteration = 1239\n",
      "\n",
      "Shrink model to first 1240 iterations.\n",
      "fold 7 lgb score: 0.48868832997604256\n",
      "Learning rate set to 0.017583\n",
      "0:\tlearn: 1.1770208\ttest: 1.1832383\tbest: 1.1832383 (0)\ttotal: 25.3ms\tremaining: 4m 12s\n",
      "100:\tlearn: 0.5899077\ttest: 0.6053616\tbest: 0.6053616 (100)\ttotal: 3.38s\tremaining: 5m 30s\n",
      "200:\tlearn: 0.5216275\ttest: 0.5435851\tbest: 0.5435851 (200)\ttotal: 6.56s\tremaining: 5m 20s\n",
      "300:\tlearn: 0.5058564\ttest: 0.5324848\tbest: 0.5324848 (300)\ttotal: 9.78s\tremaining: 5m 15s\n",
      "400:\tlearn: 0.4964953\ttest: 0.5271075\tbest: 0.5271075 (400)\ttotal: 13.1s\tremaining: 5m 12s\n",
      "500:\tlearn: 0.4890194\ttest: 0.5236147\tbest: 0.5236147 (500)\ttotal: 16.3s\tremaining: 5m 8s\n",
      "600:\tlearn: 0.4824115\ttest: 0.5211426\tbest: 0.5211426 (600)\ttotal: 19.5s\tremaining: 5m 4s\n",
      "700:\tlearn: 0.4760486\ttest: 0.5193343\tbest: 0.5193231 (699)\ttotal: 22.7s\tremaining: 5m\n",
      "800:\tlearn: 0.4698061\ttest: 0.5177939\tbest: 0.5177793 (799)\ttotal: 25.8s\tremaining: 4m 56s\n",
      "900:\tlearn: 0.4634867\ttest: 0.5164985\tbest: 0.5164482 (891)\ttotal: 29.1s\tremaining: 4m 53s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.5162726113\n",
      "bestIteration = 932\n",
      "\n",
      "Shrink model to first 933 iterations.\n",
      "fold 8 lgb score: 0.48524114780631744\n",
      "Learning rate set to 0.017583\n",
      "0:\tlearn: 1.1777768\ttest: 1.1717970\tbest: 1.1717970 (0)\ttotal: 27.4ms\tremaining: 4m 33s\n",
      "100:\tlearn: 0.5889751\ttest: 0.5780352\tbest: 0.5780352 (100)\ttotal: 3.33s\tremaining: 5m 26s\n",
      "200:\tlearn: 0.5217188\ttest: 0.5157736\tbest: 0.5157736 (200)\ttotal: 6.54s\tremaining: 5m 18s\n",
      "300:\tlearn: 0.5050747\ttest: 0.5050925\tbest: 0.5050925 (300)\ttotal: 9.71s\tremaining: 5m 12s\n",
      "400:\tlearn: 0.4952707\ttest: 0.4994789\tbest: 0.4994789 (400)\ttotal: 13s\tremaining: 5m 10s\n",
      "500:\tlearn: 0.4880431\ttest: 0.4956609\tbest: 0.4956487 (499)\ttotal: 16.3s\tremaining: 5m 8s\n",
      "600:\tlearn: 0.4816911\ttest: 0.4933037\tbest: 0.4933037 (600)\ttotal: 19.4s\tremaining: 5m 3s\n",
      "700:\tlearn: 0.4754968\ttest: 0.4917919\tbest: 0.4917919 (700)\ttotal: 22.6s\tremaining: 5m\n",
      "800:\tlearn: 0.4689998\ttest: 0.4905353\tbest: 0.4905001 (796)\ttotal: 25.7s\tremaining: 4m 55s\n",
      "900:\tlearn: 0.4623764\ttest: 0.4888751\tbest: 0.4888751 (900)\ttotal: 28.8s\tremaining: 4m 50s\n",
      "1000:\tlearn: 0.4563982\ttest: 0.4877787\tbest: 0.4877787 (1000)\ttotal: 31.9s\tremaining: 4m 46s\n",
      "1100:\tlearn: 0.4504989\ttest: 0.4866459\tbest: 0.4866459 (1100)\ttotal: 35s\tremaining: 4m 42s\n",
      "1200:\tlearn: 0.4443147\ttest: 0.4858376\tbest: 0.4858376 (1200)\ttotal: 38s\tremaining: 4m 38s\n",
      "1300:\tlearn: 0.4389378\ttest: 0.4852978\tbest: 0.4852557 (1297)\ttotal: 41s\tremaining: 4m 34s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.4850103684\n",
      "bestIteration = 1350\n",
      "\n",
      "Shrink model to first 1351 iterations.\n",
      "fold 9 lgb score: 0.5150469658357145\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'result_y' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-1b8c695acabf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"fold {fold_no} lgb score: {rmsle}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mrmsle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_result_proba_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"+-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"score: {rmsle}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result_y' is not defined"
     ]
    }
   ],
   "source": [
    "SEED = 777\n",
    "result_y_test = []\n",
    "result_y_valid = []\n",
    "cat_result_proba_test = []\n",
    "cat_result_proba_valid = []\n",
    "\n",
    "for fold_no in range(NFOLDS):\n",
    "    test_fold_no = fold_no\n",
    "    valid_fold_no = fold_no + 1\n",
    "    if valid_fold_no == NFOLDS:\n",
    "        valid_fold_no = 0\n",
    "\n",
    "    # train\n",
    "    train = df_train.copy()\n",
    "    y_train = train[~train['fold_no'].isin([test_fold_no, valid_fold_no])]['cites'].values\n",
    "    y_valid = train[train['fold_no'] == valid_fold_no]['cites'].values\n",
    "    y_test = train[train['fold_no'] == test_fold_no]['cites'].values\n",
    "\n",
    "    train = train.drop(\n",
    "        ['id', 'authors', 'title', 'comments',\n",
    "        'journal-ref', 'doi', 'report-no', 'categories', 'license',\n",
    "        'abstract', 'versions', 'authors_parsed', 'pub_publisher',\n",
    "        'update_date_y', 'first_created_date', 'last_created_date', 'doi_id', 'submitter', 'author_first', 'category_main', 'category_main_detail', 'category_name_parent_main_unique', 'category_name_parent_unique', 'category_name_unique']\n",
    "        , axis=1\n",
    "    )\n",
    "    train = train.drop(importance_0_cols, axis=1)\n",
    "    train['diff_pred_doi_cites'] = train['pred_doi_cites'] - train['doi_cites']\n",
    "    train['rate_pred_doi_cites'] = train['pred_doi_cites'] / (train['doi_cites'] + 1)\n",
    "\n",
    "    x_train = train[~train['fold_no'].isin([test_fold_no, valid_fold_no])]\n",
    "    x_valid = train[train['fold_no'] == valid_fold_no]\n",
    "    x_test = train[train['fold_no'] == test_fold_no]\n",
    "\n",
    "    # target encoding\n",
    "    #target = 'doi_cites'\n",
    "    #key = 'pred_doi_cites'\n",
    "    #x_train, x_valid, x_test = make_statics_table(target, key, x_train, x_valid, x_test)\n",
    "\n",
    "    #keys = ['update_ym', 'license_label', 'pub_publisher_label', 'category_main_label', 'category_name_parent_label', 'category_name_parent_main_label', 'category_name_label', 'pred_doi_cites']\n",
    "    '''\n",
    "    keys = ['update_ym', 'license_label', 'pub_publisher_label', 'category_main_label', 'category_name_parent_label', 'category_name_parent_main_label']\n",
    "    target = 'cites'\n",
    "    for key in keys:\n",
    "        x_train, x_valid, x_test = make_statics_table(target, key, x_train, x_valid, x_test)\n",
    "    '''\n",
    "    \n",
    "    # drop\n",
    "    x_train = x_train.drop(['cites', 'fold_no'], axis=1)\n",
    "    x_valid = x_valid.drop(['cites', 'fold_no'], axis=1)\n",
    "    x_test = x_test.drop(['cites', 'fold_no'], axis=1)\n",
    "\n",
    "\n",
    "    cat_features = np.where(x_train.dtypes == 'category')[0]\n",
    "    train_pool = Pool(x_train, y_train, cat_features=cat_features)\n",
    "    validate_pool = Pool(x_valid, y_valid, cat_features=cat_features)\n",
    "    test_pool = Pool(x_test, y_test, cat_features=cat_features)\n",
    "    \n",
    "    model = CatBoostRegressor(\n",
    "        loss_function='RMSE',\n",
    "        eval_metric=\"RMSE\",\n",
    "        colsample_bylevel=0.052922169991835896,\n",
    "        depth=6,\n",
    "        boosting_type='Ordered',\n",
    "        bootstrap_type='MVS',\n",
    "        iterations=10000,\n",
    "        random_seed=SEED,\n",
    "    )\n",
    "    #model = lgb.LGBMRegressor(**lgb_params)\n",
    "    model.fit(train_pool,\n",
    "                eval_set=validate_pool,\n",
    "                verbose=100,\n",
    "                early_stopping_rounds=20,\n",
    "    )\n",
    "\n",
    "    pickle.dump(model, open(f'../models/cat_{fold_no}.pickle', 'wb'))\n",
    "\n",
    "    fold_result_test = model.predict(x_test)\n",
    "    fold_result_valid = model.predict(x_valid)\n",
    "    result_y_test.extend(y_test)\n",
    "    result_y_valid.extend(y_valid)\n",
    "    cat_result_proba_test.extend(fold_result_test)\n",
    "    cat_result_proba_valid.extend(fold_result_valid)\n",
    "\n",
    "    rmsle = mean_squared_error(y_test, fold_result_test, squared=False)\n",
    "    print(f\"fold {fold_no} lgb score: {rmsle}\")\n",
    "\n",
    "print(\"valid\", mean_squared_error(result_y_valid, cat_result_proba_valid, squared=False))\n",
    "print(\"test\", mean_squared_error(result_y_test, cat_result_proba_test, squared=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "valid 0.49898602590195396\ntest 0.4992670981883152\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         lgb       cat  doi_cites\n",
       "0   1.616322  1.668196   1.386294\n",
       "1   1.150935  1.139835   3.663562\n",
       "2   2.656537  2.606077   2.564949\n",
       "3   2.070312  2.100779   1.791759\n",
       "4   2.703382  2.741201   2.833213\n",
       "5   2.786024  2.910353   2.995732\n",
       "6   1.261919  1.377220   1.386294\n",
       "7   1.120770  1.370008   1.609438\n",
       "8   2.982970  2.812338   2.639057\n",
       "9   1.282977  1.284003   1.386294\n",
       "10  3.813396  3.629246   3.988984\n",
       "11  2.720791  2.583534   2.302585\n",
       "12  1.146577  1.054056   1.386294\n",
       "13  1.543638  1.343105   1.609438\n",
       "14  1.033559  1.071919   1.098612\n",
       "15  3.015166  3.076558   3.295837\n",
       "16  2.446892  2.413149   3.295837\n",
       "17  4.480386  4.324300   4.406719\n",
       "18  2.683346  2.579798   2.708050\n",
       "19  3.154612  3.095361   3.258097\n",
       "20  1.916189  2.024655   2.079442\n",
       "21  1.887372  1.735898   1.791759\n",
       "22  0.959000  1.008256   1.609438\n",
       "23  1.456664  1.511432   2.197225\n",
       "24  3.725619  3.654744   3.761200\n",
       "25  3.838671  4.055715   3.988984\n",
       "26  2.930383  3.112599   3.295837\n",
       "27  0.970967  1.262890   0.693147\n",
       "28  1.204491  1.242046   1.098612\n",
       "29  1.883186  2.005726   2.564949\n",
       "30  2.534327  2.593886   2.833213\n",
       "31  3.592106  3.458176   3.526361\n",
       "32  3.233806  3.155772   2.890372\n",
       "33  2.176769  2.255284   2.302585\n",
       "34  3.115343  2.937782   3.784190\n",
       "35  3.906003  4.053593   3.737670\n",
       "36  2.654146  2.690449   2.944439\n",
       "37  4.362378  4.386840   4.477337\n",
       "38  3.079152  3.111184   2.995732\n",
       "39  1.792171  1.762892   1.386294\n",
       "40  2.407396  2.282677   2.079442\n",
       "41  2.591042  2.542966   2.564949\n",
       "42  2.121600  1.922499   2.564949\n",
       "43  2.721298  2.475445   1.386294\n",
       "44  1.867874  2.001374   2.079442\n",
       "45  1.256762  1.404472   0.693147\n",
       "46  1.650983  1.688101   1.945910\n",
       "47  1.930965  2.038517   2.397895\n",
       "48  2.077082  2.046443   2.564949\n",
       "49  2.450542  2.395711   2.833213"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lgb</th>\n      <th>cat</th>\n      <th>doi_cites</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.616322</td>\n      <td>1.668196</td>\n      <td>1.386294</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.150935</td>\n      <td>1.139835</td>\n      <td>3.663562</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.656537</td>\n      <td>2.606077</td>\n      <td>2.564949</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.070312</td>\n      <td>2.100779</td>\n      <td>1.791759</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.703382</td>\n      <td>2.741201</td>\n      <td>2.833213</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2.786024</td>\n      <td>2.910353</td>\n      <td>2.995732</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1.261919</td>\n      <td>1.377220</td>\n      <td>1.386294</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1.120770</td>\n      <td>1.370008</td>\n      <td>1.609438</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2.982970</td>\n      <td>2.812338</td>\n      <td>2.639057</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1.282977</td>\n      <td>1.284003</td>\n      <td>1.386294</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3.813396</td>\n      <td>3.629246</td>\n      <td>3.988984</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2.720791</td>\n      <td>2.583534</td>\n      <td>2.302585</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.146577</td>\n      <td>1.054056</td>\n      <td>1.386294</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1.543638</td>\n      <td>1.343105</td>\n      <td>1.609438</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1.033559</td>\n      <td>1.071919</td>\n      <td>1.098612</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>3.015166</td>\n      <td>3.076558</td>\n      <td>3.295837</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2.446892</td>\n      <td>2.413149</td>\n      <td>3.295837</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>4.480386</td>\n      <td>4.324300</td>\n      <td>4.406719</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2.683346</td>\n      <td>2.579798</td>\n      <td>2.708050</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>3.154612</td>\n      <td>3.095361</td>\n      <td>3.258097</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>1.916189</td>\n      <td>2.024655</td>\n      <td>2.079442</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1.887372</td>\n      <td>1.735898</td>\n      <td>1.791759</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.959000</td>\n      <td>1.008256</td>\n      <td>1.609438</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1.456664</td>\n      <td>1.511432</td>\n      <td>2.197225</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>3.725619</td>\n      <td>3.654744</td>\n      <td>3.761200</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>3.838671</td>\n      <td>4.055715</td>\n      <td>3.988984</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2.930383</td>\n      <td>3.112599</td>\n      <td>3.295837</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.970967</td>\n      <td>1.262890</td>\n      <td>0.693147</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>1.204491</td>\n      <td>1.242046</td>\n      <td>1.098612</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>1.883186</td>\n      <td>2.005726</td>\n      <td>2.564949</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>2.534327</td>\n      <td>2.593886</td>\n      <td>2.833213</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>3.592106</td>\n      <td>3.458176</td>\n      <td>3.526361</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>3.233806</td>\n      <td>3.155772</td>\n      <td>2.890372</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>2.176769</td>\n      <td>2.255284</td>\n      <td>2.302585</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>3.115343</td>\n      <td>2.937782</td>\n      <td>3.784190</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>3.906003</td>\n      <td>4.053593</td>\n      <td>3.737670</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>2.654146</td>\n      <td>2.690449</td>\n      <td>2.944439</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>4.362378</td>\n      <td>4.386840</td>\n      <td>4.477337</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>3.079152</td>\n      <td>3.111184</td>\n      <td>2.995732</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>1.792171</td>\n      <td>1.762892</td>\n      <td>1.386294</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>2.407396</td>\n      <td>2.282677</td>\n      <td>2.079442</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>2.591042</td>\n      <td>2.542966</td>\n      <td>2.564949</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>2.121600</td>\n      <td>1.922499</td>\n      <td>2.564949</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>2.721298</td>\n      <td>2.475445</td>\n      <td>1.386294</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>1.867874</td>\n      <td>2.001374</td>\n      <td>2.079442</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>1.256762</td>\n      <td>1.404472</td>\n      <td>0.693147</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>1.650983</td>\n      <td>1.688101</td>\n      <td>1.945910</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>1.930965</td>\n      <td>2.038517</td>\n      <td>2.397895</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>2.077082</td>\n      <td>2.046443</td>\n      <td>2.564949</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>2.450542</td>\n      <td>2.395711</td>\n      <td>2.833213</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "df_result = pd.DataFrame([lgb_result_proba_test, cat_result_proba_test, result_y_test]).T.rename(columns={0:'lgb', 1:'cat', 2:'doi_cites', 3:'pred_doi_cites', 4:'y'})\n",
    "df_result.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((15117, 3), 15117)"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "df_result.shape, len(result_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y = result_y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.25, random_state=0)\n",
    "#model = linear_model.ElasticNet(alpha=0.7, l1_ratio=0.7, max_iter=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.49743021455230063"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "mean_squared_error(y_test, pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's rmse: 0.507305\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5073053637597862"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_pre_filter': False,\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 0,\n",
    "    'min_child_samples': 20,\n",
    "    'num_iterations': 2000,\n",
    "    'early_stopping_round': 50,\n",
    "}\n",
    "model = lgb.LGBMRegressor(**lgb_params)\n",
    "model.fit(X_train, y_train,\n",
    "    eval_set=(X_test, y_test),\n",
    "    eval_metric='rmse',\n",
    "    verbose=100,\n",
    ")\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "mean_squared_error(y_test, pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['submitter_label', 'doi_id_label', 'author_first_label',\n",
       "       'pub_publisher_label', 'license_label', 'category_main_label',\n",
       "       'category_name_parent_label', 'category_name_parent_main_label',\n",
       "       'category_name_label'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "x_train.columns[categorical_features_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 0.5029752173028096 lgb normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  \n",
       "4               0.060425              -0.227604              -0.043873   \n",
       "\n",
       "   rbert_vec_raw_768_699  rbert_vec_raw_768_700  rbert_vec_raw_768_701  \\\n",
       "0              -0.125990              -0.074972               0.258740   \n",
       "1              -0.201592              -0.094257               0.140190   \n",
       "2              -0.129978              -0.057161               0.231705   \n",
       "3              -0.099062              -0.123162               0.202115   \n",
       "4              -0.122353              -0.137685               0.176160   \n",
       "\n",
       "   rbert_vec_raw_768_702  rbert_vec_raw_768_703  rbert_vec_raw_768_704  \\\n",
       "0              -0.824300              -0.240840               0.010494   \n",
       "1              -0.766507              -0.137215               0.151194   \n",
       "2              -0.802713              -0.146444               0.046169   \n",
       "3              -0.796550              -0.204647               0.084613   \n",
       "4              -0.741568               0.000557               0.011738   \n",
       "\n",
       "   rbert_vec_raw_768_705  rbert_vec_raw_768_706  rbert_vec_raw_768_707  \\\n",
       "0               0.009743              -0.035558               0.230279   \n",
       "1               0.033688              -0.009695               0.233568   \n",
       "2               0.045916              -0.073564               0.196521   \n",
       "3               0.085180              -0.105909               0.206075   \n",
       "4              -0.051742              -0.085747               0.288524   \n",
       "\n",
       "   rbert_vec_raw_768_708  rbert_vec_raw_768_709  rbert_vec_raw_768_710  \\\n",
       "0              -0.198116               0.198006               0.214464   \n",
       "1              -0.191690               0.174944               0.268931   \n",
       "2              -0.133488               0.168456               0.206797   \n",
       "3              -0.169015               0.264994               0.169813   \n",
       "4              -0.155850               0.244002               0.205743   \n",
       "\n",
       "   rbert_vec_raw_768_711  rbert_vec_raw_768_712  rbert_vec_raw_768_713  \\\n",
       "0               0.114637              -0.234757               0.168230   \n",
       "1               0.015305              -0.248654               0.152268   \n",
       "2               0.123613              -0.172510               0.051626   \n",
       "3               0.072487              -0.294009               0.140317   \n",
       "4              -0.019203              -0.097291               0.208094   \n",
       "\n",
       "   rbert_vec_raw_768_714  rbert_vec_raw_768_715  rbert_vec_raw_768_716  \\\n",
       "0              -0.221062               0.068228              -0.308377   \n",
       "1               0.088289               0.105007              -0.349722   \n",
       "2               0.012986               0.149887              -0.236040   \n",
       "3              -0.179734               0.018937              -0.235079   \n",
       "4              -0.030838               0.118386              -0.247853   \n",
       "\n",
       "   rbert_vec_raw_768_717  rbert_vec_raw_768_718  rbert_vec_raw_768_719  \\\n",
       "0              -0.041138              -0.396182               0.007108   \n",
       "1              -0.041510              -0.403001              -0.079653   \n",
       "2               0.028193              -0.476537              -0.085902   \n",
       "3              -0.034906              -0.394411              -0.017301   \n",
       "4              -0.006473              -0.369708              -0.091226   \n",
       "\n",
       "   rbert_vec_raw_768_720  rbert_vec_raw_768_721  rbert_vec_raw_768_722  \\\n",
       "0               0.308709               0.045967               0.198250   \n",
       "1               0.574540               0.136599               0.160080   \n",
       "2               0.309771               0.067193               0.075443   \n",
       "3               0.381582               0.101955               0.202956   \n",
       "4               0.587550               0.044841              -0.033097   \n",
       "\n",
       "   rbert_vec_raw_768_723  rbert_vec_raw_768_724  rbert_vec_raw_768_725  \\\n",
       "0               0.161404              -0.418363              -0.030801   \n",
       "1               0.338430              -0.428547              -0.040832   \n",
       "2               0.201039              -0.389641              -0.034003   \n",
       "3               0.227578              -0.407289               0.003627   \n",
       "4               0.336011              -0.436082              -0.046020   \n",
       "\n",
       "   rbert_vec_raw_768_726  rbert_vec_raw_768_727  rbert_vec_raw_768_728  \\\n",
       "0               0.178136               0.262133              -0.184873   \n",
       "1               0.166718               0.379569               0.009165   \n",
       "2               0.144847               0.361557              -0.073972   \n",
       "3               0.145120               0.332201              -0.135765   \n",
       "4               0.021364               0.283768              -0.018955   \n",
       "\n",
       "   rbert_vec_raw_768_729  rbert_vec_raw_768_730  rbert_vec_raw_768_731  \\\n",
       "0              -0.155423              -0.049266              -3.918618   \n",
       "1              -0.069855              -0.014186              -3.708467   \n",
       "2              -0.051607               0.076390              -3.763675   \n",
       "3              -0.132857              -0.032216              -3.812458   \n",
       "4              -0.038653               0.019657              -3.768578   \n",
       "\n",
       "   rbert_vec_raw_768_732  rbert_vec_raw_768_733  rbert_vec_raw_768_734  \\\n",
       "0              -0.297293              -1.955519               0.020843   \n",
       "1              -0.591583              -1.993726               0.082165   \n",
       "2              -0.400752              -1.998005               0.112026   \n",
       "3              -0.313736              -1.906796               0.013529   \n",
       "4              -0.324513              -1.877376               0.030798   \n",
       "\n",
       "   rbert_vec_raw_768_735  rbert_vec_raw_768_736  rbert_vec_raw_768_737  \\\n",
       "0               0.288394              -0.178661               0.186991   \n",
       "1               0.064541              -0.076794               0.212564   \n",
       "2               0.201483              -0.143264               0.181641   \n",
       "3               0.221811              -0.058853               0.211268   \n",
       "4               0.218780              -0.030397               0.148808   \n",
       "\n",
       "   rbert_vec_raw_768_738  rbert_vec_raw_768_739  rbert_vec_raw_768_740  \\\n",
       "0               0.212213              -0.343043               0.061037   \n",
       "1               0.285754              -0.357906               0.092024   \n",
       "2               0.187613              -0.395876               0.221800   \n",
       "3               0.279637              -0.325947               0.079062   \n",
       "4               0.183898              -0.343392              -0.020549   \n",
       "\n",
       "   rbert_vec_raw_768_741  rbert_vec_raw_768_742  rbert_vec_raw_768_743  \\\n",
       "0              -0.492675               0.200838               0.257117   \n",
       "1              -0.456410              -0.004129               0.173820   \n",
       "2              -0.433550               0.120441               0.187026   \n",
       "3              -0.525971               0.170618               0.157170   \n",
       "4              -0.484158               0.122724               0.116699   \n",
       "\n",
       "   rbert_vec_raw_768_744  rbert_vec_raw_768_745  rbert_vec_raw_768_746  \\\n",
       "0               0.267061               0.171412               0.213542   \n",
       "1               0.317651               0.008917               0.237783   \n",
       "2               0.264432               0.092117               0.180155   \n",
       "3               0.290191               0.047002               0.136171   \n",
       "4               0.138952              -0.018857               0.119898   \n",
       "\n",
       "   rbert_vec_raw_768_747  rbert_vec_raw_768_748  rbert_vec_raw_768_749  \\\n",
       "0              -0.076566              -0.004100              -0.816764   \n",
       "1              -0.184110               0.343919              -0.652336   \n",
       "2              -0.130565               0.163252              -0.637579   \n",
       "3              -0.088502               0.100978              -0.728087   \n",
       "4              -0.139858               0.189453              -0.814305   \n",
       "\n",
       "   rbert_vec_raw_768_750  rbert_vec_raw_768_751  rbert_vec_raw_768_752  \\\n",
       "0              -0.419798              -0.745892              -0.939142   \n",
       "1              -0.461633              -0.791143              -1.079459   \n",
       "2              -0.497798              -0.766127              -1.059294   \n",
       "3              -0.388375              -0.905938              -1.100813   \n",
       "4              -0.277532              -0.709534              -0.868354   \n",
       "\n",
       "   rbert_vec_raw_768_753  rbert_vec_raw_768_754  rbert_vec_raw_768_755  \\\n",
       "0               0.126276               0.821421               0.103536   \n",
       "1               0.053497               0.840875               0.125879   \n",
       "2               0.026800               0.750261               0.122869   \n",
       "3               0.069214               0.831719               0.062435   \n",
       "4              -0.000258               0.697891               0.114870   \n",
       "\n",
       "   rbert_vec_raw_768_756  rbert_vec_raw_768_757  rbert_vec_raw_768_758  \\\n",
       "0               0.198197              -0.251622              -0.188746   \n",
       "1               0.441573              -0.112534              -0.291659   \n",
       "2               0.234986              -0.112272              -0.137269   \n",
       "3               0.270292              -0.242251              -0.125205   \n",
       "4               0.314430              -0.150647              -0.155554   \n",
       "\n",
       "   rbert_vec_raw_768_759  rbert_vec_raw_768_760  rbert_vec_raw_768_761  \\\n",
       "0              -0.124210              -0.327753               0.266884   \n",
       "1               0.047304              -0.167982               0.166911   \n",
       "2              -0.013252              -0.276987               0.185075   \n",
       "3              -0.031401              -0.296945               0.219789   \n",
       "4               0.053789              -0.211846               0.175895   \n",
       "\n",
       "   rbert_vec_raw_768_762  rbert_vec_raw_768_763  rbert_vec_raw_768_764  \\\n",
       "0               0.058829               0.219104              -0.057867   \n",
       "1              -0.003861               0.399774               0.202404   \n",
       "2               0.054808               0.349297               0.028149   \n",
       "3               0.013650               0.231994               0.029659   \n",
       "4               0.066672               0.290528               0.094371   \n",
       "\n",
       "   rbert_vec_raw_768_765  rbert_vec_raw_768_766  rbert_vec_raw_768_767  \\\n",
       "0               0.124466              -0.132686              -0.003723   \n",
       "1               0.199137              -0.219354               0.079108   \n",
       "2               0.186575              -0.187830               0.138273   \n",
       "3               0.101472              -0.195010              -0.016275   \n",
       "4               0.129390              -0.113531               0.063748   \n",
       "\n",
       "   robaerta_proba_doi_cites  pred_doi_cites  fold_no  \\\n",
       "0                  3.026657        2.587398        4   \n",
       "1                  3.222497        2.955035        6   \n",
       "2                  3.109099        1.318706        2   \n",
       "3                  3.450085        4.212484        3   \n",
       "4                  2.378609        2.434969        3   \n",
       "\n",
       "   te_pred_doi_cites_doi_cites_count  te_pred_doi_cites_doi_cites_min  \\\n",
       "0                                  1                         2.197225   \n",
       "1                                  1                         4.812184   \n",
       "2                                  1                         1.945910   \n",
       "3                                  1                         3.555348   \n",
       "4                                  1                         2.995732   \n",
       "\n",
       "   te_pred_doi_cites_doi_cites_max  te_pred_doi_cites_doi_cites_std  \\\n",
       "0                         2.197225                              NaN   \n",
       "1                         4.812184                              NaN   \n",
       "2                         1.945910                              NaN   \n",
       "3                         3.555348                              NaN   \n",
       "4                         2.995732                              NaN   \n",
       "\n",
       "   te_pred_doi_cites_doi_cites_mean  te_pred_doi_cites_doi_cites_median  \\\n",
       "0                          2.197225                            2.197225   \n",
       "1                          4.812184                            4.812184   \n",
       "2                          1.945910                            1.945910   \n",
       "3                          3.555348                            3.555348   \n",
       "4                          2.995732                            2.995732   \n",
       "\n",
       "   diff_doi_cites_pred_doi_cites  rate_doi_cites_pred_doi_cites  \\\n",
       "0                       0.117783                       1.056642   \n",
       "1                      -0.429563                       0.918050   \n",
       "2                      -0.251314                       0.885622   \n",
       "3                       2.456736                       3.236217   \n",
       "4                       0.287682                       1.106232   \n",
       "\n",
       "   te_update_ym_cites_count  te_update_ym_cites_min  te_update_ym_cites_max  \\\n",
       "0                      1003                0.693147                6.297109   \n",
       "1                        71                0.693147                5.241747   \n",
       "2                        60                0.693147                6.633318   \n",
       "3                       791                0.693147                7.050989   \n",
       "4                        39                0.693147                5.746203   \n",
       "\n",
       "   te_update_ym_cites_std  te_update_ym_cites_mean  te_update_ym_cites_median  \\\n",
       "0                1.096367                 2.652902                   2.639057   \n",
       "1                1.204318                 2.729506                   2.772589   \n",
       "2                1.159934                 1.578621                   1.242453   \n",
       "3                1.209240                 2.534984                   2.484907   \n",
       "4                1.135174                 2.946943                   2.995732   \n",
       "\n",
       "   diff_doi_cites_update_ym  rate_doi_cites_update_ym  \\\n",
       "0                 -0.455678                  0.828234   \n",
       "1                  2.082679                  1.763024   \n",
       "2                  0.367289                  1.232664   \n",
       "3                  1.020364                  1.402513   \n",
       "4                  0.048789                  1.016556   \n",
       "\n",
       "   te_license_label_cites_count  te_license_label_cites_min  \\\n",
       "0                          8034                    0.693147   \n",
       "1                          8034                    0.693147   \n",
       "2                          8034                    0.693147   \n",
       "3                          3828                    0.693147   \n",
       "4                          3828                    0.693147   \n",
       "\n",
       "   te_license_label_cites_max  te_license_label_cites_std  \\\n",
       "0                    8.861067                    1.141813   \n",
       "1                    8.861067                    1.141813   \n",
       "2                    8.861067                    1.141813   \n",
       "3                    9.073833                    1.263901   \n",
       "4                    9.073833                    1.263901   \n",
       "\n",
       "   te_license_label_cites_mean  te_license_label_cites_median  \\\n",
       "0                     2.498216                       2.484907   \n",
       "1                     2.498216                       2.484907   \n",
       "2                     2.498216                       2.484907   \n",
       "3                     2.771316                       2.708050   \n",
       "4                     2.771316                       2.708050   \n",
       "\n",
       "   diff_doi_cites_license_label  rate_doi_cites_license_label  \\\n",
       "0                     -0.300991                      0.879518   \n",
       "1                      2.313969                      1.926249   \n",
       "2                     -0.552305                      0.778920   \n",
       "3                      0.784032                      1.282909   \n",
       "4                      0.224416                      1.080978   \n",
       "\n",
       "   te_pub_publisher_label_cites_count  te_pub_publisher_label_cites_min  \\\n",
       "0                                 416                          0.693147   \n",
       "1                                1441                          0.693147   \n",
       "2                                 147                          0.693147   \n",
       "3                                 417                          0.693147   \n",
       "4                                1157                          0.693147   \n",
       "\n",
       "   te_pub_publisher_label_cites_max  te_pub_publisher_label_cites_std  \\\n",
       "0                          6.186209                          1.045891   \n",
       "1                          8.861067                          1.215321   \n",
       "2                          6.904751                          1.458698   \n",
       "3                          9.073833                          1.256370   \n",
       "4                          6.860664                          1.100287   \n",
       "\n",
       "   te_pub_publisher_label_cites_mean  te_pub_publisher_label_cites_median  \\\n",
       "0                           2.343152                             2.302585   \n",
       "1                           2.726504                             2.708050   \n",
       "2                           3.335794                             3.367296   \n",
       "3                           3.338261                             3.332205   \n",
       "4                           2.427170                             2.302585   \n",
       "\n",
       "   diff_doi_cites_pub_publisher_label  rate_doi_cites_pub_publisher_label  \\\n",
       "0                           -0.145927                            0.937722   \n",
       "1                            2.085680                            1.764965   \n",
       "2                           -1.389884                            0.583342   \n",
       "3                            0.217087                            1.065030   \n",
       "4                            0.568562                            1.234249   \n",
       "\n",
       "   te_category_main_label_cites_count  te_category_main_label_cites_min  \\\n",
       "0                                2678                          0.693147   \n",
       "1                                2678                          0.693147   \n",
       "2                                 951                          0.693147   \n",
       "3                                2678                          0.693147   \n",
       "4                                1044                          0.693147   \n",
       "\n",
       "   te_category_main_label_cites_max  te_category_main_label_cites_std  \\\n",
       "0                          9.073833                          1.224161   \n",
       "1                          9.073833                          1.224161   \n",
       "2                          6.562444                          1.138468   \n",
       "3                          9.073833                          1.224161   \n",
       "4                          8.228978                          1.184952   \n",
       "\n",
       "   te_category_main_label_cites_mean  te_category_main_label_cites_median  \\\n",
       "0                           2.847509                             2.833213   \n",
       "1                           2.847509                             2.833213   \n",
       "2                           2.414652                             2.397895   \n",
       "3                           2.847509                             2.833213   \n",
       "4                           2.583916                             2.564949   \n",
       "\n",
       "   diff_doi_cites_category_main_label  rate_doi_cites_category_main_label  \\\n",
       "0                           -0.650284                            0.771630   \n",
       "1                            1.964676                            1.689963   \n",
       "2                           -0.468741                            0.805876   \n",
       "3                            0.707839                            1.248582   \n",
       "4                            0.411816                            1.159377   \n",
       "\n",
       "   te_pred_doi_cites_cites_count  te_pred_doi_cites_cites_min  \\\n",
       "0                              1                     2.079442   \n",
       "1                              1                     5.241747   \n",
       "2                              1                     2.197225   \n",
       "3                              1                     1.098612   \n",
       "4                              1                     2.708050   \n",
       "\n",
       "   te_pred_doi_cites_cites_max  te_pred_doi_cites_cites_std  \\\n",
       "0                     2.079442                          NaN   \n",
       "1                     5.241747                          NaN   \n",
       "2                     2.197225                          NaN   \n",
       "3                     1.098612                          NaN   \n",
       "4                     2.708050                          NaN   \n",
       "\n",
       "   te_pred_doi_cites_cites_mean  te_pred_doi_cites_cites_median  \n",
       "0                      2.079442                        2.079442  \n",
       "1                      5.241747                        5.241747  \n",
       "2                      2.197225                        2.197225  \n",
       "3                      1.098612                        1.098612  \n",
       "4                      2.708050                        2.708050  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>doi_cites</th>\n      <th>pub_journals</th>\n      <th>pub_dois</th>\n      <th>update_year</th>\n      <th>first_created_year</th>\n      <th>last_created_year</th>\n      <th>update_month</th>\n      <th>first_created_month</th>\n      <th>last_created_month</th>\n      <th>update_ym</th>\n      <th>first_created_ym</th>\n      <th>last_created_ym</th>\n      <th>update_day</th>\n      <th>first_created_day</th>\n      <th>last_created_day</th>\n      <th>update_date_unixtime</th>\n      <th>first_created_unixtime</th>\n      <th>last_created_unixtime</th>\n      <th>diff_update_date_unixtime</th>\n      <th>diff_created_unixtime</th>\n      <th>num_created</th>\n      <th>update_date_days</th>\n      <th>first_created_days</th>\n      <th>last_created_days</th>\n      <th>diff_created_days</th>\n      <th>rate_created_days</th>\n      <th>author_num</th>\n      <th>astro-ph.co</th>\n      <th>astro-ph.ep</th>\n      <th>astro-ph.ga</th>\n      <th>astro-ph.he</th>\n      <th>astro-ph.im</th>\n      <th>astro-ph.sr</th>\n      <th>cond-mat.dis-nn</th>\n      <th>cond-mat.mes-hall</th>\n      <th>cond-mat.mtrl-sci</th>\n      <th>cond-mat.other</th>\n      <th>cond-mat.quant-gas</th>\n      <th>cond-mat.soft</th>\n      <th>cond-mat.stat-mech</th>\n      <th>cond-mat.str-el</th>\n      <th>cond-mat.supr-con</th>\n      <th>cs.ai</th>\n      <th>cs.ar</th>\n      <th>cs.cc</th>\n      <th>cs.ce</th>\n      <th>cs.cg</th>\n      <th>cs.cl</th>\n      <th>cs.cr</th>\n      <th>cs.cv</th>\n      <th>cs.cy</th>\n      <th>cs.db</th>\n      <th>cs.dc</th>\n      <th>cs.dl</th>\n      <th>cs.dm</th>\n      <th>cs.ds</th>\n      <th>cs.et</th>\n      <th>cs.fl</th>\n      <th>cs.gl</th>\n      <th>cs.gr</th>\n      <th>cs.gt</th>\n      <th>cs.hc</th>\n      <th>cs.ir</th>\n      <th>cs.it</th>\n      <th>cs.lg</th>\n      <th>cs.lo</th>\n      <th>cs.ma</th>\n      <th>cs.mm</th>\n      <th>cs.ms</th>\n      <th>cs.na</th>\n      <th>cs.ne</th>\n      <th>cs.ni</th>\n      <th>cs.oh</th>\n      <th>cs.os</th>\n      <th>cs.pf</th>\n      <th>cs.pl</th>\n      <th>cs.ro</th>\n      <th>cs.sc</th>\n      <th>cs.sd</th>\n      <th>cs.se</th>\n      <th>cs.si</th>\n      <th>cs.sy</th>\n      <th>econ.em</th>\n      <th>econ.gn</th>\n      <th>econ.th</th>\n      <th>eess.as</th>\n      <th>eess.iv</th>\n      <th>eess.sp</th>\n      <th>eess.sy</th>\n      <th>math.ac</th>\n      <th>math.ag</th>\n      <th>math.ap</th>\n      <th>math.at</th>\n      <th>math.ca</th>\n      <th>math.co</th>\n      <th>math.ct</th>\n      <th>math.cv</th>\n      <th>math.dg</th>\n      <th>math.ds</th>\n      <th>math.fa</th>\n      <th>math.gm</th>\n      <th>math.gn</th>\n      <th>math.gr</th>\n      <th>math.gt</th>\n      <th>math.ho</th>\n      <th>math.it</th>\n      <th>math.kt</th>\n      <th>math.lo</th>\n      <th>math.mg</th>\n      <th>math.mp</th>\n      <th>math.na</th>\n      <th>math.nt</th>\n      <th>math.oa</th>\n      <th>math.oc</th>\n      <th>math.pr</th>\n      <th>math.qa</th>\n      <th>math.ra</th>\n      <th>math.rt</th>\n      <th>math.sg</th>\n      <th>math.sp</th>\n      <th>math.st</th>\n      <th>nlin.ao</th>\n      <th>nlin.cd</th>\n      <th>nlin.cg</th>\n      <th>nlin.ps</th>\n      <th>nlin.si</th>\n      <th>physics.acc-ph</th>\n      <th>physics.ao-ph</th>\n      <th>physics.app-ph</th>\n      <th>physics.atm-clus</th>\n      <th>physics.atom-ph</th>\n      <th>physics.bio-ph</th>\n      <th>physics.chem-ph</th>\n      <th>physics.class-ph</th>\n      <th>physics.comp-ph</th>\n      <th>physics.data-an</th>\n      <th>physics.ed-ph</th>\n      <th>physics.flu-dyn</th>\n      <th>physics.gen-ph</th>\n      <th>physics.geo-ph</th>\n      <th>physics.hist-ph</th>\n      <th>physics.ins-det</th>\n      <th>physics.med-ph</th>\n      <th>physics.optics</th>\n      <th>physics.plasm-ph</th>\n      <th>physics.pop-ph</th>\n      <th>physics.soc-ph</th>\n      <th>physics.space-ph</th>\n      <th>q-bio.bm</th>\n      <th>q-bio.cb</th>\n      <th>q-bio.gn</th>\n      <th>q-bio.mn</th>\n      <th>q-bio.nc</th>\n      <th>q-bio.ot</th>\n      <th>q-bio.pe</th>\n      <th>q-bio.qm</th>\n      <th>q-bio.sc</th>\n      <th>q-bio.to</th>\n      <th>q-fin.cp</th>\n      <th>q-fin.ec</th>\n      <th>q-fin.gn</th>\n      <th>q-fin.mf</th>\n      <th>q-fin.pm</th>\n      <th>q-fin.pr</th>\n      <th>q-fin.rm</th>\n      <th>q-fin.st</th>\n      <th>q-fin.tr</th>\n      <th>stat.ap</th>\n      <th>stat.co</th>\n      <th>stat.me</th>\n      <th>stat.ml</th>\n      <th>stat.ot</th>\n      <th>stat.th</th>\n      <th>acc-phys</th>\n      <th>adap-org</th>\n      <th>alg-geom</th>\n      <th>ao-sci</th>\n      <th>astro-ph</th>\n      <th>atom-ph</th>\n      <th>bayes-an</th>\n      <th>chao-dyn</th>\n      <th>chem-ph</th>\n      <th>cmp-lg</th>\n      <th>comp-gas</th>\n      <th>cond-mat</th>\n      <th>dg-ga</th>\n      <th>funct-an</th>\n      <th>gr-qc</th>\n      <th>hep-ex</th>\n      <th>hep-lat</th>\n      <th>hep-ph</th>\n      <th>hep-th</th>\n      <th>math-ph</th>\n      <th>mtrl-th</th>\n      <th>nucl-ex</th>\n      <th>nucl-th</th>\n      <th>patt-sol</th>\n      <th>plasm-ph</th>\n      <th>q-alg</th>\n      <th>q-bio</th>\n      <th>q-fin</th>\n      <th>quant-ph</th>\n      <th>solv-int</th>\n      <th>supr-con</th>\n      <th>acc</th>\n      <th>adap</th>\n      <th>alg</th>\n      <th>ao</th>\n      <th>astro</th>\n      <th>atom</th>\n      <th>bayes</th>\n      <th>chao</th>\n      <th>chem</th>\n      <th>cmp</th>\n      <th>comp</th>\n      <th>cond</th>\n      <th>cs</th>\n      <th>dg</th>\n      <th>econ</th>\n      <th>eess</th>\n      <th>funct</th>\n      <th>gr</th>\n      <th>hep</th>\n      <th>math</th>\n      <th>mtrl</th>\n      <th>nlin</th>\n      <th>nucl</th>\n      <th>patt</th>\n      <th>physics</th>\n      <th>plasm</th>\n      <th>q</th>\n      <th>quant</th>\n      <th>solv</th>\n      <th>stat</th>\n      <th>supr</th>\n      <th>submitter_label</th>\n      <th>doi_id_label</th>\n      <th>author_first_label</th>\n      <th>pub_publisher_label</th>\n      <th>license_label</th>\n      <th>category_main_label</th>\n      <th>doi_cites_mean_author_first_label</th>\n      <th>doi_cites_count_author_first_label</th>\n      <th>doi_cites_sum_author_first_label</th>\n      <th>doi_cites_min_author_first_label</th>\n      <th>doi_cites_max_author_first_label</th>\n      <th>doi_cites_median_author_first_label</th>\n      <th>doi_cites_std_author_first_label</th>\n      <th>doi_cites_q10_author_first_label</th>\n      <th>doi_cites_q25_author_first_label</th>\n      <th>doi_cites_q75_author_first_label</th>\n      <th>doi_cites_mean_doi_id_label</th>\n      <th>doi_cites_count_doi_id_label</th>\n      <th>doi_cites_sum_doi_id_label</th>\n      <th>doi_cites_min_doi_id_label</th>\n      <th>doi_cites_max_doi_id_label</th>\n      <th>doi_cites_median_doi_id_label</th>\n      <th>doi_cites_std_doi_id_label</th>\n      <th>doi_cites_q10_doi_id_label</th>\n      <th>doi_cites_q25_doi_id_label</th>\n      <th>doi_cites_q75_doi_id_label</th>\n      <th>doi_cites_mean_pub_publisher_label</th>\n      <th>doi_cites_count_pub_publisher_label</th>\n      <th>doi_cites_sum_pub_publisher_label</th>\n      <th>doi_cites_min_pub_publisher_label</th>\n      <th>doi_cites_max_pub_publisher_label</th>\n      <th>doi_cites_median_pub_publisher_label</th>\n      <th>doi_cites_std_pub_publisher_label</th>\n      <th>doi_cites_q10_pub_publisher_label</th>\n      <th>doi_cites_q25_pub_publisher_label</th>\n      <th>doi_cites_q75_pub_publisher_label</th>\n      <th>doi_cites_mean_submitter_label</th>\n      <th>doi_cites_count_submitter_label</th>\n      <th>doi_cites_sum_submitter_label</th>\n      <th>doi_cites_min_submitter_label</th>\n      <th>doi_cites_max_submitter_label</th>\n      <th>doi_cites_median_submitter_label</th>\n      <th>doi_cites_std_submitter_label</th>\n      <th>doi_cites_q10_submitter_label</th>\n      <th>doi_cites_q25_submitter_label</th>\n      <th>doi_cites_q75_submitter_label</th>\n      <th>doi_cites_mean_category_main_label</th>\n      <th>doi_cites_count_category_main_label</th>\n      <th>doi_cites_sum_category_main_label</th>\n      <th>doi_cites_min_category_main_label</th>\n      <th>doi_cites_max_category_main_label</th>\n      <th>doi_cites_median_category_main_label</th>\n      <th>doi_cites_std_category_main_label</th>\n      <th>doi_cites_q10_category_main_label</th>\n      <th>doi_cites_q25_category_main_label</th>\n      <th>doi_cites_q75_category_main_label</th>\n      <th>doi_cites_mean_update_ym</th>\n      <th>doi_cites_count_update_ym</th>\n      <th>doi_cites_sum_update_ym</th>\n      <th>doi_cites_min_update_ym</th>\n      <th>doi_cites_max_update_ym</th>\n      <th>doi_cites_median_update_ym</th>\n      <th>doi_cites_std_update_ym</th>\n      <th>doi_cites_q10_update_ym</th>\n      <th>doi_cites_q25_update_ym</th>\n      <th>doi_cites_q75_update_ym</th>\n      <th>doi_cites_mean_first_created_ym</th>\n      <th>doi_cites_count_first_created_ym</th>\n      <th>doi_cites_sum_first_created_ym</th>\n      <th>doi_cites_min_first_created_ym</th>\n      <th>doi_cites_max_first_created_ym</th>\n      <th>doi_cites_median_first_created_ym</th>\n      <th>doi_cites_std_first_created_ym</th>\n      <th>doi_cites_q10_first_created_ym</th>\n      <th>doi_cites_q25_first_created_ym</th>\n      <th>doi_cites_q75_first_created_ym</th>\n      <th>diff_doi_cites_doi_cites_mean_submitter_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_submitter_label</th>\n      <th>diff_doi_cites_doi_cites_mean_doi_id_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_doi_id_label</th>\n      <th>diff_doi_cites_doi_cites_mean_author_first_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_author_first_label</th>\n      <th>diff_doi_cites_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_pub_publisher_label</th>\n      <th>diff_doi_cites_doi_cites_mean_update_ym</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_update_ym</th>\n      <th>diff_doi_cites_doi_cites_mean_first_created_ym</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_first_created_ym</th>\n      <th>diff_doi_cites_doi_cites_mean_category_main_label</th>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_main_label</th>\n      <th>rbert_vec_raw_768_0</th>\n      <th>rbert_vec_raw_768_1</th>\n      <th>rbert_vec_raw_768_2</th>\n      <th>rbert_vec_raw_768_3</th>\n      <th>rbert_vec_raw_768_4</th>\n      <th>rbert_vec_raw_768_5</th>\n      <th>rbert_vec_raw_768_6</th>\n      <th>rbert_vec_raw_768_7</th>\n      <th>rbert_vec_raw_768_8</th>\n      <th>rbert_vec_raw_768_9</th>\n      <th>rbert_vec_raw_768_10</th>\n      <th>rbert_vec_raw_768_11</th>\n      <th>rbert_vec_raw_768_12</th>\n      <th>rbert_vec_raw_768_13</th>\n      <th>rbert_vec_raw_768_14</th>\n      <th>rbert_vec_raw_768_15</th>\n      <th>rbert_vec_raw_768_16</th>\n      <th>rbert_vec_raw_768_17</th>\n      <th>rbert_vec_raw_768_18</th>\n      <th>rbert_vec_raw_768_19</th>\n      <th>rbert_vec_raw_768_20</th>\n      <th>rbert_vec_raw_768_21</th>\n      <th>rbert_vec_raw_768_22</th>\n      <th>rbert_vec_raw_768_23</th>\n      <th>rbert_vec_raw_768_24</th>\n      <th>rbert_vec_raw_768_25</th>\n      <th>rbert_vec_raw_768_26</th>\n      <th>rbert_vec_raw_768_27</th>\n      <th>rbert_vec_raw_768_28</th>\n      <th>rbert_vec_raw_768_29</th>\n      <th>rbert_vec_raw_768_30</th>\n      <th>rbert_vec_raw_768_31</th>\n      <th>rbert_vec_raw_768_32</th>\n      <th>rbert_vec_raw_768_33</th>\n      <th>rbert_vec_raw_768_34</th>\n      <th>rbert_vec_raw_768_35</th>\n      <th>rbert_vec_raw_768_36</th>\n      <th>rbert_vec_raw_768_37</th>\n      <th>rbert_vec_raw_768_38</th>\n      <th>rbert_vec_raw_768_39</th>\n      <th>rbert_vec_raw_768_40</th>\n      <th>rbert_vec_raw_768_41</th>\n      <th>rbert_vec_raw_768_42</th>\n      <th>rbert_vec_raw_768_43</th>\n      <th>rbert_vec_raw_768_44</th>\n      <th>rbert_vec_raw_768_45</th>\n      <th>rbert_vec_raw_768_46</th>\n      <th>rbert_vec_raw_768_47</th>\n      <th>rbert_vec_raw_768_48</th>\n      <th>rbert_vec_raw_768_49</th>\n      <th>rbert_vec_raw_768_50</th>\n      <th>rbert_vec_raw_768_51</th>\n      <th>rbert_vec_raw_768_52</th>\n      <th>rbert_vec_raw_768_53</th>\n      <th>rbert_vec_raw_768_54</th>\n      <th>rbert_vec_raw_768_55</th>\n      <th>rbert_vec_raw_768_56</th>\n      <th>rbert_vec_raw_768_57</th>\n      <th>rbert_vec_raw_768_58</th>\n      <th>rbert_vec_raw_768_59</th>\n      <th>rbert_vec_raw_768_60</th>\n      <th>rbert_vec_raw_768_61</th>\n      <th>rbert_vec_raw_768_62</th>\n      <th>rbert_vec_raw_768_63</th>\n      <th>rbert_vec_raw_768_64</th>\n      <th>rbert_vec_raw_768_65</th>\n      <th>rbert_vec_raw_768_66</th>\n      <th>rbert_vec_raw_768_67</th>\n      <th>rbert_vec_raw_768_68</th>\n      <th>rbert_vec_raw_768_69</th>\n      <th>rbert_vec_raw_768_70</th>\n      <th>rbert_vec_raw_768_71</th>\n      <th>rbert_vec_raw_768_72</th>\n      <th>rbert_vec_raw_768_73</th>\n      <th>rbert_vec_raw_768_74</th>\n      <th>rbert_vec_raw_768_75</th>\n      <th>rbert_vec_raw_768_76</th>\n      <th>rbert_vec_raw_768_77</th>\n      <th>rbert_vec_raw_768_78</th>\n      <th>rbert_vec_raw_768_79</th>\n      <th>rbert_vec_raw_768_80</th>\n      <th>rbert_vec_raw_768_81</th>\n      <th>rbert_vec_raw_768_82</th>\n      <th>rbert_vec_raw_768_83</th>\n      <th>rbert_vec_raw_768_84</th>\n      <th>rbert_vec_raw_768_85</th>\n      <th>rbert_vec_raw_768_86</th>\n      <th>rbert_vec_raw_768_87</th>\n      <th>rbert_vec_raw_768_88</th>\n      <th>rbert_vec_raw_768_89</th>\n      <th>rbert_vec_raw_768_90</th>\n      <th>rbert_vec_raw_768_91</th>\n      <th>rbert_vec_raw_768_92</th>\n      <th>rbert_vec_raw_768_93</th>\n      <th>rbert_vec_raw_768_94</th>\n      <th>rbert_vec_raw_768_95</th>\n      <th>rbert_vec_raw_768_96</th>\n      <th>rbert_vec_raw_768_97</th>\n      <th>rbert_vec_raw_768_98</th>\n      <th>rbert_vec_raw_768_99</th>\n      <th>rbert_vec_raw_768_100</th>\n      <th>rbert_vec_raw_768_101</th>\n      <th>rbert_vec_raw_768_102</th>\n      <th>rbert_vec_raw_768_103</th>\n      <th>rbert_vec_raw_768_104</th>\n      <th>rbert_vec_raw_768_105</th>\n      <th>rbert_vec_raw_768_106</th>\n      <th>rbert_vec_raw_768_107</th>\n      <th>rbert_vec_raw_768_108</th>\n      <th>rbert_vec_raw_768_109</th>\n      <th>rbert_vec_raw_768_110</th>\n      <th>rbert_vec_raw_768_111</th>\n      <th>rbert_vec_raw_768_112</th>\n      <th>rbert_vec_raw_768_113</th>\n      <th>rbert_vec_raw_768_114</th>\n      <th>rbert_vec_raw_768_115</th>\n      <th>rbert_vec_raw_768_116</th>\n      <th>rbert_vec_raw_768_117</th>\n      <th>rbert_vec_raw_768_118</th>\n      <th>rbert_vec_raw_768_119</th>\n      <th>rbert_vec_raw_768_120</th>\n      <th>rbert_vec_raw_768_121</th>\n      <th>rbert_vec_raw_768_122</th>\n      <th>rbert_vec_raw_768_123</th>\n      <th>rbert_vec_raw_768_124</th>\n      <th>rbert_vec_raw_768_125</th>\n      <th>rbert_vec_raw_768_126</th>\n      <th>rbert_vec_raw_768_127</th>\n      <th>rbert_vec_raw_768_128</th>\n      <th>rbert_vec_raw_768_129</th>\n      <th>rbert_vec_raw_768_130</th>\n      <th>rbert_vec_raw_768_131</th>\n      <th>rbert_vec_raw_768_132</th>\n      <th>rbert_vec_raw_768_133</th>\n      <th>rbert_vec_raw_768_134</th>\n      <th>rbert_vec_raw_768_135</th>\n      <th>rbert_vec_raw_768_136</th>\n      <th>rbert_vec_raw_768_137</th>\n      <th>rbert_vec_raw_768_138</th>\n      <th>rbert_vec_raw_768_139</th>\n      <th>rbert_vec_raw_768_140</th>\n      <th>rbert_vec_raw_768_141</th>\n      <th>rbert_vec_raw_768_142</th>\n      <th>rbert_vec_raw_768_143</th>\n      <th>rbert_vec_raw_768_144</th>\n      <th>rbert_vec_raw_768_145</th>\n      <th>rbert_vec_raw_768_146</th>\n      <th>rbert_vec_raw_768_147</th>\n      <th>rbert_vec_raw_768_148</th>\n      <th>rbert_vec_raw_768_149</th>\n      <th>rbert_vec_raw_768_150</th>\n      <th>rbert_vec_raw_768_151</th>\n      <th>rbert_vec_raw_768_152</th>\n      <th>rbert_vec_raw_768_153</th>\n      <th>rbert_vec_raw_768_154</th>\n      <th>rbert_vec_raw_768_155</th>\n      <th>rbert_vec_raw_768_156</th>\n      <th>rbert_vec_raw_768_157</th>\n      <th>rbert_vec_raw_768_158</th>\n      <th>rbert_vec_raw_768_159</th>\n      <th>rbert_vec_raw_768_160</th>\n      <th>rbert_vec_raw_768_161</th>\n      <th>rbert_vec_raw_768_162</th>\n      <th>rbert_vec_raw_768_163</th>\n      <th>rbert_vec_raw_768_164</th>\n      <th>rbert_vec_raw_768_165</th>\n      <th>rbert_vec_raw_768_166</th>\n      <th>rbert_vec_raw_768_167</th>\n      <th>rbert_vec_raw_768_168</th>\n      <th>rbert_vec_raw_768_169</th>\n      <th>rbert_vec_raw_768_170</th>\n      <th>rbert_vec_raw_768_171</th>\n      <th>rbert_vec_raw_768_172</th>\n      <th>rbert_vec_raw_768_173</th>\n      <th>rbert_vec_raw_768_174</th>\n      <th>rbert_vec_raw_768_175</th>\n      <th>rbert_vec_raw_768_176</th>\n      <th>rbert_vec_raw_768_177</th>\n      <th>rbert_vec_raw_768_178</th>\n      <th>rbert_vec_raw_768_179</th>\n      <th>rbert_vec_raw_768_180</th>\n      <th>rbert_vec_raw_768_181</th>\n      <th>rbert_vec_raw_768_182</th>\n      <th>rbert_vec_raw_768_183</th>\n      <th>rbert_vec_raw_768_184</th>\n      <th>rbert_vec_raw_768_185</th>\n      <th>rbert_vec_raw_768_186</th>\n      <th>rbert_vec_raw_768_187</th>\n      <th>rbert_vec_raw_768_188</th>\n      <th>rbert_vec_raw_768_189</th>\n      <th>rbert_vec_raw_768_190</th>\n      <th>rbert_vec_raw_768_191</th>\n      <th>rbert_vec_raw_768_192</th>\n      <th>rbert_vec_raw_768_193</th>\n      <th>rbert_vec_raw_768_194</th>\n      <th>rbert_vec_raw_768_195</th>\n      <th>rbert_vec_raw_768_196</th>\n      <th>rbert_vec_raw_768_197</th>\n      <th>rbert_vec_raw_768_198</th>\n      <th>rbert_vec_raw_768_199</th>\n      <th>rbert_vec_raw_768_200</th>\n      <th>rbert_vec_raw_768_201</th>\n      <th>rbert_vec_raw_768_202</th>\n      <th>rbert_vec_raw_768_203</th>\n      <th>rbert_vec_raw_768_204</th>\n      <th>rbert_vec_raw_768_205</th>\n      <th>rbert_vec_raw_768_206</th>\n      <th>rbert_vec_raw_768_207</th>\n      <th>rbert_vec_raw_768_208</th>\n      <th>rbert_vec_raw_768_209</th>\n      <th>rbert_vec_raw_768_210</th>\n      <th>rbert_vec_raw_768_211</th>\n      <th>rbert_vec_raw_768_212</th>\n      <th>rbert_vec_raw_768_213</th>\n      <th>rbert_vec_raw_768_214</th>\n      <th>rbert_vec_raw_768_215</th>\n      <th>rbert_vec_raw_768_216</th>\n      <th>rbert_vec_raw_768_217</th>\n      <th>rbert_vec_raw_768_218</th>\n      <th>rbert_vec_raw_768_219</th>\n      <th>rbert_vec_raw_768_220</th>\n      <th>rbert_vec_raw_768_221</th>\n      <th>rbert_vec_raw_768_222</th>\n      <th>rbert_vec_raw_768_223</th>\n      <th>rbert_vec_raw_768_224</th>\n      <th>rbert_vec_raw_768_225</th>\n      <th>rbert_vec_raw_768_226</th>\n      <th>rbert_vec_raw_768_227</th>\n      <th>rbert_vec_raw_768_228</th>\n      <th>rbert_vec_raw_768_229</th>\n      <th>rbert_vec_raw_768_230</th>\n      <th>rbert_vec_raw_768_231</th>\n      <th>rbert_vec_raw_768_232</th>\n      <th>rbert_vec_raw_768_233</th>\n      <th>rbert_vec_raw_768_234</th>\n      <th>rbert_vec_raw_768_235</th>\n      <th>rbert_vec_raw_768_236</th>\n      <th>rbert_vec_raw_768_237</th>\n      <th>rbert_vec_raw_768_238</th>\n      <th>rbert_vec_raw_768_239</th>\n      <th>rbert_vec_raw_768_240</th>\n      <th>rbert_vec_raw_768_241</th>\n      <th>rbert_vec_raw_768_242</th>\n      <th>rbert_vec_raw_768_243</th>\n      <th>rbert_vec_raw_768_244</th>\n      <th>rbert_vec_raw_768_245</th>\n      <th>rbert_vec_raw_768_246</th>\n      <th>rbert_vec_raw_768_247</th>\n      <th>rbert_vec_raw_768_248</th>\n      <th>rbert_vec_raw_768_249</th>\n      <th>rbert_vec_raw_768_250</th>\n      <th>rbert_vec_raw_768_251</th>\n      <th>rbert_vec_raw_768_252</th>\n      <th>rbert_vec_raw_768_253</th>\n      <th>rbert_vec_raw_768_254</th>\n      <th>rbert_vec_raw_768_255</th>\n      <th>rbert_vec_raw_768_256</th>\n      <th>rbert_vec_raw_768_257</th>\n      <th>rbert_vec_raw_768_258</th>\n      <th>rbert_vec_raw_768_259</th>\n      <th>rbert_vec_raw_768_260</th>\n      <th>rbert_vec_raw_768_261</th>\n      <th>rbert_vec_raw_768_262</th>\n      <th>rbert_vec_raw_768_263</th>\n      <th>rbert_vec_raw_768_264</th>\n      <th>rbert_vec_raw_768_265</th>\n      <th>rbert_vec_raw_768_266</th>\n      <th>rbert_vec_raw_768_267</th>\n      <th>rbert_vec_raw_768_268</th>\n      <th>rbert_vec_raw_768_269</th>\n      <th>rbert_vec_raw_768_270</th>\n      <th>rbert_vec_raw_768_271</th>\n      <th>rbert_vec_raw_768_272</th>\n      <th>rbert_vec_raw_768_273</th>\n      <th>rbert_vec_raw_768_274</th>\n      <th>rbert_vec_raw_768_275</th>\n      <th>rbert_vec_raw_768_276</th>\n      <th>rbert_vec_raw_768_277</th>\n      <th>rbert_vec_raw_768_278</th>\n      <th>rbert_vec_raw_768_279</th>\n      <th>rbert_vec_raw_768_280</th>\n      <th>rbert_vec_raw_768_281</th>\n      <th>rbert_vec_raw_768_282</th>\n      <th>rbert_vec_raw_768_283</th>\n      <th>rbert_vec_raw_768_284</th>\n      <th>rbert_vec_raw_768_285</th>\n      <th>rbert_vec_raw_768_286</th>\n      <th>rbert_vec_raw_768_287</th>\n      <th>rbert_vec_raw_768_288</th>\n      <th>rbert_vec_raw_768_289</th>\n      <th>rbert_vec_raw_768_290</th>\n      <th>rbert_vec_raw_768_291</th>\n      <th>rbert_vec_raw_768_292</th>\n      <th>rbert_vec_raw_768_293</th>\n      <th>rbert_vec_raw_768_294</th>\n      <th>rbert_vec_raw_768_295</th>\n      <th>rbert_vec_raw_768_296</th>\n      <th>rbert_vec_raw_768_297</th>\n      <th>rbert_vec_raw_768_298</th>\n      <th>rbert_vec_raw_768_299</th>\n      <th>rbert_vec_raw_768_300</th>\n      <th>rbert_vec_raw_768_301</th>\n      <th>rbert_vec_raw_768_302</th>\n      <th>rbert_vec_raw_768_303</th>\n      <th>rbert_vec_raw_768_304</th>\n      <th>rbert_vec_raw_768_305</th>\n      <th>rbert_vec_raw_768_306</th>\n      <th>rbert_vec_raw_768_307</th>\n      <th>rbert_vec_raw_768_308</th>\n      <th>rbert_vec_raw_768_309</th>\n      <th>rbert_vec_raw_768_310</th>\n      <th>rbert_vec_raw_768_311</th>\n      <th>rbert_vec_raw_768_312</th>\n      <th>rbert_vec_raw_768_313</th>\n      <th>rbert_vec_raw_768_314</th>\n      <th>rbert_vec_raw_768_315</th>\n      <th>rbert_vec_raw_768_316</th>\n      <th>rbert_vec_raw_768_317</th>\n      <th>rbert_vec_raw_768_318</th>\n      <th>rbert_vec_raw_768_319</th>\n      <th>rbert_vec_raw_768_320</th>\n      <th>rbert_vec_raw_768_321</th>\n      <th>rbert_vec_raw_768_322</th>\n      <th>rbert_vec_raw_768_323</th>\n      <th>rbert_vec_raw_768_324</th>\n      <th>rbert_vec_raw_768_325</th>\n      <th>rbert_vec_raw_768_326</th>\n      <th>rbert_vec_raw_768_327</th>\n      <th>rbert_vec_raw_768_328</th>\n      <th>rbert_vec_raw_768_329</th>\n      <th>rbert_vec_raw_768_330</th>\n      <th>rbert_vec_raw_768_331</th>\n      <th>rbert_vec_raw_768_332</th>\n      <th>rbert_vec_raw_768_333</th>\n      <th>rbert_vec_raw_768_334</th>\n      <th>rbert_vec_raw_768_335</th>\n      <th>rbert_vec_raw_768_336</th>\n      <th>rbert_vec_raw_768_337</th>\n      <th>rbert_vec_raw_768_338</th>\n      <th>rbert_vec_raw_768_339</th>\n      <th>rbert_vec_raw_768_340</th>\n      <th>rbert_vec_raw_768_341</th>\n      <th>rbert_vec_raw_768_342</th>\n      <th>rbert_vec_raw_768_343</th>\n      <th>rbert_vec_raw_768_344</th>\n      <th>rbert_vec_raw_768_345</th>\n      <th>rbert_vec_raw_768_346</th>\n      <th>rbert_vec_raw_768_347</th>\n      <th>rbert_vec_raw_768_348</th>\n      <th>rbert_vec_raw_768_349</th>\n      <th>rbert_vec_raw_768_350</th>\n      <th>rbert_vec_raw_768_351</th>\n      <th>rbert_vec_raw_768_352</th>\n      <th>rbert_vec_raw_768_353</th>\n      <th>rbert_vec_raw_768_354</th>\n      <th>rbert_vec_raw_768_355</th>\n      <th>rbert_vec_raw_768_356</th>\n      <th>rbert_vec_raw_768_357</th>\n      <th>rbert_vec_raw_768_358</th>\n      <th>rbert_vec_raw_768_359</th>\n      <th>rbert_vec_raw_768_360</th>\n      <th>rbert_vec_raw_768_361</th>\n      <th>rbert_vec_raw_768_362</th>\n      <th>rbert_vec_raw_768_363</th>\n      <th>rbert_vec_raw_768_364</th>\n      <th>rbert_vec_raw_768_365</th>\n      <th>rbert_vec_raw_768_366</th>\n      <th>rbert_vec_raw_768_367</th>\n      <th>rbert_vec_raw_768_368</th>\n      <th>rbert_vec_raw_768_369</th>\n      <th>rbert_vec_raw_768_370</th>\n      <th>rbert_vec_raw_768_371</th>\n      <th>rbert_vec_raw_768_372</th>\n      <th>rbert_vec_raw_768_373</th>\n      <th>rbert_vec_raw_768_374</th>\n      <th>rbert_vec_raw_768_375</th>\n      <th>rbert_vec_raw_768_376</th>\n      <th>rbert_vec_raw_768_377</th>\n      <th>rbert_vec_raw_768_378</th>\n      <th>rbert_vec_raw_768_379</th>\n      <th>rbert_vec_raw_768_380</th>\n      <th>rbert_vec_raw_768_381</th>\n      <th>rbert_vec_raw_768_382</th>\n      <th>rbert_vec_raw_768_383</th>\n      <th>rbert_vec_raw_768_384</th>\n      <th>rbert_vec_raw_768_385</th>\n      <th>rbert_vec_raw_768_386</th>\n      <th>rbert_vec_raw_768_387</th>\n      <th>rbert_vec_raw_768_388</th>\n      <th>rbert_vec_raw_768_389</th>\n      <th>rbert_vec_raw_768_390</th>\n      <th>rbert_vec_raw_768_391</th>\n      <th>rbert_vec_raw_768_392</th>\n      <th>rbert_vec_raw_768_393</th>\n      <th>rbert_vec_raw_768_394</th>\n      <th>rbert_vec_raw_768_395</th>\n      <th>rbert_vec_raw_768_396</th>\n      <th>rbert_vec_raw_768_397</th>\n      <th>rbert_vec_raw_768_398</th>\n      <th>rbert_vec_raw_768_399</th>\n      <th>rbert_vec_raw_768_400</th>\n      <th>rbert_vec_raw_768_401</th>\n      <th>rbert_vec_raw_768_402</th>\n      <th>rbert_vec_raw_768_403</th>\n      <th>rbert_vec_raw_768_404</th>\n      <th>rbert_vec_raw_768_405</th>\n      <th>rbert_vec_raw_768_406</th>\n      <th>rbert_vec_raw_768_407</th>\n      <th>rbert_vec_raw_768_408</th>\n      <th>rbert_vec_raw_768_409</th>\n      <th>rbert_vec_raw_768_410</th>\n      <th>rbert_vec_raw_768_411</th>\n      <th>rbert_vec_raw_768_412</th>\n      <th>rbert_vec_raw_768_413</th>\n      <th>rbert_vec_raw_768_414</th>\n      <th>rbert_vec_raw_768_415</th>\n      <th>rbert_vec_raw_768_416</th>\n      <th>rbert_vec_raw_768_417</th>\n      <th>rbert_vec_raw_768_418</th>\n      <th>rbert_vec_raw_768_419</th>\n      <th>rbert_vec_raw_768_420</th>\n      <th>rbert_vec_raw_768_421</th>\n      <th>rbert_vec_raw_768_422</th>\n      <th>rbert_vec_raw_768_423</th>\n      <th>rbert_vec_raw_768_424</th>\n      <th>rbert_vec_raw_768_425</th>\n      <th>rbert_vec_raw_768_426</th>\n      <th>rbert_vec_raw_768_427</th>\n      <th>rbert_vec_raw_768_428</th>\n      <th>rbert_vec_raw_768_429</th>\n      <th>rbert_vec_raw_768_430</th>\n      <th>rbert_vec_raw_768_431</th>\n      <th>rbert_vec_raw_768_432</th>\n      <th>rbert_vec_raw_768_433</th>\n      <th>rbert_vec_raw_768_434</th>\n      <th>rbert_vec_raw_768_435</th>\n      <th>rbert_vec_raw_768_436</th>\n      <th>rbert_vec_raw_768_437</th>\n      <th>rbert_vec_raw_768_438</th>\n      <th>rbert_vec_raw_768_439</th>\n      <th>rbert_vec_raw_768_440</th>\n      <th>rbert_vec_raw_768_441</th>\n      <th>rbert_vec_raw_768_442</th>\n      <th>rbert_vec_raw_768_443</th>\n      <th>rbert_vec_raw_768_444</th>\n      <th>rbert_vec_raw_768_445</th>\n      <th>rbert_vec_raw_768_446</th>\n      <th>rbert_vec_raw_768_447</th>\n      <th>rbert_vec_raw_768_448</th>\n      <th>rbert_vec_raw_768_449</th>\n      <th>rbert_vec_raw_768_450</th>\n      <th>rbert_vec_raw_768_451</th>\n      <th>rbert_vec_raw_768_452</th>\n      <th>rbert_vec_raw_768_453</th>\n      <th>rbert_vec_raw_768_454</th>\n      <th>rbert_vec_raw_768_455</th>\n      <th>rbert_vec_raw_768_456</th>\n      <th>rbert_vec_raw_768_457</th>\n      <th>rbert_vec_raw_768_458</th>\n      <th>rbert_vec_raw_768_459</th>\n      <th>rbert_vec_raw_768_460</th>\n      <th>rbert_vec_raw_768_461</th>\n      <th>rbert_vec_raw_768_462</th>\n      <th>rbert_vec_raw_768_463</th>\n      <th>rbert_vec_raw_768_464</th>\n      <th>rbert_vec_raw_768_465</th>\n      <th>rbert_vec_raw_768_466</th>\n      <th>rbert_vec_raw_768_467</th>\n      <th>rbert_vec_raw_768_468</th>\n      <th>rbert_vec_raw_768_469</th>\n      <th>rbert_vec_raw_768_470</th>\n      <th>rbert_vec_raw_768_471</th>\n      <th>rbert_vec_raw_768_472</th>\n      <th>rbert_vec_raw_768_473</th>\n      <th>rbert_vec_raw_768_474</th>\n      <th>rbert_vec_raw_768_475</th>\n      <th>rbert_vec_raw_768_476</th>\n      <th>rbert_vec_raw_768_477</th>\n      <th>rbert_vec_raw_768_478</th>\n      <th>rbert_vec_raw_768_479</th>\n      <th>rbert_vec_raw_768_480</th>\n      <th>rbert_vec_raw_768_481</th>\n      <th>rbert_vec_raw_768_482</th>\n      <th>rbert_vec_raw_768_483</th>\n      <th>rbert_vec_raw_768_484</th>\n      <th>rbert_vec_raw_768_485</th>\n      <th>rbert_vec_raw_768_486</th>\n      <th>rbert_vec_raw_768_487</th>\n      <th>rbert_vec_raw_768_488</th>\n      <th>rbert_vec_raw_768_489</th>\n      <th>rbert_vec_raw_768_490</th>\n      <th>rbert_vec_raw_768_491</th>\n      <th>rbert_vec_raw_768_492</th>\n      <th>rbert_vec_raw_768_493</th>\n      <th>rbert_vec_raw_768_494</th>\n      <th>rbert_vec_raw_768_495</th>\n      <th>rbert_vec_raw_768_496</th>\n      <th>rbert_vec_raw_768_497</th>\n      <th>rbert_vec_raw_768_498</th>\n      <th>rbert_vec_raw_768_499</th>\n      <th>rbert_vec_raw_768_500</th>\n      <th>rbert_vec_raw_768_501</th>\n      <th>rbert_vec_raw_768_502</th>\n      <th>rbert_vec_raw_768_503</th>\n      <th>rbert_vec_raw_768_504</th>\n      <th>rbert_vec_raw_768_505</th>\n      <th>rbert_vec_raw_768_506</th>\n      <th>rbert_vec_raw_768_507</th>\n      <th>rbert_vec_raw_768_508</th>\n      <th>rbert_vec_raw_768_509</th>\n      <th>rbert_vec_raw_768_510</th>\n      <th>rbert_vec_raw_768_511</th>\n      <th>rbert_vec_raw_768_512</th>\n      <th>rbert_vec_raw_768_513</th>\n      <th>rbert_vec_raw_768_514</th>\n      <th>rbert_vec_raw_768_515</th>\n      <th>rbert_vec_raw_768_516</th>\n      <th>rbert_vec_raw_768_517</th>\n      <th>rbert_vec_raw_768_518</th>\n      <th>rbert_vec_raw_768_519</th>\n      <th>rbert_vec_raw_768_520</th>\n      <th>rbert_vec_raw_768_521</th>\n      <th>rbert_vec_raw_768_522</th>\n      <th>rbert_vec_raw_768_523</th>\n      <th>rbert_vec_raw_768_524</th>\n      <th>rbert_vec_raw_768_525</th>\n      <th>rbert_vec_raw_768_526</th>\n      <th>rbert_vec_raw_768_527</th>\n      <th>rbert_vec_raw_768_528</th>\n      <th>rbert_vec_raw_768_529</th>\n      <th>rbert_vec_raw_768_530</th>\n      <th>rbert_vec_raw_768_531</th>\n      <th>rbert_vec_raw_768_532</th>\n      <th>rbert_vec_raw_768_533</th>\n      <th>rbert_vec_raw_768_534</th>\n      <th>rbert_vec_raw_768_535</th>\n      <th>rbert_vec_raw_768_536</th>\n      <th>rbert_vec_raw_768_537</th>\n      <th>rbert_vec_raw_768_538</th>\n      <th>rbert_vec_raw_768_539</th>\n      <th>rbert_vec_raw_768_540</th>\n      <th>rbert_vec_raw_768_541</th>\n      <th>rbert_vec_raw_768_542</th>\n      <th>rbert_vec_raw_768_543</th>\n      <th>rbert_vec_raw_768_544</th>\n      <th>rbert_vec_raw_768_545</th>\n      <th>rbert_vec_raw_768_546</th>\n      <th>rbert_vec_raw_768_547</th>\n      <th>rbert_vec_raw_768_548</th>\n      <th>rbert_vec_raw_768_549</th>\n      <th>rbert_vec_raw_768_550</th>\n      <th>rbert_vec_raw_768_551</th>\n      <th>rbert_vec_raw_768_552</th>\n      <th>rbert_vec_raw_768_553</th>\n      <th>rbert_vec_raw_768_554</th>\n      <th>rbert_vec_raw_768_555</th>\n      <th>rbert_vec_raw_768_556</th>\n      <th>rbert_vec_raw_768_557</th>\n      <th>rbert_vec_raw_768_558</th>\n      <th>rbert_vec_raw_768_559</th>\n      <th>rbert_vec_raw_768_560</th>\n      <th>rbert_vec_raw_768_561</th>\n      <th>rbert_vec_raw_768_562</th>\n      <th>rbert_vec_raw_768_563</th>\n      <th>rbert_vec_raw_768_564</th>\n      <th>rbert_vec_raw_768_565</th>\n      <th>rbert_vec_raw_768_566</th>\n      <th>rbert_vec_raw_768_567</th>\n      <th>rbert_vec_raw_768_568</th>\n      <th>rbert_vec_raw_768_569</th>\n      <th>rbert_vec_raw_768_570</th>\n      <th>rbert_vec_raw_768_571</th>\n      <th>rbert_vec_raw_768_572</th>\n      <th>rbert_vec_raw_768_573</th>\n      <th>rbert_vec_raw_768_574</th>\n      <th>rbert_vec_raw_768_575</th>\n      <th>rbert_vec_raw_768_576</th>\n      <th>rbert_vec_raw_768_577</th>\n      <th>rbert_vec_raw_768_578</th>\n      <th>rbert_vec_raw_768_579</th>\n      <th>rbert_vec_raw_768_580</th>\n      <th>rbert_vec_raw_768_581</th>\n      <th>rbert_vec_raw_768_582</th>\n      <th>rbert_vec_raw_768_583</th>\n      <th>rbert_vec_raw_768_584</th>\n      <th>rbert_vec_raw_768_585</th>\n      <th>rbert_vec_raw_768_586</th>\n      <th>rbert_vec_raw_768_587</th>\n      <th>rbert_vec_raw_768_588</th>\n      <th>rbert_vec_raw_768_589</th>\n      <th>rbert_vec_raw_768_590</th>\n      <th>rbert_vec_raw_768_591</th>\n      <th>rbert_vec_raw_768_592</th>\n      <th>rbert_vec_raw_768_593</th>\n      <th>rbert_vec_raw_768_594</th>\n      <th>rbert_vec_raw_768_595</th>\n      <th>rbert_vec_raw_768_596</th>\n      <th>rbert_vec_raw_768_597</th>\n      <th>rbert_vec_raw_768_598</th>\n      <th>rbert_vec_raw_768_599</th>\n      <th>rbert_vec_raw_768_600</th>\n      <th>rbert_vec_raw_768_601</th>\n      <th>rbert_vec_raw_768_602</th>\n      <th>rbert_vec_raw_768_603</th>\n      <th>rbert_vec_raw_768_604</th>\n      <th>rbert_vec_raw_768_605</th>\n      <th>rbert_vec_raw_768_606</th>\n      <th>rbert_vec_raw_768_607</th>\n      <th>rbert_vec_raw_768_608</th>\n      <th>rbert_vec_raw_768_609</th>\n      <th>rbert_vec_raw_768_610</th>\n      <th>rbert_vec_raw_768_611</th>\n      <th>rbert_vec_raw_768_612</th>\n      <th>rbert_vec_raw_768_613</th>\n      <th>rbert_vec_raw_768_614</th>\n      <th>rbert_vec_raw_768_615</th>\n      <th>rbert_vec_raw_768_616</th>\n      <th>rbert_vec_raw_768_617</th>\n      <th>rbert_vec_raw_768_618</th>\n      <th>rbert_vec_raw_768_619</th>\n      <th>rbert_vec_raw_768_620</th>\n      <th>rbert_vec_raw_768_621</th>\n      <th>rbert_vec_raw_768_622</th>\n      <th>rbert_vec_raw_768_623</th>\n      <th>rbert_vec_raw_768_624</th>\n      <th>rbert_vec_raw_768_625</th>\n      <th>rbert_vec_raw_768_626</th>\n      <th>rbert_vec_raw_768_627</th>\n      <th>rbert_vec_raw_768_628</th>\n      <th>rbert_vec_raw_768_629</th>\n      <th>rbert_vec_raw_768_630</th>\n      <th>rbert_vec_raw_768_631</th>\n      <th>rbert_vec_raw_768_632</th>\n      <th>rbert_vec_raw_768_633</th>\n      <th>rbert_vec_raw_768_634</th>\n      <th>rbert_vec_raw_768_635</th>\n      <th>rbert_vec_raw_768_636</th>\n      <th>rbert_vec_raw_768_637</th>\n      <th>rbert_vec_raw_768_638</th>\n      <th>rbert_vec_raw_768_639</th>\n      <th>rbert_vec_raw_768_640</th>\n      <th>rbert_vec_raw_768_641</th>\n      <th>rbert_vec_raw_768_642</th>\n      <th>rbert_vec_raw_768_643</th>\n      <th>rbert_vec_raw_768_644</th>\n      <th>rbert_vec_raw_768_645</th>\n      <th>rbert_vec_raw_768_646</th>\n      <th>rbert_vec_raw_768_647</th>\n      <th>rbert_vec_raw_768_648</th>\n      <th>rbert_vec_raw_768_649</th>\n      <th>rbert_vec_raw_768_650</th>\n      <th>rbert_vec_raw_768_651</th>\n      <th>rbert_vec_raw_768_652</th>\n      <th>rbert_vec_raw_768_653</th>\n      <th>rbert_vec_raw_768_654</th>\n      <th>rbert_vec_raw_768_655</th>\n      <th>rbert_vec_raw_768_656</th>\n      <th>rbert_vec_raw_768_657</th>\n      <th>rbert_vec_raw_768_658</th>\n      <th>rbert_vec_raw_768_659</th>\n      <th>rbert_vec_raw_768_660</th>\n      <th>rbert_vec_raw_768_661</th>\n      <th>rbert_vec_raw_768_662</th>\n      <th>rbert_vec_raw_768_663</th>\n      <th>rbert_vec_raw_768_664</th>\n      <th>rbert_vec_raw_768_665</th>\n      <th>rbert_vec_raw_768_666</th>\n      <th>rbert_vec_raw_768_667</th>\n      <th>rbert_vec_raw_768_668</th>\n      <th>rbert_vec_raw_768_669</th>\n      <th>rbert_vec_raw_768_670</th>\n      <th>rbert_vec_raw_768_671</th>\n      <th>rbert_vec_raw_768_672</th>\n      <th>rbert_vec_raw_768_673</th>\n      <th>rbert_vec_raw_768_674</th>\n      <th>rbert_vec_raw_768_675</th>\n      <th>rbert_vec_raw_768_676</th>\n      <th>rbert_vec_raw_768_677</th>\n      <th>rbert_vec_raw_768_678</th>\n      <th>rbert_vec_raw_768_679</th>\n      <th>rbert_vec_raw_768_680</th>\n      <th>rbert_vec_raw_768_681</th>\n      <th>rbert_vec_raw_768_682</th>\n      <th>rbert_vec_raw_768_683</th>\n      <th>rbert_vec_raw_768_684</th>\n      <th>rbert_vec_raw_768_685</th>\n      <th>rbert_vec_raw_768_686</th>\n      <th>rbert_vec_raw_768_687</th>\n      <th>rbert_vec_raw_768_688</th>\n      <th>rbert_vec_raw_768_689</th>\n      <th>rbert_vec_raw_768_690</th>\n      <th>rbert_vec_raw_768_691</th>\n      <th>rbert_vec_raw_768_692</th>\n      <th>rbert_vec_raw_768_693</th>\n      <th>rbert_vec_raw_768_694</th>\n      <th>rbert_vec_raw_768_695</th>\n      <th>rbert_vec_raw_768_696</th>\n      <th>rbert_vec_raw_768_697</th>\n      <th>rbert_vec_raw_768_698</th>\n      <th>rbert_vec_raw_768_699</th>\n      <th>rbert_vec_raw_768_700</th>\n      <th>rbert_vec_raw_768_701</th>\n      <th>rbert_vec_raw_768_702</th>\n      <th>rbert_vec_raw_768_703</th>\n      <th>rbert_vec_raw_768_704</th>\n      <th>rbert_vec_raw_768_705</th>\n      <th>rbert_vec_raw_768_706</th>\n      <th>rbert_vec_raw_768_707</th>\n      <th>rbert_vec_raw_768_708</th>\n      <th>rbert_vec_raw_768_709</th>\n      <th>rbert_vec_raw_768_710</th>\n      <th>rbert_vec_raw_768_711</th>\n      <th>rbert_vec_raw_768_712</th>\n      <th>rbert_vec_raw_768_713</th>\n      <th>rbert_vec_raw_768_714</th>\n      <th>rbert_vec_raw_768_715</th>\n      <th>rbert_vec_raw_768_716</th>\n      <th>rbert_vec_raw_768_717</th>\n      <th>rbert_vec_raw_768_718</th>\n      <th>rbert_vec_raw_768_719</th>\n      <th>rbert_vec_raw_768_720</th>\n      <th>rbert_vec_raw_768_721</th>\n      <th>rbert_vec_raw_768_722</th>\n      <th>rbert_vec_raw_768_723</th>\n      <th>rbert_vec_raw_768_724</th>\n      <th>rbert_vec_raw_768_725</th>\n      <th>rbert_vec_raw_768_726</th>\n      <th>rbert_vec_raw_768_727</th>\n      <th>rbert_vec_raw_768_728</th>\n      <th>rbert_vec_raw_768_729</th>\n      <th>rbert_vec_raw_768_730</th>\n      <th>rbert_vec_raw_768_731</th>\n      <th>rbert_vec_raw_768_732</th>\n      <th>rbert_vec_raw_768_733</th>\n      <th>rbert_vec_raw_768_734</th>\n      <th>rbert_vec_raw_768_735</th>\n      <th>rbert_vec_raw_768_736</th>\n      <th>rbert_vec_raw_768_737</th>\n      <th>rbert_vec_raw_768_738</th>\n      <th>rbert_vec_raw_768_739</th>\n      <th>rbert_vec_raw_768_740</th>\n      <th>rbert_vec_raw_768_741</th>\n      <th>rbert_vec_raw_768_742</th>\n      <th>rbert_vec_raw_768_743</th>\n      <th>rbert_vec_raw_768_744</th>\n      <th>rbert_vec_raw_768_745</th>\n      <th>rbert_vec_raw_768_746</th>\n      <th>rbert_vec_raw_768_747</th>\n      <th>rbert_vec_raw_768_748</th>\n      <th>rbert_vec_raw_768_749</th>\n      <th>rbert_vec_raw_768_750</th>\n      <th>rbert_vec_raw_768_751</th>\n      <th>rbert_vec_raw_768_752</th>\n      <th>rbert_vec_raw_768_753</th>\n      <th>rbert_vec_raw_768_754</th>\n      <th>rbert_vec_raw_768_755</th>\n      <th>rbert_vec_raw_768_756</th>\n      <th>rbert_vec_raw_768_757</th>\n      <th>rbert_vec_raw_768_758</th>\n      <th>rbert_vec_raw_768_759</th>\n      <th>rbert_vec_raw_768_760</th>\n      <th>rbert_vec_raw_768_761</th>\n      <th>rbert_vec_raw_768_762</th>\n      <th>rbert_vec_raw_768_763</th>\n      <th>rbert_vec_raw_768_764</th>\n      <th>rbert_vec_raw_768_765</th>\n      <th>rbert_vec_raw_768_766</th>\n      <th>rbert_vec_raw_768_767</th>\n      <th>robaerta_proba_doi_cites</th>\n      <th>pred_doi_cites</th>\n      <th>fold_no</th>\n      <th>te_pred_doi_cites_doi_cites_count</th>\n      <th>te_pred_doi_cites_doi_cites_min</th>\n      <th>te_pred_doi_cites_doi_cites_max</th>\n      <th>te_pred_doi_cites_doi_cites_std</th>\n      <th>te_pred_doi_cites_doi_cites_mean</th>\n      <th>te_pred_doi_cites_doi_cites_median</th>\n      <th>diff_doi_cites_pred_doi_cites</th>\n      <th>rate_doi_cites_pred_doi_cites</th>\n      <th>te_update_ym_cites_count</th>\n      <th>te_update_ym_cites_min</th>\n      <th>te_update_ym_cites_max</th>\n      <th>te_update_ym_cites_std</th>\n      <th>te_update_ym_cites_mean</th>\n      <th>te_update_ym_cites_median</th>\n      <th>diff_doi_cites_update_ym</th>\n      <th>rate_doi_cites_update_ym</th>\n      <th>te_license_label_cites_count</th>\n      <th>te_license_label_cites_min</th>\n      <th>te_license_label_cites_max</th>\n      <th>te_license_label_cites_std</th>\n      <th>te_license_label_cites_mean</th>\n      <th>te_license_label_cites_median</th>\n      <th>diff_doi_cites_license_label</th>\n      <th>rate_doi_cites_license_label</th>\n      <th>te_pub_publisher_label_cites_count</th>\n      <th>te_pub_publisher_label_cites_min</th>\n      <th>te_pub_publisher_label_cites_max</th>\n      <th>te_pub_publisher_label_cites_std</th>\n      <th>te_pub_publisher_label_cites_mean</th>\n      <th>te_pub_publisher_label_cites_median</th>\n      <th>diff_doi_cites_pub_publisher_label</th>\n      <th>rate_doi_cites_pub_publisher_label</th>\n      <th>te_category_main_label_cites_count</th>\n      <th>te_category_main_label_cites_min</th>\n      <th>te_category_main_label_cites_max</th>\n      <th>te_category_main_label_cites_std</th>\n      <th>te_category_main_label_cites_mean</th>\n      <th>te_category_main_label_cites_median</th>\n      <th>diff_doi_cites_category_main_label</th>\n      <th>rate_doi_cites_category_main_label</th>\n      <th>te_pred_doi_cites_cites_count</th>\n      <th>te_pred_doi_cites_cites_min</th>\n      <th>te_pred_doi_cites_cites_max</th>\n      <th>te_pred_doi_cites_cites_std</th>\n      <th>te_pred_doi_cites_cites_mean</th>\n      <th>te_pred_doi_cites_cites_median</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.197225</td>\n      <td>5.831882</td>\n      <td>1091568.0</td>\n      <td>2015</td>\n      <td>2014</td>\n      <td>2014</td>\n      <td>6</td>\n      <td>3</td>\n      <td>3</td>\n      <td>201506</td>\n      <td>201403</td>\n      <td>201403</td>\n      <td>19</td>\n      <td>27</td>\n      <td>27</td>\n      <td>1.434672e+09</td>\n      <td>1.395941e+09</td>\n      <td>1.395941e+09</td>\n      <td>38730860</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>16605</td>\n      <td>16156</td>\n      <td>16156</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3033</td>\n      <td>53</td>\n      <td>60233</td>\n      <td>344</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1.975857</td>\n      <td>6310</td>\n      <td>12467.660571</td>\n      <td>0.000000</td>\n      <td>8.491670</td>\n      <td>1.945910</td>\n      <td>1.362610</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>2.890372</td>\n      <td>2.072728</td>\n      <td>29728</td>\n      <td>61618.072584</td>\n      <td>0.0</td>\n      <td>8.079308</td>\n      <td>2.079442</td>\n      <td>1.299295</td>\n      <td>0.000000</td>\n      <td>1.098612</td>\n      <td>3.044522</td>\n      <td>2.072728</td>\n      <td>29728</td>\n      <td>61618.072584</td>\n      <td>0.0</td>\n      <td>8.079308</td>\n      <td>2.079442</td>\n      <td>1.299295</td>\n      <td>0.000000</td>\n      <td>1.098612</td>\n      <td>3.044522</td>\n      <td>2.280909</td>\n      <td>43</td>\n      <td>98.079103</td>\n      <td>0.000000</td>\n      <td>4.094345</td>\n      <td>2.302585</td>\n      <td>0.998816</td>\n      <td>0.774240</td>\n      <td>1.945910</td>\n      <td>2.917405</td>\n      <td>2.640078</td>\n      <td>195890</td>\n      <td>517164.893335</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>2.772589</td>\n      <td>1.436436</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.663562</td>\n      <td>2.415190</td>\n      <td>75054</td>\n      <td>181269.657847</td>\n      <td>0.0</td>\n      <td>9.532134</td>\n      <td>2.484907</td>\n      <td>1.274504</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.295837</td>\n      <td>2.303734</td>\n      <td>4588</td>\n      <td>10569.530184</td>\n      <td>0.0</td>\n      <td>6.881411</td>\n      <td>2.302585</td>\n      <td>1.259877</td>\n      <td>0.693147</td>\n      <td>1.386294</td>\n      <td>3.178054</td>\n      <td>-0.083685</td>\n      <td>0.963311</td>\n      <td>0.124496</td>\n      <td>1.060064</td>\n      <td>0.221367</td>\n      <td>1.112036</td>\n      <td>0.124496</td>\n      <td>1.060064</td>\n      <td>-0.217965</td>\n      <td>0.909752</td>\n      <td>-0.106509</td>\n      <td>0.953767</td>\n      <td>-0.442853</td>\n      <td>0.832257</td>\n      <td>-0.177162</td>\n      <td>0.040575</td>\n      <td>0.031812</td>\n      <td>0.219589</td>\n      <td>0.157455</td>\n      <td>0.197949</td>\n      <td>0.022600</td>\n      <td>0.457351</td>\n      <td>-0.066790</td>\n      <td>0.009790</td>\n      <td>0.106237</td>\n      <td>0.008966</td>\n      <td>0.270964</td>\n      <td>-0.092499</td>\n      <td>1.761488</td>\n      <td>-0.268730</td>\n      <td>0.093436</td>\n      <td>0.171133</td>\n      <td>0.194344</td>\n      <td>0.210109</td>\n      <td>-0.037696</td>\n      <td>0.640040</td>\n      <td>0.024497</td>\n      <td>-0.174025</td>\n      <td>0.437692</td>\n      <td>0.139189</td>\n      <td>0.133526</td>\n      <td>-0.096871</td>\n      <td>-1.105201</td>\n      <td>0.830772</td>\n      <td>0.087428</td>\n      <td>0.746169</td>\n      <td>-0.914887</td>\n      <td>0.225666</td>\n      <td>-2.476590</td>\n      <td>-0.727707</td>\n      <td>-0.164099</td>\n      <td>2.043918</td>\n      <td>-0.058883</td>\n      <td>0.003766</td>\n      <td>-0.335878</td>\n      <td>-0.151336</td>\n      <td>0.157771</td>\n      <td>-0.038954</td>\n      <td>-0.109718</td>\n      <td>0.301329</td>\n      <td>-0.059748</td>\n      <td>0.010483</td>\n      <td>0.283288</td>\n      <td>0.281284</td>\n      <td>0.229355</td>\n      <td>-0.195499</td>\n      <td>-0.340115</td>\n      <td>-0.383016</td>\n      <td>0.003452</td>\n      <td>-0.106773</td>\n      <td>0.465899</td>\n      <td>0.574278</td>\n      <td>-0.130823</td>\n      <td>-0.056405</td>\n      <td>0.139040</td>\n      <td>0.388301</td>\n      <td>0.006408</td>\n      <td>0.239137</td>\n      <td>-2.083329</td>\n      <td>0.117212</td>\n      <td>-0.115771</td>\n      <td>0.062894</td>\n      <td>-0.404673</td>\n      <td>-2.507579</td>\n      <td>0.030008</td>\n      <td>-0.399532</td>\n      <td>0.040631</td>\n      <td>-0.014740</td>\n      <td>0.818929</td>\n      <td>0.104165</td>\n      <td>0.285458</td>\n      <td>-0.047836</td>\n      <td>0.274045</td>\n      <td>0.126953</td>\n      <td>0.020760</td>\n      <td>-0.021443</td>\n      <td>0.642590</td>\n      <td>0.159625</td>\n      <td>0.463005</td>\n      <td>0.138884</td>\n      <td>0.121721</td>\n      <td>0.012317</td>\n      <td>-2.593231</td>\n      <td>0.210774</td>\n      <td>0.080882</td>\n      <td>0.201561</td>\n      <td>0.604124</td>\n      <td>0.001948</td>\n      <td>0.088672</td>\n      <td>0.218093</td>\n      <td>-1.660159</td>\n      <td>-0.839162</td>\n      <td>-0.072879</td>\n      <td>0.071614</td>\n      <td>0.358287</td>\n      <td>0.038257</td>\n      <td>-0.114978</td>\n      <td>-0.264588</td>\n      <td>0.094108</td>\n      <td>0.510717</td>\n      <td>1.733449</td>\n      <td>0.149458</td>\n      <td>0.126397</td>\n      <td>-0.176947</td>\n      <td>-0.170408</td>\n      <td>-0.034738</td>\n      <td>0.129364</td>\n      <td>0.029673</td>\n      <td>0.111585</td>\n      <td>0.083132</td>\n      <td>0.201844</td>\n      <td>0.206728</td>\n      <td>0.164638</td>\n      <td>-0.051234</td>\n      <td>-0.305508</td>\n      <td>0.340127</td>\n      <td>0.023230</td>\n      <td>0.327123</td>\n      <td>1.954383</td>\n      <td>0.220627</td>\n      <td>-0.103610</td>\n      <td>0.005175</td>\n      <td>0.026816</td>\n      <td>-0.039961</td>\n      <td>-0.446920</td>\n      <td>-0.511632</td>\n      <td>-0.473427</td>\n      <td>-0.131748</td>\n      <td>-0.074137</td>\n      <td>2.364103</td>\n      <td>-0.556862</td>\n      <td>0.658900</td>\n      <td>0.198368</td>\n      <td>-1.422330</td>\n      <td>0.034498</td>\n      <td>0.328357</td>\n      <td>-0.176316</td>\n      <td>-0.144410</td>\n      <td>0.267348</td>\n      <td>-0.074445</td>\n      <td>0.154867</td>\n      <td>-0.004681</td>\n      <td>0.149763</td>\n      <td>-0.130018</td>\n      <td>0.165379</td>\n      <td>-0.184490</td>\n      <td>0.074234</td>\n      <td>0.035723</td>\n      <td>-0.066997</td>\n      <td>0.272242</td>\n      <td>0.246196</td>\n      <td>-0.438311</td>\n      <td>1.615137</td>\n      <td>0.427056</td>\n      <td>0.337042</td>\n      <td>-0.038943</td>\n      <td>0.055756</td>\n      <td>0.210340</td>\n      <td>0.272656</td>\n      <td>-0.027734</td>\n      <td>-0.078671</td>\n      <td>-0.016879</td>\n      <td>0.045702</td>\n      <td>2.628477</td>\n      <td>-1.811311</td>\n      <td>2.102249</td>\n      <td>0.675407</td>\n      <td>0.012592</td>\n      <td>0.096764</td>\n      <td>-0.147561</td>\n      <td>-0.128479</td>\n      <td>0.177894</td>\n      <td>-0.115422</td>\n      <td>0.077280</td>\n      <td>0.110406</td>\n      <td>-0.093415</td>\n      <td>0.283838</td>\n      <td>-2.353974</td>\n      <td>1.162858</td>\n      <td>0.031733</td>\n      <td>-0.115066</td>\n      <td>-0.167801</td>\n      <td>-0.281858</td>\n      <td>0.326777</td>\n      <td>-0.448179</td>\n      <td>-0.156138</td>\n      <td>-0.021854</td>\n      <td>0.252944</td>\n      <td>0.106458</td>\n      <td>-0.234487</td>\n      <td>-0.111715</td>\n      <td>0.448182</td>\n      <td>0.486683</td>\n      <td>0.131686</td>\n      <td>-0.266955</td>\n      <td>-1.361007</td>\n      <td>-0.049065</td>\n      <td>-0.064608</td>\n      <td>3.339946</td>\n      <td>0.097154</td>\n      <td>-0.397478</td>\n      <td>-0.320212</td>\n      <td>0.162619</td>\n      <td>-0.300963</td>\n      <td>-0.017429</td>\n      <td>-0.389466</td>\n      <td>0.250998</td>\n      <td>0.173973</td>\n      <td>-0.049120</td>\n      <td>0.175812</td>\n      <td>0.116698</td>\n      <td>-2.436979</td>\n      <td>-0.117003</td>\n      <td>-0.098261</td>\n      <td>-0.059250</td>\n      <td>0.055785</td>\n      <td>-0.938275</td>\n      <td>-0.113799</td>\n      <td>-0.077730</td>\n      <td>1.833869</td>\n      <td>0.082557</td>\n      <td>-0.082771</td>\n      <td>0.073025</td>\n      <td>-0.163293</td>\n      <td>0.168310</td>\n      <td>0.771094</td>\n      <td>0.013442</td>\n      <td>0.161488</td>\n      <td>-0.105135</td>\n      <td>-0.099828</td>\n      <td>0.149962</td>\n      <td>0.218198</td>\n      <td>0.138891</td>\n      <td>-0.127859</td>\n      <td>-0.542496</td>\n      <td>0.117916</td>\n      <td>-0.145400</td>\n      <td>0.056014</td>\n      <td>-2.400601</td>\n      <td>0.266558</td>\n      <td>0.182339</td>\n      <td>-0.171575</td>\n      <td>0.090119</td>\n      <td>-0.062044</td>\n      <td>0.110755</td>\n      <td>0.211157</td>\n      <td>-0.374894</td>\n      <td>-0.038100</td>\n      <td>0.617938</td>\n      <td>0.072157</td>\n      <td>0.075126</td>\n      <td>0.030757</td>\n      <td>-0.238993</td>\n      <td>0.070640</td>\n      <td>-0.004450</td>\n      <td>-0.025512</td>\n      <td>0.060729</td>\n      <td>1.896597</td>\n      <td>-0.081278</td>\n      <td>-1.419425</td>\n      <td>0.305349</td>\n      <td>-0.144463</td>\n      <td>-0.028137</td>\n      <td>-0.417937</td>\n      <td>0.027069</td>\n      <td>1.823623</td>\n      <td>0.135928</td>\n      <td>0.279970</td>\n      <td>0.149716</td>\n      <td>2.667881</td>\n      <td>-0.304123</td>\n      <td>0.124811</td>\n      <td>-3.380638</td>\n      <td>-0.230439</td>\n      <td>0.151774</td>\n      <td>-0.106095</td>\n      <td>2.268460</td>\n      <td>0.291547</td>\n      <td>-0.165465</td>\n      <td>-0.005392</td>\n      <td>-0.473945</td>\n      <td>-0.054873</td>\n      <td>-1.811842</td>\n      <td>0.086534</td>\n      <td>0.078607</td>\n      <td>0.097960</td>\n      <td>-0.380417</td>\n      <td>-0.136308</td>\n      <td>-0.041102</td>\n      <td>1.175967</td>\n      <td>-0.043439</td>\n      <td>-0.186521</td>\n      <td>0.076933</td>\n      <td>2.081131</td>\n      <td>1.092557</td>\n      <td>0.099329</td>\n      <td>0.511400</td>\n      <td>0.251352</td>\n      <td>0.026832</td>\n      <td>0.116784</td>\n      <td>-0.580458</td>\n      <td>-0.020857</td>\n      <td>0.539677</td>\n      <td>0.107181</td>\n      <td>0.343884</td>\n      <td>0.093129</td>\n      <td>0.069732</td>\n      <td>0.066160</td>\n      <td>0.025072</td>\n      <td>0.050601</td>\n      <td>-0.170542</td>\n      <td>0.091406</td>\n      <td>0.153839</td>\n      <td>-0.693481</td>\n      <td>-0.307847</td>\n      <td>0.085467</td>\n      <td>-0.110733</td>\n      <td>-0.025265</td>\n      <td>-2.514244</td>\n      <td>0.218100</td>\n      <td>0.051555</td>\n      <td>-0.071849</td>\n      <td>-0.033625</td>\n      <td>0.396798</td>\n      <td>-0.239480</td>\n      <td>0.382372</td>\n      <td>-0.152272</td>\n      <td>0.072296</td>\n      <td>0.298961</td>\n      <td>0.228029</td>\n      <td>-0.130030</td>\n      <td>-0.361593</td>\n      <td>0.218805</td>\n      <td>0.108234</td>\n      <td>-0.332411</td>\n      <td>-0.054848</td>\n      <td>-0.319904</td>\n      <td>-0.040819</td>\n      <td>0.240858</td>\n      <td>0.046785</td>\n      <td>0.023965</td>\n      <td>-0.050435</td>\n      <td>0.280022</td>\n      <td>-0.289779</td>\n      <td>-0.104922</td>\n      <td>-0.042606</td>\n      <td>-0.300852</td>\n      <td>-0.320200</td>\n      <td>0.228576</td>\n      <td>0.081917</td>\n      <td>-2.854576</td>\n      <td>-0.248066</td>\n      <td>-0.304316</td>\n      <td>-0.076131</td>\n      <td>0.150011</td>\n      <td>0.305855</td>\n      <td>0.363664</td>\n      <td>-0.158253</td>\n      <td>0.063583</td>\n      <td>-0.311423</td>\n      <td>0.047354</td>\n      <td>-0.050124</td>\n      <td>0.006139</td>\n      <td>-0.044060</td>\n      <td>1.079078</td>\n      <td>0.522937</td>\n      <td>0.340724</td>\n      <td>0.116205</td>\n      <td>-0.153995</td>\n      <td>0.075715</td>\n      <td>-0.220623</td>\n      <td>-0.094960</td>\n      <td>0.059401</td>\n      <td>-0.271611</td>\n      <td>-0.166118</td>\n      <td>0.037045</td>\n      <td>0.090737</td>\n      <td>-0.244622</td>\n      <td>0.207681</td>\n      <td>-0.086833</td>\n      <td>0.105332</td>\n      <td>0.149631</td>\n      <td>0.575362</td>\n      <td>-0.019076</td>\n      <td>0.104248</td>\n      <td>-0.110618</td>\n      <td>0.160277</td>\n      <td>0.061136</td>\n      <td>-0.259597</td>\n      <td>-0.032720</td>\n      <td>0.411125</td>\n      <td>-0.204966</td>\n      <td>0.218637</td>\n      <td>0.041885</td>\n      <td>-0.413181</td>\n      <td>-0.097323</td>\n      <td>-0.044138</td>\n      <td>0.252018</td>\n      <td>-0.036811</td>\n      <td>1.632427</td>\n      <td>-0.210471</td>\n      <td>-0.060572</td>\n      <td>0.062437</td>\n      <td>0.232953</td>\n      <td>-0.036175</td>\n      <td>2.529381</td>\n      <td>-0.070228</td>\n      <td>-0.097729</td>\n      <td>-0.216002</td>\n      <td>-0.051232</td>\n      <td>-0.032238</td>\n      <td>0.094778</td>\n      <td>-0.003144</td>\n      <td>0.023444</td>\n      <td>-0.213006</td>\n      <td>0.005119</td>\n      <td>0.080720</td>\n      <td>-0.090810</td>\n      <td>-0.286104</td>\n      <td>-0.238237</td>\n      <td>0.431598</td>\n      <td>0.070619</td>\n      <td>-0.256229</td>\n      <td>0.314685</td>\n      <td>0.064888</td>\n      <td>0.011341</td>\n      <td>-0.245777</td>\n      <td>0.163133</td>\n      <td>-0.084693</td>\n      <td>-0.116258</td>\n      <td>1.998967</td>\n      <td>0.087246</td>\n      <td>-0.629274</td>\n      <td>-0.229100</td>\n      <td>-0.262853</td>\n      <td>-0.078284</td>\n      <td>-0.116869</td>\n      <td>0.128459</td>\n      <td>-0.105923</td>\n      <td>-0.095073</td>\n      <td>0.022644</td>\n      <td>0.135669</td>\n      <td>-0.303392</td>\n      <td>-0.462883</td>\n      <td>-0.095307</td>\n      <td>0.245333</td>\n      <td>-0.074186</td>\n      <td>-1.578492</td>\n      <td>0.150023</td>\n      <td>2.144652</td>\n      <td>0.010519</td>\n      <td>0.019600</td>\n      <td>-0.136826</td>\n      <td>0.103503</td>\n      <td>0.203120</td>\n      <td>0.012878</td>\n      <td>0.178566</td>\n      <td>0.015869</td>\n      <td>0.243503</td>\n      <td>0.395536</td>\n      <td>-0.110988</td>\n      <td>0.558919</td>\n      <td>0.112184</td>\n      <td>0.092198</td>\n      <td>0.103707</td>\n      <td>-0.915578</td>\n      <td>-0.423408</td>\n      <td>0.133698</td>\n      <td>0.090898</td>\n      <td>0.187741</td>\n      <td>-0.461914</td>\n      <td>0.337999</td>\n      <td>0.232108</td>\n      <td>0.141146</td>\n      <td>1.225202</td>\n      <td>0.441282</td>\n      <td>0.093731</td>\n      <td>0.238544</td>\n      <td>0.244064</td>\n      <td>-0.028392</td>\n      <td>-0.002571</td>\n      <td>0.259496</td>\n      <td>-0.013259</td>\n      <td>0.077961</td>\n      <td>-0.604996</td>\n      <td>-0.184417</td>\n      <td>-0.102347</td>\n      <td>0.618171</td>\n      <td>-0.468131</td>\n      <td>0.586874</td>\n      <td>-0.158692</td>\n      <td>0.070995</td>\n      <td>-0.105360</td>\n      <td>0.181787</td>\n      <td>0.249843</td>\n      <td>1.700492</td>\n      <td>-0.004595</td>\n      <td>0.155420</td>\n      <td>0.035483</td>\n      <td>-0.100525</td>\n      <td>-0.151472</td>\n      <td>0.137295</td>\n      <td>-0.058530</td>\n      <td>-0.054506</td>\n      <td>-0.212934</td>\n      <td>-2.161172</td>\n      <td>-0.265470</td>\n      <td>0.322820</td>\n      <td>0.025805</td>\n      <td>0.644916</td>\n      <td>0.110094</td>\n      <td>0.011677</td>\n      <td>-0.132298</td>\n      <td>0.229728</td>\n      <td>-0.233181</td>\n      <td>-0.160186</td>\n      <td>0.057111</td>\n      <td>0.317793</td>\n      <td>-0.129320</td>\n      <td>0.207173</td>\n      <td>0.483480</td>\n      <td>0.013354</td>\n      <td>0.070192</td>\n      <td>-0.122317</td>\n      <td>0.301371</td>\n      <td>-0.033025</td>\n      <td>0.154867</td>\n      <td>-1.885714</td>\n      <td>-0.219424</td>\n      <td>0.053800</td>\n      <td>0.227758</td>\n      <td>0.195066</td>\n      <td>0.307997</td>\n      <td>0.044713</td>\n      <td>-0.037292</td>\n      <td>0.120480</td>\n      <td>0.219560</td>\n      <td>-0.030262</td>\n      <td>-1.384143</td>\n      <td>-0.381816</td>\n      <td>0.454982</td>\n      <td>0.019704</td>\n      <td>0.078214</td>\n      <td>-0.101639</td>\n      <td>-0.161910</td>\n      <td>-0.157801</td>\n      <td>0.535551</td>\n      <td>0.185828</td>\n      <td>-0.319736</td>\n      <td>1.613077</td>\n      <td>0.041437</td>\n      <td>0.055049</td>\n      <td>-2.214453</td>\n      <td>-0.114641</td>\n      <td>-0.188238</td>\n      <td>-0.021826</td>\n      <td>-0.272334</td>\n      <td>0.611012</td>\n      <td>-0.448702</td>\n      <td>-0.144010</td>\n      <td>-1.012211</td>\n      <td>0.047929</td>\n      <td>0.319470</td>\n      <td>0.028701</td>\n      <td>0.880147</td>\n      <td>0.299435</td>\n      <td>0.016198</td>\n      <td>0.344211</td>\n      <td>0.215319</td>\n      <td>-0.063521</td>\n      <td>-0.823203</td>\n      <td>0.535293</td>\n      <td>0.122845</td>\n      <td>0.160939</td>\n      <td>1.060224</td>\n      <td>0.073841</td>\n      <td>-0.279970</td>\n      <td>0.092112</td>\n      <td>0.022589</td>\n      <td>0.300994</td>\n      <td>-1.223316</td>\n      <td>0.250526</td>\n      <td>15.822091</td>\n      <td>-0.293939</td>\n      <td>-0.225852</td>\n      <td>0.065358</td>\n      <td>-0.196876</td>\n      <td>0.087030</td>\n      <td>-0.036008</td>\n      <td>0.013691</td>\n      <td>0.192849</td>\n      <td>0.004463</td>\n      <td>-0.189876</td>\n      <td>0.000603</td>\n      <td>0.357863</td>\n      <td>-0.085517</td>\n      <td>0.032815</td>\n      <td>0.204696</td>\n      <td>0.315695</td>\n      <td>0.061444</td>\n      <td>0.217520</td>\n      <td>-0.112574</td>\n      <td>0.419957</td>\n      <td>0.015652</td>\n      <td>0.270032</td>\n      <td>0.214610</td>\n      <td>0.270370</td>\n      <td>0.351678</td>\n      <td>0.182666</td>\n      <td>0.029343</td>\n      <td>0.150769</td>\n      <td>1.614666</td>\n      <td>0.010060</td>\n      <td>0.833348</td>\n      <td>0.076869</td>\n      <td>-2.815283</td>\n      <td>-0.181786</td>\n      <td>-0.516675</td>\n      <td>-0.023858</td>\n      <td>0.019006</td>\n      <td>0.108533</td>\n      <td>0.101123</td>\n      <td>-0.037032</td>\n      <td>0.209284</td>\n      <td>0.128844</td>\n      <td>-0.536315</td>\n      <td>0.564388</td>\n      <td>0.154100</td>\n      <td>0.135937</td>\n      <td>0.280091</td>\n      <td>0.094150</td>\n      <td>-0.242711</td>\n      <td>-0.119878</td>\n      <td>0.094247</td>\n      <td>0.289991</td>\n      <td>-0.158389</td>\n      <td>-0.051196</td>\n      <td>-0.166133</td>\n      <td>-0.048548</td>\n      <td>-0.066555</td>\n      <td>0.156262</td>\n      <td>0.221374</td>\n      <td>-1.172621</td>\n      <td>-0.179384</td>\n      <td>0.208057</td>\n      <td>0.196055</td>\n      <td>-0.191868</td>\n      <td>0.296852</td>\n      <td>0.419609</td>\n      <td>-0.243087</td>\n      <td>0.384583</td>\n      <td>-0.071843</td>\n      <td>-0.308457</td>\n      <td>0.188357</td>\n      <td>0.161250</td>\n      <td>-0.505534</td>\n      <td>-0.078022</td>\n      <td>-0.085979</td>\n      <td>-0.223631</td>\n      <td>0.125725</td>\n      <td>0.094609</td>\n      <td>-0.111266</td>\n      <td>-0.098372</td>\n      <td>-0.008651</td>\n      <td>0.292853</td>\n      <td>0.033137</td>\n      <td>-0.460100</td>\n      <td>0.098051</td>\n      <td>-0.258710</td>\n      <td>0.126392</td>\n      <td>0.043422</td>\n      <td>0.585374</td>\n      <td>-0.086863</td>\n      <td>-0.017255</td>\n      <td>0.158541</td>\n      <td>-0.008609</td>\n      <td>0.245970</td>\n      <td>-0.527509</td>\n      <td>0.323708</td>\n      <td>0.086802</td>\n      <td>0.004911</td>\n      <td>-0.097889</td>\n      <td>-0.548347</td>\n      <td>0.080195</td>\n      <td>-0.660253</td>\n      <td>-0.131541</td>\n      <td>0.185146</td>\n      <td>1.540391</td>\n      <td>0.235880</td>\n      <td>-0.294624</td>\n      <td>0.132456</td>\n      <td>-0.269979</td>\n      <td>0.099906</td>\n      <td>-0.125990</td>\n      <td>-0.074972</td>\n      <td>0.258740</td>\n      <td>-0.824300</td>\n      <td>-0.240840</td>\n      <td>0.010494</td>\n      <td>0.009743</td>\n      <td>-0.035558</td>\n      <td>0.230279</td>\n      <td>-0.198116</td>\n      <td>0.198006</td>\n      <td>0.214464</td>\n      <td>0.114637</td>\n      <td>-0.234757</td>\n      <td>0.168230</td>\n      <td>-0.221062</td>\n      <td>0.068228</td>\n      <td>-0.308377</td>\n      <td>-0.041138</td>\n      <td>-0.396182</td>\n      <td>0.007108</td>\n      <td>0.308709</td>\n      <td>0.045967</td>\n      <td>0.198250</td>\n      <td>0.161404</td>\n      <td>-0.418363</td>\n      <td>-0.030801</td>\n      <td>0.178136</td>\n      <td>0.262133</td>\n      <td>-0.184873</td>\n      <td>-0.155423</td>\n      <td>-0.049266</td>\n      <td>-3.918618</td>\n      <td>-0.297293</td>\n      <td>-1.955519</td>\n      <td>0.020843</td>\n      <td>0.288394</td>\n      <td>-0.178661</td>\n      <td>0.186991</td>\n      <td>0.212213</td>\n      <td>-0.343043</td>\n      <td>0.061037</td>\n      <td>-0.492675</td>\n      <td>0.200838</td>\n      <td>0.257117</td>\n      <td>0.267061</td>\n      <td>0.171412</td>\n      <td>0.213542</td>\n      <td>-0.076566</td>\n      <td>-0.004100</td>\n      <td>-0.816764</td>\n      <td>-0.419798</td>\n      <td>-0.745892</td>\n      <td>-0.939142</td>\n      <td>0.126276</td>\n      <td>0.821421</td>\n      <td>0.103536</td>\n      <td>0.198197</td>\n      <td>-0.251622</td>\n      <td>-0.188746</td>\n      <td>-0.124210</td>\n      <td>-0.327753</td>\n      <td>0.266884</td>\n      <td>0.058829</td>\n      <td>0.219104</td>\n      <td>-0.057867</td>\n      <td>0.124466</td>\n      <td>-0.132686</td>\n      <td>-0.003723</td>\n      <td>3.026657</td>\n      <td>2.587398</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2.197225</td>\n      <td>2.197225</td>\n      <td>NaN</td>\n      <td>2.197225</td>\n      <td>2.197225</td>\n      <td>0.117783</td>\n      <td>1.056642</td>\n      <td>1003</td>\n      <td>0.693147</td>\n      <td>6.297109</td>\n      <td>1.096367</td>\n      <td>2.652902</td>\n      <td>2.639057</td>\n      <td>-0.455678</td>\n      <td>0.828234</td>\n      <td>8034</td>\n      <td>0.693147</td>\n      <td>8.861067</td>\n      <td>1.141813</td>\n      <td>2.498216</td>\n      <td>2.484907</td>\n      <td>-0.300991</td>\n      <td>0.879518</td>\n      <td>416</td>\n      <td>0.693147</td>\n      <td>6.186209</td>\n      <td>1.045891</td>\n      <td>2.343152</td>\n      <td>2.302585</td>\n      <td>-0.145927</td>\n      <td>0.937722</td>\n      <td>2678</td>\n      <td>0.693147</td>\n      <td>9.073833</td>\n      <td>1.224161</td>\n      <td>2.847509</td>\n      <td>2.833213</td>\n      <td>-0.650284</td>\n      <td>0.771630</td>\n      <td>1</td>\n      <td>2.079442</td>\n      <td>2.079442</td>\n      <td>NaN</td>\n      <td>2.079442</td>\n      <td>2.079442</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.812184</td>\n      <td>4.605170</td>\n      <td>510044.0</td>\n      <td>2014</td>\n      <td>2014</td>\n      <td>2014</td>\n      <td>10</td>\n      <td>5</td>\n      <td>9</td>\n      <td>201410</td>\n      <td>201405</td>\n      <td>201409</td>\n      <td>17</td>\n      <td>22</td>\n      <td>26</td>\n      <td>1.413504e+09</td>\n      <td>1.400785e+09</td>\n      <td>1.411756e+09</td>\n      <td>12719189</td>\n      <td>10971274.0</td>\n      <td>2</td>\n      <td>16360</td>\n      <td>16212</td>\n      <td>16339</td>\n      <td>127</td>\n      <td>63.500000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>122888</td>\n      <td>49</td>\n      <td>70004</td>\n      <td>209</td>\n      <td>0</td>\n      <td>4</td>\n      <td>3.462599</td>\n      <td>17</td>\n      <td>58.864177</td>\n      <td>1.945910</td>\n      <td>4.812184</td>\n      <td>3.218876</td>\n      <td>0.885703</td>\n      <td>2.417859</td>\n      <td>2.890372</td>\n      <td>4.158883</td>\n      <td>2.313685</td>\n      <td>105982</td>\n      <td>245208.992755</td>\n      <td>0.0</td>\n      <td>8.746080</td>\n      <td>2.302585</td>\n      <td>1.347253</td>\n      <td>0.693147</td>\n      <td>1.386294</td>\n      <td>3.295837</td>\n      <td>2.313685</td>\n      <td>105982</td>\n      <td>245208.992755</td>\n      <td>0.0</td>\n      <td>8.746080</td>\n      <td>2.302585</td>\n      <td>1.347253</td>\n      <td>0.693147</td>\n      <td>1.386294</td>\n      <td>3.295837</td>\n      <td>3.557392</td>\n      <td>16</td>\n      <td>56.918266</td>\n      <td>2.197225</td>\n      <td>4.812184</td>\n      <td>3.275540</td>\n      <td>0.820869</td>\n      <td>2.699081</td>\n      <td>2.969392</td>\n      <td>4.241321</td>\n      <td>2.640078</td>\n      <td>195890</td>\n      <td>517164.893335</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>2.772589</td>\n      <td>1.436436</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.663562</td>\n      <td>2.310727</td>\n      <td>4152</td>\n      <td>9594.140257</td>\n      <td>0.0</td>\n      <td>7.280008</td>\n      <td>2.302585</td>\n      <td>1.355448</td>\n      <td>0.693147</td>\n      <td>1.386294</td>\n      <td>3.218876</td>\n      <td>2.258292</td>\n      <td>4372</td>\n      <td>9873.250453</td>\n      <td>0.0</td>\n      <td>7.730175</td>\n      <td>2.302585</td>\n      <td>1.231808</td>\n      <td>0.693147</td>\n      <td>1.386294</td>\n      <td>3.091042</td>\n      <td>1.254793</td>\n      <td>1.352728</td>\n      <td>2.498499</td>\n      <td>2.079879</td>\n      <td>1.349586</td>\n      <td>1.389761</td>\n      <td>2.498499</td>\n      <td>2.079879</td>\n      <td>2.501457</td>\n      <td>2.082541</td>\n      <td>2.553893</td>\n      <td>2.130896</td>\n      <td>2.172106</td>\n      <td>1.822743</td>\n      <td>-0.151919</td>\n      <td>0.164424</td>\n      <td>0.070339</td>\n      <td>0.036665</td>\n      <td>0.034719</td>\n      <td>-0.290146</td>\n      <td>0.028694</td>\n      <td>0.393194</td>\n      <td>-0.231535</td>\n      <td>0.148632</td>\n      <td>0.037068</td>\n      <td>-0.001566</td>\n      <td>0.387036</td>\n      <td>-0.099940</td>\n      <td>1.729701</td>\n      <td>-0.514761</td>\n      <td>0.062940</td>\n      <td>0.106854</td>\n      <td>-0.057348</td>\n      <td>-0.020047</td>\n      <td>0.012124</td>\n      <td>0.666923</td>\n      <td>0.258121</td>\n      <td>-0.243020</td>\n      <td>0.433961</td>\n      <td>0.152797</td>\n      <td>0.245113</td>\n      <td>-0.081684</td>\n      <td>-1.277016</td>\n      <td>0.892337</td>\n      <td>0.176921</td>\n      <td>0.881571</td>\n      <td>-0.952881</td>\n      <td>0.174807</td>\n      <td>-2.506780</td>\n      <td>-0.571296</td>\n      <td>-0.222642</td>\n      <td>1.833139</td>\n      <td>-0.118989</td>\n      <td>-0.037166</td>\n      <td>-0.268579</td>\n      <td>0.119906</td>\n      <td>0.004784</td>\n      <td>-0.042307</td>\n      <td>0.072045</td>\n      <td>0.243240</td>\n      <td>-0.023208</td>\n      <td>0.096890</td>\n      <td>0.104138</td>\n      <td>0.049292</td>\n      <td>0.258484</td>\n      <td>-0.028110</td>\n      <td>-0.528706</td>\n      <td>-0.364189</td>\n      <td>0.084927</td>\n      <td>-0.047436</td>\n      <td>0.567769</td>\n      <td>0.653926</td>\n      <td>-0.005437</td>\n      <td>0.002436</td>\n      <td>0.166112</td>\n      <td>0.244592</td>\n      <td>-0.184095</td>\n      <td>0.189920</td>\n      <td>-1.838804</td>\n      <td>0.032892</td>\n      <td>-0.049409</td>\n      <td>0.016488</td>\n      <td>-0.401157</td>\n      <td>-2.459249</td>\n      <td>0.081463</td>\n      <td>-0.507396</td>\n      <td>0.008090</td>\n      <td>-0.133888</td>\n      <td>0.738181</td>\n      <td>0.237094</td>\n      <td>0.090749</td>\n      <td>-0.359563</td>\n      <td>0.156505</td>\n      <td>0.085250</td>\n      <td>0.106221</td>\n      <td>-0.005918</td>\n      <td>0.379876</td>\n      <td>-0.013810</td>\n      <td>0.469553</td>\n      <td>0.123529</td>\n      <td>0.278972</td>\n      <td>0.013047</td>\n      <td>-2.494086</td>\n      <td>0.091893</td>\n      <td>0.050619</td>\n      <td>0.072058</td>\n      <td>0.473731</td>\n      <td>0.215042</td>\n      <td>-0.094404</td>\n      <td>0.035752</td>\n      <td>-1.565716</td>\n      <td>-0.629441</td>\n      <td>-0.087820</td>\n      <td>0.095126</td>\n      <td>0.221732</td>\n      <td>0.205123</td>\n      <td>0.008623</td>\n      <td>-0.044629</td>\n      <td>0.049562</td>\n      <td>0.400052</td>\n      <td>1.610574</td>\n      <td>0.175849</td>\n      <td>0.083861</td>\n      <td>-0.060892</td>\n      <td>-0.307426</td>\n      <td>-0.078453</td>\n      <td>-0.098274</td>\n      <td>0.164445</td>\n      <td>0.212244</td>\n      <td>0.261736</td>\n      <td>0.354385</td>\n      <td>0.235996</td>\n      <td>0.077539</td>\n      <td>0.093511</td>\n      <td>-0.295784</td>\n      <td>0.295669</td>\n      <td>-0.177012</td>\n      <td>0.214302</td>\n      <td>2.024448</td>\n      <td>0.030927</td>\n      <td>-0.366780</td>\n      <td>0.080402</td>\n      <td>0.226900</td>\n      <td>-0.121414</td>\n      <td>-0.549308</td>\n      <td>-0.648643</td>\n      <td>-0.398560</td>\n      <td>-0.002369</td>\n      <td>-0.048275</td>\n      <td>2.429694</td>\n      <td>-0.563302</td>\n      <td>0.680386</td>\n      <td>0.249354</td>\n      <td>-1.398988</td>\n      <td>-0.116884</td>\n      <td>0.228830</td>\n      <td>-0.230369</td>\n      <td>-0.270499</td>\n      <td>0.590439</td>\n      <td>-0.124515</td>\n      <td>0.019093</td>\n      <td>0.207813</td>\n      <td>0.057525</td>\n      <td>0.003758</td>\n      <td>0.101787</td>\n      <td>-0.193255</td>\n      <td>0.026730</td>\n      <td>0.140879</td>\n      <td>-0.133573</td>\n      <td>0.367288</td>\n      <td>0.070894</td>\n      <td>-0.273916</td>\n      <td>1.753311</td>\n      <td>0.476324</td>\n      <td>0.300358</td>\n      <td>-0.125042</td>\n      <td>0.105168</td>\n      <td>0.400843</td>\n      <td>0.071425</td>\n      <td>-0.079656</td>\n      <td>0.004569</td>\n      <td>0.240837</td>\n      <td>0.055948</td>\n      <td>2.562024</td>\n      <td>-1.915245</td>\n      <td>1.931701</td>\n      <td>0.689537</td>\n      <td>-0.076363</td>\n      <td>0.283702</td>\n      <td>-0.072233</td>\n      <td>-0.008425</td>\n      <td>0.006931</td>\n      <td>-0.180455</td>\n      <td>0.018393</td>\n      <td>0.280631</td>\n      <td>-0.150083</td>\n      <td>0.439877</td>\n      <td>-2.516978</td>\n      <td>1.240154</td>\n      <td>-0.257553</td>\n      <td>-0.184012</td>\n      <td>-0.140266</td>\n      <td>-0.334285</td>\n      <td>0.345371</td>\n      <td>-0.570592</td>\n      <td>-0.186548</td>\n      <td>0.094842</td>\n      <td>0.109596</td>\n      <td>-0.019910</td>\n      <td>-0.020032</td>\n      <td>-0.086996</td>\n      <td>0.409417</td>\n      <td>0.290271</td>\n      <td>0.117621</td>\n      <td>-0.274684</td>\n      <td>-1.280097</td>\n      <td>-0.070983</td>\n      <td>0.117835</td>\n      <td>3.031759</td>\n      <td>-0.068311</td>\n      <td>-0.174937</td>\n      <td>-0.440589</td>\n      <td>0.098930</td>\n      <td>-0.094490</td>\n      <td>-0.217006</td>\n      <td>-0.262350</td>\n      <td>0.083453</td>\n      <td>0.344863</td>\n      <td>0.115683</td>\n      <td>0.315848</td>\n      <td>0.343707</td>\n      <td>-2.803661</td>\n      <td>-0.354291</td>\n      <td>-0.289866</td>\n      <td>-0.033451</td>\n      <td>-0.043122</td>\n      <td>-0.886860</td>\n      <td>-0.355123</td>\n      <td>0.028857</td>\n      <td>2.033006</td>\n      <td>0.142631</td>\n      <td>-0.025116</td>\n      <td>0.141639</td>\n      <td>-0.357822</td>\n      <td>0.272178</td>\n      <td>0.769346</td>\n      <td>0.110242</td>\n      <td>0.232803</td>\n      <td>-0.111568</td>\n      <td>0.008478</td>\n      <td>0.286177</td>\n      <td>0.103214</td>\n      <td>0.198305</td>\n      <td>0.038057</td>\n      <td>-0.562072</td>\n      <td>0.043948</td>\n      <td>0.058712</td>\n      <td>-0.083025</td>\n      <td>-2.415932</td>\n      <td>0.190685</td>\n      <td>0.362162</td>\n      <td>-0.324424</td>\n      <td>0.112708</td>\n      <td>-0.109901</td>\n      <td>0.168498</td>\n      <td>0.111365</td>\n      <td>-0.345083</td>\n      <td>-0.082122</td>\n      <td>0.551285</td>\n      <td>0.255635</td>\n      <td>0.025544</td>\n      <td>-0.037368</td>\n      <td>-0.159811</td>\n      <td>-0.188935</td>\n      <td>-0.068413</td>\n      <td>0.107664</td>\n      <td>-0.105496</td>\n      <td>1.930817</td>\n      <td>-0.145218</td>\n      <td>-1.235909</td>\n      <td>0.325697</td>\n      <td>-0.135129</td>\n      <td>-0.065198</td>\n      <td>-0.378396</td>\n      <td>0.025759</td>\n      <td>1.766490</td>\n      <td>0.187029</td>\n      <td>0.327793</td>\n      <td>0.274584</td>\n      <td>2.602602</td>\n      <td>0.011677</td>\n      <td>0.206744</td>\n      <td>-3.507809</td>\n      <td>-0.361280</td>\n      <td>0.192333</td>\n      <td>0.114876</td>\n      <td>2.067370</td>\n      <td>0.346214</td>\n      <td>0.084443</td>\n      <td>-0.026947</td>\n      <td>-0.307767</td>\n      <td>-0.158774</td>\n      <td>-1.719255</td>\n      <td>0.123622</td>\n      <td>0.088402</td>\n      <td>0.106691</td>\n      <td>-0.308075</td>\n      <td>0.019779</td>\n      <td>-0.092850</td>\n      <td>1.443902</td>\n      <td>-0.049734</td>\n      <td>0.030276</td>\n      <td>0.198428</td>\n      <td>1.865828</td>\n      <td>1.158940</td>\n      <td>0.019601</td>\n      <td>0.413710</td>\n      <td>0.187452</td>\n      <td>-0.095632</td>\n      <td>0.181788</td>\n      <td>-0.524692</td>\n      <td>-0.123330</td>\n      <td>0.524592</td>\n      <td>0.143700</td>\n      <td>0.370351</td>\n      <td>0.003509</td>\n      <td>0.213226</td>\n      <td>0.145369</td>\n      <td>0.138129</td>\n      <td>-0.068682</td>\n      <td>-0.205514</td>\n      <td>0.205617</td>\n      <td>0.316274</td>\n      <td>-0.615527</td>\n      <td>-0.182902</td>\n      <td>0.050981</td>\n      <td>0.072929</td>\n      <td>-0.120510</td>\n      <td>-2.399167</td>\n      <td>0.247773</td>\n      <td>-0.027927</td>\n      <td>0.069620</td>\n      <td>0.033353</td>\n      <td>0.296183</td>\n      <td>-0.321210</td>\n      <td>0.301555</td>\n      <td>0.059902</td>\n      <td>0.149956</td>\n      <td>0.176933</td>\n      <td>0.277550</td>\n      <td>-0.237042</td>\n      <td>-0.435120</td>\n      <td>0.101241</td>\n      <td>0.020318</td>\n      <td>-0.229482</td>\n      <td>0.146072</td>\n      <td>-0.323755</td>\n      <td>-0.062908</td>\n      <td>0.144934</td>\n      <td>-0.191260</td>\n      <td>-0.007968</td>\n      <td>-0.209067</td>\n      <td>0.286510</td>\n      <td>-0.190347</td>\n      <td>-0.126290</td>\n      <td>-0.012667</td>\n      <td>-0.298197</td>\n      <td>-0.529448</td>\n      <td>0.082021</td>\n      <td>-0.048211</td>\n      <td>-2.961158</td>\n      <td>-0.186818</td>\n      <td>-0.122479</td>\n      <td>0.257014</td>\n      <td>0.093953</td>\n      <td>0.289130</td>\n      <td>0.252585</td>\n      <td>-0.036869</td>\n      <td>0.139050</td>\n      <td>-0.558978</td>\n      <td>-0.095369</td>\n      <td>-0.077019</td>\n      <td>0.122258</td>\n      <td>0.207551</td>\n      <td>1.158958</td>\n      <td>0.429084</td>\n      <td>0.297641</td>\n      <td>0.008332</td>\n      <td>-0.256805</td>\n      <td>0.024351</td>\n      <td>-0.144494</td>\n      <td>-0.000099</td>\n      <td>-0.156173</td>\n      <td>-0.206374</td>\n      <td>-0.229927</td>\n      <td>0.124755</td>\n      <td>0.096193</td>\n      <td>-0.228400</td>\n      <td>0.317358</td>\n      <td>-0.284253</td>\n      <td>-0.101167</td>\n      <td>0.123308</td>\n      <td>0.610646</td>\n      <td>0.111745</td>\n      <td>-0.051682</td>\n      <td>0.058757</td>\n      <td>0.267427</td>\n      <td>0.123767</td>\n      <td>-0.314890</td>\n      <td>-0.058952</td>\n      <td>0.306851</td>\n      <td>0.121462</td>\n      <td>0.251914</td>\n      <td>0.032505</td>\n      <td>-0.364834</td>\n      <td>-0.086208</td>\n      <td>0.046284</td>\n      <td>0.067692</td>\n      <td>0.114693</td>\n      <td>1.887044</td>\n      <td>-0.251274</td>\n      <td>-0.225252</td>\n      <td>0.101141</td>\n      <td>0.310902</td>\n      <td>-0.118585</td>\n      <td>2.555898</td>\n      <td>0.278240</td>\n      <td>-0.059608</td>\n      <td>-0.330878</td>\n      <td>-0.179448</td>\n      <td>0.034438</td>\n      <td>-0.087032</td>\n      <td>-0.172155</td>\n      <td>0.177130</td>\n      <td>-0.135443</td>\n      <td>-0.035702</td>\n      <td>0.067213</td>\n      <td>-0.088586</td>\n      <td>-0.313352</td>\n      <td>-0.360200</td>\n      <td>0.286111</td>\n      <td>0.141720</td>\n      <td>-0.185340</td>\n      <td>0.337427</td>\n      <td>0.005537</td>\n      <td>0.064887</td>\n      <td>-0.083093</td>\n      <td>0.214161</td>\n      <td>-0.147097</td>\n      <td>-0.156253</td>\n      <td>2.020263</td>\n      <td>0.139344</td>\n      <td>-0.778590</td>\n      <td>-0.075678</td>\n      <td>-0.455053</td>\n      <td>-0.089388</td>\n      <td>-0.085184</td>\n      <td>0.229967</td>\n      <td>-0.204640</td>\n      <td>-0.176410</td>\n      <td>0.157522</td>\n      <td>0.061491</td>\n      <td>-0.627472</td>\n      <td>-0.233144</td>\n      <td>-0.110974</td>\n      <td>0.261672</td>\n      <td>-0.185464</td>\n      <td>-1.323093</td>\n      <td>0.146174</td>\n      <td>2.291961</td>\n      <td>-0.038002</td>\n      <td>-0.027410</td>\n      <td>-0.092332</td>\n      <td>0.009229</td>\n      <td>0.413759</td>\n      <td>-0.161852</td>\n      <td>0.158293</td>\n      <td>-0.030290</td>\n      <td>0.235251</td>\n      <td>0.203837</td>\n      <td>-0.098864</td>\n      <td>0.394712</td>\n      <td>0.008275</td>\n      <td>0.240496</td>\n      <td>-0.020147</td>\n      <td>-0.932685</td>\n      <td>-0.469263</td>\n      <td>-0.018727</td>\n      <td>0.153063</td>\n      <td>0.146592</td>\n      <td>-0.560373</td>\n      <td>0.251904</td>\n      <td>0.433597</td>\n      <td>0.151299</td>\n      <td>1.171400</td>\n      <td>0.473205</td>\n      <td>0.162715</td>\n      <td>0.241250</td>\n      <td>0.130757</td>\n      <td>-0.034031</td>\n      <td>-0.108028</td>\n      <td>0.363107</td>\n      <td>0.116180</td>\n      <td>-0.021556</td>\n      <td>-0.665398</td>\n      <td>-0.159891</td>\n      <td>-0.121735</td>\n      <td>0.379535</td>\n      <td>-0.459163</td>\n      <td>0.407180</td>\n      <td>-0.075914</td>\n      <td>0.152923</td>\n      <td>0.064105</td>\n      <td>0.040492</td>\n      <td>0.141919</td>\n      <td>1.806803</td>\n      <td>-0.051680</td>\n      <td>0.372172</td>\n      <td>0.119274</td>\n      <td>0.030204</td>\n      <td>-0.073295</td>\n      <td>0.151053</td>\n      <td>-0.012912</td>\n      <td>0.021601</td>\n      <td>-0.021308</td>\n      <td>-2.246757</td>\n      <td>-0.239993</td>\n      <td>0.224773</td>\n      <td>-0.006464</td>\n      <td>0.360688</td>\n      <td>-0.112231</td>\n      <td>0.064517</td>\n      <td>-0.150548</td>\n      <td>0.038087</td>\n      <td>-0.260672</td>\n      <td>-0.149075</td>\n      <td>0.011921</td>\n      <td>0.238152</td>\n      <td>-0.193100</td>\n      <td>0.270614</td>\n      <td>0.442918</td>\n      <td>-0.057084</td>\n      <td>0.287970</td>\n      <td>-0.074903</td>\n      <td>0.381705</td>\n      <td>0.028985</td>\n      <td>0.308847</td>\n      <td>-1.990040</td>\n      <td>-0.226734</td>\n      <td>0.066200</td>\n      <td>0.072457</td>\n      <td>0.000491</td>\n      <td>0.349080</td>\n      <td>0.119157</td>\n      <td>-0.303383</td>\n      <td>0.168547</td>\n      <td>0.043352</td>\n      <td>-0.105565</td>\n      <td>-1.359656</td>\n      <td>-0.536941</td>\n      <td>0.287319</td>\n      <td>0.184718</td>\n      <td>-0.108925</td>\n      <td>0.004664</td>\n      <td>-0.160138</td>\n      <td>-0.086793</td>\n      <td>0.448416</td>\n      <td>0.199831</td>\n      <td>-0.566441</td>\n      <td>1.712669</td>\n      <td>-0.066894</td>\n      <td>0.127602</td>\n      <td>-2.258015</td>\n      <td>-0.022484</td>\n      <td>-0.360272</td>\n      <td>0.027002</td>\n      <td>-0.168777</td>\n      <td>0.583397</td>\n      <td>-0.499323</td>\n      <td>-0.092745</td>\n      <td>-1.050407</td>\n      <td>0.164120</td>\n      <td>0.293879</td>\n      <td>0.101792</td>\n      <td>0.864475</td>\n      <td>0.382499</td>\n      <td>-0.076339</td>\n      <td>0.026944</td>\n      <td>0.323894</td>\n      <td>-0.094514</td>\n      <td>-0.715805</td>\n      <td>0.674772</td>\n      <td>0.121623</td>\n      <td>0.241752</td>\n      <td>1.183422</td>\n      <td>-0.097667</td>\n      <td>0.001432</td>\n      <td>0.003126</td>\n      <td>-0.001244</td>\n      <td>0.277276</td>\n      <td>-1.182100</td>\n      <td>0.403843</td>\n      <td>15.553185</td>\n      <td>-0.372938</td>\n      <td>-0.050561</td>\n      <td>0.056788</td>\n      <td>-0.208312</td>\n      <td>0.168762</td>\n      <td>0.151427</td>\n      <td>0.037620</td>\n      <td>0.572874</td>\n      <td>-0.006761</td>\n      <td>-0.234047</td>\n      <td>-0.048970</td>\n      <td>0.156726</td>\n      <td>-0.056132</td>\n      <td>0.094340</td>\n      <td>0.074749</td>\n      <td>0.127371</td>\n      <td>0.016461</td>\n      <td>0.170581</td>\n      <td>0.104563</td>\n      <td>0.366803</td>\n      <td>-0.077828</td>\n      <td>0.182071</td>\n      <td>0.051257</td>\n      <td>0.092744</td>\n      <td>0.351979</td>\n      <td>0.069919</td>\n      <td>0.140736</td>\n      <td>0.111602</td>\n      <td>1.452005</td>\n      <td>0.022420</td>\n      <td>0.582984</td>\n      <td>0.038011</td>\n      <td>-3.002893</td>\n      <td>-0.230123</td>\n      <td>-0.197030</td>\n      <td>0.163921</td>\n      <td>0.209723</td>\n      <td>0.024682</td>\n      <td>0.114945</td>\n      <td>-0.028971</td>\n      <td>0.208363</td>\n      <td>0.113172</td>\n      <td>-0.665051</td>\n      <td>0.508760</td>\n      <td>0.143695</td>\n      <td>0.119762</td>\n      <td>0.286777</td>\n      <td>0.031980</td>\n      <td>-0.001094</td>\n      <td>-0.063991</td>\n      <td>0.020547</td>\n      <td>0.073094</td>\n      <td>-0.286973</td>\n      <td>0.048861</td>\n      <td>-0.134137</td>\n      <td>0.250595</td>\n      <td>-0.048538</td>\n      <td>0.028061</td>\n      <td>0.095534</td>\n      <td>-1.101001</td>\n      <td>-0.023357</td>\n      <td>0.189023</td>\n      <td>0.085924</td>\n      <td>0.073344</td>\n      <td>0.456503</td>\n      <td>0.373931</td>\n      <td>-0.245915</td>\n      <td>0.547515</td>\n      <td>0.024903</td>\n      <td>-0.372423</td>\n      <td>0.080572</td>\n      <td>0.073819</td>\n      <td>-0.326122</td>\n      <td>-0.105866</td>\n      <td>-0.054877</td>\n      <td>-0.157585</td>\n      <td>0.178068</td>\n      <td>0.101500</td>\n      <td>0.011944</td>\n      <td>-0.093229</td>\n      <td>-0.208350</td>\n      <td>0.186226</td>\n      <td>0.062414</td>\n      <td>-0.182860</td>\n      <td>0.000611</td>\n      <td>-0.492310</td>\n      <td>0.167412</td>\n      <td>0.063648</td>\n      <td>0.591303</td>\n      <td>-0.069620</td>\n      <td>-0.062570</td>\n      <td>0.187370</td>\n      <td>0.049593</td>\n      <td>0.144560</td>\n      <td>-0.441664</td>\n      <td>0.283955</td>\n      <td>0.197611</td>\n      <td>-0.026639</td>\n      <td>0.138178</td>\n      <td>-0.331778</td>\n      <td>-0.028185</td>\n      <td>-0.808046</td>\n      <td>0.187521</td>\n      <td>0.104550</td>\n      <td>1.474983</td>\n      <td>0.204089</td>\n      <td>-0.557227</td>\n      <td>0.059868</td>\n      <td>-0.356101</td>\n      <td>0.058076</td>\n      <td>-0.201592</td>\n      <td>-0.094257</td>\n      <td>0.140190</td>\n      <td>-0.766507</td>\n      <td>-0.137215</td>\n      <td>0.151194</td>\n      <td>0.033688</td>\n      <td>-0.009695</td>\n      <td>0.233568</td>\n      <td>-0.191690</td>\n      <td>0.174944</td>\n      <td>0.268931</td>\n      <td>0.015305</td>\n      <td>-0.248654</td>\n      <td>0.152268</td>\n      <td>0.088289</td>\n      <td>0.105007</td>\n      <td>-0.349722</td>\n      <td>-0.041510</td>\n      <td>-0.403001</td>\n      <td>-0.079653</td>\n      <td>0.574540</td>\n      <td>0.136599</td>\n      <td>0.160080</td>\n      <td>0.338430</td>\n      <td>-0.428547</td>\n      <td>-0.040832</td>\n      <td>0.166718</td>\n      <td>0.379569</td>\n      <td>0.009165</td>\n      <td>-0.069855</td>\n      <td>-0.014186</td>\n      <td>-3.708467</td>\n      <td>-0.591583</td>\n      <td>-1.993726</td>\n      <td>0.082165</td>\n      <td>0.064541</td>\n      <td>-0.076794</td>\n      <td>0.212564</td>\n      <td>0.285754</td>\n      <td>-0.357906</td>\n      <td>0.092024</td>\n      <td>-0.456410</td>\n      <td>-0.004129</td>\n      <td>0.173820</td>\n      <td>0.317651</td>\n      <td>0.008917</td>\n      <td>0.237783</td>\n      <td>-0.184110</td>\n      <td>0.343919</td>\n      <td>-0.652336</td>\n      <td>-0.461633</td>\n      <td>-0.791143</td>\n      <td>-1.079459</td>\n      <td>0.053497</td>\n      <td>0.840875</td>\n      <td>0.125879</td>\n      <td>0.441573</td>\n      <td>-0.112534</td>\n      <td>-0.291659</td>\n      <td>0.047304</td>\n      <td>-0.167982</td>\n      <td>0.166911</td>\n      <td>-0.003861</td>\n      <td>0.399774</td>\n      <td>0.202404</td>\n      <td>0.199137</td>\n      <td>-0.219354</td>\n      <td>0.079108</td>\n      <td>3.222497</td>\n      <td>2.955035</td>\n      <td>6</td>\n      <td>1</td>\n      <td>4.812184</td>\n      <td>4.812184</td>\n      <td>NaN</td>\n      <td>4.812184</td>\n      <td>4.812184</td>\n      <td>-0.429563</td>\n      <td>0.918050</td>\n      <td>71</td>\n      <td>0.693147</td>\n      <td>5.241747</td>\n      <td>1.204318</td>\n      <td>2.729506</td>\n      <td>2.772589</td>\n      <td>2.082679</td>\n      <td>1.763024</td>\n      <td>8034</td>\n      <td>0.693147</td>\n      <td>8.861067</td>\n      <td>1.141813</td>\n      <td>2.498216</td>\n      <td>2.484907</td>\n      <td>2.313969</td>\n      <td>1.926249</td>\n      <td>1441</td>\n      <td>0.693147</td>\n      <td>8.861067</td>\n      <td>1.215321</td>\n      <td>2.726504</td>\n      <td>2.708050</td>\n      <td>2.085680</td>\n      <td>1.764965</td>\n      <td>2678</td>\n      <td>0.693147</td>\n      <td>9.073833</td>\n      <td>1.224161</td>\n      <td>2.847509</td>\n      <td>2.833213</td>\n      <td>1.964676</td>\n      <td>1.689963</td>\n      <td>1</td>\n      <td>5.241747</td>\n      <td>5.241747</td>\n      <td>NaN</td>\n      <td>5.241747</td>\n      <td>5.241747</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.945910</td>\n      <td>5.105945</td>\n      <td>866445.0</td>\n      <td>2020</td>\n      <td>2018</td>\n      <td>2019</td>\n      <td>5</td>\n      <td>7</td>\n      <td>5</td>\n      <td>202005</td>\n      <td>201807</td>\n      <td>201905</td>\n      <td>19</td>\n      <td>3</td>\n      <td>9</td>\n      <td>1.589846e+09</td>\n      <td>1.530609e+09</td>\n      <td>1.557403e+09</td>\n      <td>59237321</td>\n      <td>26794202.0</td>\n      <td>2</td>\n      <td>18401</td>\n      <td>17715</td>\n      <td>18025</td>\n      <td>310</td>\n      <td>155.000000</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>52993</td>\n      <td>22</td>\n      <td>50948</td>\n      <td>327</td>\n      <td>0</td>\n      <td>29</td>\n      <td>1.572833</td>\n      <td>3</td>\n      <td>4.718499</td>\n      <td>0.693147</td>\n      <td>2.079442</td>\n      <td>1.945910</td>\n      <td>0.764750</td>\n      <td>0.943700</td>\n      <td>1.319529</td>\n      <td>2.012676</td>\n      <td>3.094469</td>\n      <td>11868</td>\n      <td>36725.162265</td>\n      <td>0.0</td>\n      <td>9.263786</td>\n      <td>3.135494</td>\n      <td>1.694765</td>\n      <td>0.693147</td>\n      <td>1.945910</td>\n      <td>4.317488</td>\n      <td>3.094469</td>\n      <td>11868</td>\n      <td>36725.162265</td>\n      <td>0.0</td>\n      <td>9.263786</td>\n      <td>3.135494</td>\n      <td>1.694765</td>\n      <td>0.693147</td>\n      <td>1.945910</td>\n      <td>4.317488</td>\n      <td>1.572833</td>\n      <td>3</td>\n      <td>4.718499</td>\n      <td>0.693147</td>\n      <td>2.079442</td>\n      <td>1.945910</td>\n      <td>0.764750</td>\n      <td>0.943700</td>\n      <td>1.319529</td>\n      <td>2.012676</td>\n      <td>1.880741</td>\n      <td>70889</td>\n      <td>133323.877828</td>\n      <td>0.0</td>\n      <td>8.405591</td>\n      <td>1.791759</td>\n      <td>1.330488</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>2.833213</td>\n      <td>0.235188</td>\n      <td>6130</td>\n      <td>1441.699761</td>\n      <td>0.0</td>\n      <td>6.516193</td>\n      <td>0.000000</td>\n      <td>0.611910</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.221578</td>\n      <td>5360</td>\n      <td>6547.656998</td>\n      <td>0.0</td>\n      <td>5.141664</td>\n      <td>1.098612</td>\n      <td>0.966951</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>1.945910</td>\n      <td>0.373077</td>\n      <td>1.237201</td>\n      <td>-1.148559</td>\n      <td>0.628835</td>\n      <td>0.373077</td>\n      <td>1.237201</td>\n      <td>-1.148559</td>\n      <td>0.628835</td>\n      <td>1.710723</td>\n      <td>8.273865</td>\n      <td>0.724332</td>\n      <td>1.592948</td>\n      <td>0.065169</td>\n      <td>1.034651</td>\n      <td>-0.056842</td>\n      <td>0.024134</td>\n      <td>0.077527</td>\n      <td>0.002172</td>\n      <td>0.111476</td>\n      <td>-0.054090</td>\n      <td>-0.021401</td>\n      <td>0.341266</td>\n      <td>-0.163869</td>\n      <td>0.044457</td>\n      <td>0.193856</td>\n      <td>-0.008626</td>\n      <td>0.329635</td>\n      <td>0.018120</td>\n      <td>1.811508</td>\n      <td>-0.376844</td>\n      <td>0.112333</td>\n      <td>0.135414</td>\n      <td>0.035637</td>\n      <td>0.119704</td>\n      <td>0.028732</td>\n      <td>0.665100</td>\n      <td>0.203662</td>\n      <td>-0.184751</td>\n      <td>0.481317</td>\n      <td>0.098841</td>\n      <td>0.149961</td>\n      <td>-0.113411</td>\n      <td>-1.256245</td>\n      <td>0.905980</td>\n      <td>0.104848</td>\n      <td>0.804375</td>\n      <td>-0.986573</td>\n      <td>0.210285</td>\n      <td>-2.495247</td>\n      <td>-0.595127</td>\n      <td>-0.034170</td>\n      <td>1.863049</td>\n      <td>-0.143966</td>\n      <td>0.045171</td>\n      <td>-0.305713</td>\n      <td>-0.058646</td>\n      <td>-0.003764</td>\n      <td>-0.051510</td>\n      <td>-0.009675</td>\n      <td>0.335237</td>\n      <td>0.089173</td>\n      <td>0.043896</td>\n      <td>0.107442</td>\n      <td>0.182818</td>\n      <td>0.216040</td>\n      <td>0.001688</td>\n      <td>-0.408833</td>\n      <td>-0.371012</td>\n      <td>0.137077</td>\n      <td>-0.123600</td>\n      <td>0.515492</td>\n      <td>0.569739</td>\n      <td>0.039195</td>\n      <td>0.064551</td>\n      <td>0.175405</td>\n      <td>0.154157</td>\n      <td>-0.171991</td>\n      <td>0.236044</td>\n      <td>-1.897585</td>\n      <td>0.005933</td>\n      <td>-0.055588</td>\n      <td>0.040125</td>\n      <td>-0.315086</td>\n      <td>-2.526258</td>\n      <td>0.089856</td>\n      <td>-0.403909</td>\n      <td>0.033191</td>\n      <td>-0.138435</td>\n      <td>0.791871</td>\n      <td>0.097823</td>\n      <td>0.135091</td>\n      <td>-0.321452</td>\n      <td>0.146382</td>\n      <td>0.144274</td>\n      <td>0.141355</td>\n      <td>-0.008055</td>\n      <td>0.472540</td>\n      <td>0.052181</td>\n      <td>0.453003</td>\n      <td>0.117840</td>\n      <td>0.194709</td>\n      <td>-0.032242</td>\n      <td>-2.506124</td>\n      <td>0.150000</td>\n      <td>0.040412</td>\n      <td>0.037129</td>\n      <td>0.522422</td>\n      <td>0.171392</td>\n      <td>-0.043173</td>\n      <td>0.104884</td>\n      <td>-1.508898</td>\n      <td>-0.552044</td>\n      <td>-0.032686</td>\n      <td>0.153504</td>\n      <td>0.231386</td>\n      <td>0.118627</td>\n      <td>0.072543</td>\n      <td>-0.090304</td>\n      <td>0.169708</td>\n      <td>0.315754</td>\n      <td>1.668699</td>\n      <td>0.124504</td>\n      <td>0.014337</td>\n      <td>-0.125640</td>\n      <td>-0.239297</td>\n      <td>-0.008114</td>\n      <td>-0.091556</td>\n      <td>0.124892</td>\n      <td>0.279448</td>\n      <td>0.196491</td>\n      <td>0.306162</td>\n      <td>0.224352</td>\n      <td>0.053228</td>\n      <td>-0.045489</td>\n      <td>-0.274130</td>\n      <td>0.284709</td>\n      <td>-0.067332</td>\n      <td>0.178634</td>\n      <td>1.925830</td>\n      <td>0.102527</td>\n      <td>-0.270740</td>\n      <td>0.044345</td>\n      <td>0.122166</td>\n      <td>-0.010134</td>\n      <td>-0.574814</td>\n      <td>-0.561045</td>\n      <td>-0.446151</td>\n      <td>-0.102594</td>\n      <td>0.017651</td>\n      <td>2.254713</td>\n      <td>-0.578229</td>\n      <td>0.702613</td>\n      <td>0.150888</td>\n      <td>-1.453365</td>\n      <td>-0.029931</td>\n      <td>0.227453</td>\n      <td>-0.149535</td>\n      <td>-0.204706</td>\n      <td>0.479813</td>\n      <td>-0.084938</td>\n      <td>0.032982</td>\n      <td>0.175073</td>\n      <td>0.116954</td>\n      <td>-0.023457</td>\n      <td>0.043565</td>\n      <td>-0.298888</td>\n      <td>0.060980</td>\n      <td>0.185994</td>\n      <td>-0.018926</td>\n      <td>0.173560</td>\n      <td>0.139335</td>\n      <td>-0.386804</td>\n      <td>1.687680</td>\n      <td>0.486983</td>\n      <td>0.351559</td>\n      <td>-0.115893</td>\n      <td>0.025709</td>\n      <td>0.329914</td>\n      <td>0.180383</td>\n      <td>-0.053614</td>\n      <td>0.017443</td>\n      <td>0.231203</td>\n      <td>0.114651</td>\n      <td>2.483434</td>\n      <td>-1.806570</td>\n      <td>2.017472</td>\n      <td>0.623664</td>\n      <td>0.033400</td>\n      <td>0.223727</td>\n      <td>-0.080224</td>\n      <td>0.083298</td>\n      <td>0.078782</td>\n      <td>-0.091046</td>\n      <td>0.068809</td>\n      <td>0.218594</td>\n      <td>-0.080184</td>\n      <td>0.277207</td>\n      <td>-2.506025</td>\n      <td>1.197491</td>\n      <td>-0.085689</td>\n      <td>-0.174635</td>\n      <td>-0.134841</td>\n      <td>-0.277650</td>\n      <td>0.286878</td>\n      <td>-0.447910</td>\n      <td>-0.220592</td>\n      <td>0.153917</td>\n      <td>0.148613</td>\n      <td>0.039814</td>\n      <td>-0.155728</td>\n      <td>-0.081096</td>\n      <td>0.389137</td>\n      <td>0.262927</td>\n      <td>0.143403</td>\n      <td>-0.491894</td>\n      <td>-1.244793</td>\n      <td>-0.130878</td>\n      <td>0.031742</td>\n      <td>3.150254</td>\n      <td>0.028888</td>\n      <td>-0.254138</td>\n      <td>-0.310756</td>\n      <td>0.194871</td>\n      <td>-0.224938</td>\n      <td>-0.141935</td>\n      <td>-0.235722</td>\n      <td>0.192708</td>\n      <td>0.412804</td>\n      <td>0.088120</td>\n      <td>0.261432</td>\n      <td>0.202775</td>\n      <td>-2.667027</td>\n      <td>-0.282549</td>\n      <td>-0.259832</td>\n      <td>-0.091370</td>\n      <td>-0.044187</td>\n      <td>-0.884903</td>\n      <td>-0.227404</td>\n      <td>-0.005934</td>\n      <td>2.003923</td>\n      <td>0.078385</td>\n      <td>-0.006061</td>\n      <td>0.134492</td>\n      <td>-0.376658</td>\n      <td>0.183801</td>\n      <td>0.721697</td>\n      <td>0.011328</td>\n      <td>0.162650</td>\n      <td>-0.099771</td>\n      <td>-0.004149</td>\n      <td>0.138004</td>\n      <td>0.109714</td>\n      <td>0.111614</td>\n      <td>0.007013</td>\n      <td>-0.448557</td>\n      <td>0.104244</td>\n      <td>-0.059339</td>\n      <td>-0.037429</td>\n      <td>-2.451160</td>\n      <td>0.149168</td>\n      <td>0.259654</td>\n      <td>-0.228018</td>\n      <td>0.074740</td>\n      <td>-0.171474</td>\n      <td>0.212024</td>\n      <td>0.173075</td>\n      <td>-0.331100</td>\n      <td>0.025703</td>\n      <td>0.620389</td>\n      <td>0.275188</td>\n      <td>0.053262</td>\n      <td>-0.041892</td>\n      <td>-0.178539</td>\n      <td>-0.053122</td>\n      <td>-0.001618</td>\n      <td>0.111925</td>\n      <td>-0.052350</td>\n      <td>1.873505</td>\n      <td>-0.050993</td>\n      <td>-1.353496</td>\n      <td>0.394978</td>\n      <td>-0.127919</td>\n      <td>-0.083099</td>\n      <td>-0.371884</td>\n      <td>0.042236</td>\n      <td>1.801705</td>\n      <td>0.147710</td>\n      <td>0.254955</td>\n      <td>0.212227</td>\n      <td>2.576202</td>\n      <td>-0.070726</td>\n      <td>0.148632</td>\n      <td>-3.422003</td>\n      <td>-0.290363</td>\n      <td>0.290093</td>\n      <td>0.040177</td>\n      <td>2.253440</td>\n      <td>0.201972</td>\n      <td>0.081102</td>\n      <td>-0.025090</td>\n      <td>-0.361630</td>\n      <td>-0.112134</td>\n      <td>-1.830406</td>\n      <td>0.205584</td>\n      <td>0.149620</td>\n      <td>0.215501</td>\n      <td>-0.316010</td>\n      <td>0.019102</td>\n      <td>-0.076710</td>\n      <td>1.317520</td>\n      <td>-0.028470</td>\n      <td>0.006781</td>\n      <td>0.200507</td>\n      <td>1.908410</td>\n      <td>1.169262</td>\n      <td>-0.005556</td>\n      <td>0.433259</td>\n      <td>0.207491</td>\n      <td>-0.098600</td>\n      <td>0.173922</td>\n      <td>-0.617099</td>\n      <td>-0.010868</td>\n      <td>0.407780</td>\n      <td>0.072026</td>\n      <td>0.392575</td>\n      <td>0.062297</td>\n      <td>0.197143</td>\n      <td>0.089461</td>\n      <td>0.051428</td>\n      <td>-0.038423</td>\n      <td>-0.233132</td>\n      <td>0.179221</td>\n      <td>0.192130</td>\n      <td>-0.722591</td>\n      <td>-0.226184</td>\n      <td>0.001483</td>\n      <td>0.089187</td>\n      <td>-0.010298</td>\n      <td>-2.306751</td>\n      <td>0.287809</td>\n      <td>-0.002073</td>\n      <td>0.021750</td>\n      <td>0.060082</td>\n      <td>0.283628</td>\n      <td>-0.364457</td>\n      <td>0.287073</td>\n      <td>0.016222</td>\n      <td>0.134690</td>\n      <td>0.183493</td>\n      <td>0.300665</td>\n      <td>-0.264968</td>\n      <td>-0.349118</td>\n      <td>0.145562</td>\n      <td>0.037069</td>\n      <td>-0.316856</td>\n      <td>0.073806</td>\n      <td>-0.343595</td>\n      <td>-0.037100</td>\n      <td>0.182167</td>\n      <td>-0.106184</td>\n      <td>-0.012129</td>\n      <td>-0.088767</td>\n      <td>0.228625</td>\n      <td>-0.205651</td>\n      <td>-0.029269</td>\n      <td>0.006856</td>\n      <td>-0.279565</td>\n      <td>-0.362457</td>\n      <td>0.050524</td>\n      <td>0.045667</td>\n      <td>-3.032517</td>\n      <td>-0.204518</td>\n      <td>-0.143533</td>\n      <td>0.200500</td>\n      <td>0.127080</td>\n      <td>0.249758</td>\n      <td>0.274883</td>\n      <td>0.061158</td>\n      <td>0.131590</td>\n      <td>-0.439356</td>\n      <td>-0.230691</td>\n      <td>-0.121624</td>\n      <td>0.031395</td>\n      <td>0.007230</td>\n      <td>0.998293</td>\n      <td>0.448496</td>\n      <td>0.320850</td>\n      <td>-0.026372</td>\n      <td>-0.225820</td>\n      <td>0.022117</td>\n      <td>-0.163722</td>\n      <td>-0.000585</td>\n      <td>-0.081620</td>\n      <td>-0.263277</td>\n      <td>-0.134790</td>\n      <td>0.091172</td>\n      <td>0.015396</td>\n      <td>-0.207458</td>\n      <td>0.296093</td>\n      <td>-0.240750</td>\n      <td>0.004467</td>\n      <td>0.115073</td>\n      <td>0.548440</td>\n      <td>0.087003</td>\n      <td>-0.055420</td>\n      <td>-0.085359</td>\n      <td>0.067766</td>\n      <td>0.125534</td>\n      <td>-0.326463</td>\n      <td>-0.125293</td>\n      <td>0.280062</td>\n      <td>-0.101059</td>\n      <td>0.237139</td>\n      <td>0.023231</td>\n      <td>-0.452875</td>\n      <td>-0.004467</td>\n      <td>-0.076859</td>\n      <td>0.106541</td>\n      <td>0.024990</td>\n      <td>1.772140</td>\n      <td>-0.166953</td>\n      <td>-0.277041</td>\n      <td>0.157278</td>\n      <td>0.286098</td>\n      <td>0.080473</td>\n      <td>2.540668</td>\n      <td>0.123556</td>\n      <td>-0.049834</td>\n      <td>-0.395198</td>\n      <td>-0.165545</td>\n      <td>0.013189</td>\n      <td>0.040974</td>\n      <td>-0.147202</td>\n      <td>0.142431</td>\n      <td>-0.185525</td>\n      <td>-0.010101</td>\n      <td>0.026927</td>\n      <td>-0.142773</td>\n      <td>-0.157937</td>\n      <td>-0.266347</td>\n      <td>0.341345</td>\n      <td>0.070413</td>\n      <td>-0.255057</td>\n      <td>0.230562</td>\n      <td>0.053993</td>\n      <td>0.023075</td>\n      <td>-0.180786</td>\n      <td>0.202608</td>\n      <td>-0.154401</td>\n      <td>-0.103665</td>\n      <td>2.034712</td>\n      <td>0.150287</td>\n      <td>-0.659897</td>\n      <td>-0.194316</td>\n      <td>-0.439122</td>\n      <td>-0.149376</td>\n      <td>-0.079960</td>\n      <td>0.078856</td>\n      <td>-0.157934</td>\n      <td>-0.285321</td>\n      <td>-0.017812</td>\n      <td>0.044464</td>\n      <td>-0.489260</td>\n      <td>-0.193046</td>\n      <td>-0.065896</td>\n      <td>0.336117</td>\n      <td>-0.086014</td>\n      <td>-1.261396</td>\n      <td>0.156138</td>\n      <td>2.394937</td>\n      <td>0.023842</td>\n      <td>0.053864</td>\n      <td>-0.046104</td>\n      <td>-0.032422</td>\n      <td>0.302536</td>\n      <td>-0.115504</td>\n      <td>0.086991</td>\n      <td>-0.005310</td>\n      <td>0.222087</td>\n      <td>0.283678</td>\n      <td>0.030226</td>\n      <td>0.422670</td>\n      <td>0.054652</td>\n      <td>0.149554</td>\n      <td>0.046082</td>\n      <td>-0.979786</td>\n      <td>-0.518095</td>\n      <td>0.047828</td>\n      <td>0.124062</td>\n      <td>0.200028</td>\n      <td>-0.435055</td>\n      <td>0.167789</td>\n      <td>0.302036</td>\n      <td>0.100425</td>\n      <td>1.163692</td>\n      <td>0.463162</td>\n      <td>0.185636</td>\n      <td>0.197276</td>\n      <td>0.099728</td>\n      <td>-0.109266</td>\n      <td>-0.042058</td>\n      <td>0.291771</td>\n      <td>0.101698</td>\n      <td>0.052568</td>\n      <td>-0.555177</td>\n      <td>-0.176158</td>\n      <td>-0.027588</td>\n      <td>0.491958</td>\n      <td>-0.446667</td>\n      <td>0.431137</td>\n      <td>-0.067139</td>\n      <td>0.266253</td>\n      <td>0.009277</td>\n      <td>0.164425</td>\n      <td>0.154216</td>\n      <td>1.678438</td>\n      <td>-0.009528</td>\n      <td>0.212294</td>\n      <td>0.071359</td>\n      <td>-0.078748</td>\n      <td>-0.106866</td>\n      <td>0.078964</td>\n      <td>0.037510</td>\n      <td>-0.151701</td>\n      <td>-0.090715</td>\n      <td>-2.204904</td>\n      <td>-0.136698</td>\n      <td>0.241396</td>\n      <td>0.021319</td>\n      <td>0.419903</td>\n      <td>0.004256</td>\n      <td>0.071470</td>\n      <td>-0.114078</td>\n      <td>0.074556</td>\n      <td>-0.263726</td>\n      <td>-0.133134</td>\n      <td>0.017339</td>\n      <td>0.263289</td>\n      <td>-0.082252</td>\n      <td>0.250185</td>\n      <td>0.460629</td>\n      <td>-0.016840</td>\n      <td>0.175422</td>\n      <td>0.157566</td>\n      <td>0.419569</td>\n      <td>-0.025988</td>\n      <td>0.261276</td>\n      <td>-1.843011</td>\n      <td>-0.134013</td>\n      <td>0.014627</td>\n      <td>-0.048810</td>\n      <td>0.097481</td>\n      <td>0.315081</td>\n      <td>0.091456</td>\n      <td>-0.257366</td>\n      <td>0.170851</td>\n      <td>0.050739</td>\n      <td>-0.086404</td>\n      <td>-1.325074</td>\n      <td>-0.404412</td>\n      <td>0.272161</td>\n      <td>0.080645</td>\n      <td>-0.054186</td>\n      <td>0.166837</td>\n      <td>-0.219296</td>\n      <td>-0.075931</td>\n      <td>0.503294</td>\n      <td>0.280621</td>\n      <td>-0.504979</td>\n      <td>1.623201</td>\n      <td>-0.012388</td>\n      <td>0.125677</td>\n      <td>-2.300368</td>\n      <td>0.105271</td>\n      <td>-0.254472</td>\n      <td>-0.006396</td>\n      <td>-0.227216</td>\n      <td>0.610685</td>\n      <td>-0.450610</td>\n      <td>-0.143536</td>\n      <td>-1.101053</td>\n      <td>0.182532</td>\n      <td>0.272050</td>\n      <td>0.104461</td>\n      <td>0.774926</td>\n      <td>0.310726</td>\n      <td>-0.044036</td>\n      <td>0.197949</td>\n      <td>0.167628</td>\n      <td>-0.120342</td>\n      <td>-0.762763</td>\n      <td>0.654617</td>\n      <td>0.003306</td>\n      <td>0.133462</td>\n      <td>1.142177</td>\n      <td>0.016386</td>\n      <td>-0.042429</td>\n      <td>-0.064496</td>\n      <td>0.007013</td>\n      <td>0.325139</td>\n      <td>-1.073495</td>\n      <td>0.426291</td>\n      <td>15.844813</td>\n      <td>-0.306415</td>\n      <td>-0.079105</td>\n      <td>0.198903</td>\n      <td>-0.231324</td>\n      <td>0.119051</td>\n      <td>0.077108</td>\n      <td>0.005179</td>\n      <td>0.303829</td>\n      <td>-0.053929</td>\n      <td>-0.186148</td>\n      <td>-0.021015</td>\n      <td>0.242072</td>\n      <td>-0.070203</td>\n      <td>0.058864</td>\n      <td>0.119841</td>\n      <td>0.165088</td>\n      <td>0.060464</td>\n      <td>0.158488</td>\n      <td>-0.011235</td>\n      <td>0.395918</td>\n      <td>-0.063851</td>\n      <td>0.180751</td>\n      <td>0.027858</td>\n      <td>0.112992</td>\n      <td>0.213551</td>\n      <td>0.104593</td>\n      <td>0.167317</td>\n      <td>0.090190</td>\n      <td>1.572100</td>\n      <td>0.025217</td>\n      <td>0.675744</td>\n      <td>0.057000</td>\n      <td>-2.962146</td>\n      <td>-0.272924</td>\n      <td>-0.365076</td>\n      <td>0.061387</td>\n      <td>0.174849</td>\n      <td>0.098550</td>\n      <td>0.061867</td>\n      <td>0.025853</td>\n      <td>0.203269</td>\n      <td>-0.007888</td>\n      <td>-0.462947</td>\n      <td>0.525456</td>\n      <td>0.110210</td>\n      <td>0.168101</td>\n      <td>0.215915</td>\n      <td>-0.007926</td>\n      <td>-0.023232</td>\n      <td>0.033094</td>\n      <td>0.059309</td>\n      <td>0.095212</td>\n      <td>-0.185200</td>\n      <td>-0.018416</td>\n      <td>-0.172998</td>\n      <td>0.130920</td>\n      <td>-0.095067</td>\n      <td>-0.021346</td>\n      <td>0.124015</td>\n      <td>-1.089467</td>\n      <td>-0.106027</td>\n      <td>0.133312</td>\n      <td>0.188547</td>\n      <td>-0.086923</td>\n      <td>0.356702</td>\n      <td>0.369333</td>\n      <td>-0.261264</td>\n      <td>0.420348</td>\n      <td>-0.029035</td>\n      <td>-0.303828</td>\n      <td>0.126919</td>\n      <td>0.101516</td>\n      <td>-0.436137</td>\n      <td>-0.108258</td>\n      <td>-0.017772</td>\n      <td>-0.287531</td>\n      <td>0.159387</td>\n      <td>0.167649</td>\n      <td>0.131180</td>\n      <td>-0.027852</td>\n      <td>-0.138670</td>\n      <td>0.219817</td>\n      <td>0.084599</td>\n      <td>-0.308141</td>\n      <td>0.142244</td>\n      <td>-0.357205</td>\n      <td>0.102737</td>\n      <td>0.022313</td>\n      <td>0.546061</td>\n      <td>-0.247005</td>\n      <td>-0.004627</td>\n      <td>0.201054</td>\n      <td>-0.014128</td>\n      <td>0.133573</td>\n      <td>-0.421171</td>\n      <td>0.252873</td>\n      <td>0.219634</td>\n      <td>-0.027937</td>\n      <td>0.065881</td>\n      <td>-0.391664</td>\n      <td>-0.004864</td>\n      <td>-0.711784</td>\n      <td>0.143438</td>\n      <td>0.032284</td>\n      <td>1.463696</td>\n      <td>0.133402</td>\n      <td>-0.477880</td>\n      <td>0.124315</td>\n      <td>-0.358670</td>\n      <td>0.039477</td>\n      <td>-0.129978</td>\n      <td>-0.057161</td>\n      <td>0.231705</td>\n      <td>-0.802713</td>\n      <td>-0.146444</td>\n      <td>0.046169</td>\n      <td>0.045916</td>\n      <td>-0.073564</td>\n      <td>0.196521</td>\n      <td>-0.133488</td>\n      <td>0.168456</td>\n      <td>0.206797</td>\n      <td>0.123613</td>\n      <td>-0.172510</td>\n      <td>0.051626</td>\n      <td>0.012986</td>\n      <td>0.149887</td>\n      <td>-0.236040</td>\n      <td>0.028193</td>\n      <td>-0.476537</td>\n      <td>-0.085902</td>\n      <td>0.309771</td>\n      <td>0.067193</td>\n      <td>0.075443</td>\n      <td>0.201039</td>\n      <td>-0.389641</td>\n      <td>-0.034003</td>\n      <td>0.144847</td>\n      <td>0.361557</td>\n      <td>-0.073972</td>\n      <td>-0.051607</td>\n      <td>0.076390</td>\n      <td>-3.763675</td>\n      <td>-0.400752</td>\n      <td>-1.998005</td>\n      <td>0.112026</td>\n      <td>0.201483</td>\n      <td>-0.143264</td>\n      <td>0.181641</td>\n      <td>0.187613</td>\n      <td>-0.395876</td>\n      <td>0.221800</td>\n      <td>-0.433550</td>\n      <td>0.120441</td>\n      <td>0.187026</td>\n      <td>0.264432</td>\n      <td>0.092117</td>\n      <td>0.180155</td>\n      <td>-0.130565</td>\n      <td>0.163252</td>\n      <td>-0.637579</td>\n      <td>-0.497798</td>\n      <td>-0.766127</td>\n      <td>-1.059294</td>\n      <td>0.026800</td>\n      <td>0.750261</td>\n      <td>0.122869</td>\n      <td>0.234986</td>\n      <td>-0.112272</td>\n      <td>-0.137269</td>\n      <td>-0.013252</td>\n      <td>-0.276987</td>\n      <td>0.185075</td>\n      <td>0.054808</td>\n      <td>0.349297</td>\n      <td>0.028149</td>\n      <td>0.186575</td>\n      <td>-0.187830</td>\n      <td>0.138273</td>\n      <td>3.109099</td>\n      <td>1.318706</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1.945910</td>\n      <td>1.945910</td>\n      <td>NaN</td>\n      <td>1.945910</td>\n      <td>1.945910</td>\n      <td>-0.251314</td>\n      <td>0.885622</td>\n      <td>60</td>\n      <td>0.693147</td>\n      <td>6.633318</td>\n      <td>1.159934</td>\n      <td>1.578621</td>\n      <td>1.242453</td>\n      <td>0.367289</td>\n      <td>1.232664</td>\n      <td>8034</td>\n      <td>0.693147</td>\n      <td>8.861067</td>\n      <td>1.141813</td>\n      <td>2.498216</td>\n      <td>2.484907</td>\n      <td>-0.552305</td>\n      <td>0.778920</td>\n      <td>147</td>\n      <td>0.693147</td>\n      <td>6.904751</td>\n      <td>1.458698</td>\n      <td>3.335794</td>\n      <td>3.367296</td>\n      <td>-1.389884</td>\n      <td>0.583342</td>\n      <td>951</td>\n      <td>0.693147</td>\n      <td>6.562444</td>\n      <td>1.138468</td>\n      <td>2.414652</td>\n      <td>2.397895</td>\n      <td>-0.468741</td>\n      <td>0.805876</td>\n      <td>1</td>\n      <td>2.197225</td>\n      <td>2.197225</td>\n      <td>NaN</td>\n      <td>2.197225</td>\n      <td>2.197225</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.555348</td>\n      <td>4.248495</td>\n      <td>325332.0</td>\n      <td>2009</td>\n      <td>1999</td>\n      <td>1999</td>\n      <td>10</td>\n      <td>8</td>\n      <td>12</td>\n      <td>200910</td>\n      <td>199908</td>\n      <td>199912</td>\n      <td>31</td>\n      <td>21</td>\n      <td>2</td>\n      <td>1.256947e+09</td>\n      <td>9.352608e+08</td>\n      <td>9.441679e+08</td>\n      <td>321686413</td>\n      <td>8907072.0</td>\n      <td>3</td>\n      <td>14548</td>\n      <td>10824</td>\n      <td>10927</td>\n      <td>103</td>\n      <td>34.333333</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>141917</td>\n      <td>47</td>\n      <td>106908</td>\n      <td>488</td>\n      <td>8</td>\n      <td>4</td>\n      <td>3.555348</td>\n      <td>1</td>\n      <td>3.555348</td>\n      <td>3.555348</td>\n      <td>3.555348</td>\n      <td>3.555348</td>\n      <td>NaN</td>\n      <td>3.555348</td>\n      <td>3.555348</td>\n      <td>3.555348</td>\n      <td>3.656215</td>\n      <td>28742</td>\n      <td>105086.939920</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>3.663562</td>\n      <td>1.098822</td>\n      <td>2.302585</td>\n      <td>2.995732</td>\n      <td>4.356709</td>\n      <td>3.656215</td>\n      <td>28742</td>\n      <td>105086.939920</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>3.663562</td>\n      <td>1.098822</td>\n      <td>2.302585</td>\n      <td>2.995732</td>\n      <td>4.356709</td>\n      <td>3.724077</td>\n      <td>46</td>\n      <td>171.307562</td>\n      <td>0.000000</td>\n      <td>6.651572</td>\n      <td>3.637240</td>\n      <td>1.689052</td>\n      <td>1.497866</td>\n      <td>2.753631</td>\n      <td>4.881364</td>\n      <td>2.640078</td>\n      <td>195890</td>\n      <td>517164.893335</td>\n      <td>0.0</td>\n      <td>9.241742</td>\n      <td>2.772589</td>\n      <td>1.436436</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.663562</td>\n      <td>2.621200</td>\n      <td>63528</td>\n      <td>166519.618737</td>\n      <td>0.0</td>\n      <td>8.306719</td>\n      <td>2.639057</td>\n      <td>1.384934</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.583519</td>\n      <td>2.614574</td>\n      <td>1343</td>\n      <td>3511.372690</td>\n      <td>0.0</td>\n      <td>7.584265</td>\n      <td>2.639057</td>\n      <td>1.470314</td>\n      <td>0.693147</td>\n      <td>1.609438</td>\n      <td>3.610918</td>\n      <td>-0.168729</td>\n      <td>0.954692</td>\n      <td>-0.100867</td>\n      <td>0.972412</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.100867</td>\n      <td>0.972412</td>\n      <td>0.934148</td>\n      <td>1.356382</td>\n      <td>0.940774</td>\n      <td>1.359819</td>\n      <td>0.915270</td>\n      <td>1.346683</td>\n      <td>-0.208291</td>\n      <td>-0.010926</td>\n      <td>0.083218</td>\n      <td>0.150096</td>\n      <td>-0.008250</td>\n      <td>-0.080186</td>\n      <td>0.046144</td>\n      <td>0.420460</td>\n      <td>-0.202450</td>\n      <td>0.023080</td>\n      <td>0.197276</td>\n      <td>0.112194</td>\n      <td>0.286559</td>\n      <td>0.071929</td>\n      <td>1.776792</td>\n      <td>-0.309333</td>\n      <td>0.146808</td>\n      <td>0.162415</td>\n      <td>0.142022</td>\n      <td>0.235431</td>\n      <td>-0.039786</td>\n      <td>0.741636</td>\n      <td>0.107184</td>\n      <td>-0.105181</td>\n      <td>0.409020</td>\n      <td>0.218782</td>\n      <td>0.196558</td>\n      <td>-0.093029</td>\n      <td>-1.170368</td>\n      <td>0.804693</td>\n      <td>0.124322</td>\n      <td>0.737541</td>\n      <td>-0.877775</td>\n      <td>0.260975</td>\n      <td>-2.411145</td>\n      <td>-0.637473</td>\n      <td>-0.100732</td>\n      <td>1.932431</td>\n      <td>-0.082223</td>\n      <td>0.058513</td>\n      <td>-0.191939</td>\n      <td>-0.064657</td>\n      <td>0.167725</td>\n      <td>-0.092506</td>\n      <td>-0.059745</td>\n      <td>0.233553</td>\n      <td>0.007226</td>\n      <td>0.140106</td>\n      <td>0.167100</td>\n      <td>0.274049</td>\n      <td>0.199528</td>\n      <td>-0.181837</td>\n      <td>-0.509051</td>\n      <td>-0.314096</td>\n      <td>0.004841</td>\n      <td>-0.086180</td>\n      <td>0.458136</td>\n      <td>0.711666</td>\n      <td>-0.059328</td>\n      <td>-0.039439</td>\n      <td>0.126087</td>\n      <td>0.247075</td>\n      <td>-0.187733</td>\n      <td>0.388701</td>\n      <td>-1.941070</td>\n      <td>0.000730</td>\n      <td>-0.042875</td>\n      <td>0.082130</td>\n      <td>-0.401998</td>\n      <td>-2.598048</td>\n      <td>-0.097858</td>\n      <td>-0.459862</td>\n      <td>0.130468</td>\n      <td>-0.178673</td>\n      <td>0.767624</td>\n      <td>0.124298</td>\n      <td>0.153640</td>\n      <td>-0.333841</td>\n      <td>0.223361</td>\n      <td>0.152346</td>\n      <td>0.021533</td>\n      <td>0.103443</td>\n      <td>0.532409</td>\n      <td>0.075198</td>\n      <td>0.566319</td>\n      <td>0.156937</td>\n      <td>0.189481</td>\n      <td>-0.044979</td>\n      <td>-2.612585</td>\n      <td>0.129149</td>\n      <td>0.101983</td>\n      <td>0.140736</td>\n      <td>0.561609</td>\n      <td>0.022473</td>\n      <td>-0.018595</td>\n      <td>0.017419</td>\n      <td>-1.603606</td>\n      <td>-0.871463</td>\n      <td>-0.038517</td>\n      <td>0.171907</td>\n      <td>0.240751</td>\n      <td>0.084511</td>\n      <td>-0.038087</td>\n      <td>-0.009535</td>\n      <td>0.086917</td>\n      <td>0.558644</td>\n      <td>1.804594</td>\n      <td>0.128579</td>\n      <td>0.064682</td>\n      <td>-0.161358</td>\n      <td>-0.201426</td>\n      <td>-0.024702</td>\n      <td>-0.006653</td>\n      <td>0.039118</td>\n      <td>0.135469</td>\n      <td>0.219540</td>\n      <td>0.279916</td>\n      <td>0.266163</td>\n      <td>0.029719</td>\n      <td>-0.047633</td>\n      <td>-0.261265</td>\n      <td>0.266995</td>\n      <td>0.023398</td>\n      <td>0.332948</td>\n      <td>1.852291</td>\n      <td>0.064059</td>\n      <td>-0.221144</td>\n      <td>-0.057262</td>\n      <td>0.169315</td>\n      <td>-0.070071</td>\n      <td>-0.511633</td>\n      <td>-0.449601</td>\n      <td>-0.486559</td>\n      <td>-0.070251</td>\n      <td>0.023410</td>\n      <td>2.353786</td>\n      <td>-0.630395</td>\n      <td>0.640899</td>\n      <td>0.145830</td>\n      <td>-1.445593</td>\n      <td>-0.061567</td>\n      <td>0.299150</td>\n      <td>-0.120716</td>\n      <td>-0.169679</td>\n      <td>0.404521</td>\n      <td>-0.097405</td>\n      <td>0.071880</td>\n      <td>0.173230</td>\n      <td>0.099267</td>\n      <td>-0.106117</td>\n      <td>0.020077</td>\n      <td>-0.243323</td>\n      <td>0.054529</td>\n      <td>0.139890</td>\n      <td>-0.004724</td>\n      <td>0.457711</td>\n      <td>0.184554</td>\n      <td>-0.423282</td>\n      <td>1.647679</td>\n      <td>0.313112</td>\n      <td>0.394969</td>\n      <td>-0.062272</td>\n      <td>0.036974</td>\n      <td>0.288891</td>\n      <td>0.296403</td>\n      <td>-0.163806</td>\n      <td>-0.214395</td>\n      <td>0.212026</td>\n      <td>0.000124</td>\n      <td>2.555810</td>\n      <td>-1.838967</td>\n      <td>1.992343</td>\n      <td>0.687347</td>\n      <td>-0.122267</td>\n      <td>0.067168</td>\n      <td>-0.078775</td>\n      <td>-0.032405</td>\n      <td>0.210341</td>\n      <td>-0.097730</td>\n      <td>0.078898</td>\n      <td>0.172516</td>\n      <td>-0.096750</td>\n      <td>0.348617</td>\n      <td>-2.505918</td>\n      <td>1.268387</td>\n      <td>-0.015233</td>\n      <td>-0.113351</td>\n      <td>-0.058425</td>\n      <td>-0.217675</td>\n      <td>0.250800</td>\n      <td>-0.556520</td>\n      <td>-0.193254</td>\n      <td>0.010202</td>\n      <td>0.220472</td>\n      <td>0.097995</td>\n      <td>-0.226445</td>\n      <td>-0.076758</td>\n      <td>0.408853</td>\n      <td>0.394347</td>\n      <td>0.160727</td>\n      <td>-0.311775</td>\n      <td>-1.289350</td>\n      <td>-0.081816</td>\n      <td>0.025895</td>\n      <td>3.207915</td>\n      <td>0.090487</td>\n      <td>-0.141846</td>\n      <td>-0.381764</td>\n      <td>0.263704</td>\n      <td>-0.219380</td>\n      <td>-0.118338</td>\n      <td>-0.315832</td>\n      <td>0.133408</td>\n      <td>0.414030</td>\n      <td>-0.173241</td>\n      <td>0.190469</td>\n      <td>0.198712</td>\n      <td>-2.523486</td>\n      <td>-0.191513</td>\n      <td>-0.152962</td>\n      <td>0.050075</td>\n      <td>-0.010045</td>\n      <td>-0.846456</td>\n      <td>-0.287519</td>\n      <td>-0.093624</td>\n      <td>1.959135</td>\n      <td>0.052163</td>\n      <td>-0.156001</td>\n      <td>0.170417</td>\n      <td>-0.275222</td>\n      <td>0.084156</td>\n      <td>0.803454</td>\n      <td>-0.004285</td>\n      <td>0.251668</td>\n      <td>-0.138459</td>\n      <td>0.068153</td>\n      <td>0.109065</td>\n      <td>0.175499</td>\n      <td>0.083710</td>\n      <td>-0.167410</td>\n      <td>-0.753625</td>\n      <td>0.169018</td>\n      <td>-0.050623</td>\n      <td>-0.037526</td>\n      <td>-2.394194</td>\n      <td>0.159489</td>\n      <td>0.191821</td>\n      <td>-0.318941</td>\n      <td>0.098131</td>\n      <td>-0.080081</td>\n      <td>0.158861</td>\n      <td>0.433901</td>\n      <td>-0.279326</td>\n      <td>-0.071081</td>\n      <td>0.543207</td>\n      <td>0.252345</td>\n      <td>0.058751</td>\n      <td>0.131210</td>\n      <td>-0.261168</td>\n      <td>-0.198618</td>\n      <td>-0.036632</td>\n      <td>0.070011</td>\n      <td>0.020527</td>\n      <td>2.016599</td>\n      <td>0.038830</td>\n      <td>-1.290535</td>\n      <td>0.353242</td>\n      <td>-0.118498</td>\n      <td>-0.059929</td>\n      <td>-0.386447</td>\n      <td>0.087913</td>\n      <td>1.765314</td>\n      <td>0.205492</td>\n      <td>0.239984</td>\n      <td>0.152804</td>\n      <td>2.663770</td>\n      <td>-0.141089</td>\n      <td>0.091385</td>\n      <td>-3.421282</td>\n      <td>-0.332130</td>\n      <td>0.108499</td>\n      <td>-0.009616</td>\n      <td>2.188776</td>\n      <td>0.255904</td>\n      <td>-0.034881</td>\n      <td>0.056723</td>\n      <td>-0.477477</td>\n      <td>-0.268958</td>\n      <td>-1.930601</td>\n      <td>0.244555</td>\n      <td>0.174821</td>\n      <td>0.219638</td>\n      <td>-0.273490</td>\n      <td>0.054807</td>\n      <td>-0.071126</td>\n      <td>1.404045</td>\n      <td>0.004108</td>\n      <td>-0.033647</td>\n      <td>0.153876</td>\n      <td>2.019505</td>\n      <td>1.131256</td>\n      <td>-0.026437</td>\n      <td>0.459597</td>\n      <td>0.207811</td>\n      <td>-0.181748</td>\n      <td>0.104864</td>\n      <td>-0.645482</td>\n      <td>-0.017064</td>\n      <td>0.628516</td>\n      <td>0.026366</td>\n      <td>0.384188</td>\n      <td>-0.016993</td>\n      <td>0.155208</td>\n      <td>0.115347</td>\n      <td>0.054118</td>\n      <td>-0.024089</td>\n      <td>-0.298569</td>\n      <td>0.064265</td>\n      <td>0.221265</td>\n      <td>-0.629967</td>\n      <td>-0.240979</td>\n      <td>0.100654</td>\n      <td>-0.069907</td>\n      <td>-0.070397</td>\n      <td>-2.379597</td>\n      <td>0.255050</td>\n      <td>-0.067504</td>\n      <td>0.022967</td>\n      <td>-0.035241</td>\n      <td>0.356725</td>\n      <td>-0.444231</td>\n      <td>0.431514</td>\n      <td>0.026566</td>\n      <td>0.009751</td>\n      <td>0.224113</td>\n      <td>0.251205</td>\n      <td>-0.241261</td>\n      <td>-0.399054</td>\n      <td>0.244097</td>\n      <td>-0.079142</td>\n      <td>-0.327230</td>\n      <td>0.052349</td>\n      <td>-0.306724</td>\n      <td>0.020707</td>\n      <td>0.253127</td>\n      <td>-0.087338</td>\n      <td>-0.087830</td>\n      <td>-0.084611</td>\n      <td>0.419029</td>\n      <td>-0.059954</td>\n      <td>0.025546</td>\n      <td>0.041498</td>\n      <td>-0.254276</td>\n      <td>-0.390722</td>\n      <td>0.042174</td>\n      <td>0.013076</td>\n      <td>-2.857336</td>\n      <td>-0.185217</td>\n      <td>-0.254679</td>\n      <td>0.018244</td>\n      <td>0.126393</td>\n      <td>0.354632</td>\n      <td>0.379288</td>\n      <td>-0.093672</td>\n      <td>0.072067</td>\n      <td>-0.330987</td>\n      <td>-0.121968</td>\n      <td>0.019468</td>\n      <td>-0.026040</td>\n      <td>-0.015894</td>\n      <td>1.104023</td>\n      <td>0.490189</td>\n      <td>0.246370</td>\n      <td>0.065125</td>\n      <td>-0.098130</td>\n      <td>0.036091</td>\n      <td>-0.207514</td>\n      <td>-0.134827</td>\n      <td>-0.112595</td>\n      <td>-0.208325</td>\n      <td>-0.169273</td>\n      <td>0.073377</td>\n      <td>0.028040</td>\n      <td>-0.293768</td>\n      <td>0.224554</td>\n      <td>-0.208772</td>\n      <td>-0.036270</td>\n      <td>0.060038</td>\n      <td>0.509903</td>\n      <td>0.085959</td>\n      <td>-0.057937</td>\n      <td>-0.028370</td>\n      <td>0.160520</td>\n      <td>0.004224</td>\n      <td>-0.355430</td>\n      <td>-0.014851</td>\n      <td>0.484404</td>\n      <td>-0.017259</td>\n      <td>0.363049</td>\n      <td>0.050382</td>\n      <td>-0.494631</td>\n      <td>-0.148666</td>\n      <td>0.013060</td>\n      <td>0.082447</td>\n      <td>0.113793</td>\n      <td>1.744074</td>\n      <td>-0.104651</td>\n      <td>-0.139658</td>\n      <td>0.173880</td>\n      <td>0.183233</td>\n      <td>-0.067716</td>\n      <td>2.485960</td>\n      <td>0.051862</td>\n      <td>-0.109655</td>\n      <td>-0.340113</td>\n      <td>-0.100898</td>\n      <td>0.081306</td>\n      <td>0.063807</td>\n      <td>-0.292851</td>\n      <td>0.180262</td>\n      <td>-0.178909</td>\n      <td>-0.086794</td>\n      <td>-0.006490</td>\n      <td>-0.171399</td>\n      <td>-0.307505</td>\n      <td>-0.347830</td>\n      <td>0.401314</td>\n      <td>0.133021</td>\n      <td>-0.269792</td>\n      <td>0.335339</td>\n      <td>0.042842</td>\n      <td>0.017449</td>\n      <td>-0.180792</td>\n      <td>0.149203</td>\n      <td>-0.225829</td>\n      <td>-0.113747</td>\n      <td>2.002280</td>\n      <td>0.134932</td>\n      <td>-0.692661</td>\n      <td>-0.132664</td>\n      <td>-0.330120</td>\n      <td>-0.175362</td>\n      <td>-0.170062</td>\n      <td>0.029008</td>\n      <td>-0.129776</td>\n      <td>-0.133392</td>\n      <td>-0.002454</td>\n      <td>0.121127</td>\n      <td>-0.418404</td>\n      <td>-0.309105</td>\n      <td>-0.063905</td>\n      <td>0.265288</td>\n      <td>-0.077628</td>\n      <td>-1.357990</td>\n      <td>0.135468</td>\n      <td>2.216243</td>\n      <td>0.048628</td>\n      <td>0.011779</td>\n      <td>-0.069871</td>\n      <td>0.079845</td>\n      <td>0.330591</td>\n      <td>-0.115614</td>\n      <td>0.217454</td>\n      <td>-0.054093</td>\n      <td>0.239651</td>\n      <td>0.377111</td>\n      <td>0.016356</td>\n      <td>0.580147</td>\n      <td>0.176018</td>\n      <td>0.159482</td>\n      <td>0.066138</td>\n      <td>-0.997297</td>\n      <td>-0.406922</td>\n      <td>-0.025751</td>\n      <td>0.246825</td>\n      <td>0.236218</td>\n      <td>-0.374182</td>\n      <td>0.283653</td>\n      <td>0.240770</td>\n      <td>0.156542</td>\n      <td>1.159808</td>\n      <td>0.392672</td>\n      <td>0.138935</td>\n      <td>0.265259</td>\n      <td>0.223429</td>\n      <td>-0.058707</td>\n      <td>-0.069433</td>\n      <td>0.226698</td>\n      <td>-0.010465</td>\n      <td>0.006579</td>\n      <td>-0.475847</td>\n      <td>-0.195371</td>\n      <td>-0.098609</td>\n      <td>0.585837</td>\n      <td>-0.469254</td>\n      <td>0.582671</td>\n      <td>-0.114554</td>\n      <td>-0.012496</td>\n      <td>-0.100632</td>\n      <td>0.216511</td>\n      <td>0.199610</td>\n      <td>1.779944</td>\n      <td>0.024791</td>\n      <td>0.208478</td>\n      <td>0.028069</td>\n      <td>-0.055857</td>\n      <td>-0.111874</td>\n      <td>0.210685</td>\n      <td>-0.067999</td>\n      <td>-0.108225</td>\n      <td>-0.038617</td>\n      <td>-2.200426</td>\n      <td>-0.177147</td>\n      <td>0.219382</td>\n      <td>-0.040248</td>\n      <td>0.595902</td>\n      <td>0.098881</td>\n      <td>0.123644</td>\n      <td>-0.351162</td>\n      <td>0.014326</td>\n      <td>-0.332253</td>\n      <td>-0.070752</td>\n      <td>0.059858</td>\n      <td>0.258543</td>\n      <td>-0.065095</td>\n      <td>0.349410</td>\n      <td>0.436748</td>\n      <td>-0.018930</td>\n      <td>0.275581</td>\n      <td>-0.120089</td>\n      <td>0.366175</td>\n      <td>-0.018887</td>\n      <td>0.248874</td>\n      <td>-1.912786</td>\n      <td>-0.233995</td>\n      <td>0.029044</td>\n      <td>0.140569</td>\n      <td>0.152168</td>\n      <td>0.311290</td>\n      <td>0.107242</td>\n      <td>-0.126861</td>\n      <td>0.126954</td>\n      <td>0.089758</td>\n      <td>-0.054519</td>\n      <td>-1.397378</td>\n      <td>-0.383100</td>\n      <td>0.387510</td>\n      <td>0.050146</td>\n      <td>0.018349</td>\n      <td>0.049488</td>\n      <td>-0.196376</td>\n      <td>-0.078164</td>\n      <td>0.427996</td>\n      <td>0.235573</td>\n      <td>-0.405791</td>\n      <td>1.700030</td>\n      <td>0.086266</td>\n      <td>0.061431</td>\n      <td>-2.342282</td>\n      <td>0.028502</td>\n      <td>-0.242585</td>\n      <td>-0.037277</td>\n      <td>-0.203769</td>\n      <td>0.650433</td>\n      <td>-0.369655</td>\n      <td>-0.171448</td>\n      <td>-1.121116</td>\n      <td>0.161571</td>\n      <td>0.361442</td>\n      <td>0.051398</td>\n      <td>1.051563</td>\n      <td>0.353890</td>\n      <td>-0.018994</td>\n      <td>0.196987</td>\n      <td>0.161014</td>\n      <td>-0.013524</td>\n      <td>-0.774762</td>\n      <td>0.668775</td>\n      <td>-0.056611</td>\n      <td>0.050784</td>\n      <td>1.015317</td>\n      <td>0.031618</td>\n      <td>-0.025416</td>\n      <td>-0.028334</td>\n      <td>-0.033147</td>\n      <td>0.277414</td>\n      <td>-1.281865</td>\n      <td>0.438884</td>\n      <td>15.658729</td>\n      <td>-0.384239</td>\n      <td>-0.179232</td>\n      <td>0.127206</td>\n      <td>-0.241648</td>\n      <td>0.201075</td>\n      <td>0.018560</td>\n      <td>0.051595</td>\n      <td>0.320928</td>\n      <td>-0.014993</td>\n      <td>-0.289971</td>\n      <td>-0.075338</td>\n      <td>0.308649</td>\n      <td>-0.015771</td>\n      <td>0.105882</td>\n      <td>0.105947</td>\n      <td>0.127456</td>\n      <td>0.078088</td>\n      <td>0.222367</td>\n      <td>-0.032851</td>\n      <td>0.304265</td>\n      <td>-0.070498</td>\n      <td>0.267309</td>\n      <td>0.052221</td>\n      <td>0.076381</td>\n      <td>0.396377</td>\n      <td>0.163754</td>\n      <td>0.041400</td>\n      <td>0.110767</td>\n      <td>1.638090</td>\n      <td>-0.029457</td>\n      <td>0.859024</td>\n      <td>0.030129</td>\n      <td>-2.927337</td>\n      <td>-0.140893</td>\n      <td>-0.331484</td>\n      <td>0.020357</td>\n      <td>0.158915</td>\n      <td>0.086444</td>\n      <td>0.193896</td>\n      <td>0.052242</td>\n      <td>0.108272</td>\n      <td>0.130650</td>\n      <td>-0.512819</td>\n      <td>0.474549</td>\n      <td>0.115467</td>\n      <td>0.129627</td>\n      <td>0.304608</td>\n      <td>0.123892</td>\n      <td>-0.046219</td>\n      <td>-0.104719</td>\n      <td>0.074052</td>\n      <td>0.167908</td>\n      <td>-0.159478</td>\n      <td>0.078277</td>\n      <td>-0.184950</td>\n      <td>0.010801</td>\n      <td>-0.092773</td>\n      <td>0.112173</td>\n      <td>0.070382</td>\n      <td>-1.117281</td>\n      <td>-0.167809</td>\n      <td>0.252590</td>\n      <td>0.187945</td>\n      <td>-0.086037</td>\n      <td>0.376501</td>\n      <td>0.369049</td>\n      <td>-0.163570</td>\n      <td>0.549421</td>\n      <td>-0.000795</td>\n      <td>-0.294876</td>\n      <td>0.134238</td>\n      <td>0.070635</td>\n      <td>-0.421189</td>\n      <td>-0.062440</td>\n      <td>-0.049646</td>\n      <td>-0.100289</td>\n      <td>0.120832</td>\n      <td>0.172514</td>\n      <td>0.060347</td>\n      <td>-0.114613</td>\n      <td>-0.147195</td>\n      <td>0.139910</td>\n      <td>0.010566</td>\n      <td>-0.365051</td>\n      <td>0.137368</td>\n      <td>-0.385252</td>\n      <td>0.151488</td>\n      <td>-0.010181</td>\n      <td>0.642636</td>\n      <td>-0.158496</td>\n      <td>-0.053182</td>\n      <td>0.251289</td>\n      <td>-0.084205</td>\n      <td>0.260213</td>\n      <td>-0.492204</td>\n      <td>0.301423</td>\n      <td>0.228388</td>\n      <td>-0.024414</td>\n      <td>0.063679</td>\n      <td>-0.428558</td>\n      <td>-0.078686</td>\n      <td>-0.749097</td>\n      <td>-0.009131</td>\n      <td>0.046799</td>\n      <td>1.569970</td>\n      <td>0.203062</td>\n      <td>-0.443468</td>\n      <td>-0.004288</td>\n      <td>-0.266038</td>\n      <td>0.094545</td>\n      <td>-0.099062</td>\n      <td>-0.123162</td>\n      <td>0.202115</td>\n      <td>-0.796550</td>\n      <td>-0.204647</td>\n      <td>0.084613</td>\n      <td>0.085180</td>\n      <td>-0.105909</td>\n      <td>0.206075</td>\n      <td>-0.169015</td>\n      <td>0.264994</td>\n      <td>0.169813</td>\n      <td>0.072487</td>\n      <td>-0.294009</td>\n      <td>0.140317</td>\n      <td>-0.179734</td>\n      <td>0.018937</td>\n      <td>-0.235079</td>\n      <td>-0.034906</td>\n      <td>-0.394411</td>\n      <td>-0.017301</td>\n      <td>0.381582</td>\n      <td>0.101955</td>\n      <td>0.202956</td>\n      <td>0.227578</td>\n      <td>-0.407289</td>\n      <td>0.003627</td>\n      <td>0.145120</td>\n      <td>0.332201</td>\n      <td>-0.135765</td>\n      <td>-0.132857</td>\n      <td>-0.032216</td>\n      <td>-3.812458</td>\n      <td>-0.313736</td>\n      <td>-1.906796</td>\n      <td>0.013529</td>\n      <td>0.221811</td>\n      <td>-0.058853</td>\n      <td>0.211268</td>\n      <td>0.279637</td>\n      <td>-0.325947</td>\n      <td>0.079062</td>\n      <td>-0.525971</td>\n      <td>0.170618</td>\n      <td>0.157170</td>\n      <td>0.290191</td>\n      <td>0.047002</td>\n      <td>0.136171</td>\n      <td>-0.088502</td>\n      <td>0.100978</td>\n      <td>-0.728087</td>\n      <td>-0.388375</td>\n      <td>-0.905938</td>\n      <td>-1.100813</td>\n      <td>0.069214</td>\n      <td>0.831719</td>\n      <td>0.062435</td>\n      <td>0.270292</td>\n      <td>-0.242251</td>\n      <td>-0.125205</td>\n      <td>-0.031401</td>\n      <td>-0.296945</td>\n      <td>0.219789</td>\n      <td>0.013650</td>\n      <td>0.231994</td>\n      <td>0.029659</td>\n      <td>0.101472</td>\n      <td>-0.195010</td>\n      <td>-0.016275</td>\n      <td>3.450085</td>\n      <td>4.212484</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3.555348</td>\n      <td>3.555348</td>\n      <td>NaN</td>\n      <td>3.555348</td>\n      <td>3.555348</td>\n      <td>2.456736</td>\n      <td>3.236217</td>\n      <td>791</td>\n      <td>0.693147</td>\n      <td>7.050989</td>\n      <td>1.209240</td>\n      <td>2.534984</td>\n      <td>2.484907</td>\n      <td>1.020364</td>\n      <td>1.402513</td>\n      <td>3828</td>\n      <td>0.693147</td>\n      <td>9.073833</td>\n      <td>1.263901</td>\n      <td>2.771316</td>\n      <td>2.708050</td>\n      <td>0.784032</td>\n      <td>1.282909</td>\n      <td>417</td>\n      <td>0.693147</td>\n      <td>9.073833</td>\n      <td>1.256370</td>\n      <td>3.338261</td>\n      <td>3.332205</td>\n      <td>0.217087</td>\n      <td>1.065030</td>\n      <td>2678</td>\n      <td>0.693147</td>\n      <td>9.073833</td>\n      <td>1.224161</td>\n      <td>2.847509</td>\n      <td>2.833213</td>\n      <td>0.707839</td>\n      <td>1.248582</td>\n      <td>1</td>\n      <td>1.098612</td>\n      <td>1.098612</td>\n      <td>NaN</td>\n      <td>1.098612</td>\n      <td>1.098612</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.995732</td>\n      <td>7.909857</td>\n      <td>4329354.0</td>\n      <td>2010</td>\n      <td>2001</td>\n      <td>2001</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>201005</td>\n      <td>200103</td>\n      <td>200105</td>\n      <td>28</td>\n      <td>23</td>\n      <td>4</td>\n      <td>1.275005e+09</td>\n      <td>9.853557e+08</td>\n      <td>9.889858e+08</td>\n      <td>289649078</td>\n      <td>3630052.0</td>\n      <td>2</td>\n      <td>14757</td>\n      <td>11404</td>\n      <td>11446</td>\n      <td>42</td>\n      <td>21.000000</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>179924</td>\n      <td>10</td>\n      <td>74502</td>\n      <td>440</td>\n      <td>8</td>\n      <td>20</td>\n      <td>2.145841</td>\n      <td>37</td>\n      <td>79.396107</td>\n      <td>0.000000</td>\n      <td>4.905275</td>\n      <td>2.397895</td>\n      <td>1.270819</td>\n      <td>0.000000</td>\n      <td>1.098612</td>\n      <td>3.178054</td>\n      <td>1.707669</td>\n      <td>69644</td>\n      <td>118928.891167</td>\n      <td>0.0</td>\n      <td>7.730175</td>\n      <td>1.609438</td>\n      <td>1.254878</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>2.564949</td>\n      <td>1.746283</td>\n      <td>85372</td>\n      <td>149083.676440</td>\n      <td>0.0</td>\n      <td>7.730175</td>\n      <td>1.791759</td>\n      <td>1.249781</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>2.639057</td>\n      <td>2.418981</td>\n      <td>43</td>\n      <td>104.016187</td>\n      <td>0.000000</td>\n      <td>4.905275</td>\n      <td>2.772589</td>\n      <td>1.196306</td>\n      <td>0.693147</td>\n      <td>1.868835</td>\n      <td>3.178054</td>\n      <td>2.162204</td>\n      <td>82437</td>\n      <td>178245.632013</td>\n      <td>0.0</td>\n      <td>8.479076</td>\n      <td>2.197225</td>\n      <td>1.360693</td>\n      <td>0.000000</td>\n      <td>1.098612</td>\n      <td>3.135494</td>\n      <td>2.728472</td>\n      <td>2548</td>\n      <td>6952.146967</td>\n      <td>0.0</td>\n      <td>7.510978</td>\n      <td>2.772589</td>\n      <td>1.313177</td>\n      <td>0.693147</td>\n      <td>1.945910</td>\n      <td>3.637586</td>\n      <td>2.707929</td>\n      <td>1941</td>\n      <td>5256.090936</td>\n      <td>0.0</td>\n      <td>7.250636</td>\n      <td>2.772589</td>\n      <td>1.438365</td>\n      <td>0.693147</td>\n      <td>1.791759</td>\n      <td>3.663562</td>\n      <td>0.576751</td>\n      <td>1.238427</td>\n      <td>1.288063</td>\n      <td>1.754282</td>\n      <td>0.849892</td>\n      <td>1.396065</td>\n      <td>1.249449</td>\n      <td>1.715491</td>\n      <td>0.267260</td>\n      <td>1.097952</td>\n      <td>0.287803</td>\n      <td>1.106282</td>\n      <td>0.833528</td>\n      <td>1.385499</td>\n      <td>-0.187380</td>\n      <td>0.145397</td>\n      <td>0.042867</td>\n      <td>0.091747</td>\n      <td>0.073396</td>\n      <td>-0.021361</td>\n      <td>0.092727</td>\n      <td>0.461332</td>\n      <td>-0.110972</td>\n      <td>0.017662</td>\n      <td>0.148059</td>\n      <td>-0.081682</td>\n      <td>0.307757</td>\n      <td>-0.022940</td>\n      <td>1.732171</td>\n      <td>-0.404425</td>\n      <td>0.232586</td>\n      <td>0.059912</td>\n      <td>-0.078345</td>\n      <td>0.252584</td>\n      <td>-0.018222</td>\n      <td>0.734836</td>\n      <td>0.044877</td>\n      <td>-0.048249</td>\n      <td>0.118653</td>\n      <td>0.185329</td>\n      <td>0.008177</td>\n      <td>-0.102454</td>\n      <td>-1.034392</td>\n      <td>0.816351</td>\n      <td>0.016014</td>\n      <td>0.753829</td>\n      <td>-0.867097</td>\n      <td>0.192586</td>\n      <td>-2.425286</td>\n      <td>-0.612009</td>\n      <td>-0.101969</td>\n      <td>1.786419</td>\n      <td>0.012609</td>\n      <td>0.016724</td>\n      <td>-0.232541</td>\n      <td>0.033428</td>\n      <td>-0.009051</td>\n      <td>-0.067578</td>\n      <td>0.054633</td>\n      <td>0.178886</td>\n      <td>0.037466</td>\n      <td>0.135309</td>\n      <td>0.171914</td>\n      <td>0.065489</td>\n      <td>0.312873</td>\n      <td>-0.065808</td>\n      <td>-0.299652</td>\n      <td>-0.307785</td>\n      <td>0.099331</td>\n      <td>-0.003821</td>\n      <td>0.567964</td>\n      <td>0.684359</td>\n      <td>-0.033729</td>\n      <td>-0.026250</td>\n      <td>0.142653</td>\n      <td>0.332960</td>\n      <td>-0.065651</td>\n      <td>0.318467</td>\n      <td>-1.769145</td>\n      <td>0.075743</td>\n      <td>-0.009804</td>\n      <td>-0.038626</td>\n      <td>-0.297279</td>\n      <td>-2.414447</td>\n      <td>-0.038408</td>\n      <td>-0.314786</td>\n      <td>-0.005732</td>\n      <td>0.014603</td>\n      <td>0.684424</td>\n      <td>0.090653</td>\n      <td>0.129101</td>\n      <td>-0.318063</td>\n      <td>0.321654</td>\n      <td>0.045372</td>\n      <td>0.083384</td>\n      <td>0.002869</td>\n      <td>0.251840</td>\n      <td>0.152930</td>\n      <td>0.326370</td>\n      <td>0.064168</td>\n      <td>0.093157</td>\n      <td>0.004896</td>\n      <td>-2.514675</td>\n      <td>0.144044</td>\n      <td>0.083127</td>\n      <td>0.024578</td>\n      <td>0.434071</td>\n      <td>0.097451</td>\n      <td>-0.100557</td>\n      <td>0.019803</td>\n      <td>-1.491407</td>\n      <td>-0.832966</td>\n      <td>-0.032468</td>\n      <td>0.102626</td>\n      <td>0.259525</td>\n      <td>0.061175</td>\n      <td>-0.024313</td>\n      <td>-0.016062</td>\n      <td>0.066748</td>\n      <td>0.359099</td>\n      <td>1.702562</td>\n      <td>0.153157</td>\n      <td>-0.048811</td>\n      <td>-0.067856</td>\n      <td>-0.166743</td>\n      <td>-0.070074</td>\n      <td>-0.046894</td>\n      <td>0.103432</td>\n      <td>0.145019</td>\n      <td>0.264992</td>\n      <td>0.206026</td>\n      <td>0.148249</td>\n      <td>0.079008</td>\n      <td>-0.005477</td>\n      <td>-0.131134</td>\n      <td>0.161990</td>\n      <td>-0.093254</td>\n      <td>0.209557</td>\n      <td>1.920110</td>\n      <td>0.096089</td>\n      <td>-0.235787</td>\n      <td>0.025200</td>\n      <td>0.124689</td>\n      <td>0.015753</td>\n      <td>-0.507947</td>\n      <td>-0.452399</td>\n      <td>-0.370156</td>\n      <td>-0.114861</td>\n      <td>0.091304</td>\n      <td>2.220559</td>\n      <td>-0.612926</td>\n      <td>0.671514</td>\n      <td>0.117427</td>\n      <td>-1.500174</td>\n      <td>0.085345</td>\n      <td>0.180046</td>\n      <td>-0.012982</td>\n      <td>-0.231229</td>\n      <td>0.266041</td>\n      <td>-0.014513</td>\n      <td>0.059450</td>\n      <td>0.134018</td>\n      <td>0.036182</td>\n      <td>-0.043965</td>\n      <td>0.008869</td>\n      <td>-0.153529</td>\n      <td>0.073723</td>\n      <td>0.063554</td>\n      <td>-0.141321</td>\n      <td>0.328091</td>\n      <td>0.180086</td>\n      <td>-0.209761</td>\n      <td>1.697075</td>\n      <td>0.448811</td>\n      <td>0.389430</td>\n      <td>-0.049153</td>\n      <td>-0.000387</td>\n      <td>0.113767</td>\n      <td>0.298940</td>\n      <td>-0.117455</td>\n      <td>-0.175730</td>\n      <td>0.154398</td>\n      <td>0.138273</td>\n      <td>2.566446</td>\n      <td>-1.807923</td>\n      <td>2.032891</td>\n      <td>0.642066</td>\n      <td>-0.053015</td>\n      <td>0.226126</td>\n      <td>-0.094345</td>\n      <td>-0.036774</td>\n      <td>0.105494</td>\n      <td>-0.166624</td>\n      <td>0.008655</td>\n      <td>0.175703</td>\n      <td>-0.081307</td>\n      <td>0.353235</td>\n      <td>-2.365701</td>\n      <td>0.980985</td>\n      <td>-0.161744</td>\n      <td>-0.028764</td>\n      <td>-0.078987</td>\n      <td>-0.169303</td>\n      <td>0.280238</td>\n      <td>-0.319408</td>\n      <td>-0.171438</td>\n      <td>0.083330</td>\n      <td>0.133894</td>\n      <td>0.033672</td>\n      <td>-0.115560</td>\n      <td>-0.043981</td>\n      <td>0.230242</td>\n      <td>0.257162</td>\n      <td>0.132305</td>\n      <td>-0.236100</td>\n      <td>-1.076464</td>\n      <td>0.046142</td>\n      <td>-0.012706</td>\n      <td>3.190663</td>\n      <td>0.100709</td>\n      <td>-0.301624</td>\n      <td>-0.405904</td>\n      <td>0.131458</td>\n      <td>-0.233304</td>\n      <td>-0.095763</td>\n      <td>-0.108915</td>\n      <td>-0.086663</td>\n      <td>0.196314</td>\n      <td>-0.061726</td>\n      <td>0.158183</td>\n      <td>0.239789</td>\n      <td>-2.246501</td>\n      <td>-0.148002</td>\n      <td>-0.152085</td>\n      <td>-0.056916</td>\n      <td>-0.015134</td>\n      <td>-0.895529</td>\n      <td>-0.231494</td>\n      <td>-0.086118</td>\n      <td>1.858994</td>\n      <td>0.122235</td>\n      <td>-0.061654</td>\n      <td>-0.004929</td>\n      <td>-0.214011</td>\n      <td>0.060358</td>\n      <td>0.603084</td>\n      <td>0.017818</td>\n      <td>0.210879</td>\n      <td>-0.142381</td>\n      <td>-0.070831</td>\n      <td>0.268282</td>\n      <td>0.149472</td>\n      <td>0.049750</td>\n      <td>-0.033004</td>\n      <td>-0.475461</td>\n      <td>0.080947</td>\n      <td>-0.164550</td>\n      <td>-0.062583</td>\n      <td>-2.306530</td>\n      <td>0.290093</td>\n      <td>0.182371</td>\n      <td>-0.204939</td>\n      <td>0.005158</td>\n      <td>0.009534</td>\n      <td>0.153010</td>\n      <td>0.103650</td>\n      <td>-0.293298</td>\n      <td>-0.072656</td>\n      <td>0.552348</td>\n      <td>0.042984</td>\n      <td>0.001588</td>\n      <td>0.107664</td>\n      <td>-0.290890</td>\n      <td>-0.016343</td>\n      <td>0.057698</td>\n      <td>0.031012</td>\n      <td>-0.005055</td>\n      <td>1.893795</td>\n      <td>-0.021059</td>\n      <td>-1.304884</td>\n      <td>0.272340</td>\n      <td>-0.137524</td>\n      <td>-0.106324</td>\n      <td>-0.417957</td>\n      <td>-0.066685</td>\n      <td>1.757590</td>\n      <td>0.157748</td>\n      <td>0.181897</td>\n      <td>0.109985</td>\n      <td>2.423291</td>\n      <td>0.151764</td>\n      <td>0.061456</td>\n      <td>-3.478415</td>\n      <td>-0.229757</td>\n      <td>0.152382</td>\n      <td>-0.047759</td>\n      <td>2.357267</td>\n      <td>0.223840</td>\n      <td>0.138511</td>\n      <td>0.001968</td>\n      <td>-0.279085</td>\n      <td>-0.195155</td>\n      <td>-1.910486</td>\n      <td>0.178989</td>\n      <td>0.112412</td>\n      <td>0.163494</td>\n      <td>-0.179234</td>\n      <td>0.139706</td>\n      <td>-0.024894</td>\n      <td>1.404210</td>\n      <td>0.038409</td>\n      <td>-0.089508</td>\n      <td>0.199482</td>\n      <td>1.834189</td>\n      <td>1.033714</td>\n      <td>0.067323</td>\n      <td>0.208381</td>\n      <td>0.173402</td>\n      <td>-0.117624</td>\n      <td>0.235604</td>\n      <td>-0.468047</td>\n      <td>-0.018331</td>\n      <td>0.553543</td>\n      <td>0.021239</td>\n      <td>0.224792</td>\n      <td>-0.038629</td>\n      <td>0.039813</td>\n      <td>0.218176</td>\n      <td>0.025046</td>\n      <td>-0.071873</td>\n      <td>-0.071090</td>\n      <td>-0.050543</td>\n      <td>0.137773</td>\n      <td>-0.677745</td>\n      <td>-0.156858</td>\n      <td>0.009576</td>\n      <td>0.036731</td>\n      <td>-0.052305</td>\n      <td>-2.260051</td>\n      <td>0.146993</td>\n      <td>-0.012935</td>\n      <td>0.043423</td>\n      <td>-0.117022</td>\n      <td>0.287592</td>\n      <td>-0.036265</td>\n      <td>0.376516</td>\n      <td>-0.099693</td>\n      <td>0.084041</td>\n      <td>0.174254</td>\n      <td>0.203815</td>\n      <td>-0.106393</td>\n      <td>-0.362104</td>\n      <td>0.178062</td>\n      <td>0.117283</td>\n      <td>-0.423528</td>\n      <td>-0.026239</td>\n      <td>-0.311260</td>\n      <td>-0.048824</td>\n      <td>0.114188</td>\n      <td>-0.067165</td>\n      <td>-0.007000</td>\n      <td>-0.035685</td>\n      <td>0.367277</td>\n      <td>-0.213153</td>\n      <td>-0.044977</td>\n      <td>-0.110997</td>\n      <td>-0.262060</td>\n      <td>-0.384119</td>\n      <td>0.050995</td>\n      <td>0.135775</td>\n      <td>-2.887919</td>\n      <td>-0.179541</td>\n      <td>-0.213835</td>\n      <td>-0.006154</td>\n      <td>0.088981</td>\n      <td>0.182848</td>\n      <td>0.317700</td>\n      <td>-0.149029</td>\n      <td>0.167177</td>\n      <td>-0.546108</td>\n      <td>-0.131718</td>\n      <td>0.044155</td>\n      <td>0.068826</td>\n      <td>0.054670</td>\n      <td>0.984523</td>\n      <td>0.385723</td>\n      <td>0.312443</td>\n      <td>-0.009449</td>\n      <td>-0.104133</td>\n      <td>0.001509</td>\n      <td>-0.202240</td>\n      <td>-0.022862</td>\n      <td>-0.069508</td>\n      <td>-0.215396</td>\n      <td>-0.322772</td>\n      <td>0.104961</td>\n      <td>0.121022</td>\n      <td>-0.065504</td>\n      <td>0.158373</td>\n      <td>-0.169121</td>\n      <td>0.129776</td>\n      <td>0.109671</td>\n      <td>0.518899</td>\n      <td>0.168845</td>\n      <td>0.087282</td>\n      <td>0.034171</td>\n      <td>0.098574</td>\n      <td>0.031332</td>\n      <td>-0.368003</td>\n      <td>-0.111130</td>\n      <td>0.229563</td>\n      <td>0.162961</td>\n      <td>0.294590</td>\n      <td>-0.054456</td>\n      <td>-0.345892</td>\n      <td>-0.038721</td>\n      <td>0.008215</td>\n      <td>0.122527</td>\n      <td>0.007750</td>\n      <td>1.598903</td>\n      <td>-0.138952</td>\n      <td>-0.056813</td>\n      <td>0.053615</td>\n      <td>0.295866</td>\n      <td>0.005762</td>\n      <td>2.627607</td>\n      <td>0.031670</td>\n      <td>-0.072003</td>\n      <td>-0.377022</td>\n      <td>-0.053818</td>\n      <td>-0.022433</td>\n      <td>0.064419</td>\n      <td>-0.165590</td>\n      <td>-0.037903</td>\n      <td>-0.244135</td>\n      <td>0.010811</td>\n      <td>-0.032834</td>\n      <td>-0.133434</td>\n      <td>-0.326949</td>\n      <td>-0.306970</td>\n      <td>0.253018</td>\n      <td>0.056711</td>\n      <td>-0.075639</td>\n      <td>0.393701</td>\n      <td>0.068553</td>\n      <td>0.083607</td>\n      <td>-0.279721</td>\n      <td>0.275026</td>\n      <td>-0.005184</td>\n      <td>0.019542</td>\n      <td>1.843857</td>\n      <td>-0.021117</td>\n      <td>-0.649182</td>\n      <td>-0.180874</td>\n      <td>-0.335896</td>\n      <td>-0.118478</td>\n      <td>0.031979</td>\n      <td>0.152168</td>\n      <td>-0.208604</td>\n      <td>-0.160579</td>\n      <td>0.125156</td>\n      <td>0.051888</td>\n      <td>-0.451882</td>\n      <td>-0.135818</td>\n      <td>-0.115391</td>\n      <td>0.316893</td>\n      <td>-0.165745</td>\n      <td>-0.751606</td>\n      <td>0.120666</td>\n      <td>2.236530</td>\n      <td>-0.041005</td>\n      <td>-0.023330</td>\n      <td>-0.148738</td>\n      <td>0.032427</td>\n      <td>0.450715</td>\n      <td>-0.076456</td>\n      <td>0.123699</td>\n      <td>0.009947</td>\n      <td>0.337450</td>\n      <td>0.202159</td>\n      <td>-0.075959</td>\n      <td>0.562078</td>\n      <td>-0.093621</td>\n      <td>0.324092</td>\n      <td>0.035906</td>\n      <td>-0.889501</td>\n      <td>-0.370093</td>\n      <td>-0.091545</td>\n      <td>0.117753</td>\n      <td>0.140190</td>\n      <td>-0.299377</td>\n      <td>0.029388</td>\n      <td>0.343551</td>\n      <td>0.218692</td>\n      <td>1.134175</td>\n      <td>0.298837</td>\n      <td>0.134503</td>\n      <td>0.180290</td>\n      <td>0.185334</td>\n      <td>-0.012042</td>\n      <td>-0.047619</td>\n      <td>0.190545</td>\n      <td>-0.018983</td>\n      <td>0.069052</td>\n      <td>-0.326265</td>\n      <td>-0.121601</td>\n      <td>0.025725</td>\n      <td>0.380883</td>\n      <td>-0.422817</td>\n      <td>0.423724</td>\n      <td>-0.146876</td>\n      <td>-0.019544</td>\n      <td>-0.072202</td>\n      <td>0.055910</td>\n      <td>0.257128</td>\n      <td>1.520159</td>\n      <td>0.014785</td>\n      <td>0.040694</td>\n      <td>0.093235</td>\n      <td>0.013008</td>\n      <td>-0.158661</td>\n      <td>0.197508</td>\n      <td>-0.081468</td>\n      <td>-0.109806</td>\n      <td>0.034318</td>\n      <td>-2.152943</td>\n      <td>-0.150228</td>\n      <td>0.135618</td>\n      <td>-0.031336</td>\n      <td>0.374404</td>\n      <td>-0.038125</td>\n      <td>0.033804</td>\n      <td>-0.044011</td>\n      <td>0.189994</td>\n      <td>-0.290810</td>\n      <td>-0.076751</td>\n      <td>0.055066</td>\n      <td>0.094413</td>\n      <td>-0.011358</td>\n      <td>0.329585</td>\n      <td>0.465333</td>\n      <td>0.031094</td>\n      <td>0.140568</td>\n      <td>0.089856</td>\n      <td>0.250899</td>\n      <td>0.024797</td>\n      <td>0.175454</td>\n      <td>-1.659980</td>\n      <td>-0.277828</td>\n      <td>0.054596</td>\n      <td>0.144084</td>\n      <td>0.108964</td>\n      <td>0.305567</td>\n      <td>-0.028825</td>\n      <td>-0.092508</td>\n      <td>0.053494</td>\n      <td>0.070822</td>\n      <td>-0.021621</td>\n      <td>-1.237372</td>\n      <td>-0.437703</td>\n      <td>0.374536</td>\n      <td>0.036562</td>\n      <td>0.126518</td>\n      <td>0.029002</td>\n      <td>-0.234288</td>\n      <td>-0.136523</td>\n      <td>0.567952</td>\n      <td>0.307315</td>\n      <td>-0.405664</td>\n      <td>1.578434</td>\n      <td>0.231085</td>\n      <td>-0.019642</td>\n      <td>-2.275623</td>\n      <td>0.056835</td>\n      <td>-0.214364</td>\n      <td>0.050980</td>\n      <td>-0.153820</td>\n      <td>0.653449</td>\n      <td>-0.223131</td>\n      <td>-0.070954</td>\n      <td>-1.086275</td>\n      <td>0.187581</td>\n      <td>0.352656</td>\n      <td>0.049867</td>\n      <td>0.729927</td>\n      <td>0.160435</td>\n      <td>-0.117220</td>\n      <td>0.322336</td>\n      <td>0.286748</td>\n      <td>-0.083758</td>\n      <td>-0.766978</td>\n      <td>0.568926</td>\n      <td>0.114184</td>\n      <td>-0.002756</td>\n      <td>1.079590</td>\n      <td>-0.000061</td>\n      <td>-0.045825</td>\n      <td>0.049232</td>\n      <td>-0.008646</td>\n      <td>0.246747</td>\n      <td>-1.228021</td>\n      <td>0.514326</td>\n      <td>16.450823</td>\n      <td>-0.326872</td>\n      <td>-0.235271</td>\n      <td>0.074833</td>\n      <td>-0.132499</td>\n      <td>0.100140</td>\n      <td>-0.046721</td>\n      <td>-0.046752</td>\n      <td>0.373024</td>\n      <td>-0.062352</td>\n      <td>-0.151560</td>\n      <td>-0.142718</td>\n      <td>0.113890</td>\n      <td>0.059737</td>\n      <td>0.136326</td>\n      <td>0.067625</td>\n      <td>0.163933</td>\n      <td>0.051400</td>\n      <td>0.244366</td>\n      <td>-0.108563</td>\n      <td>0.331426</td>\n      <td>-0.075972</td>\n      <td>0.266956</td>\n      <td>-0.035423</td>\n      <td>0.023705</td>\n      <td>0.256270</td>\n      <td>0.101773</td>\n      <td>0.048937</td>\n      <td>0.040774</td>\n      <td>1.560507</td>\n      <td>0.060618</td>\n      <td>0.689802</td>\n      <td>0.038729</td>\n      <td>-2.896327</td>\n      <td>-0.190275</td>\n      <td>-0.354980</td>\n      <td>-0.319749</td>\n      <td>0.202815</td>\n      <td>0.044535</td>\n      <td>0.074249</td>\n      <td>0.036319</td>\n      <td>0.152618</td>\n      <td>0.028272</td>\n      <td>-0.306225</td>\n      <td>0.419981</td>\n      <td>0.112814</td>\n      <td>0.286193</td>\n      <td>0.266369</td>\n      <td>-0.074123</td>\n      <td>-0.105692</td>\n      <td>-0.054460</td>\n      <td>0.080881</td>\n      <td>0.160070</td>\n      <td>-0.142029</td>\n      <td>0.011122</td>\n      <td>-0.167130</td>\n      <td>0.124838</td>\n      <td>-0.148576</td>\n      <td>0.042996</td>\n      <td>0.182056</td>\n      <td>-1.078335</td>\n      <td>-0.230087</td>\n      <td>0.093823</td>\n      <td>-0.052736</td>\n      <td>-0.075713</td>\n      <td>0.366752</td>\n      <td>0.451099</td>\n      <td>-0.202294</td>\n      <td>0.186338</td>\n      <td>-0.025228</td>\n      <td>-0.275641</td>\n      <td>0.054520</td>\n      <td>0.214516</td>\n      <td>-0.436491</td>\n      <td>-0.095601</td>\n      <td>-0.007449</td>\n      <td>-0.377080</td>\n      <td>0.095078</td>\n      <td>0.203332</td>\n      <td>-0.018805</td>\n      <td>-0.010719</td>\n      <td>-0.132318</td>\n      <td>0.232512</td>\n      <td>0.049093</td>\n      <td>-0.276242</td>\n      <td>0.096070</td>\n      <td>-0.300126</td>\n      <td>0.032640</td>\n      <td>0.125544</td>\n      <td>0.502857</td>\n      <td>-0.168638</td>\n      <td>-0.033188</td>\n      <td>0.110764</td>\n      <td>-0.037772</td>\n      <td>0.177882</td>\n      <td>-0.478104</td>\n      <td>0.246282</td>\n      <td>0.142282</td>\n      <td>0.112222</td>\n      <td>-0.054025</td>\n      <td>-0.300294</td>\n      <td>0.038212</td>\n      <td>-0.693776</td>\n      <td>0.142295</td>\n      <td>0.079722</td>\n      <td>1.377992</td>\n      <td>0.183432</td>\n      <td>-0.478156</td>\n      <td>0.060425</td>\n      <td>-0.227604</td>\n      <td>-0.043873</td>\n      <td>-0.122353</td>\n      <td>-0.137685</td>\n      <td>0.176160</td>\n      <td>-0.741568</td>\n      <td>0.000557</td>\n      <td>0.011738</td>\n      <td>-0.051742</td>\n      <td>-0.085747</td>\n      <td>0.288524</td>\n      <td>-0.155850</td>\n      <td>0.244002</td>\n      <td>0.205743</td>\n      <td>-0.019203</td>\n      <td>-0.097291</td>\n      <td>0.208094</td>\n      <td>-0.030838</td>\n      <td>0.118386</td>\n      <td>-0.247853</td>\n      <td>-0.006473</td>\n      <td>-0.369708</td>\n      <td>-0.091226</td>\n      <td>0.587550</td>\n      <td>0.044841</td>\n      <td>-0.033097</td>\n      <td>0.336011</td>\n      <td>-0.436082</td>\n      <td>-0.046020</td>\n      <td>0.021364</td>\n      <td>0.283768</td>\n      <td>-0.018955</td>\n      <td>-0.038653</td>\n      <td>0.019657</td>\n      <td>-3.768578</td>\n      <td>-0.324513</td>\n      <td>-1.877376</td>\n      <td>0.030798</td>\n      <td>0.218780</td>\n      <td>-0.030397</td>\n      <td>0.148808</td>\n      <td>0.183898</td>\n      <td>-0.343392</td>\n      <td>-0.020549</td>\n      <td>-0.484158</td>\n      <td>0.122724</td>\n      <td>0.116699</td>\n      <td>0.138952</td>\n      <td>-0.018857</td>\n      <td>0.119898</td>\n      <td>-0.139858</td>\n      <td>0.189453</td>\n      <td>-0.814305</td>\n      <td>-0.277532</td>\n      <td>-0.709534</td>\n      <td>-0.868354</td>\n      <td>-0.000258</td>\n      <td>0.697891</td>\n      <td>0.114870</td>\n      <td>0.314430</td>\n      <td>-0.150647</td>\n      <td>-0.155554</td>\n      <td>0.053789</td>\n      <td>-0.211846</td>\n      <td>0.175895</td>\n      <td>0.066672</td>\n      <td>0.290528</td>\n      <td>0.094371</td>\n      <td>0.129390</td>\n      <td>-0.113531</td>\n      <td>0.063748</td>\n      <td>2.378609</td>\n      <td>2.434969</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2.995732</td>\n      <td>2.995732</td>\n      <td>NaN</td>\n      <td>2.995732</td>\n      <td>2.995732</td>\n      <td>0.287682</td>\n      <td>1.106232</td>\n      <td>39</td>\n      <td>0.693147</td>\n      <td>5.746203</td>\n      <td>1.135174</td>\n      <td>2.946943</td>\n      <td>2.995732</td>\n      <td>0.048789</td>\n      <td>1.016556</td>\n      <td>3828</td>\n      <td>0.693147</td>\n      <td>9.073833</td>\n      <td>1.263901</td>\n      <td>2.771316</td>\n      <td>2.708050</td>\n      <td>0.224416</td>\n      <td>1.080978</td>\n      <td>1157</td>\n      <td>0.693147</td>\n      <td>6.860664</td>\n      <td>1.100287</td>\n      <td>2.427170</td>\n      <td>2.302585</td>\n      <td>0.568562</td>\n      <td>1.234249</td>\n      <td>1044</td>\n      <td>0.693147</td>\n      <td>8.228978</td>\n      <td>1.184952</td>\n      <td>2.583916</td>\n      <td>2.564949</td>\n      <td>0.411816</td>\n      <td>1.159377</td>\n      <td>1</td>\n      <td>2.708050</td>\n      <td>2.708050</td>\n      <td>NaN</td>\n      <td>2.708050</td>\n      <td>2.708050</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                      importance\n",
       "diff_doi_cites_doi_cites_mean_category_main_label   45452.756427\n",
       "te_diff_license_label_cites_mean                     9845.287023\n",
       "diff_rate_doi_cites_doi_cites_mean_category_mai...   9263.424620\n",
       "te_rate_license_label_cites_mean                     3971.043035\n",
       "diff_doi_cites_doi_cites_mean_doi_id_label           1151.217911\n",
       "doi_cites                                             696.934825\n",
       "diff_doi_cites_doi_cites_mean_update_ym               596.695166\n",
       "last_created_ym                                       393.102887\n",
       "diff_rate_doi_cites_doi_cites_mean_pub_publishe...    359.453805\n",
       "category_main_label                                   283.692540\n",
       "doi_id_label                                          226.679651\n",
       "te_category_main_label_diff_cites_doi_cites_mean      202.381430\n",
       "doi_cites_count_first_created_ym                      198.516430\n",
       "te_pub_publisher_label_diff_cites_doi_cites_mean      170.220198\n",
       "doi_cites_sum_first_created_ym                        165.948261\n",
       "diff_doi_cites_doi_cites_mean_pub_publisher_label     163.542940\n",
       "first_created_unixtime                                163.072149\n",
       "last_created_unixtime                                 129.288408\n",
       "te_rate_category_main_label_cites_mean                113.572617\n",
       "diff_created_unixtime                                 110.820619\n",
       "te_update_ym_diff_cites_doi_cites_std                 104.890610\n",
       "diff_update_date_unixtime                             103.064480\n",
       "te_rate_license_label_diff_cites_doi_cites_mean        95.723860\n",
       "first_created_ym                                       93.638841\n",
       "diff_doi_cites_doi_cites_mean_first_created_ym         93.117649\n",
       "te_update_ym_diff_cites_doi_cites_mean                 82.143879\n",
       "te_rate_update_ym_cites_mean                           80.368801\n",
       "te_diff_license_label_diff_cites_doi_cites_mean        79.809981\n",
       "pub_publisher_label                                    74.156529\n",
       "te_category_main_label_diff_cites_doi_cites_median     71.135279\n",
       "te_update_ym_diff_cites_doi_cites_max                  69.370690\n",
       "doi_cites_q75_submitter_label                          64.461880\n",
       "rate_created_days                                      61.067469\n",
       "rbert_vec_raw_768_349                                  60.952601\n",
       "diff_rate_doi_cites_doi_cites_mean_update_ym           60.118180\n",
       "te_pub_publisher_label_diff_cites_doi_cites_median     59.833529\n",
       "te_pub_publisher_label_cites_mean                      58.040840\n",
       "cond-mat.mtrl-sci                                      56.920780\n",
       "rbert_vec_raw_768_633                                  52.669210\n",
       "rbert_vec_raw_768_164                                  51.165220\n",
       "cond-mat.str-el                                        50.408690\n",
       "rbert_vec_raw_768_526                                  48.760190\n",
       "rbert_vec_raw_768_193                                  47.376630\n",
       "rbert_vec_raw_768_447                                  45.303631\n",
       "first_created_year                                     43.945062\n",
       "author_num                                             43.578070\n",
       "hep-th                                                 42.389950\n",
       "rbert_vec_raw_768_35                                   41.581860\n",
       "rbert_vec_raw_768_756                                  40.041250\n",
       "rbert_vec_raw_768_692                                  39.180329"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>diff_doi_cites_doi_cites_mean_category_main_label</th>\n      <td>45452.756427</td>\n    </tr>\n    <tr>\n      <th>te_diff_license_label_cites_mean</th>\n      <td>9845.287023</td>\n    </tr>\n    <tr>\n      <th>diff_rate_doi_cites_doi_cites_mean_category_main_label</th>\n      <td>9263.424620</td>\n    </tr>\n    <tr>\n      <th>te_rate_license_label_cites_mean</th>\n      <td>3971.043035</td>\n    </tr>\n    <tr>\n      <th>diff_doi_cites_doi_cites_mean_doi_id_label</th>\n      <td>1151.217911</td>\n    </tr>\n    <tr>\n      <th>doi_cites</th>\n      <td>696.934825</td>\n    </tr>\n    <tr>\n      <th>diff_doi_cites_doi_cites_mean_update_ym</th>\n      <td>596.695166</td>\n    </tr>\n    <tr>\n      <th>last_created_ym</th>\n      <td>393.102887</td>\n    </tr>\n    <tr>\n      <th>diff_rate_doi_cites_doi_cites_mean_pub_publisher_label</th>\n      <td>359.453805</td>\n    </tr>\n    <tr>\n      <th>category_main_label</th>\n      <td>283.692540</td>\n    </tr>\n    <tr>\n      <th>doi_id_label</th>\n      <td>226.679651</td>\n    </tr>\n    <tr>\n      <th>te_category_main_label_diff_cites_doi_cites_mean</th>\n      <td>202.381430</td>\n    </tr>\n    <tr>\n      <th>doi_cites_count_first_created_ym</th>\n      <td>198.516430</td>\n    </tr>\n    <tr>\n      <th>te_pub_publisher_label_diff_cites_doi_cites_mean</th>\n      <td>170.220198</td>\n    </tr>\n    <tr>\n      <th>doi_cites_sum_first_created_ym</th>\n      <td>165.948261</td>\n    </tr>\n    <tr>\n      <th>diff_doi_cites_doi_cites_mean_pub_publisher_label</th>\n      <td>163.542940</td>\n    </tr>\n    <tr>\n      <th>first_created_unixtime</th>\n      <td>163.072149</td>\n    </tr>\n    <tr>\n      <th>last_created_unixtime</th>\n      <td>129.288408</td>\n    </tr>\n    <tr>\n      <th>te_rate_category_main_label_cites_mean</th>\n      <td>113.572617</td>\n    </tr>\n    <tr>\n      <th>diff_created_unixtime</th>\n      <td>110.820619</td>\n    </tr>\n    <tr>\n      <th>te_update_ym_diff_cites_doi_cites_std</th>\n      <td>104.890610</td>\n    </tr>\n    <tr>\n      <th>diff_update_date_unixtime</th>\n      <td>103.064480</td>\n    </tr>\n    <tr>\n      <th>te_rate_license_label_diff_cites_doi_cites_mean</th>\n      <td>95.723860</td>\n    </tr>\n    <tr>\n      <th>first_created_ym</th>\n      <td>93.638841</td>\n    </tr>\n    <tr>\n      <th>diff_doi_cites_doi_cites_mean_first_created_ym</th>\n      <td>93.117649</td>\n    </tr>\n    <tr>\n      <th>te_update_ym_diff_cites_doi_cites_mean</th>\n      <td>82.143879</td>\n    </tr>\n    <tr>\n      <th>te_rate_update_ym_cites_mean</th>\n      <td>80.368801</td>\n    </tr>\n    <tr>\n      <th>te_diff_license_label_diff_cites_doi_cites_mean</th>\n      <td>79.809981</td>\n    </tr>\n    <tr>\n      <th>pub_publisher_label</th>\n      <td>74.156529</td>\n    </tr>\n    <tr>\n      <th>te_category_main_label_diff_cites_doi_cites_median</th>\n      <td>71.135279</td>\n    </tr>\n    <tr>\n      <th>te_update_ym_diff_cites_doi_cites_max</th>\n      <td>69.370690</td>\n    </tr>\n    <tr>\n      <th>doi_cites_q75_submitter_label</th>\n      <td>64.461880</td>\n    </tr>\n    <tr>\n      <th>rate_created_days</th>\n      <td>61.067469</td>\n    </tr>\n    <tr>\n      <th>rbert_vec_raw_768_349</th>\n      <td>60.952601</td>\n    </tr>\n    <tr>\n      <th>diff_rate_doi_cites_doi_cites_mean_update_ym</th>\n      <td>60.118180</td>\n    </tr>\n    <tr>\n      <th>te_pub_publisher_label_diff_cites_doi_cites_median</th>\n      <td>59.833529</td>\n    </tr>\n    <tr>\n      <th>te_pub_publisher_label_cites_mean</th>\n      <td>58.040840</td>\n    </tr>\n    <tr>\n      <th>cond-mat.mtrl-sci</th>\n      <td>56.920780</td>\n    </tr>\n    <tr>\n      <th>rbert_vec_raw_768_633</th>\n      <td>52.669210</td>\n    </tr>\n    <tr>\n      <th>rbert_vec_raw_768_164</th>\n      <td>51.165220</td>\n    </tr>\n    <tr>\n      <th>cond-mat.str-el</th>\n      <td>50.408690</td>\n    </tr>\n    <tr>\n      <th>rbert_vec_raw_768_526</th>\n      <td>48.760190</td>\n    </tr>\n    <tr>\n      <th>rbert_vec_raw_768_193</th>\n      <td>47.376630</td>\n    </tr>\n    <tr>\n      <th>rbert_vec_raw_768_447</th>\n      <td>45.303631</td>\n    </tr>\n    <tr>\n      <th>first_created_year</th>\n      <td>43.945062</td>\n    </tr>\n    <tr>\n      <th>author_num</th>\n      <td>43.578070</td>\n    </tr>\n    <tr>\n      <th>hep-th</th>\n      <td>42.389950</td>\n    </tr>\n    <tr>\n      <th>rbert_vec_raw_768_35</th>\n      <td>41.581860</td>\n    </tr>\n    <tr>\n      <th>rbert_vec_raw_768_756</th>\n      <td>40.041250</td>\n    </tr>\n    <tr>\n      <th>rbert_vec_raw_768_692</th>\n      <td>39.180329</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "feature = x_train.copy()\n",
    "df_feature = pd.DataFrame(model.booster_.feature_importance(importance_type='gain'), index=feature.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "df_feature.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                    importance\n",
       "rbert_vec_raw_768_665                      0.0\n",
       "rbert_vec_raw_768_697                      0.0\n",
       "cs.ds                                      0.0\n",
       "rbert_vec_raw_768_695                      0.0\n",
       "rbert_vec_raw_768_693                      0.0\n",
       "...                                        ...\n",
       "rbert_vec_raw_768_341                      0.0\n",
       "bayes-an                                   0.0\n",
       "atom-ph                                    0.0\n",
       "rbert_vec_raw_768_344                      0.0\n",
       "rate_doi_cites_category_main_label         0.0\n",
       "\n",
       "[400 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>rbert_vec_raw_768_665</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>rbert_vec_raw_768_697</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>cs.ds</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>rbert_vec_raw_768_695</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>rbert_vec_raw_768_693</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>rbert_vec_raw_768_341</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>bayes-an</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>atom-ph</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>rbert_vec_raw_768_344</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>rate_doi_cites_category_main_label</th>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "df_feature.tail(400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}