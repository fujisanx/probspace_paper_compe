{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gensim\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error,mean_squared_log_error\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/fujimotoat/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Stopwordsのダウンロード\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StopWord の再定義\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "# 句読点の追加。string.punctuation = ['!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~']\n",
    "punctuation = list(string.punctuation)\n",
    "\n",
    "# 手動で追加\n",
    "org_stop = [\"Subject\"]\n",
    "\n",
    "# stopwordsの定義更新\n",
    "add_stop = punctuation + org_stop\n",
    "stop.update(add_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# htmlの分割\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# []で囲まれた文章の削除（脚注、linkなど）\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "# URLの削除\n",
    "def remove_URL(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "# stopwordsの削除\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            if i.strip().isalpha():\n",
    "                final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "# 上記の関数をまとめて適用する関数を定義\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train: 100%|██████████| 851524/851524 [00:12<00:00, 68024.00it/s]\n",
      "test: 100%|██████████| 59084/59084 [00:14<00:00, 4190.64it/s]\n",
      "(15117, 301)\n",
      "(15117,)\n",
      "(59084, 301)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 各種定数の設定\n",
    "# ------------------------------------------------------------------------------\n",
    "NFOLDS = 5\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 各種パスの設定\n",
    "# ------------------------------------------------------------------------------\n",
    "DATA_PATH = Path(\"../data/\")\n",
    "FEATURE_PATH = Path(\"../features/\")\n",
    "MODEL_PATH = Path(\"../data/\")\n",
    "\n",
    "train_file = DATA_PATH / \"train_data.json\"\n",
    "test_file = DATA_PATH / \"test_data.json\"\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# ベクトル化のモデルインスタンス作成\n",
    "# ------------------------------------------------------------------------------\n",
    "emb_model = gensim.models.KeyedVectors.load_word2vec_format(MODEL_PATH / \"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# JSONを読み込むためのイテレータの定義\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_data_iter(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        for jason_line in f:\n",
    "            yield jason_line\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 訓練データとテストデータの読み込み（特徴量作成）\n",
    "# ------------------------------------------------------------------------------\n",
    "train = []\n",
    "train_feat = []\n",
    "target = []\n",
    "train_iter = get_data_iter(train_file)\n",
    "\n",
    "for line in tqdm(train_iter, desc=\"train\", total=851_524):\n",
    "    data = json.loads(line)\n",
    "    if 'cites' in data:  # 'cites' が nan のデータは除外\n",
    "        # 前処理の関数の適用（追加した部分）\n",
    "        data['abstract']=denoise_text(data['abstract'])\n",
    "       ###############################################\n",
    "        abstract = data['abstract']\n",
    "        doi_cites = [np.log1p(int(data['doi_cites']))]\n",
    "        cites = int(data['cites'])\n",
    "        # 'abstract' を gensim でベクトル化\n",
    "        emb_abstract = np.mean([emb_model[w] for w in abstract.split(' ') if w in emb_model], axis=0)\n",
    "        train.append(emb_abstract)\n",
    "        train_feat.append(doi_cites)\n",
    "        target.append(cites)\n",
    "\n",
    "test = []\n",
    "test_feat = []\n",
    "test_index = []\n",
    "test_iter = get_data_iter(test_file)\n",
    "for line in tqdm(test_iter, desc=\"test\", total=59_084):\n",
    "    data = json.loads(line)\n",
    "    # 前処理の関数の適用（追加した部分）\n",
    "    data['abstract']=denoise_text(data['abstract'])\n",
    "    ###############################################\n",
    "    abstract = data['abstract']\n",
    "    doi_cites = [np.log1p(int(data['doi_cites']))]\n",
    "    emb_abstract = np.mean([emb_model[w] for w in abstract.split(' ') if w in emb_model], axis=0)\n",
    "    test.append(emb_abstract)\n",
    "    test_feat.append(doi_cites)\n",
    "    test_index.append(data['id'])\n",
    "\n",
    "train = np.concatenate([np.array(train), np.array(train_feat)], axis=1)\n",
    "target = np.array(np.log1p(target))\n",
    "test = np.concatenate([np.array(test), np.array(test_feat)], axis=1)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 各データのサイズ表示\n",
    "# ------------------------------------------------------------------------------\n",
    "print(train.shape)\n",
    "print(target.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################3\n",
    "### LGBで学習、予測する関数の定義\n",
    "########################################################\n",
    "def Train_and_Pred(train,target,test):\n",
    "    # --------------------------------------\n",
    "    # パラメータ定義\n",
    "    # --------------------------------------\n",
    "    lgb_params = {\n",
    "                    'objective': 'root_mean_squared_error',\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'n_estimators': 50000,\n",
    "                    'colsample_bytree': 0.5,\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 3,\n",
    "                    'reg_alpha': 8,\n",
    "                    'reg_lambda': 2,\n",
    "                    'random_state': SEED,\n",
    "                    \"bagging_fraction\": 0.5520399476847848,\n",
    "                    \"bagging_freq\": 1,\n",
    "                    \"feature_fraction\": 0.4436319472771827,\n",
    "                    \"lambda_l1\": 0.01113869595673112,\n",
    "                    \"lambda_l2\": 8.706009358617911e-07,\n",
    "                    \"learning_rate\": 0.012307412937706345,\n",
    "                    \"min_child_samples\": 18,\n",
    "                    \"num_leaves\": 8,        \n",
    "                  }\n",
    "\n",
    "    # --------------------------------------\n",
    "    # 学習と予測\n",
    "    # --------------------------------------\n",
    "    kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "    lgb_oof = np.zeros(train.shape[0])\n",
    "    lgb_pred = 0\n",
    "\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kf.split(X=train)):\n",
    "        X_train, y_train = train[trn_idx], target[trn_idx]\n",
    "        X_valid, y_valid = train[val_idx], target[val_idx]\n",
    "        X_test = test\n",
    "\n",
    "        # LightGBM\n",
    "        model = lgb.LGBMRegressor(**lgb_params)\n",
    "        model.fit(X_train, y_train,\n",
    "                  eval_set=(X_valid, y_valid),\n",
    "                  eval_metric='rmse',\n",
    "                  verbose=False,\n",
    "                  early_stopping_rounds=500\n",
    "                  )\n",
    "\n",
    "        lgb_oof[val_idx] = model.predict(X_valid)\n",
    "        lgb_pred += model.predict(X_test) / NFOLDS\n",
    "        rmsle = mean_squared_error(y_valid, lgb_oof[val_idx], squared=False)\n",
    "        print(f\"fold {fold} lgb score: {rmsle}\")\n",
    "\n",
    "    rmsle = mean_squared_error(target, lgb_oof, squared=False)\n",
    "    print(\"+-\" * 40)\n",
    "    print(f\"score: {rmsle}\")\n",
    "    print(f\"model score: {model.score(train, target)}\")\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # 提出ファイルの作成\n",
    "    # ------------------------------------------------------------------------------\n",
    "    test_predicted = np.expm1(lgb_pred)\n",
    "\n",
    "    submit_df = pd.DataFrame({'id': test_index})\n",
    "    submit_df['cites'] = np.where(test_predicted < 0, 0, test_predicted)\n",
    "    submit_df.to_csv(\"submission.csv\", index=False)\n",
    "    return rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.01113869595673112, reg_alpha=8 will be ignored. Current value: lambda_l1=0.01113869595673112\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5520399476847848, subsample=0.5 will be ignored. Current value: bagging_fraction=0.5520399476847848\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4436319472771827, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.4436319472771827\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=3 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.706009358617911e-07, reg_lambda=2 will be ignored. Current value: lambda_l2=8.706009358617911e-07\n",
      "fold 0 lgb score: 0.5707246114150425\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01113869595673112, reg_alpha=8 will be ignored. Current value: lambda_l1=0.01113869595673112\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5520399476847848, subsample=0.5 will be ignored. Current value: bagging_fraction=0.5520399476847848\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4436319472771827, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.4436319472771827\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=3 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.706009358617911e-07, reg_lambda=2 will be ignored. Current value: lambda_l2=8.706009358617911e-07\n",
      "fold 1 lgb score: 0.5405980557585585\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01113869595673112, reg_alpha=8 will be ignored. Current value: lambda_l1=0.01113869595673112\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5520399476847848, subsample=0.5 will be ignored. Current value: bagging_fraction=0.5520399476847848\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4436319472771827, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.4436319472771827\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=3 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.706009358617911e-07, reg_lambda=2 will be ignored. Current value: lambda_l2=8.706009358617911e-07\n",
      "fold 2 lgb score: 0.5695753270477643\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01113869595673112, reg_alpha=8 will be ignored. Current value: lambda_l1=0.01113869595673112\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5520399476847848, subsample=0.5 will be ignored. Current value: bagging_fraction=0.5520399476847848\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4436319472771827, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.4436319472771827\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=3 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.706009358617911e-07, reg_lambda=2 will be ignored. Current value: lambda_l2=8.706009358617911e-07\n",
      "fold 3 lgb score: 0.5490203518710194\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01113869595673112, reg_alpha=8 will be ignored. Current value: lambda_l1=0.01113869595673112\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5520399476847848, subsample=0.5 will be ignored. Current value: bagging_fraction=0.5520399476847848\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4436319472771827, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.4436319472771827\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=3 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.706009358617911e-07, reg_lambda=2 will be ignored. Current value: lambda_l2=8.706009358617911e-07\n",
      "fold 4 lgb score: 0.5473995460551688\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "score: 0.5556003415937129\n",
      "model score: 0.8445692066089614\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5556003415937129"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "Train_and_Pred(train,target,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}